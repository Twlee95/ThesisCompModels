{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "804c32fd-cb58-4fc2-9262-e9edd9c34b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math\n",
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a33d2-541f-4b3e-bb5e-62a9a6517af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class adversarial(nn.Module):\n",
    "    def __init__(self, batch_size, hid_dim, attn_size):\n",
    "        super(adversarial,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hid_dim = hid_dim\n",
    "        self.attn_size = attn_size\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "        self.w = nn.init.xavier_normal_(torch.empty(batch_size, hid_dim+attn_size, 1, requires_grad=True).to(self.device))\n",
    "        self.b = torch.zeros(batch_size,requires_grad=True).to(self.device)\n",
    "        self._1 = torch.tensor([[1.0]]).to(self.device)\n",
    "        self._0 = torch.tensor([[0.0]]).to(self.device)\n",
    "        \n",
    "    def forward(self, es, true_y):\n",
    "\n",
    "        # w : 128, 20,1\n",
    "        # b : 128\n",
    "        y_hat = (torch.bmm(es.unsqueeze(1), self.w).squeeze() + self.b).squeeze()\n",
    "        \n",
    "        ll = _1- y_hat*true_y\n",
    "        l =ll.tolist()[0]\n",
    "        _list = [0 for i in range(self.batch_size)]\n",
    "        \n",
    "        consi = torch.tensor([l > _list]).to(self.device)\n",
    "        \n",
    "        # w : 128, 20,1\n",
    "        gs = -torch.bmm(self.w, true_y.unsqueeze(1).unsqueeze(1))\n",
    "        # print(gs.size()) # [128, 20, 1]\n",
    "        # print(gs[0].size()) # 20, 1 \n",
    "        # print(-true_y*w)\n",
    "        # print((-true_y*w).size())\n",
    "        eps = 0.1\n",
    "        norm = torch.norm(gs,dim=1)\n",
    "        # print(norm.size()) [128, 1]\n",
    "        norm_list = norm.squeeze(1).tolist()\n",
    "        # print(norm_list) 128\n",
    "        \n",
    "        radv = torch.empty(self.batch_size, self.hid_dim + self.attn_size, 1).to(self.device)\n",
    "        for i, norm in enumerate(norm_list):\n",
    "            if norm == 0:\n",
    "                radv[i] = torch.zeros(self.hid_dim + self.attn_size, 1).to(self.device)\n",
    "            elif norm > 0:\n",
    "                radv[i] = (eps * (gs/norm))[i]\n",
    "        \n",
    "        # radv = radv * consi [128, 20, 1]\n",
    "        # es [128, 20]\n",
    "\n",
    "        eadv = radv + es.unsqueeze(2)\n",
    "        \n",
    "        # w : 128, 20,1\n",
    "        y_hat_adv = torch.bmm(eadv.transpose(1,2), self.w).squeeze() + self.b\n",
    "        \n",
    "\n",
    "        return y_hat, y_hat_adv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_64",
   "language": "python",
   "name": "py38_64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
