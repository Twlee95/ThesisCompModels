{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from attention.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import attention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class attLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, batch_size, dropout,use_bn, attn_head,\n",
    "                 attn_size, activation=\"ReLU\"):\n",
    "        super(attLSTM,self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.attn_head = attn_head\n",
    "        self.attn_size = attn_size\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn\n",
    "        self.activation = getattr(nn, activation)()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        ## 파이토치에 있는 lstm모듈\n",
    "        ## output dim 은 self.regressor에서 사용됨\n",
    "        \n",
    "        self.feature_linear = nn.Linear(11,self.hidden_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "        self.attention = attention.Attention(self.batch_size,self.attn_head, self.attn_size, self.hidden_dim,\n",
    "                                             self.hidden_dim, self.dropout)\n",
    "        self.regression_input_size = self.attn_size + self.hidden_dim\n",
    "        self.regressor = self.make_regressor()\n",
    "        self.init_hidden_ = self.init_hidden()\n",
    "\n",
    "\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        h = torch.empty(self.num_layers, self.batch_size, self.hidden_dim).to(device)\n",
    "        c = torch.empty(self.num_layers, self.batch_size, self.hidden_dim).to(device)\n",
    "        return (nn.init.xavier_normal_(h),\n",
    "                nn.init.xavier_normal_(c))\n",
    "    \n",
    "    def make_regressor(self): # 간단한 MLP를 만드는 함수\n",
    "        layers = []\n",
    "        if self.use_bn:\n",
    "            layers.append(nn.BatchNorm1d(self.regression_input_size))  ##  nn.BatchNorm1d\n",
    "        layers.append(nn.Dropout(self.dropout))    ##  nn.Dropout\n",
    "\n",
    "        ## hidden dim을 outputdim으로 바꿔주는 MLP\n",
    "        layers.append(nn.Linear(self.regression_input_size, 1))\n",
    "        regressor = nn.Sequential(*layers)\n",
    "        return regressor\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        input_x : [10, 64, 10]\n",
    "        '''\n",
    "\n",
    "        # self.hidden 각각의 layer의 모든 hidden state 를 갖고있음\n",
    "        ## LSTM의 hidden state에는 tuple로 cell state포함, 0번째는 hidden state tensor, 1번째는 cell state\n",
    "\n",
    "        input_x = self.tanh(self.feature_linear(x.float())).transpose(0,1).float().to(self.device)\n",
    "\n",
    "        lstm_out, self.hidden = self.lstm(input_x, self.init_hidden_)\n",
    "\n",
    "        hidden_states = self.hidden[0] ## 0번째가 히든스테이트임 1번째는 cell state\n",
    "        #  print(hidden_states.size()) [1, 128, 10]\n",
    "        attn_applied, attn_weights = self.attention(lstm_out, lstm_out)\n",
    "        ## lstm_out : 각 time step에서의 lstm 모델의 output 값\n",
    "        ## lstm_out[-1] : 맨마지막의 아웃풋 값으로 그 다음을 예측하고싶은 것이기 때문에 -1을 해줌\n",
    "\n",
    "        es = torch.cat([attn_applied, self.hidden[0]], dim=2).view(self.batch_size, -1)\n",
    "                                                             \n",
    "        return es, attn_weights, attn_applied"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py38_64",
   "language": "python",
   "name": "py38_64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
