{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import import_ipynb\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train(Informer, Informer_optimizer,args,partition):\n",
    "    train_loader = DataLoader(partition[\"train\"],batch_size= args.batch_size,shuffle=False,drop_last=True)\n",
    "    \n",
    "    Informer.train()\n",
    "    train_loss = 0.0\n",
    "    for (x,y) in train_loader:\n",
    "        Informer.zero_grad()\n",
    "        Informer_optimizer.zero_grad()\n",
    "\n",
    "        x = x.float().to(args.device) # 64,10 40\n",
    "        y = y.float().squeeze().to(args.device)\n",
    "\n",
    "        Informer_out = Informer(x)\n",
    "\n",
    "        loss = args.loss_fn(Informer_out, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        Informer_optimizer.step() ## parameter 갱신\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    return Informer, train_loss\n",
    "\n",
    "def validation(Informer, args, partition):\n",
    "    val_loader = DataLoader(partition[\"val\"],batch_size= args.batch_size,shuffle=False,drop_last=True)\n",
    "\n",
    "    Informer.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (x,y) in val_loader:\n",
    "\n",
    "            x = x.float().to(args.device) # 64,10 40\n",
    "            y = y.float().squeeze().to(args.device)\n",
    "\n",
    "            Informer_out = Informer(x)\n",
    "                        \n",
    "            loss = args.loss_fn(Informer_out, y)      \n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    return Informer, val_loss\n",
    "\n",
    "def test(Informer, args, partition):\n",
    "    test_loader = DataLoader(partition[\"test\"],batch_size= args.batch_size,shuffle=False,drop_last=True)\n",
    "    \n",
    "    Informer.eval()\n",
    "    ACC_metric = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (x,y) in test_loader:\n",
    "\n",
    "            x = x.float().to(args.device) # 64,10 40\n",
    "            y = y.float().squeeze().to(args.device)\n",
    "\n",
    "            Informer_out = Informer(x)\n",
    "\n",
    "            output_ = torch.where(Informer_out >= 0.5, 1.0, 0.0)\n",
    "\n",
    "            output_.requires_grad = True\n",
    "\n",
    "            perc_y_pred = output_.cpu().detach().numpy()     \n",
    "            perc_y_true =  y.cpu().detach().numpy()\n",
    "            acc = accuracy_score(perc_y_true, perc_y_pred)\n",
    "\n",
    "            ACC_metric += acc\n",
    "\n",
    "    ACC_metric = ACC_metric / len(test_loader)\n",
    "     \n",
    "    return ACC_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====== Argument initializtion ======#\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "#=============== Device ===============#\n",
    "args.device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#================ Path ================#\n",
    "\n",
    "args.save_file_path = \"D:\\\\Informer_results\"\n",
    "\n",
    "#========= Base Hyperparameter =========#\n",
    "args.batch_size = 32\n",
    "args.lr = 0.00005\n",
    "args.L2 = 0.00001\n",
    "args.epoch = 100\n",
    "args.dropout = 0.15\n",
    "args.loss_fn = nn.BCELoss()\n",
    "\n",
    "#===== Transformer Hyperparameter =====#\n",
    "from models.model import Informer\n",
    "\n",
    "args.Informer = Informer\n",
    "args.input_feature_size = 40\n",
    "\n",
    "args.Informer_feature_size = 64\n",
    "\n",
    "args.nhead = 4\n",
    "\n",
    "args.nlayer = 1\n",
    "args.ts_len = 10\n",
    "args.target_len = 1\n",
    "# trans_feature_size / trans_nhead => int 필수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Stock_Dataset.ipynb\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lab\\Desktop\\Informer_\\main_new.ipynb 셀 3\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lab/Desktop/Informer_/main_new.ipynb#W2sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m testset \u001b[39m=\u001b[39m StockDataset(data[\u001b[39m2\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lab/Desktop/Informer_/main_new.ipynb#W2sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m partition \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m: trainset, \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m: valset, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m: testset}     \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lab/Desktop/Informer_/main_new.ipynb#W2sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m Informer, train_loss \u001b[39m=\u001b[39m train(Informer,Informer_optimizer, args, partition)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lab/Desktop/Informer_/main_new.ipynb#W2sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m Informer, validation_loss \u001b[39m=\u001b[39m validation(Informer, args, partition)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lab/Desktop/Informer_/main_new.ipynb#W2sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m## .state_dict() : model의 parameter(W)만을 저장하는것임 => 다시 불러올 때 모델의 파라미터를 알고있어야함\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\lab\\Desktop\\Informer_\\main_new.ipynb 셀 3\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(Informer, Informer_optimizer, args, partition)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lab/Desktop/Informer_/main_new.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m Informer_out \u001b[39m=\u001b[39m Informer(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lab/Desktop/Informer_/main_new.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mloss_fn(Informer_out, y)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lab/Desktop/Informer_/main_new.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lab/Desktop/Informer_/main_new.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m Informer_optimizer\u001b[39m.\u001b[39mstep() \u001b[39m## parameter 갱신\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lab/Desktop/Informer_/main_new.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\lab\\anaconda3\\envs\\taewon\\lib\\site-packages\\torch\\tensor.py:245\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    237\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    238\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    239\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    244\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 245\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\lab\\anaconda3\\envs\\taewon\\lib\\site-packages\\torch\\autograd\\__init__.py:145\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m--> 145\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[0;32m    146\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    147\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "## 실행파일\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from Stock_dataloader_csv_ti import stock_csv_read\n",
    "from Stock_Dataset import StockDataset\n",
    "\n",
    "\n",
    "args.data_list = os.listdir(r\"C:\\Users\\lab\\Desktop\\Informer_\\data\\kdd17\\price_long_50\")\n",
    "\n",
    "with open(args.save_file_path + '\\\\' + 'Informer_result_t.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"model\", \"stock\", \"entire_exp_time\",  \"avg_test_ACC\", \"avg_test_ACC_std\"])\n",
    "\n",
    "    for data in args.data_list:\n",
    "        est = time.time()\n",
    "\n",
    "        stock = data.split('.')[0]\n",
    "\n",
    "        setattr(args, 'symbol', stock)\n",
    "        args.new_file_path = args.save_file_path + '\\\\' + \"Informer_\" + args.symbol\n",
    "        os.makedirs(args.new_file_path)\n",
    "        \n",
    "        csv_read = stock_csv_read(data, args.ts_len,args.target_len)\n",
    "        split_data_list = csv_read.cv_split()\n",
    "\n",
    "        with open(args.new_file_path + '\\\\'+ str(args.symbol)+'test_acc_list' +'.csv', 'w',newline='') as alist:\n",
    "            www = csv.writer(alist)\n",
    "            www.writerow([\"acc_list\"])\n",
    "\n",
    "            ACC_cv = []\n",
    "            for i, data in enumerate(split_data_list):\n",
    "                \n",
    "                args.sp_ith = i\n",
    "\n",
    "                args.split_file_path = args.new_file_path + \"\\\\\" + str(i) +\"th_iter\"\n",
    "                os.makedirs(args.split_file_path)\n",
    "                ## Model                 \n",
    "                Informer = args.Informer(5, 1, factor=5, d_model=512, n_heads=8, e_layers=1, d_ff=512, \n",
    "                dropout=0.0, attn='prob', embed='fixed', freq='h', activation='gelu', output_attention = False, distil=True)\n",
    "                \n",
    "                Informer.to(args.device)\n",
    "\n",
    "                ##Optimizer\n",
    "                Informer_optimizer = optim.Adam(Informer.parameters(), lr=args.lr, weight_decay=args.L2)\n",
    "\n",
    "                ## training\n",
    "                Train_losses = []\n",
    "                Validation_losses = []\n",
    "                for epoch in range(args.epoch):\n",
    "                    ts = time.time()\n",
    "\n",
    "                    trainset = StockDataset(data[0])\n",
    "                    valset = StockDataset(data[1])\n",
    "                    testset = StockDataset(data[2])\n",
    "\n",
    "\n",
    "                    partition = {'train': trainset, 'val': valset, 'test': testset}     \n",
    "\n",
    "                    Informer, train_loss = train(Informer,Informer_optimizer, args, partition)\n",
    "                    Informer, validation_loss = validation(Informer, args, partition)\n",
    "\n",
    "\n",
    "                    ## .state_dict() : model의 parameter(W)만을 저장하는것임 => 다시 불러올 때 모델의 파라미터를 알고있어야함\n",
    "                    if len(Validation_losses) == 0:\n",
    "                        torch.save(Informer.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_Informer' +'.pt')\n",
    "                    elif min(Validation_losses) > validation_loss:\n",
    "                        torch.save(Informer.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_Informer' +'.pt')\n",
    "                    \n",
    "                    Train_losses.append(train_loss)\n",
    "                    Validation_losses.append(validation_loss)\n",
    "                    \n",
    "                    te = time.time()\n",
    "\n",
    "                    print('Epoch {}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "                    .format(epoch, train_loss, validation_loss, te - ts))\n",
    "\n",
    "                ## Test\n",
    "                # state_dict로 저장했기 때문에 model의 hyperparameter를 불러와야함\n",
    "                Informer = args.Informer(5, 1, factor=5, d_model=512, n_heads=8, e_layers=1, d_ff=512, \n",
    "                dropout=0.0, attn='prob', embed='fixed', freq='h', activation='gelu', output_attention = False, distil=True)\n",
    "                \n",
    "                Informer.to(args.device)\n",
    "\n",
    "                # Model_selection\n",
    "                min_val_losses = Validation_losses.index(min(Validation_losses)) ## 10 epoch일 경우 0번째~9번째 까지로 나옴\n",
    "\n",
    "                Informer.load_state_dict(torch.load(args.split_file_path + '\\\\' + str(min_val_losses) +'_Informer' + '.pt'))\n",
    "\n",
    "                ACC = test(Informer, args, partition)\n",
    "                www.writerow([ACC])\n",
    "                print('ACC: {}'.format(ACC))\n",
    "\n",
    "                with open(args.split_file_path + '\\\\'+ str(min_val_losses)+'Epoch_test_metric' +'.csv', 'w') as fd:\n",
    "                    print('ACC: {}'.format(ACC), file=fd)\n",
    "\n",
    "                result = {}\n",
    "\n",
    "                result['train_losses'] = Train_losses\n",
    "                result['val_losses'] = Validation_losses\n",
    "                result['ACC'] = ACC\n",
    "\n",
    "                eet = time.time()\n",
    "                entire_exp_time = eet - est\n",
    "                \n",
    "\n",
    "                ## draw loss curve\n",
    "                fig = plt.figure()\n",
    "                plt.plot(result['train_losses'])\n",
    "                plt.plot(result['val_losses'])\n",
    "                plt.legend(['train_losses', 'val_losses'], fontsize=15)\n",
    "                plt.xlabel('epoch', fontsize=15)\n",
    "                plt.ylabel('loss', fontsize=15)\n",
    "                plt.grid()\n",
    "                plt.savefig(args.split_file_path + '\\\\' + 'fig' + '.png')\n",
    "                plt.close(fig)\n",
    "                ACC_cv.append(result['ACC'])\n",
    "\n",
    "        ACC_cv_ar = np.array(ACC_cv)\n",
    "        acc_avg = np.mean(ACC_cv_ar)\n",
    "        acc_std = np.std(ACC_cv_ar)\n",
    "\n",
    "        wr.writerow([\"Informer\", args.symbol, entire_exp_time, acc_avg, acc_std])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('taewon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "013403e7ebf8f35ee0411721c7e4b108aa3c3f8cb903b89610d110413a68ec3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
