{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "650a3f6d-1839-43f9-aeb9-3a995d6e31b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from attention.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import attention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class att_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, batch_size, dropout,use_bn, attn_head,\n",
    "                 attn_size, activation=\"ReLU\"):\n",
    "        super(att_LSTM,self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.attn_head = attn_head\n",
    "        self.attn_size = attn_size\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn\n",
    "        self.activation = getattr(nn, activation)()\n",
    "\n",
    "        ## 파이토치에 있는 lstm모듈\n",
    "        ## output dim 은 self.regressor에서 사용됨\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "        self.attention = attention.Attention(self.attn_head, self.attn_size, self.hidden_dim,\n",
    "                                             self.hidden_dim, self.dropout)\n",
    "        self.regression_input_size = self.attn_size + self.hidden_dim\n",
    "        self.regressor = self.make_regressor()\n",
    "        self.linear1 = nn.Linear(11,10)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        h = torch.empty(self.num_layers, self.batch_size, self.hidden_dim)\n",
    "        c = torch.empty(self.num_layers, self.batch_size, self.hidden_dim)\n",
    "        return (nn.init.xavier_normal_(h),\n",
    "                nn.init.xavier_normal_(c))\n",
    "\n",
    "    def make_regressor(self): # 간단한 MLP를 만드는 함수\n",
    "        layers = []\n",
    "        if self.use_bn:\n",
    "            layers.append(nn.BatchNorm1d(self.regression_input_size))  ##  nn.BatchNorm1d\n",
    "        layers.append(nn.Dropout(self.dropout))    ##  nn.Dropout\n",
    "\n",
    "        ## hidden dim을 outputdim으로 바꿔주는 MLP\n",
    "        layers.append(nn.Linear(self.regression_input_size, self.hidden_dim))\n",
    "        regressor = nn.Sequential(*layers)\n",
    "        return regressor\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tanh1(self.linear1(x.float())).transpose(0,1).float()\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        # self.hidden # 0번째가 히든스테이트임 1번째는 cell state\n",
    "        # print(self.hidden[0].size()) # [1, 128, 10] # final hidden state\n",
    "        # print(lstm_out.size()) # [10, 128, 10] sequence length, batch size, feature length\n",
    "        hidden_context, attn_weights = self.attention(lstm_out, self.hidden[0], lstm_out)\n",
    "        ## lstm_out : 각 time step에서의 lstm 모델의 output 값\n",
    "        ## lstm_out[-1] : 맨마지막의 아웃풋 값으로 그 다음을 예측하고싶은 것이기 때문에 -1을 해줌\n",
    "        return hidden_context, attn_weights\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_64",
   "language": "python",
   "name": "py38_64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
