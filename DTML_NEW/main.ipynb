{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db823349-554f-4e1c-8b9f-47a4503d8fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Stock_Dataset.ipynb\n",
      "importing Jupyter notebook from AttLSTM.ipynb\n",
      "importing Jupyter notebook from attention.ipynb\n",
      "importing Jupyter notebook from Transformer_Encoder.ipynb\n",
      "importing Jupyter notebook from metric.ipynb\n",
      "importing Jupyter notebook from loss_fn1.ipynb\n",
      "importing Jupyter notebook from Stock_datasets_csv.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\lab\\Desktop\\DTML_NEW\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from Stock_Dataset import StockDataset\n",
    "import argparse\n",
    "from AttLSTM import att_LSTM\n",
    "from Transformer_Encoder import Transformer\n",
    "import numpy as np\n",
    "import time\n",
    "from metric import metric_acc as ACC\n",
    "from metric import metric_mcc as MCC\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "from loss_fn1 import Selective_Regularization\n",
    "from Stock_datasets_csv import stock_csv_read\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "def train(att_LSTM,transformer,                                  ## Model\n",
    "          att_LSTM_optimizer, transformer_optimizer,   ## Optimizer\n",
    "          Partition, args):                                      ## Data, loss function, argument\n",
    "    trainloader = DataLoader(Partition['train'],\n",
    "                             batch_size = args.batch_size,\n",
    "                             shuffle=False, drop_last=True)\n",
    "\n",
    "    att_LSTM.train()\n",
    "    transformer.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for j, data in enumerate(trainloader):\n",
    "        data_out_list = []\n",
    "        for i in range(len(data)):\n",
    "\n",
    "            att_LSTM_optimizer.zero_grad()\n",
    "            transformer_optimizer.zero_grad()\n",
    "            \n",
    "            x_input = data[i][0].to(args.device)\n",
    "            \n",
    "            if i == 1:\n",
    "                true_y = data[i][1].squeeze().float().to(args.device)\n",
    "            \n",
    "            att_LSTM.hidden = [hidden.to(args.device) for hidden in att_LSTM.init_hidden()]\n",
    "            \n",
    "            # 'list' object has no attribute 'float'\n",
    "\n",
    "            hidden_context, attn_weights = att_LSTM(x_input)\n",
    "\n",
    "            data_out_list.append(hidden_context)\n",
    "\n",
    "        index_output = data_out_list[0]  # torch.Size([128, 10])\n",
    "        stock_output = data_out_list[1] # torch.Size([128, 10])\n",
    "        \n",
    "\n",
    "        Norm_ = nn.LayerNorm(10, device=args.device)\n",
    "        \n",
    "        index_output = args.r * Norm_(index_output) + args.b\n",
    "        stock_output = args.r * Norm_(stock_output) + args.b\n",
    "\n",
    "\n",
    "        Transformer_input = index_output* args.market_beta + stock_output\n",
    "        \n",
    "\n",
    "        output1,wp,bp = transformer(Transformer_input.transpose(0,1).unsqueeze(2))\n",
    "\n",
    "        # output_ = torch.where(output1 >= 0.5, 1.0, 0.0)\n",
    "        # output_.requires_grad=True\n",
    "        \n",
    "        loss = args.loss_fn(output1, true_y, wp.squeeze(), bp)\n",
    "        loss.backward()\n",
    "\n",
    "        att_LSTM_optimizer.step() ## parameter 갱신\n",
    "        transformer_optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    return att_LSTM, transformer, train_loss\n",
    "\n",
    "\n",
    "def validation(att_LSTM,transformer,\n",
    "               partition, args):\n",
    "    valloader = DataLoader(partition['val'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    att_LSTM.eval()\n",
    "    transformer.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j, data in enumerate(valloader):\n",
    "\n",
    "            data_out_list = []\n",
    "            for i in range(len(data)):\n",
    "                \n",
    "                x_input = data[i][0].to(args.device)\n",
    "\n",
    "                if i == 1:\n",
    "                    true_y = data[i][1].squeeze().float().to(args.device)\n",
    "                    \n",
    "                att_LSTM.hidden = [hidden.to(args.device) for hidden in att_LSTM.init_hidden()]\n",
    "\n",
    "\n",
    "                hidden_context, attn_weights = att_LSTM(x_input)\n",
    "\n",
    "                data_out_list.append(hidden_context)\n",
    "\n",
    "            index_output = data_out_list[0]  # torch.Size([128, 10])\n",
    "            stock_output = data_out_list[1]  # torch.Size([128, 10])\n",
    "\n",
    "            Norm_ = nn.LayerNorm(10, device=args.device)\n",
    "            index_output = args.r * Norm_(index_output) + args.b\n",
    "            stock_output = args.r * Norm_(stock_output) + args.b\n",
    "\n",
    "            Transformer_input = index_output* args.market_beta  + stock_output\n",
    "\n",
    "            output1,wp,bp = transformer(Transformer_input.transpose(0,1).unsqueeze(2))\n",
    "\n",
    "            loss = args.loss_fn(output1, true_y, wp.squeeze(), bp)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss = val_loss / len(valloader)\n",
    "        return att_LSTM, transformer, val_loss\n",
    "\n",
    "\n",
    "def test(att_LSTM, transformer,\n",
    "               partition, args):\n",
    "    testloader = DataLoader(partition['test'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    att_LSTM.eval()\n",
    "    transformer.eval()\n",
    "\n",
    "    ACC_metric = 0.0\n",
    "    MCC_metric = 0.0\n",
    "    with torch.no_grad():\n",
    "        for j, data in enumerate(testloader):\n",
    "\n",
    "            data_out_list = []\n",
    "            for i in range(len(data)):\n",
    "                \n",
    "                x_input = data[i][0].to(args.device)\n",
    "\n",
    "                if i == 1:\n",
    "                    true_y = data[i][1].squeeze().float().to(args.device)\n",
    "\n",
    "                att_LSTM.hidden = [hidden.to(args.device) for hidden in att_LSTM.init_hidden()]\n",
    "\n",
    "\n",
    "                hidden_context, attn_weights = att_LSTM(x_input)\n",
    "\n",
    "                data_out_list.append(hidden_context)\n",
    "\n",
    "            index_output = data_out_list[0]  # torch.Size([128, 10])\n",
    "            stock_output = data_out_list[1]  # torch.Size([128, 10])\n",
    "\n",
    "            Norm_ = nn.LayerNorm(10, device=args.device)\n",
    "            index_output = args.r * Norm_(index_output) + args.b\n",
    "            stock_output = args.r * Norm_(stock_output) + args.b\n",
    "\n",
    "            Transformer_input = index_output * args.market_beta + stock_output\n",
    "\n",
    "            output1,wp,bp = transformer(Transformer_input.transpose(0,1).unsqueeze(2))\n",
    "\n",
    "            output_ = torch.where(output1 >= 0.5, 1.0, 0.0)\n",
    "\n",
    "            output_.requires_grad = True\n",
    "\n",
    "\n",
    "            perc_y_pred = output_.cpu().detach().numpy()\n",
    "            perc_y_true = true_y.cpu().detach().numpy()\n",
    "            acc = accuracy_score(perc_y_true, perc_y_pred)\n",
    "            mcc = matthews_corrcoef(perc_y_true, perc_y_pred)\n",
    "\n",
    "\n",
    "            ACC_metric += acc\n",
    "            MCC_metric += mcc\n",
    "\n",
    "        ACC_metric = ACC_metric / len(testloader)\n",
    "        MCC_metric = MCC_metric / len(testloader)\n",
    "\n",
    "        return ACC_metric, MCC_metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd115972-567d-475b-ac08-15dedfc7d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# ========= experiment setting ========== #\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args.save_file_path = \"D:\\\\dtml_results\"\n",
    "\n",
    "\n",
    "# ====== hyperparameter ======= #\n",
    "args.batch_size = 64\n",
    "args.x_frames = 10\n",
    "args.y_frames = 1\n",
    "args.input_dim = 10\n",
    "args.output_dim = 1\n",
    "args.dropout = 0.2\n",
    "args.use_bn = True\n",
    "args.loss_fn = Selective_Regularization  ## loss function for classification : cross entropy\n",
    "args.optim = 'Adam'\n",
    "args.lr = 0.0005\n",
    "args.l2 = 0.00001 #?\n",
    "args.epoch = 100\n",
    "# ============= model ================== #\n",
    "args.att_LSTM = att_LSTM\n",
    "args.transformer = Transformer\n",
    "\n",
    "# ====== att_lstm hyperparameter ======= #\n",
    "args.hid_dim = 10\n",
    "args.attention_head = 1\n",
    "args.attn_size = 10\n",
    "args.num_layers = 1\n",
    "args.decoder_x_frames = 1\n",
    "\n",
    "# ====== transformer hyperparameter ======= #\n",
    "args.trans_feature_size = 250\n",
    "args.trans_num_laysers = 1\n",
    "args.trans_nhead = 10\n",
    "args.market_beta = 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fdb0bdf-b811-4e2c-a9a4-1289b194c7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss(train/val) 5.01314/5.00140. Took 0.27 sec\n",
      "Epoch 1, Loss(train/val) 4.97845/4.95474. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.97583/4.95910. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.97460/4.96682. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.97504/4.96939. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.97738/4.98318. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.97592/5.00148. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.97815/5.01197. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.97635/5.02269. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.97619/5.01285. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.97364/4.99450. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.96919/4.99595. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.96992/4.99807. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.96802/5.00059. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.96826/5.00816. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.96783/5.01201. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.96696/5.00583. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.96815/5.00186. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.96496/4.99801. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.96286/5.00883. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.96136/5.00693. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.96326/5.01015. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.96099/5.00948. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.96222/5.01981. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.95839/5.02359. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.95988/5.02080. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.95821/5.02619. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.95810/5.03280. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.96063/5.01663. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.95734/5.01063. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.95679/5.02903. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.95622/5.02785. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.95846/5.01032. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.95316/5.01729. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.95345/5.01820. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.95392/5.02303. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.95088/5.02279. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.95377/5.02167. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.95221/5.00604. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.95098/5.02191. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.94956/5.04841. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.95575/5.02313. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.94907/5.05132. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.95537/5.01536. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.95127/5.01137. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.94675/5.02376. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.94308/5.02203. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 4.94432/5.03573. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.94841/5.02600. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.95050/5.01173. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.94505/5.01338. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.94774/5.01373. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.95145/5.00679. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.95056/5.02270. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.95019/5.02195. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.94526/5.01245. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.94535/5.02388. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.94701/5.02026. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.94179/5.02399. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.94386/5.01259. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.94039/5.02618. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.94284/5.01528. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.94801/5.02541. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.94113/5.02522. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.94098/5.04384. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.93956/5.02252. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.94197/5.03089. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.94221/5.03814. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.93349/5.03710. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.93312/5.06452. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.93047/5.04570. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.94152/5.03713. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.92769/5.05922. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.93623/5.03443. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.93568/5.03523. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.93730/5.02800. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.93717/5.06326. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.93780/5.03655. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.93334/5.03909. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.94004/5.04429. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.95345/5.00050. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.94693/5.00890. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.93702/5.05212. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.93421/5.03179. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.93724/5.03016. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.93063/5.05662. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.93397/5.07809. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.93765/5.01468. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.93309/5.02751. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.93914/5.02773. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.93348/5.02986. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.93307/5.04190. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.93138/5.07285. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.93525/5.03849. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.93208/5.06925. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.93218/5.05805. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.93643/5.03867. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.92762/5.05645. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.92936/5.04382. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.92881/5.05331. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.09759000729485331\n",
      "Epoch 0, Loss(train/val) 4.84664/4.82539. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.81921/4.80405. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.81870/4.80226. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.81308/4.81029. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.81464/4.80812. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.81460/4.80558. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.80904/4.80834. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.81407/4.80890. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.81319/4.80825. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.80945/4.80814. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.81142/4.80842. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.80959/4.81348. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.80655/4.83135. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.80593/4.83173. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.80785/4.83055. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.80322/4.84047. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.80095/4.84634. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.80223/4.84556. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.79537/4.86162. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.80077/4.84958. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.79855/4.85891. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.79796/4.87376. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.79417/4.87317. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.79397/4.90615. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.81325/4.86225. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.80367/4.87112. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.79913/4.88230. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.79946/4.88047. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.79762/4.88030. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.79603/4.87820. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.79726/4.89063. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.79394/4.90883. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.79703/4.87694. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.79853/4.88477. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78951/4.89754. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.79041/4.88643. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.79563/4.89396. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.79313/4.88801. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.79234/4.87955. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.79120/4.91321. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.78886/4.90030. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.78755/4.88225. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.78635/4.93045. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.78951/4.88182. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.79141/4.90374. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.79031/4.88856. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.78714/4.90135. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.78338/4.91660. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.78645/4.90347. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.78945/4.89863. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.78341/4.90495. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.79124/4.91882. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.78143/4.91731. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.78874/4.88393. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.78336/4.91609. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.78726/4.87438. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.78101/4.94314. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.78431/4.89752. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.78171/4.90519. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.78680/4.88527. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.78477/4.91363. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.78308/4.89681. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.78076/4.91039. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.77658/4.94808. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.77989/4.89162. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.78074/4.90901. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.78193/4.91892. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.77718/4.90385. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.78505/4.89056. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.78294/4.89643. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.78356/4.90912. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78065/4.91971. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.77966/4.90728. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.79055/4.90311. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.78855/4.91092. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.78825/4.91140. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78706/4.89344. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78077/4.92284. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.78187/4.90933. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.78302/4.91527. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.78191/4.92640. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.78062/4.89959. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.78278/4.91882. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.77796/4.93350. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.77970/4.90542. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.78190/4.91741. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.78236/4.89475. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.77650/4.91839. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.77621/4.93391. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.77383/4.93019. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.77717/4.91282. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.76836/4.93655. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.77803/4.92725. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.77303/4.93585. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.77595/4.92524. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.77587/4.90630. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.76758/4.92399. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.77376/4.92490. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77385/4.91925. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.76988/4.94197. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.0657512425947884\n",
      "Epoch 0, Loss(train/val) 4.91886/4.87482. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.86414/4.90172. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.86250/4.90512. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.86497/4.90461. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.86190/4.90328. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.86375/4.89018. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.86308/4.89122. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.85924/4.89326. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.85979/4.89143. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.85651/4.89423. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.86167/4.88919. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.85530/4.89618. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85989/4.89265. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.85791/4.89161. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.85296/4.89021. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.85201/4.89574. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.85391/4.90216. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.85608/4.89430. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.85372/4.88953. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85312/4.89055. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.85295/4.89306. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.85150/4.88709. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.85092/4.89099. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.84659/4.88690. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.84853/4.89529. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.85141/4.88283. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.85322/4.88781. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.85484/4.86709. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.85333/4.87462. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84931/4.87890. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.85011/4.88567. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.84510/4.88941. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84783/4.90090. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.84528/4.90328. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.84498/4.90722. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.85240/4.89955. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.85165/4.91386. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84898/4.91443. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84598/4.90552. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84654/4.90497. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.84506/4.90146. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.84632/4.92646. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84435/4.91613. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.84111/4.92639. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.84860/4.88681. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.84620/4.92698. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.84495/4.90662. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.84565/4.90036. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84439/4.89183. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.84103/4.89242. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.84001/4.91004. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.83671/4.92854. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.84355/4.90845. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.84420/4.93911. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.84950/4.90251. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.84669/4.92010. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.83995/4.91085. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.84288/4.90331. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.84143/4.92670. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.83791/4.91190. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.83512/4.92847. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83932/4.93243. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.83673/4.91594. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.83398/4.92085. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.83476/4.92145. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.83788/4.90947. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.83556/4.92568. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.83710/4.94420. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.83484/4.92102. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83843/4.92482. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.83780/4.93839. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.83903/4.91707. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.83603/4.94290. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.84354/4.90637. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83838/4.91918. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83579/4.92992. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.83518/4.96159. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.83891/4.95318. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.83427/4.93285. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.83159/4.96678. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83556/4.94399. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.83856/4.93358. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.83345/4.93601. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.83085/4.96937. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83537/4.95640. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83386/4.97022. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.82782/4.98052. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.83781/4.92405. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83097/4.94258. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.83407/4.96933. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.83367/4.94895. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.83571/4.94429. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83011/4.93899. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83507/4.96012. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.83396/4.95008. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.83082/4.96767. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.82748/5.01109. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.82786/4.93480. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83249/4.96332. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.82739/4.97660. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.1563263498701806\n",
      "Epoch 0, Loss(train/val) 5.01101/4.93303. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.91961/4.92222. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.91824/4.91206. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.91679/4.90732. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.91824/4.90425. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.91596/4.90492. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.91506/4.90755. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.91384/4.91157. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.91563/4.91590. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.91232/4.92188. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.91419/4.92389. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.91435/4.92367. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.91202/4.93004. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.91418/4.92169. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.90823/4.92526. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.90958/4.92850. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.91293/4.92001. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.90816/4.92209. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.91121/4.91974. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.91303/4.92189. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.90975/4.92497. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.90766/4.92905. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.91097/4.92720. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.91049/4.92511. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.90943/4.92799. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.90781/4.92576. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.90815/4.91935. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.90707/4.92195. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.91070/4.92357. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.91276/4.92259. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.90763/4.93429. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.90955/4.92778. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.90720/4.92613. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.90640/4.92792. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.90313/4.92330. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.90588/4.93046. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.90601/4.93392. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.90221/4.93409. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.90549/4.93006. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.90371/4.93515. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.90782/4.93471. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.90395/4.93814. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.90620/4.93632. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.90478/4.93766. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.90415/4.93727. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.89986/4.93787. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.90304/4.93677. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.90313/4.93732. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.90200/4.94124. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.90125/4.92854. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.89993/4.92641. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.90144/4.93432. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.89786/4.94377. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.89979/4.93982. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.89567/4.95221. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.89861/4.94400. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.89746/4.94647. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.89371/4.94558. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.89517/4.95631. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.90727/4.93618. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.90048/4.94758. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89956/4.93564. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.89690/4.95795. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.89494/4.93290. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.89425/4.94689. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.89709/4.93675. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.89491/4.93591. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.89394/4.93433. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.89492/4.93010. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.89132/4.94780. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.89643/4.92310. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.89304/4.94659. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.89238/4.94287. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88823/4.96683. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.88239/4.97076. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.88890/4.95239. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88989/4.95801. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.89611/4.94805. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.88799/4.94694. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88969/4.95538. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.88468/4.95697. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.87907/4.96436. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.88745/4.96102. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.87629/4.96581. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.88692/4.94572. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.88431/4.94777. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.88337/4.96536. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.89059/4.96093. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.88369/4.96088. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88398/4.97186. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.88791/4.94398. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.87489/4.95280. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.88691/4.96403. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.88365/4.96582. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.88184/4.94455. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.89514/4.95117. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.88199/4.94621. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.88885/4.96488. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.88101/4.96359. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.87482/4.98370. Took 0.08 sec\n",
      "ACC: 0.625, MCC: 0.24305875451990117\n",
      "Epoch 0, Loss(train/val) 4.75492/4.79098. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.74274/4.81670. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.74771/4.78337. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.74727/4.72972. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.73595/4.72524. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.72796/4.73369. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.73420/4.73618. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.73367/4.72910. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.73150/4.73046. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.73014/4.72784. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.73058/4.72739. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.72817/4.73097. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.73111/4.72342. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.72569/4.72815. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.72594/4.73253. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.73059/4.72769. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.72615/4.72455. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.72439/4.73204. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.72619/4.72931. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.72762/4.72750. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.72576/4.72415. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.71784/4.74229. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.72078/4.73511. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.72071/4.72982. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.71823/4.75319. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.72220/4.73558. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.72086/4.72601. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.71663/4.73677. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.71862/4.73143. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.72027/4.72865. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.71641/4.74972. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.72026/4.73476. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.71654/4.74871. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.71607/4.72915. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.71703/4.73271. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.71629/4.72655. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.71279/4.72709. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.71187/4.73057. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.71234/4.78147. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.72466/4.73431. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.71359/4.73150. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.70557/4.75651. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.71649/4.74892. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.71437/4.73492. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.70960/4.73354. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.70573/4.76879. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.71616/4.74596. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.70656/4.75098. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.71131/4.74431. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.70455/4.74064. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.70263/4.76523. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.70911/4.75158. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.70908/4.74926. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.70157/4.76451. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.70824/4.75437. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.69985/4.78259. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.70953/4.74557. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.70479/4.79935. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.71058/4.75300. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.70657/4.74557. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.70675/4.74903. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.70267/4.76364. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.70565/4.75635. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.70169/4.75045. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.69908/4.76655. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.70071/4.74602. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.69661/4.77072. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.70959/4.73808. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.69644/4.76931. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.69659/4.74722. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.69146/4.76545. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.70082/4.77115. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.70464/4.75638. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.70647/4.72929. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.69661/4.76349. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.69751/4.76138. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.69779/4.75821. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.70037/4.73630. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.69783/4.76487. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.69876/4.77359. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.70030/4.75339. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.69580/4.75219. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.69550/4.74660. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.69629/4.76757. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.69756/4.75703. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.69440/4.76200. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.69557/4.75770. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.70578/4.76138. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.70373/4.74270. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.70931/4.72876. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.69863/4.74190. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.69829/4.73710. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.69788/4.75678. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.69254/4.76843. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.69378/4.75384. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.70128/4.76426. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.69992/4.73515. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.69901/4.74599. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.70014/4.74032. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.70298/4.72808. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.16150120428611295\n",
      "Epoch 0, Loss(train/val) 5.05370/4.97759. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.98629/4.98497. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.98659/4.99414. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.99375/5.00518. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.99165/5.00847. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.98901/5.00573. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.98288/5.00178. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.98031/4.99928. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.97674/5.00144. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.97528/5.01048. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.98203/4.98996. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.98033/4.97697. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.97509/4.97407. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.97243/4.97461. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.97414/4.97926. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.97341/4.97908. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.97544/4.98369. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.97164/4.98340. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.96802/4.98803. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.97074/4.99095. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.97107/4.98681. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.96718/4.98699. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.96519/4.98743. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.96446/4.98522. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.96689/4.98245. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.96202/4.98487. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.96423/4.97606. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.95956/4.97462. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.96495/4.98703. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.96054/4.98112. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.96397/4.98446. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.96431/4.98116. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.96310/4.98742. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.95979/4.98887. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.95542/4.98381. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.96124/4.98766. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.95590/4.97876. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.97105/5.00469. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.97771/4.99025. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.96637/4.99846. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.96427/4.99603. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.96437/4.98620. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.96208/4.98960. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.95787/4.97888. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.96380/4.98516. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.95819/4.98277. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.95508/4.98799. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.96077/4.98821. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.96181/4.98379. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.96861/4.97716. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.97911/4.98393. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.97616/4.98331. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.97142/4.99270. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.96847/4.98672. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.96832/4.99716. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.96329/4.97891. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.96302/4.98696. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.96295/4.98056. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.96338/4.97605. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.96366/4.97731. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.96361/4.97562. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.95577/4.98624. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.95751/4.97807. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.95800/4.98673. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.95184/4.97966. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.95686/4.98549. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.95787/4.98328. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.95428/4.98122. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.95201/4.98586. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.95730/4.98341. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.95087/4.99276. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.95230/4.98929. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.95087/4.97307. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.94797/4.97376. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.95323/4.96677. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.94833/4.96926. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.95247/4.97046. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.95188/4.96776. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.94649/4.96311. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.95682/4.97689. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.95242/4.97200. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.94652/4.97584. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.94669/4.97979. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.95136/4.96033. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.94790/4.95637. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.95249/4.97252. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.94887/4.97667. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.94402/4.97495. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.95073/4.96444. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.94743/4.96596. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.94587/4.96015. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.94946/4.97671. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.95117/4.97553. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.95009/4.95683. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.95019/4.96377. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.95062/4.96975. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.94920/5.01013. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.94067/4.97273. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.94768/5.02676. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.97213/4.96513. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.06362847629757777\n",
      "Epoch 0, Loss(train/val) 4.92514/4.81297. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.83327/4.82299. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.83381/4.80458. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.82679/4.80442. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.82557/4.80494. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.82241/4.80553. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.82246/4.80812. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.82484/4.80997. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.82494/4.81233. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.82371/4.81329. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.81951/4.81572. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.81642/4.81809. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.81589/4.82208. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.81718/4.81783. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.81615/4.81945. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.81624/4.82942. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.81542/4.81375. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.80985/4.81052. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.81251/4.82161. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.81656/4.82061. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.81183/4.82210. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.80800/4.81515. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.81033/4.81189. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.81213/4.81028. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.80605/4.81357. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.80817/4.80491. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.80251/4.81514. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.80659/4.80403. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.80586/4.80831. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.80224/4.80658. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.80697/4.81460. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.80209/4.81009. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.80219/4.80386. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.80460/4.80928. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.80512/4.80976. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.80537/4.80778. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.80311/4.80961. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.80503/4.80378. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.79737/4.80407. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.80277/4.81791. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.79861/4.81817. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.79946/4.80664. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.79873/4.81542. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.79992/4.80571. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.79692/4.81731. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.79596/4.81532. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.79944/4.81264. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.79514/4.81731. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.79015/4.82629. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.79973/4.81484. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.79742/4.82906. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.79686/4.82283. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.79085/4.82610. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.80013/4.82040. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.79058/4.83598. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.79467/4.81311. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.79405/4.83283. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.79197/4.81947. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.78845/4.82221. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.78671/4.83851. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.78733/4.82939. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.78280/4.83349. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.78604/4.85886. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.78939/4.84522. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.78981/4.86877. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.79508/4.82981. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.78785/4.84304. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.78261/4.83265. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.78300/4.84767. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.78360/4.85068. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.77740/4.85345. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78035/4.87466. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.78126/4.84128. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.78330/4.84767. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.77752/4.86549. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.78124/4.85682. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78051/4.88761. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78367/4.84792. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.77846/4.87808. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.77277/4.87221. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.77891/4.89106. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.78012/4.85437. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.77671/4.87172. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.77887/4.87561. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.78016/4.87549. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.78120/4.87420. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.77978/4.87868. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.77484/4.88437. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.76597/4.87574. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.77400/4.87884. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.77737/4.87078. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.77748/4.86204. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.77073/4.87997. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.77986/4.87159. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.77622/4.91739. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.78025/4.87646. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.76355/4.89261. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.77099/4.92409. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77211/4.90711. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.76994/4.91123. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.02938413738897565\n",
      "Epoch 0, Loss(train/val) 5.04878/4.97115. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.94017/4.97806. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.94208/4.98429. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.94531/4.96477. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.93596/4.96362. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.92968/4.96592. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.92915/4.97262. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.92700/4.97421. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.92630/4.96957. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.92259/4.97569. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.92636/4.97596. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.92556/4.97822. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.92260/4.97018. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.92119/4.96524. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.92276/4.97186. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.91765/4.97623. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.92713/4.96626. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.92273/4.97400. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.92624/4.96282. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.92099/4.96971. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.91779/4.97215. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.92019/4.97400. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.92208/4.97130. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.91825/4.97658. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.92158/4.96510. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.91910/4.97422. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.91897/4.97236. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.92197/4.97102. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.91908/4.97527. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.91796/4.97754. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.91457/4.97893. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.91822/4.97310. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.91941/4.98305. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.92921/4.95548. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.92504/4.97415. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.92071/4.96558. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.92277/4.95731. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.91845/4.97276. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.92317/4.96818. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.92199/4.95531. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.92112/4.96916. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.92089/4.97162. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.91687/4.97623. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.92050/4.96966. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.91594/4.97664. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.91589/4.97828. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.91767/4.97551. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.91246/4.97333. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.91233/4.97611. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.91610/4.98451. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.91686/4.96985. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.91420/4.99791. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.91571/4.96048. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.91127/4.98958. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.91505/4.99299. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.91832/4.95912. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.91655/4.98945. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.91825/4.94219. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.91744/4.98097. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.91977/4.97412. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.91788/4.98646. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.91280/5.00705. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.91336/4.97720. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.91652/5.00942. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.91461/4.97007. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.90831/5.00457. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.91619/4.99663. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.90789/4.99956. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.90811/5.02087. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.91165/4.97149. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.91613/4.97218. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.91424/4.99295. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.91635/4.96467. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.91889/4.97389. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.91240/4.98601. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.91275/4.97360. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.91052/4.99687. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.91199/4.99437. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.90870/4.98241. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.91254/4.98817. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.91517/4.97341. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.90776/4.99045. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.90726/5.00565. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.90538/4.96890. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.91224/4.99230. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.91329/4.97552. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.90149/5.01192. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.90082/5.01555. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.90846/4.97611. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.90886/5.00562. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.90451/4.97853. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.90128/5.01883. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.90807/4.96512. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.90301/5.00601. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.90977/4.99689. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.90522/4.98889. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.90367/4.99218. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.90299/4.99861. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.90673/5.00621. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.90220/4.99504. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.12855839970025792\n",
      "Epoch 0, Loss(train/val) 5.03666/5.02632. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.00680/4.97363. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.00215/4.97007. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.99938/4.97386. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.99799/4.96952. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.99354/4.97235. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.99346/4.97547. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.99326/4.98140. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.99326/4.97366. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.99436/4.97796. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.99233/4.98730. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.99461/4.99821. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.99162/5.00876. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.99331/5.02647. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.99977/5.02575. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.99002/5.00526. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.98814/5.00091. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.98598/5.00049. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.99012/5.01628. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.98590/5.01872. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.98872/5.01414. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.99442/5.00682. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.98702/4.99952. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.98948/5.00379. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.98988/5.00495. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.98193/5.01253. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.98866/5.01089. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.98970/5.02373. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.98386/5.01615. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.98873/5.00494. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.98409/5.02180. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.98746/5.00379. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.98281/5.01637. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.98373/5.01108. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.98449/5.01159. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.98430/5.02445. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.98098/5.02060. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.98518/5.01817. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.98498/5.02251. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.97867/5.01718. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.97700/5.05835. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.98641/5.02760. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.97883/5.02594. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.99430/5.01310. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.98671/5.02510. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.98448/5.00956. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.98225/5.00990. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.98231/5.01028. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.98420/5.01485. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.98053/5.02276. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.97945/5.01802. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.98362/5.02611. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.98916/5.02847. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.98325/5.02549. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.97271/5.01704. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.97855/5.02714. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.97296/5.02018. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.97863/5.03945. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.97937/5.03632. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.97706/5.04395. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.97437/5.03233. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.97476/5.01997. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.97731/5.07002. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.97641/5.03013. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.97922/5.03729. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.97637/5.02899. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.97409/5.04644. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.97381/5.03034. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.97213/5.05450. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.97369/5.03833. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.97305/5.06109. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.97273/5.04318. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.97650/5.03139. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.97787/5.04041. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.97304/5.04128. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.97076/5.03999. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.96813/5.06296. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.97204/5.06954. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.97840/5.03993. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.97059/5.03961. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.96863/5.05521. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.96686/5.07817. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.97731/5.03309. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.96962/5.03693. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.96900/5.04195. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.96928/5.04868. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.96891/5.03416. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.96966/5.06202. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.96749/5.03234. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.96735/5.08257. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.97223/5.03787. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.97031/5.03649. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.97086/5.03707. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.96544/5.04782. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.96823/5.07107. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.00168/4.97916. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.99323/4.99304. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.98517/5.03666. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.98498/4.96377. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.98977/4.97112. Took 0.09 sec\n",
      "ACC: 0.578125, MCC: 0.1463182264089704\n",
      "Epoch 0, Loss(train/val) 4.71318/4.69194. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.67599/4.67224. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.67384/4.67349. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.67464/4.67609. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.67432/4.67820. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.67661/4.68053. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.67536/4.68325. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.67395/4.68313. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.67516/4.68302. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.66951/4.68668. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.67118/4.68774. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.67075/4.68542. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.66863/4.68696. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.66793/4.67561. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.67368/4.67407. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.67115/4.67223. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.67117/4.67273. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.66785/4.67297. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.66869/4.67417. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.66714/4.67397. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.66374/4.68197. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.66936/4.67863. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.66443/4.68436. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.66207/4.68656. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.66863/4.68446. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.66612/4.68792. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.66554/4.68385. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.66198/4.68714. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.65908/4.70455. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.66254/4.69187. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.66183/4.69233. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.65791/4.70589. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.65627/4.69382. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.65758/4.70594. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.66212/4.69413. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.65734/4.70816. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.65005/4.70075. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.65062/4.71469. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.65390/4.70918. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.65385/4.70799. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.65939/4.68816. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.65298/4.70473. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.64603/4.72407. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.65035/4.71454. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.65241/4.73791. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.66017/4.69849. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.65302/4.72110. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.65627/4.70730. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.65828/4.70931. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.65880/4.70936. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.65546/4.70434. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.65158/4.70546. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.65139/4.70493. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.64963/4.71022. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.64909/4.71990. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.64992/4.70668. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.64494/4.71767. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.64785/4.74300. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.64738/4.71396. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.64319/4.72643. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.64495/4.72567. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.65041/4.71583. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.64620/4.72342. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.64183/4.73858. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.64502/4.71360. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.64807/4.73349. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.64568/4.73420. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.64056/4.74808. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.64154/4.74182. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.64392/4.71161. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.64731/4.71982. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.64156/4.73732. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.64291/4.73970. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.64369/4.73639. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.63706/4.74902. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.64190/4.75685. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.64107/4.74102. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.64183/4.74336. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.64167/4.74101. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.63796/4.73866. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.64090/4.75953. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.64324/4.71154. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.67385/4.69021. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.66594/4.69019. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.66397/4.69705. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.65881/4.69542. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.65262/4.69640. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.64931/4.71541. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.64545/4.73061. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.63882/4.75413. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.65293/4.70455. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.65373/4.72258. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.64500/4.72793. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.64398/4.75062. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.64405/4.74305. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.64290/4.74464. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.63959/4.74233. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.64141/4.74153. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.63915/4.74398. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.64042/4.74499. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.07570682517832987\n",
      "Epoch 0, Loss(train/val) 4.92467/4.84718. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.89246/4.88068. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 4.85777/4.85920. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.83800/4.84187. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.84244/4.85159. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.84476/4.85084. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.84393/4.85320. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.84409/4.84984. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.84182/4.85545. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.83981/4.84673. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.84089/4.85288. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.84311/4.85201. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.83938/4.85523. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.83909/4.85611. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.83594/4.85598. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.83870/4.85859. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.83906/4.85700. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.83542/4.85551. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.83332/4.85743. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.83089/4.85427. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.83507/4.85641. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.83444/4.85005. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.83697/4.84855. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.83097/4.85945. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.83197/4.84898. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.83176/4.84747. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.83090/4.86572. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82621/4.84311. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.83013/4.85147. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.82697/4.86039. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.82592/4.85038. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.83149/4.85656. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.82483/4.85600. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.82768/4.85721. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.82969/4.85455. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.82338/4.85151. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.82442/4.84526. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.82055/4.86355. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.82265/4.85683. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.82074/4.87349. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.82786/4.86453. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.82060/4.85072. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.82225/4.85197. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.82205/4.84970. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.82056/4.85651. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.81846/4.86657. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.82242/4.85016. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.81506/4.84509. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.82254/4.85164. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.81899/4.87027. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.82233/4.86771. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.82148/4.83480. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.82342/4.87307. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.82028/4.84874. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.81297/4.85239. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.81935/4.87561. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.81273/4.85534. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.82242/4.85592. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.81173/4.86980. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.81858/4.85078. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.81756/4.85204. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.80638/4.86301. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.81666/4.84691. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.81559/4.84478. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.81093/4.85118. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.81411/4.86569. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.82360/4.82121. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.82024/4.85983. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.81010/4.86318. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.81280/4.86825. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.81691/4.86224. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.82250/4.85642. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.81237/4.86371. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.81371/4.84866. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.81299/4.86218. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.81200/4.84321. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.81000/4.84904. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.81040/4.86327. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.81590/4.83904. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.81108/4.85247. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.81865/4.89352. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.81419/4.84335. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.81380/4.86804. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.80957/4.83989. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.81318/4.84775. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.81219/4.86334. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.80887/4.86250. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.80679/4.84410. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.81991/4.87322. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.82755/4.86175. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.82212/4.86799. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.81985/4.87188. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.81836/4.88580. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.81701/4.87410. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.81359/4.86782. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.81970/4.86401. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.81612/4.84519. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.81161/4.85912. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.82163/4.85327. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.81804/4.86098. Took 0.09 sec\n",
      "ACC: 0.46875, MCC: -0.08818077954471167\n",
      "Epoch 0, Loss(train/val) 5.02112/4.97087. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.95192/4.97660. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.95258/4.91829. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.92242/4.92320. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.92534/4.93074. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.92840/4.92841. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.92952/4.92491. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.93083/4.92829. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.92823/4.92562. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.92486/4.92712. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.92490/4.92393. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.92080/4.92239. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.92316/4.92011. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.92305/4.91765. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.91947/4.91666. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.91986/4.91559. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.91952/4.91765. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.91723/4.91843. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.91752/4.91711. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.92279/4.92071. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.92583/4.92256. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.92228/4.91828. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.92030/4.92038. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.91809/4.91983. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.91952/4.91672. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.92074/4.91826. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.91720/4.91986. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.91712/4.91840. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.91524/4.91958. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.91667/4.91880. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.91546/4.91661. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.91182/4.91451. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.91243/4.92071. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.91036/4.92385. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.91454/4.91834. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.91026/4.92225. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.90954/4.92722. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.91210/4.91913. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.91290/4.91983. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.90755/4.92526. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.90850/4.92913. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.90301/4.92498. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.90700/4.93157. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.90119/4.92267. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.90587/4.93355. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.90733/4.93328. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.90255/4.93822. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.90315/4.94433. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.90374/4.94340. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.90192/4.94449. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.90108/4.95115. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.89488/4.96158. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.90299/4.96674. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.90179/4.94667. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.89256/4.96444. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.89292/4.96477. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.89443/4.97507. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.89464/4.96513. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.89099/4.96064. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.90438/4.94094. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.89264/4.95852. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89610/4.97257. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.89821/4.95305. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.89809/4.96945. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.89148/4.96976. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.88796/4.97886. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.89473/4.96331. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.89231/4.97699. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.89193/4.98158. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.88558/4.98938. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.88418/4.99132. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.88495/4.98864. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.89000/4.97490. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88937/4.98446. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.88025/5.00594. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.88709/5.00954. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88008/5.00029. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.88205/4.99535. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.88180/4.97950. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88435/4.97692. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.88534/4.99403. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.88370/4.99174. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.88381/4.97881. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.88092/4.99349. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.88698/4.99619. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.88447/5.00169. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.88286/4.99131. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.87293/5.00951. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.88226/4.98344. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.88251/4.98939. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.87809/5.01729. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.88407/5.00754. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.87952/4.98205. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.88315/5.02789. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.88012/4.98917. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.87430/5.03495. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.88206/5.01363. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.88385/4.99039. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.88625/5.00229. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.88091/5.01859. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.09202163616785992\n",
      "Epoch 0, Loss(train/val) 4.88321/4.81404. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.85130/4.83213. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.85144/4.82935. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.85873/4.83176. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.86573/4.87822. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.85801/4.88854. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.84948/4.86123. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.84949/4.86890. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.85007/4.87154. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.84891/4.87004. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.84806/4.87129. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.84977/4.87302. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.85001/4.88317. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.84863/4.88374. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.84353/4.88036. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.84591/4.87317. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.84857/4.89492. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.85127/4.89036. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.84465/4.87970. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.84265/4.88687. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.84010/4.89809. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.84547/4.90658. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.84056/4.90500. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.84119/4.89053. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.83787/4.90971. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.83913/4.90594. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.83649/4.91052. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.83191/4.91631. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.83383/4.91776. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.83578/4.91692. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.83085/4.93925. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.82888/4.92661. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84124/4.92427. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.83471/4.90009. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.83432/4.91150. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.83586/4.91804. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.84080/4.88132. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.83750/4.89125. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.83393/4.89874. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.83860/4.88700. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.83689/4.88658. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.83146/4.91484. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.83270/4.90603. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.82967/4.90296. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.83024/4.92215. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.83008/4.93248. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.83455/4.91841. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.83412/4.91412. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.82813/4.90751. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.83035/4.92407. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.83332/4.93469. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.83065/4.93618. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.82610/4.93888. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.82286/4.93448. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.82749/4.93604. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.82701/4.92525. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.82423/4.92829. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.82616/4.92085. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.82455/4.90806. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.83746/4.90171. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.82876/4.89930. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83115/4.88969. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.83068/4.92789. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.82854/4.92978. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.82961/4.89405. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.82697/4.90927. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.82761/4.90614. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.82172/4.93464. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.82561/4.90488. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.81719/4.93906. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.81585/4.93192. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.81077/4.95546. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.81400/4.93053. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.81967/4.92251. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.81954/4.93172. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.80879/4.95255. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.81187/4.91455. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.81320/4.93781. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.81663/4.95463. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.81243/4.93732. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.81999/4.93955. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.81514/4.93864. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.83660/4.88855. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.82992/4.91304. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83477/4.92172. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.82687/4.93268. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.82957/4.96295. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.82217/4.93516. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.82281/4.90381. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.81730/4.96160. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.81414/4.95506. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.81880/4.93774. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.81292/4.94690. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.81542/4.93871. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.81387/4.96614. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.81699/4.93926. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.80945/4.94715. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.80622/4.97202. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.81674/4.93331. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.82176/4.97486. Took 0.08 sec\n",
      "ACC: 0.578125, MCC: 0.15694120514358612\n",
      "Epoch 0, Loss(train/val) 4.81257/4.79918. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.81129/4.81343. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.79547/4.80058. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79785/4.79131. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.80400/4.79924. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79746/4.79837. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.79424/4.78932. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.79105/4.78872. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.79123/4.78642. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79165/4.78487. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79139/4.78801. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.79076/4.78709. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.79103/4.78845. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.78820/4.78960. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.79106/4.78896. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.78989/4.78781. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78824/4.78883. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.79028/4.79004. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.78868/4.79082. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.78830/4.79441. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.78614/4.79118. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78587/4.79039. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.78841/4.79260. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.78600/4.78917. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.78555/4.79501. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.78652/4.79168. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.78842/4.79715. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.78826/4.79654. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.78359/4.79471. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.78368/4.79336. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.78321/4.79253. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78280/4.79328. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.78344/4.80026. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78383/4.80072. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78040/4.80797. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.78148/4.80266. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.78606/4.79860. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.77910/4.79442. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.78051/4.80286. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.78494/4.80000. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.78697/4.80228. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.78181/4.79740. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.78185/4.79933. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.78197/4.80326. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.78361/4.80533. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.77765/4.80501. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77407/4.81582. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77844/4.82582. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77796/4.80987. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.77557/4.82227. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77419/4.82967. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77410/4.81818. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.77636/4.81435. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.77167/4.82609. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.77563/4.83296. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.77360/4.82529. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.77578/4.81854. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.76844/4.82482. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.77179/4.83203. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.76986/4.83110. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.77047/4.82058. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.76793/4.82673. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76583/4.82436. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.77294/4.83080. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.77270/4.81500. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.76981/4.81721. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.77253/4.81382. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.76721/4.82768. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.76780/4.83197. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76962/4.81058. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.76783/4.82509. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.76824/4.82615. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76829/4.82873. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.76827/4.82523. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.76584/4.81449. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.76693/4.82404. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.76557/4.83370. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.77139/4.81305. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.76923/4.81072. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.76848/4.83392. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.76521/4.84022. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.76030/4.84271. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.75928/4.82431. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76037/4.81097. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.76249/4.84702. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.75597/4.85248. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.76560/4.80541. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.76664/4.80688. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.76273/4.84048. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.76217/4.80925. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.75768/4.82042. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.76147/4.78911. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.76296/4.81847. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.75705/4.82350. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.76190/4.85247. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.77105/4.81831. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.78348/4.82516. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.77756/4.82339. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77660/4.81010. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.77836/4.81245. Took 0.08 sec\n",
      "ACC: 0.5, MCC: -0.011953709238683663\n",
      "Epoch 0, Loss(train/val) 4.86538/4.83907. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.84713/4.83780. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.84313/4.83904. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.84064/4.84478. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.84269/4.84746. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.84423/4.83585. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.84015/4.83165. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.83642/4.83265. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.83492/4.83296. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.83806/4.83317. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.83441/4.83335. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.83626/4.83339. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.83472/4.83414. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.83557/4.83659. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.83350/4.83748. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.83384/4.83583. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.83455/4.83764. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.83277/4.83620. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.83070/4.82647. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.83012/4.83121. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.83015/4.83728. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.82991/4.83005. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.82917/4.82997. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.82852/4.83126. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.82841/4.83148. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.82686/4.82859. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.82310/4.83464. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82620/4.83558. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.82459/4.83039. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.82595/4.82824. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.81812/4.83217. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.82169/4.83391. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.82460/4.83006. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.81924/4.82985. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.81680/4.82732. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.81887/4.83727. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.82748/4.82752. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.81959/4.82720. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.81972/4.85076. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.82499/4.83928. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.82098/4.84154. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.82181/4.84356. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.82046/4.83958. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.82120/4.83584. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.82344/4.83507. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.81969/4.83275. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.81474/4.83558. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.81881/4.83238. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.81585/4.83836. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.81945/4.82617. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.81441/4.82956. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.81360/4.83161. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.81314/4.82887. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.81542/4.82342. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.81395/4.81741. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.81658/4.83414. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.82026/4.82591. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.81041/4.83697. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.81152/4.83171. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.81277/4.82847. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.81930/4.82642. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.80967/4.83306. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.81703/4.82571. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.81060/4.83654. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.81064/4.84281. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.81861/4.82281. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.80430/4.83339. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.80900/4.82991. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.80956/4.83562. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.80812/4.86666. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.81958/4.85169. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.82369/4.83967. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.81317/4.84045. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.81372/4.84237. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.81667/4.83494. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.81189/4.83283. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.81412/4.83329. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.81625/4.80058. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.82688/4.81471. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.82043/4.82782. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.82298/4.82100. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.81605/4.82007. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.81660/4.81642. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.81140/4.81958. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.81315/4.82032. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.80998/4.82181. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.81427/4.81507. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.81203/4.83020. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.81093/4.83102. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.81164/4.81960. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.80525/4.82727. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.80858/4.83419. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.81293/4.83519. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.80461/4.82623. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.80112/4.81476. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.80749/4.83917. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.80942/4.83005. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.81014/4.82473. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.81097/4.81528. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.80878/4.83050. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.0905982365507463\n",
      "Epoch 0, Loss(train/val) 4.93144/4.88661. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.90538/4.89907. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.90404/4.88258. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.91255/4.87940. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.91445/4.87611. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.91867/4.91374. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.90332/4.91500. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.89989/4.90464. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.90183/4.91204. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.89810/4.90643. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.90083/4.90106. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.90164/4.90369. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.89988/4.90230. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.89972/4.89838. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.89728/4.90066. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.89832/4.90620. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.89311/4.90366. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.90051/4.90387. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.89482/4.91948. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.89495/4.90877. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.89391/4.91132. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.89167/4.90674. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.89077/4.90865. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.89178/4.90468. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.88146/4.92075. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.89871/4.89442. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.88896/4.92977. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.88729/4.90589. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.88337/4.91956. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.88433/4.90333. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.88624/4.91478. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.88077/4.89913. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.88631/4.91483. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.88194/4.90718. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.88600/4.91129. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.88206/4.91740. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.88688/4.90338. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.89751/4.91796. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.89467/4.90676. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.89937/4.90701. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.89121/4.90425. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.89057/4.91203. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.88942/4.90224. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.88964/4.90703. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.88692/4.90516. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.88531/4.91498. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.88722/4.91560. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.88875/4.91906. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.88255/4.92007. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.88280/4.90864. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.88409/4.93314. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.88420/4.91332. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.88304/4.92345. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.88290/4.93126. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.88012/4.91691. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.88220/4.92326. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.88208/4.91257. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.87950/4.93392. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.87883/4.92210. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.88391/4.92173. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.87863/4.92035. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.88401/4.93249. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.87905/4.92397. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.87881/4.92556. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.87875/4.92069. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.87619/4.93672. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.87902/4.93546. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.88085/4.91992. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.88314/4.92926. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.87850/4.91007. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.87691/4.93137. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.88093/4.92280. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88019/4.92306. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.87428/4.91126. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.87865/4.92012. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.87040/4.92870. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.87779/4.90930. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.88051/4.92130. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.87013/4.91627. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88051/4.90267. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.87815/4.90977. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.87518/4.93425. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.87800/4.93349. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.88129/4.91399. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.87128/4.92647. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.87415/4.91219. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.87149/4.91883. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.87109/4.94276. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.87938/4.91452. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.87558/4.92764. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.88036/4.91292. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.87630/4.92938. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.87367/4.93027. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.87212/4.91845. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.86906/4.91640. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.87512/4.91320. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.87357/4.93446. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.87454/4.93273. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.87612/4.89219. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.87598/4.90644. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: 0.010188710438961876\n",
      "Epoch 0, Loss(train/val) 5.02240/4.93480. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.96581/4.94089. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.96163/4.94217. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.96324/4.93982. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.96622/4.93949. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.96392/4.94036. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.95904/4.93963. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.95744/4.94153. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.95799/4.94389. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.95717/4.94401. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.95686/4.94522. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.95523/4.94502. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.95464/4.94382. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.95706/4.94861. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.95577/4.94949. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.95338/4.94970. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.95481/4.94653. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.94904/4.95028. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.95139/4.95634. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.95348/4.95308. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.95185/4.95307. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.94795/4.95652. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.94524/4.96317. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.94908/4.96429. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.94621/4.95858. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.94590/4.96608. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.94996/4.95646. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.94546/4.95921. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.94173/4.96240. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.94333/4.95396. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.94371/4.95901. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.93757/4.96529. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.94072/4.96599. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.94328/4.95761. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.93797/4.95974. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.94156/4.96792. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.94284/4.95662. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.94040/4.96256. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.93441/4.96929. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.93616/4.97580. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.93828/4.95641. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.93752/4.95496. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.93341/4.96392. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.93473/4.96641. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.93430/4.95675. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.93585/4.95849. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.92862/4.97901. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.93497/4.95635. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.93040/4.95589. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.93322/4.95308. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.92692/4.95831. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.93211/4.96091. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.92727/4.95802. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.92696/4.95411. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.92046/4.96809. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.92410/4.94973. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.93045/4.94964. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.92884/4.96090. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.92888/4.94337. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.92619/4.96455. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.92277/4.95526. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.92642/4.95196. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.92456/4.96599. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.92583/4.95377. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.92521/4.96385. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.92677/4.95323. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.92902/4.95970. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.91983/4.96861. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.91901/4.95736. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.92650/4.94802. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.93010/4.96793. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.92146/4.94133. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.92039/4.95658. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.93098/4.95381. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.92309/4.95418. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.91705/4.95738. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.91656/4.95085. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.91949/4.95229. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.91758/4.95449. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.92985/4.97717. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.94770/4.94981. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.92658/4.95009. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.92956/4.95138. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.92668/4.95172. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.91879/4.94865. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.91924/4.95164. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.92272/4.96687. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.93129/4.94897. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.93504/4.94607. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.93192/4.97204. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.92304/4.96838. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.92365/4.97024. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.92324/4.96407. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.92788/4.94201. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.92399/4.95782. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.92478/4.96699. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.91899/4.95971. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.92201/4.96057. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.91897/4.96026. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.92922/4.95156. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.05707352953442433\n",
      "Epoch 0, Loss(train/val) 4.82099/4.77192. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.74774/4.75307. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.74444/4.75347. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.74520/4.75826. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.74555/4.76083. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.74479/4.76179. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.74704/4.76576. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.74287/4.76761. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.74194/4.76708. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.74090/4.76583. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.73262/4.77390. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.73819/4.77396. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.73377/4.78137. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.73396/4.78970. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.73369/4.78036. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.73192/4.78188. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.73056/4.78757. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.74169/4.75060. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.73688/4.76518. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.73258/4.78509. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.73132/4.78781. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.72951/4.79707. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.73060/4.80186. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.72664/4.81244. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.73331/4.79401. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.72793/4.80556. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.73445/4.78995. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.72553/4.79886. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.72808/4.79945. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.72937/4.80572. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.72539/4.81264. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.72602/4.82298. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.72746/4.81452. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.72241/4.82635. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.72675/4.81195. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.71990/4.85771. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.74026/4.75244. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.74128/4.75724. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.73345/4.76446. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.73412/4.76671. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.73509/4.77374. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.73234/4.77811. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.73294/4.76267. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.72813/4.78116. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.72657/4.79817. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.72410/4.80831. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.72422/4.79486. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.72550/4.79795. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.72653/4.79392. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.72086/4.81154. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.72029/4.80863. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.72307/4.81319. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.72655/4.79256. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.72375/4.80661. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.71949/4.81485. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.72494/4.79543. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.71911/4.80252. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.71832/4.79692. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.71443/4.83288. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.72010/4.78523. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.71621/4.80038. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.71476/4.79555. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.71628/4.79354. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.71923/4.79841. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.71303/4.80030. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.71785/4.79818. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.71423/4.78667. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.71156/4.81756. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.70991/4.80184. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.71405/4.79108. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.71140/4.79445. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.71165/4.79584. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.70808/4.79140. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.70490/4.80025. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.70887/4.80045. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.71378/4.80664. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.70681/4.79881. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.70911/4.79160. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.70679/4.78498. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.70531/4.81768. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.70935/4.76540. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.70555/4.77373. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.71162/4.78892. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.70172/4.79069. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.70251/4.78033. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.70076/4.80003. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.70265/4.77105. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.70703/4.78894. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.70283/4.77801. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.70968/4.78534. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.70507/4.77648. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.70256/4.79078. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.70446/4.75147. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.70701/4.76237. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.70086/4.78378. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.70560/4.76563. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.70264/4.75893. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.69830/4.74415. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.70356/4.77203. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.69895/4.77286. Took 0.08 sec\n",
      "ACC: 0.359375, MCC: -0.27286371951943134\n",
      "Epoch 0, Loss(train/val) 5.04395/4.98468. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.00737/4.97834. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.00498/4.98142. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.00252/4.98154. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.00096/4.98056. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.00063/4.97917. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.99931/4.98041. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.00045/4.99195. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.99602/4.99660. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.99488/4.99728. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.99739/4.99544. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.99420/4.99444. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.99473/4.99343. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.99618/4.99961. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.99356/5.00198. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.99195/5.00095. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.99484/4.99947. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.99190/5.00313. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.99234/5.00175. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.99077/5.00163. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.99129/5.00062. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.99217/5.00412. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.98936/5.00508. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.99074/5.00343. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.98947/5.00476. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.99045/5.00740. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.98888/5.00653. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.99077/5.00413. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.98953/5.00770. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.98691/5.01077. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.98534/5.00132. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.99053/5.00382. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.99007/5.00114. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.98491/5.00535. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.98332/5.01004. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.98388/5.00338. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.98246/5.00662. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.97908/5.01132. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.98549/5.01353. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.98529/5.00235. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.98107/5.02176. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.98029/5.02226. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.98150/5.01033. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.98235/5.01296. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.98021/5.01398. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.98292/5.01180. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.98249/5.02569. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.97614/5.03275. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.98609/5.00565. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.98279/5.03235. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.98094/5.02876. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.97817/5.02708. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.98065/5.02673. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.97360/5.03460. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.97843/5.02014. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.97958/5.01972. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.97838/5.02513. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.97752/5.02626. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.97667/5.02992. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.97348/5.04018. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.97671/5.03284. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.97631/5.02619. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.97748/5.02161. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.97225/5.03868. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.97434/5.02650. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.97136/5.03895. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.97590/5.03908. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.97442/5.03084. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.97043/5.03938. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.97014/5.04120. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.98348/5.01990. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.97089/5.03174. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.97216/5.03123. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.97031/5.02257. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.96968/5.02901. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.97415/5.01810. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.97119/5.02167. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.97344/5.02625. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.97896/5.01829. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.96784/5.02561. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.97284/5.03975. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.96561/5.03579. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.96680/5.03561. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.96933/5.02911. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.97018/5.03461. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.96743/5.04526. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.96574/5.03604. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.96462/5.03463. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.96964/5.04518. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.95921/5.04383. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.97061/5.02207. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.96770/5.02355. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.96401/5.04301. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.96495/5.03062. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.97483/5.03048. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.96140/5.04203. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.96908/5.02315. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.96371/5.04319. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.96063/5.07295. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.96130/5.03540. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.07192118226600985\n",
      "Epoch 0, Loss(train/val) 5.08156/5.05941. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 5.04291/5.05516. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.04098/5.06148. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 5.03907/5.07533. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 5.04238/5.06923. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 5.03679/5.05384. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 5.03491/5.04594. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.03200/5.04791. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.02819/5.04978. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 5.02715/5.05338. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 5.02554/5.05782. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.02920/5.05551. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.02541/5.05521. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 5.02492/5.05622. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.02578/5.06284. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 5.02308/5.05828. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 5.02121/5.05350. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 5.02157/5.05816. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.02096/5.05592. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 5.01868/5.05653. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 5.02099/5.05515. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 5.02156/5.04830. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 5.01619/5.05985. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 5.01868/5.05729. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 5.01944/5.06283. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.01868/5.05492. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 5.01665/5.06090. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 5.01414/5.06646. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 5.01516/5.06889. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.01113/5.06325. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 5.01380/5.06469. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 5.01514/5.05938. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 5.01643/5.06951. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 5.01509/5.06769. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.01371/5.06997. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.01300/5.06956. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.01196/5.06838. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.00895/5.06630. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.01041/5.06933. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.01005/5.09065. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.01700/5.06228. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.01267/5.08048. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 5.01174/5.06895. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.01088/5.06202. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.00864/5.08628. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.00655/5.07671. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.01387/5.07212. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.00910/5.07257. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.00301/5.08876. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.00771/5.09003. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.00825/5.08837. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.00764/5.08917. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.00783/5.07834. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.00981/5.08876. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.00716/5.09363. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.00692/5.07843. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.01071/5.08471. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.00575/5.07992. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.00516/5.07493. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.00974/5.06995. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.00496/5.09059. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.00654/5.08044. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.00749/5.07680. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.01657/5.06488. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.01679/5.08171. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.00909/5.08309. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 5.01130/5.09125. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.00891/5.08697. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 5.01128/5.07774. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.00677/5.09762. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.00438/5.08556. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.00220/5.09339. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.00628/5.09430. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.00032/5.08659. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.01265/5.08246. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.00608/5.07101. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.00323/5.09192. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 5.00627/5.11359. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 4.99873/5.10318. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.99714/5.12744. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 5.00843/5.09995. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.99996/5.08387. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.00877/5.09150. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.00002/5.08634. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.00625/5.10106. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.00284/5.09809. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.99832/5.10241. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.99737/5.08419. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.99512/5.10824. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.00017/5.10633. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.99803/5.09653. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.99997/5.10480. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.99691/5.12606. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.99873/5.11040. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.99324/5.10697. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.99388/5.10794. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.99556/5.09781. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.99265/5.13230. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.99394/5.10881. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.99069/5.11466. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.03779786830012622\n",
      "Epoch 0, Loss(train/val) 4.93192/4.90081. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.86611/4.88063. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 4.86374/4.87599. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.86387/4.87693. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.86494/4.87484. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.86627/4.87655. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.86191/4.88226. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.86156/4.87893. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.86206/4.87645. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.86047/4.87559. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.85871/4.87424. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.86113/4.87199. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85694/4.87077. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.85760/4.87684. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.85509/4.86976. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.85809/4.86591. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.85773/4.86992. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.85615/4.87346. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.85680/4.86855. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85233/4.86841. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.85124/4.86537. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.86228/4.86636. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.85293/4.86934. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.85344/4.86584. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.85066/4.87305. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.85337/4.86962. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.85161/4.87733. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.85368/4.87009. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.84645/4.87658. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84559/4.87047. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.84777/4.86849. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.84994/4.88145. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84800/4.88460. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.84820/4.87294. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.84795/4.88990. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.84442/4.87748. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84046/4.89291. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.84112/4.88793. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84678/4.88772. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84389/4.88076. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84218/4.88520. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.83939/4.88753. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.84427/4.87328. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.84466/4.87398. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.84213/4.90672. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.84386/4.88550. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.84032/4.87121. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.84119/4.89094. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84259/4.87254. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.83769/4.88861. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.84119/4.87782. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.84029/4.87760. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.84076/4.88283. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.83733/4.89538. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.83764/4.87853. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.83472/4.92450. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.84103/4.88709. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.83527/4.89950. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.84086/4.88402. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83613/4.89877. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.83273/4.90832. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83483/4.89717. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.83551/4.88641. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.83596/4.91519. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.83768/4.88997. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.83867/4.90038. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.83387/4.88059. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.83419/4.89544. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.83383/4.93073. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83360/4.88671. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.83536/4.89936. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.82604/4.90648. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.82706/4.89088. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.83087/4.92785. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83736/4.90260. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83484/4.90316. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.82495/4.94629. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.83029/4.89409. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.82660/4.91003. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.82465/4.94841. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83487/4.87829. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.82656/4.93503. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.83051/4.91283. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.82759/4.89242. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.82440/4.91811. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.82811/4.90053. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.82631/4.90879. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.82654/4.93732. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.81969/4.88846. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.82581/4.90168. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.83212/4.93430. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.83207/4.91440. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.82606/4.90581. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.82270/4.90310. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.81754/4.90228. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.82279/4.91110. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.82417/4.92387. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.82410/4.91377. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.82472/4.88893. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.82205/4.89854. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.011958266722236254\n",
      "Epoch 0, Loss(train/val) 4.91286/4.87892. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.86222/4.88181. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.85614/4.87823. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.85366/4.87491. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.85339/4.87705. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.84838/4.87920. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.85266/4.88328. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.85062/4.88047. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.85168/4.88258. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.85319/4.88528. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.84964/4.89183. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.85001/4.88369. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.84571/4.89735. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.84831/4.88407. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.84829/4.88583. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.84792/4.89294. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.85087/4.89229. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.84600/4.89225. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.84206/4.90825. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.84462/4.89816. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.85214/4.89847. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.84638/4.90049. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.84977/4.90698. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.84327/4.89999. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.84769/4.90544. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.84650/4.89506. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.84559/4.91509. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.84273/4.90458. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.84522/4.89756. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84343/4.90602. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.84085/4.90094. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.84642/4.88999. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.85333/4.86454. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.84847/4.87845. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.84708/4.89111. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84175/4.89640. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84282/4.91650. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.83897/4.90125. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.83691/4.91799. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.83900/4.90517. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84319/4.90697. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.84276/4.90611. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84078/4.90268. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.83957/4.89068. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.83725/4.91529. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.84065/4.90296. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.83569/4.90951. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.83285/4.91321. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.83672/4.91219. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.84115/4.91820. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.83948/4.91345. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.83211/4.91856. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.83126/4.93354. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.83159/4.91561. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.83755/4.91713. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.83800/4.90807. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.83273/4.92263. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.83849/4.92342. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.83218/4.91708. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83732/4.92691. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.83228/4.91199. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83475/4.91764. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.83120/4.93138. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.83294/4.92880. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.83326/4.92184. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.82827/4.93059. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.83098/4.91732. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.83116/4.92326. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.82804/4.93267. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.82956/4.91480. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.82911/4.93690. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.83259/4.92409. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.82834/4.93690. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.82662/4.92761. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.82737/4.94094. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83020/4.91259. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.83050/4.90858. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.82817/4.93407. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.83529/4.92888. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.82886/4.94209. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.82636/4.94391. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.82713/4.92359. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.82585/4.93103. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.82609/4.93590. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.82780/4.94016. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83172/4.92184. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83234/4.91365. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.82686/4.91098. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.82520/4.90743. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.82528/4.92468. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.82657/4.93384. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.82632/4.92543. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.82291/4.93246. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83248/4.91755. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.82549/4.92787. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.82515/4.92682. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.83591/4.91524. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.83166/4.93460. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83845/4.89926. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.83591/4.91740. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.12267022872162016\n",
      "Epoch 0, Loss(train/val) 5.07454/5.04067. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.00260/5.00664. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.99397/5.00193. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.99228/5.00261. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.99189/5.00615. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.99078/5.00683. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.99118/5.00753. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.99044/5.01241. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.98922/5.01210. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.98450/5.01057. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.98799/5.01478. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.99008/5.01667. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.98461/5.01275. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.98614/5.00987. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.98178/5.02340. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.98478/5.03515. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.98704/5.02393. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.98590/5.02231. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.97884/5.04076. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.97749/5.03927. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.98285/5.00623. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.98047/5.01754. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.97389/5.04872. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.98089/5.03926. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.97647/5.03945. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.98148/5.02575. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.97322/5.06176. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.97779/5.04282. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.97343/5.06092. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.97505/5.04444. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.97379/5.05376. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.97396/5.05096. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.96963/5.07724. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.97186/5.06535. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.96869/5.07794. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.97090/5.06395. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.96943/5.05917. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.97042/5.06480. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.96805/5.06950. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.96653/5.07716. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.96540/5.06828. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.97009/5.06136. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.96604/5.07232. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.96750/5.07332. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.97110/5.06396. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.97040/5.06587. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.96930/5.05198. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.97090/5.05951. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.96512/5.08952. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.97280/5.05309. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.96348/5.08795. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.97137/5.05720. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.97213/5.07170. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.96723/5.06793. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.97182/5.07258. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.96243/5.08941. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.96613/5.08409. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.95901/5.08344. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.96913/5.05843. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.96780/5.07119. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.96465/5.07785. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.96713/5.08482. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.96296/5.07522. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.96108/5.05470. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.96230/5.07735. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.96335/5.07067. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.96458/5.06967. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.95593/5.10702. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.96317/5.07503. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.96104/5.07833. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.95797/5.08389. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.95742/5.09304. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.95567/5.08899. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.95748/5.09303. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.96018/5.08168. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.95877/5.08694. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.95919/5.09055. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.96295/5.07692. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.95556/5.08304. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.95661/5.07921. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.95498/5.05952. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.95827/5.04540. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.95829/5.06556. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.96896/5.06346. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.95302/5.07364. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.96206/5.06444. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.96066/5.08110. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.95644/5.05687. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.95331/5.07065. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.95635/5.05322. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.95824/5.06547. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.95692/5.06882. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.95279/5.10007. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.95011/5.10246. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.96274/5.07571. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.95696/5.09223. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.95396/5.11594. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.95152/5.07912. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.95020/5.10064. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.96168/5.08801. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.06262242910851495\n",
      "Epoch 0, Loss(train/val) 4.91931/4.86970. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.86537/4.86038. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.86497/4.85900. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.86008/4.85875. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.86503/4.85849. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.86045/4.85988. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.86157/4.86056. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.85778/4.86308. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.85691/4.86290. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.85672/4.86729. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.86065/4.86409. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.85993/4.86656. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85678/4.86784. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.85874/4.86840. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.85689/4.86844. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.85573/4.87301. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.85489/4.87076. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.86070/4.85642. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.85698/4.86221. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85731/4.86589. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.85624/4.86965. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.85356/4.87228. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.85492/4.87647. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.84915/4.88431. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.85426/4.88099. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.85032/4.88577. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.85076/4.89434. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.84772/4.88952. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.84813/4.89336. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84854/4.89401. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.85216/4.88635. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.84664/4.89965. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84920/4.89573. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.84722/4.90046. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.84857/4.89613. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84487/4.89538. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84174/4.90983. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84374/4.90543. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84778/4.87533. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84586/4.89882. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84420/4.89679. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.83473/4.92082. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.83931/4.91691. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.84086/4.92119. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.83817/4.92553. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.84265/4.91410. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.83702/4.91921. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.83477/4.92885. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84056/4.90846. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.83720/4.92217. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.83915/4.93174. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.83739/4.94190. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.83306/4.93056. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.83654/4.94533. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.83473/4.93971. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.82581/4.94659. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.82933/4.94342. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.83514/4.93357. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.83277/4.92490. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83220/4.95016. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.83585/4.94616. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83300/4.91978. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.83474/4.92822. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.83260/4.94401. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.82349/4.96374. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.83057/4.97181. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.83368/4.95041. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.82849/4.96250. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.83037/4.94921. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83224/4.94399. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.83054/4.92403. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.82676/4.94067. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.82564/4.97714. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.82893/4.95646. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.82837/4.96736. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.81979/4.97597. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.82852/4.93917. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.84576/4.92197. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.84308/4.93877. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.83743/4.97826. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.85838/4.89249. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.84947/4.89344. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.84809/4.90844. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.84748/4.90740. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.84697/4.90444. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.84754/4.90135. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.84414/4.90515. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.84107/4.91561. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83666/4.93702. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.83716/4.94378. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.83328/4.94492. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.83768/4.97990. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.84278/4.92616. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.84204/4.95087. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.84035/4.94620. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.83948/4.93357. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.83744/4.93372. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.83999/4.96502. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83980/4.94480. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.83807/4.93491. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.0020131159011798102\n",
      "Epoch 0, Loss(train/val) 4.93549/4.86609. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.83295/4.83464. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.83229/4.82983. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.83451/4.82966. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.83623/4.82977. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.83836/4.82907. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.83743/4.83022. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.83656/4.82847. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.83390/4.82917. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.83404/4.83084. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.83418/4.83344. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.83409/4.83050. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.83279/4.82921. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.83444/4.82526. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.83201/4.82781. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.82980/4.82782. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.83003/4.82565. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.82906/4.82732. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.82802/4.82901. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.82860/4.82585. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.83378/4.83023. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.83409/4.82980. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.83193/4.83119. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.82997/4.82815. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.82993/4.82826. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.82871/4.82855. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.82544/4.82421. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82645/4.82227. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.82675/4.82189. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.82610/4.82039. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.82485/4.82073. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.82278/4.82307. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.82467/4.83142. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.82279/4.81824. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.82226/4.81971. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.82261/4.81096. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.82552/4.81304. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.82319/4.81204. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.81750/4.80967. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.82314/4.81551. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.81674/4.81476. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.81522/4.81274. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.81673/4.82110. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.81817/4.82536. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.81616/4.82081. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.81835/4.83420. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.81603/4.82159. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.81399/4.81804. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.81783/4.81948. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.81633/4.82838. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.81318/4.83281. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.81370/4.83229. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.81313/4.82573. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.81212/4.82336. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.81091/4.83146. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.80911/4.82468. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.81759/4.82713. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.81224/4.83053. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.80899/4.83074. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.80652/4.82945. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.80576/4.82770. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.80956/4.82750. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.80862/4.83836. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.80541/4.83377. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.80482/4.83009. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.80005/4.82646. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.80575/4.83390. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.80372/4.83692. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.80280/4.85339. Took 0.12 sec\n",
      "Epoch 69, Loss(train/val) 4.79801/4.83129. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.80444/4.83616. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.80151/4.84934. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.80604/4.84543. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.80408/4.84395. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.80701/4.84008. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.80085/4.84552. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.79819/4.85066. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.80110/4.84889. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.80583/4.85243. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.81994/4.85276. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.82014/4.84196. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.81856/4.84187. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.81701/4.84504. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.81271/4.86088. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.81466/4.84959. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.80835/4.84969. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.80974/4.88248. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.81011/4.83528. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.80996/4.84424. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.81098/4.83911. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.80240/4.84801. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.81000/4.84848. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.81020/4.83431. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.80273/4.85220. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.80606/4.84781. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.79876/4.84837. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.80777/4.84473. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.80193/4.84265. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.80215/4.85566. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.79994/4.84955. Took 0.08 sec\n",
      "ACC: 0.59375, MCC: 0.20634920634920634\n",
      "Epoch 0, Loss(train/val) 4.96870/4.97146. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.95322/4.94634. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.94665/4.94706. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.94358/4.94581. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.94235/4.94813. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.94122/4.95322. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.94359/4.95288. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.94081/4.95734. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.94201/4.95551. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.94235/4.95827. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.94478/4.95517. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.94290/4.95791. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.94200/4.95158. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.94031/4.95002. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.94006/4.95396. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.93656/4.95487. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.94004/4.94975. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.93740/4.95704. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.93437/4.95289. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.93777/4.95005. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.93614/4.95232. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.93557/4.94612. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.93234/4.94116. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.93127/4.95103. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 4.93378/4.94626. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.92798/4.95094. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.93233/4.94470. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.92842/4.95091. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.93434/4.94190. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.93100/4.94747. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.92849/4.95003. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.92738/4.93735. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.92714/4.94685. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.92744/4.94059. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.92747/4.93518. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.92910/4.94033. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.92665/4.93333. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.92406/4.93937. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.92874/4.93433. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.92552/4.93566. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.91940/4.92842. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.92558/4.92637. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.92445/4.92621. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.92504/4.92556. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.92594/4.91984. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.92319/4.93182. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.91974/4.92960. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.92830/4.92441. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.92549/4.92219. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.91882/4.93119. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.92330/4.93308. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.92418/4.93051. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.91467/4.92137. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.91788/4.92218. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.91823/4.91063. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.91462/4.93668. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.92055/4.92651. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.92308/4.93180. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.92054/4.92584. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.92027/4.93508. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.91816/4.93208. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.91464/4.92709. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.91592/4.91881. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.91837/4.92069. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.92447/4.92076. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.92094/4.92695. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.91895/4.92733. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.91414/4.92478. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.91594/4.92737. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.91457/4.92012. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.90992/4.92618. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.91961/4.92272. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.90956/4.92482. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.90904/4.92152. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.91289/4.91776. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.91410/4.92737. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.90700/4.92710. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.90918/4.91997. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.91836/4.92307. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.91686/4.92718. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.92003/4.92735. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.91547/4.93086. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.91764/4.92017. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.90911/4.92926. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.90657/4.92565. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.90936/4.92088. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.90478/4.93640. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.91029/4.92187. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.91102/4.93435. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.90464/4.93710. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.91308/4.91576. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.90855/4.93060. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.91898/4.90562. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.90415/4.91805. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.90717/4.92252. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.91191/4.92456. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.91073/4.91804. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.90683/4.91308. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.90770/4.91426. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.90876/4.92816. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.0\n",
      "Epoch 0, Loss(train/val) 5.04590/4.99789. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.00683/4.99446. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.99690/4.99968. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.99869/5.00508. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.99698/5.00840. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.99402/5.01223. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.99548/5.01271. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.99336/5.01914. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.99202/5.01298. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.99214/5.01281. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.99262/5.02168. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.98822/5.02184. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.98556/5.02152. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.98578/5.01607. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.98643/5.01648. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.98569/5.00902. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.98192/5.02082. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.98592/5.01789. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.98524/5.00805. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.98385/5.02462. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.98591/5.01036. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.98285/5.01590. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.97827/5.02787. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.98322/5.01086. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.98596/5.01988. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.97996/5.01803. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.98350/5.02488. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.98036/5.02760. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.98199/5.01615. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.97647/5.01821. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.97876/5.02108. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.97939/5.01429. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.98156/5.00863. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.97235/5.01295. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.97719/5.01836. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.98107/5.01555. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.97497/5.02377. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.97674/5.01123. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.97596/5.01012. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.97443/5.01540. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.97727/5.01430. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.97400/5.01666. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.97325/5.03298. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.97430/5.01233. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 4.97634/5.01842. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.97263/5.01370. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.97544/5.01530. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.97210/5.03018. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.97386/5.01725. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.98029/5.01413. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.97054/5.01955. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.96680/5.02623. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.97342/5.02353. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 4.97119/5.02471. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.96966/5.01818. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.97237/5.02897. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.97617/5.00975. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.97214/5.02691. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.97467/5.01057. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 4.96892/5.03433. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.97000/5.02651. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.96718/5.01464. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.97304/5.02327. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.96505/5.00380. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.97138/5.02337. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.97735/5.01593. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.97089/5.01542. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.96865/5.02103. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 4.97025/5.01458. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.96362/5.02416. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.96750/5.01054. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.96746/5.00528. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.96498/5.01883. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.97043/5.02088. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.96974/5.01545. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.96433/5.01817. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.96026/5.01987. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.97027/5.02023. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.96211/5.07004. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.99495/5.00463. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.99988/5.00033. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.98647/5.00347. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.98358/5.00818. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.97255/5.01243. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.97856/4.99517. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.98682/4.99279. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.97271/5.00490. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.97422/5.00560. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.97067/5.01071. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.96650/5.01218. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.97295/5.00690. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.96395/5.01236. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.97192/4.99728. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.96588/4.99331. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.96956/5.01028. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.96987/5.00147. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.96673/5.01069. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.96284/5.01298. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.97092/5.02243. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.97253/5.00527. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.14060420405767388\n",
      "Epoch 0, Loss(train/val) 5.07756/5.05106. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.04295/5.05383. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.04127/5.06427. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 5.04668/5.05727. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 5.04773/5.05310. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.04628/5.06968. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 5.04293/5.07368. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.03973/5.06801. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.03569/5.06465. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.03515/5.06774. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 5.03720/5.06642. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.03681/5.06587. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.03824/5.06624. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.03577/5.06512. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.03349/5.06439. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.03468/5.06693. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.03488/5.07325. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.03513/5.06969. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.03729/5.06317. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 5.03441/5.06814. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.03330/5.06605. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 5.03340/5.06862. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 5.03227/5.06405. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.03207/5.06831. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.03282/5.07100. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.03251/5.07032. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.03017/5.07477. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.03254/5.07241. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.03186/5.07446. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.03020/5.07472. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.03400/5.06948. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.03067/5.07285. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.03031/5.08148. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.02786/5.08059. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 5.02520/5.08401. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 5.03078/5.07252. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.02867/5.07314. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.02655/5.08231. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.02623/5.08283. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 5.02391/5.07856. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.02394/5.09219. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.02987/5.07395. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.01923/5.09633. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.02485/5.08510. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.02075/5.08732. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.02624/5.08619. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.02269/5.07205. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.02422/5.07930. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.02359/5.07861. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.01985/5.07053. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.02331/5.07287. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.02311/5.05525. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.02438/5.06791. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.02879/5.05880. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.01955/5.06233. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.01981/5.05288. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.02287/5.06411. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.01778/5.06553. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.02379/5.04336. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.02830/5.04264. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.01955/5.03978. Took 0.07 sec\n",
      "Epoch 61, Loss(train/val) 5.01674/5.05726. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.01988/5.06322. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.01750/5.04800. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.01550/5.04723. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.01118/5.05054. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.01980/5.05162. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 5.01915/5.05542. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.01612/5.06587. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 5.01563/5.05732. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.01603/5.07742. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.02803/5.06386. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 5.02384/5.06497. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.01874/5.07142. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.01424/5.05652. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.01803/5.06532. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.01423/5.06566. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.01332/5.06080. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.01359/5.06777. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.01438/5.06131. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 5.01589/5.06511. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 5.00953/5.06855. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 5.01254/5.07255. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 5.01236/5.07038. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.01086/5.06706. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 5.01324/5.07022. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 5.01621/5.08907. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.01546/5.08709. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.01622/5.08216. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 5.01059/5.08555. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 5.00866/5.08809. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.01024/5.08603. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.01189/5.08915. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.00608/5.07925. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.01044/5.07012. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.01165/5.07865. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.01189/5.08191. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 5.01084/5.08110. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.00920/5.08700. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.00596/5.07724. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.04350001325672093\n",
      "Epoch 0, Loss(train/val) 4.80581/4.78598. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.77707/4.78877. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.77616/4.78273. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.77523/4.79119. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.77363/4.78265. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.77219/4.78251. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.77428/4.79125. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.76985/4.78962. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.77377/4.79224. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.77180/4.79155. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.77161/4.79246. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.77138/4.79238. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.77185/4.79490. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.77136/4.80049. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.76882/4.80591. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.76943/4.80483. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.77094/4.81254. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.76903/4.81720. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.76953/4.80658. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.76569/4.80165. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.76666/4.81020. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.76577/4.80761. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.76237/4.82035. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.76256/4.82065. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.76160/4.81873. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.75903/4.80301. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.75792/4.81033. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.75716/4.81619. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.75560/4.80963. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.76224/4.79902. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.76383/4.78259. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.75801/4.80166. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.75595/4.80681. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.75506/4.79954. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.75459/4.81536. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.75226/4.82078. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.75308/4.83231. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.75046/4.84209. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.75484/4.82334. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.74888/4.83814. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.75251/4.81244. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.75106/4.84291. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.74843/4.81686. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.74474/4.81922. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.74491/4.82985. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.74062/4.86131. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.74344/4.82909. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.74393/4.82859. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.74807/4.83177. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.74733/4.81553. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.74276/4.86216. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.74472/4.82936. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.74613/4.83494. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.74323/4.83068. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.74420/4.81994. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 4.74028/4.83957. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.74481/4.86563. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.74003/4.83369. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.73707/4.84777. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.74342/4.83406. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.73740/4.82048. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.74352/4.84278. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.74005/4.82132. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.73307/4.83105. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.74188/4.85524. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.73716/4.80875. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.73754/4.81927. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.73753/4.83113. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.74043/4.84280. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.73790/4.82218. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.74009/4.83936. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.73203/4.84102. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.73750/4.85488. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.73045/4.84227. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.73190/4.83752. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.73647/4.84580. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.73581/4.80627. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.73547/4.83885. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.73654/4.87451. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.72437/4.84457. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.73612/4.85839. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 4.72772/4.83785. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.73540/4.83074. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.72752/4.82281. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.72783/4.82728. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.73341/4.81025. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.72767/4.83817. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.72729/4.83587. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.72738/4.85023. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.72825/4.85761. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.73066/4.83916. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.73166/4.81492. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.71992/4.83483. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.72371/4.87246. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.78087/4.80230. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.76614/4.81251. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.76591/4.81514. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.75792/4.82388. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.75961/4.82051. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.75757/4.81992. Took 0.09 sec\n",
      "ACC: 0.34375, MCC: -0.3028893994409716\n",
      "Epoch 0, Loss(train/val) 4.73061/4.75077. Took 0.12 sec\n",
      "Epoch 1, Loss(train/val) 4.71770/4.74582. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.72538/4.71950. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.73291/4.71084. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.72387/4.70480. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.71307/4.70576. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.71501/4.70518. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.71364/4.70611. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.71290/4.70830. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.71461/4.70856. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.71490/4.70773. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.71329/4.70835. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.71317/4.70928. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 4.71387/4.71059. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.71095/4.71156. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.71035/4.71892. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.71178/4.71596. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.71012/4.72099. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.70965/4.71753. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.71036/4.72098. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.70810/4.73231. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.70642/4.73263. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.70646/4.74359. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.70247/4.74608. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.70222/4.77517. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.70053/4.74715. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.69901/4.76169. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.69965/4.76469. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.69891/4.77039. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.69633/4.76027. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.69906/4.77374. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.69678/4.78020. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.69477/4.78896. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.68886/4.80561. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.69713/4.79238. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.69582/4.79657. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.69681/4.77876. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.69477/4.78191. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.69084/4.79992. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.69455/4.78374. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.69400/4.79254. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.69256/4.81287. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.68682/4.80821. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.68722/4.81095. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.68972/4.82115. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.69192/4.79427. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 4.69703/4.75763. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.68661/4.82104. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.69064/4.82995. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.68963/4.80544. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.69474/4.78692. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.68627/4.82119. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.68888/4.82385. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.68196/4.82577. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.68598/4.82750. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.68263/4.83292. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.69016/4.81969. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.67899/4.83601. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.68361/4.85098. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.67767/4.85929. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.68426/4.82828. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.69028/4.81760. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.68518/4.83413. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.68417/4.84887. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.68472/4.83811. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.67939/4.87723. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.67765/4.86712. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.68148/4.84135. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.68411/4.84607. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.68057/4.87388. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.67820/4.83189. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.68689/4.84375. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.68186/4.85258. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.68772/4.82587. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.69053/4.80015. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.69106/4.79497. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.69079/4.75825. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.69224/4.78626. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.68725/4.77775. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.69124/4.80274. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.68145/4.81862. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.68256/4.82250. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.67732/4.84476. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.67543/4.86235. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.68486/4.86512. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.67991/4.85593. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.67426/4.85571. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.67848/4.86549. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.67138/4.86208. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.67982/4.87394. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.68250/4.83812. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.67048/4.88190. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.67394/4.86749. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.67050/4.87728. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.67738/4.84662. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.67799/4.86062. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.67485/4.86742. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.67009/4.88667. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.67176/4.84459. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.67881/4.84251. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.1014574359634967\n",
      "Epoch 0, Loss(train/val) 4.94573/4.81978. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.82682/4.82608. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 4.81824/4.83068. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.82039/4.83222. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.81832/4.82771. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.82040/4.82839. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.81574/4.83346. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.81650/4.84035. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.81502/4.84351. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.81617/4.83570. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.81655/4.84142. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.81298/4.84169. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.81379/4.85140. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.81300/4.84441. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.81356/4.84529. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.81330/4.83934. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.81398/4.84694. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.81439/4.84086. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.81044/4.84445. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.81248/4.85507. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.81010/4.84324. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.81403/4.83905. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.81088/4.85183. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.80822/4.83742. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.80934/4.84306. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.80632/4.84751. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.80797/4.84713. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.80860/4.84588. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.80776/4.84213. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.80871/4.84196. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.80404/4.85443. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.80663/4.84671. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.80708/4.85458. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.80637/4.86615. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.80106/4.85035. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.80331/4.84632. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.80074/4.86014. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.79913/4.84484. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.80891/4.87264. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.81084/4.84171. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.80744/4.83647. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.80649/4.84677. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.80750/4.86464. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.80679/4.84323. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.80389/4.83825. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.79740/4.85949. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.80009/4.85515. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.79823/4.85269. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.79947/4.85601. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.80250/4.84271. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.79916/4.84528. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.79665/4.85234. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.80427/4.84575. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.79264/4.85780. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.80545/4.83387. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.81273/4.82482. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.81157/4.82431. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.80770/4.81483. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.80680/4.82584. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.80277/4.82998. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.80251/4.83594. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.80198/4.83094. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 4.80225/4.85060. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.80144/4.84141. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.80198/4.84897. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.80259/4.85135. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.80318/4.84665. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.80168/4.85035. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.80289/4.86081. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.80128/4.85220. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.79365/4.86880. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.79925/4.86717. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.79718/4.85225. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.79678/4.87563. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.79214/4.87189. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.79133/4.87056. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.79751/4.86410. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.79700/4.87776. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.79604/4.87976. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.80000/4.85409. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.79247/4.88504. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.79230/4.88002. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.79754/4.88059. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.79169/4.87957. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.78806/4.88726. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.79625/4.87191. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.79146/4.89560. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.78726/4.89608. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.79640/4.88585. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.79199/4.88990. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.79155/4.88253. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.79188/4.88392. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.78642/4.90171. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.79052/4.88383. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.79066/4.89464. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.79070/4.90810. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.79136/4.89450. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.78930/4.86576. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.78810/4.89216. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.79085/4.88779. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.0629940788348712\n",
      "Epoch 0, Loss(train/val) 4.82781/4.81315. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.78288/4.80751. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.78614/4.81421. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.78333/4.81331. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.77914/4.81171. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.78153/4.81478. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.78355/4.81431. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.78232/4.81247. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.78225/4.81618. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.78428/4.82065. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.78635/4.82034. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.78610/4.81208. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.78675/4.80988. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.78864/4.81086. Took 0.07 sec\n",
      "Epoch 14, Loss(train/val) 4.78116/4.81310. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.77755/4.82187. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.77939/4.81942. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.77798/4.81679. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.77986/4.81978. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.77673/4.82351. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.77975/4.81860. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.77658/4.82215. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.77853/4.81944. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.77817/4.82474. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.77910/4.81745. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.77789/4.82241. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.77719/4.81863. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.77594/4.82116. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77902/4.81820. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.77846/4.82500. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.77723/4.82462. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.77407/4.83028. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.77056/4.83687. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.77776/4.82313. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.77101/4.83225. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.77366/4.83149. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.77300/4.82882. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.77191/4.83930. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.76882/4.83567. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.77054/4.83134. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.76740/4.83316. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.77551/4.82259. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.77129/4.83809. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.76643/4.83219. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.76805/4.83486. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.76628/4.83293. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.76923/4.84037. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77873/4.84447. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.76976/4.85359. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.79031/4.80329. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.78427/4.80486. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77827/4.81425. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.77678/4.82942. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.77545/4.84065. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.77558/4.84297. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.77330/4.84736. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.77535/4.84878. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.77387/4.85592. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.77203/4.85676. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 4.77075/4.85924. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.77287/4.86348. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.77312/4.85041. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76521/4.84851. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.76548/4.86026. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.76706/4.84099. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.76397/4.85580. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.76542/4.86074. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.76652/4.84639. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.76300/4.84990. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76378/4.85993. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.76197/4.85776. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.76239/4.86300. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76291/4.85227. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.76707/4.83998. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.76348/4.84622. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.76380/4.85670. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.75922/4.86488. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.76037/4.86289. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.76298/4.85033. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.76267/4.86233. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.76321/4.84482. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.76790/4.85110. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.76013/4.85925. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.75725/4.86298. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75752/4.85805. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.75789/4.86092. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.76065/4.86417. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.76000/4.87274. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.75755/4.87724. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.76156/4.85588. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.76228/4.87224. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.75854/4.86662. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.76060/4.86220. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.75681/4.86366. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.75455/4.87075. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.75792/4.86905. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.75947/4.85521. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.75723/4.86331. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.75656/4.85308. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.75726/4.86537. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.002944085489318884\n",
      "Epoch 0, Loss(train/val) 4.93020/4.90913. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.87615/4.88742. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.87274/4.89982. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.87076/4.90292. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.86842/4.91043. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.86852/4.90299. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.86775/4.90166. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.86874/4.90035. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.86538/4.90610. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.86229/4.90495. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.86104/4.91063. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.86425/4.90758. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.86270/4.90871. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.86356/4.91454. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.86096/4.92040. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.86324/4.92176. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.86080/4.93136. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.85793/4.93933. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.85827/4.93556. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85582/4.93491. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.85492/4.94194. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.85512/4.94376. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.85335/4.95454. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.85792/4.93572. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.85510/4.93914. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.85502/4.94438. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.85504/4.93783. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.85257/4.95996. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.85883/4.93011. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.85148/4.96224. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.85426/4.94136. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.85475/4.93956. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 4.85231/4.94229. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.85304/4.94214. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.85303/4.95042. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.85276/4.95063. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.85044/4.95380. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84661/4.95947. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.85090/4.94057. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84993/4.95392. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84837/4.94689. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.84900/4.95888. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84791/4.94825. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.84826/4.95571. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.84963/4.94596. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.84759/4.94707. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.84908/4.94480. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.84478/4.95990. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84754/4.93890. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.84877/4.93859. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.84616/4.95575. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.84798/4.94884. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.84607/4.93653. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.84654/4.95414. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.84289/4.95472. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.84876/4.93671. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.84629/4.94854. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.84582/4.94063. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.84469/4.94619. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.84482/4.94331. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.84601/4.93510. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.84233/4.95602. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.84441/4.94689. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.84382/4.93617. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.83919/4.94063. Took 0.07 sec\n",
      "Epoch 65, Loss(train/val) 4.84186/4.93655. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.84148/4.93717. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.84115/4.93750. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.84521/4.92634. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.84214/4.93311. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.83620/4.92708. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.84184/4.92260. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.84388/4.93205. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.83555/4.93737. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.83603/4.91807. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83780/4.91871. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.84394/4.93161. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.84200/4.94304. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.83885/4.92006. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.83697/4.92012. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.84096/4.92026. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.83437/4.93191. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.83646/4.92741. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.83327/4.91427. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83121/4.92806. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83172/4.92296. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83578/4.93628. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.83732/4.92191. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83268/4.92238. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.83442/4.91266. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.84711/4.91468. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.84030/4.93208. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83654/4.93166. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83366/4.92209. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.83093/4.91827. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.83298/4.91370. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.83168/4.91404. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.82679/4.90240. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83397/4.92096. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.83023/4.92670. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.0629940788348712\n",
      "Epoch 0, Loss(train/val) 4.94643/5.00708. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.92755/4.92062. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.92670/4.91732. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.93000/4.91354. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.93285/4.91868. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.92945/4.91403. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.92119/4.91576. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.92001/4.91486. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.92086/4.91558. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.92109/4.91619. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.91982/4.91088. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.92095/4.91134. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.92197/4.91151. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.92313/4.91232. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.92014/4.91308. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.91949/4.91300. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.91635/4.91393. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.91843/4.91625. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.91757/4.91313. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.91653/4.91283. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.91762/4.91576. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.91537/4.92011. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.91481/4.92527. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.91784/4.93016. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.91472/4.92388. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.91282/4.92911. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.91154/4.93208. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.91100/4.93850. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.91154/4.94437. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.90895/4.94649. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.90708/4.94872. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.90567/4.94703. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.90730/4.94360. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.90798/4.94376. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.90641/4.94651. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.90479/4.94758. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.90404/4.94352. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.90049/4.96343. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.90011/4.96191. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.90406/4.96484. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.89951/4.97663. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.89878/4.96338. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.90148/4.98332. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.90069/4.96728. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.89839/4.97004. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.90298/4.96789. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.89764/4.97941. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.90191/4.97480. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.90872/4.96932. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.90427/4.96625. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.90111/4.98095. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.90176/4.96934. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.89819/4.97600. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.89623/4.98097. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.89567/4.98556. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.89919/4.97995. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.89578/4.98656. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.89667/4.99947. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.89289/4.99681. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.89306/4.99832. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.89286/5.01094. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89568/4.98570. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.88897/5.01368. Took 0.07 sec\n",
      "Epoch 63, Loss(train/val) 4.89404/5.01622. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.89568/5.01194. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.88900/5.02499. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.89271/5.01352. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.88972/5.02053. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.88912/5.01886. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.88554/5.02476. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.88791/5.02455. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.89240/5.01685. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88855/5.04159. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88336/5.02301. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.88800/5.01696. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.89370/5.00989. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88617/5.03317. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.89988/4.97988. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.88686/5.02098. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.89336/5.00801. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.88837/5.00511. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.88840/5.02206. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.88938/5.00989. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.88892/5.00165. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.88682/5.00156. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.88926/5.00521. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.88286/5.02752. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.89198/5.01889. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.88656/5.01815. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88863/5.00802. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.88008/5.02666. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.88802/5.00094. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.88464/5.02482. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.88616/5.02722. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.88457/5.01643. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.88811/5.01347. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.88591/5.00978. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.88459/5.01636. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.88517/5.02170. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.88203/5.03934. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.025861699363244256\n",
      "Epoch 0, Loss(train/val) 4.83041/4.78042. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.76172/4.75754. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.75691/4.75479. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.75394/4.75404. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.75258/4.75007. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.74990/4.74514. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.75116/4.74731. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.74914/4.74513. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.74795/4.74738. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.74836/4.74539. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.74793/4.74479. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.74539/4.74472. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.74726/4.74592. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.74523/4.74532. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.74406/4.74747. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.74346/4.74736. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.74085/4.74800. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.74251/4.74328. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.74154/4.74495. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.74128/4.74705. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.73822/4.74461. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.73941/4.74032. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.74023/4.74555. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.73704/4.74449. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.73822/4.72974. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.74350/4.73093. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.74071/4.73177. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.73773/4.73155. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.73346/4.74049. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.73672/4.74953. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.73889/4.75399. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.73957/4.75565. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.73713/4.74469. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.73548/4.74646. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.73935/4.74755. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.73952/4.74095. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.73379/4.74019. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.73141/4.75266. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.73202/4.74594. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.73453/4.74478. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.73162/4.74226. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.73377/4.73807. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.72831/4.74276. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.73136/4.74488. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.73265/4.74756. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.73177/4.74021. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.73279/4.73860. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.73092/4.75188. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.72848/4.75211. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.72446/4.75356. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.73034/4.75120. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.72644/4.74179. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.72846/4.75857. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.72590/4.74983. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.72762/4.75775. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.72320/4.75477. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.72627/4.75489. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.72765/4.73562. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.72511/4.74244. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.72565/4.76934. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.73328/4.75095. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.73437/4.75892. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.73044/4.76359. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.72835/4.77328. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.72471/4.77879. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.72776/4.74760. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.73039/4.74679. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.72610/4.74130. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.72106/4.74276. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.72322/4.74592. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.71937/4.74186. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.71856/4.77688. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.72243/4.78251. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.71906/4.77237. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.72226/4.75920. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.72278/4.76898. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.72072/4.76747. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.71840/4.77457. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.72074/4.77176. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.72513/4.72054. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.72615/4.73439. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.72207/4.76610. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.72439/4.76490. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.71982/4.79044. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.71830/4.76828. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.71808/4.76878. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.72046/4.77238. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.71569/4.76785. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.71554/4.78955. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.71386/4.77388. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.71877/4.78075. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.71236/4.78414. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.71169/4.77181. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.71497/4.76655. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.71423/4.78788. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.71634/4.77378. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.71155/4.76180. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.71429/4.76912. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.71618/4.76530. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.70974/4.78725. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.08304547985373996\n",
      "Epoch 0, Loss(train/val) 4.73924/4.71777. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.74314/4.70793. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.73006/4.70497. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.72584/4.70520. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.72429/4.70558. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.72374/4.70670. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.72581/4.71035. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.72706/4.71642. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.72835/4.73337. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.72716/4.74121. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.72588/4.74435. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.72292/4.73714. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.72074/4.73350. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.72382/4.73708. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.72083/4.74364. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.72129/4.74731. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.72108/4.74667. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.71895/4.75620. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.71813/4.75969. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.71617/4.75926. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.71775/4.75381. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.72299/4.74312. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.71540/4.76341. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.72362/4.75151. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.71784/4.76051. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.71731/4.75375. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.71806/4.75483. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.71863/4.75060. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.71752/4.75656. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.71577/4.76167. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.71360/4.75781. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.71665/4.75461. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.72033/4.75148. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.71424/4.74920. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.71773/4.74150. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.71566/4.73921. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.71676/4.75112. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.71868/4.75719. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.71477/4.76932. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.71466/4.76056. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.70855/4.76510. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.72507/4.73099. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.72023/4.74200. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.71481/4.75390. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.71582/4.75499. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.71637/4.73753. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.71510/4.74494. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.71633/4.75570. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.71239/4.76233. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.70897/4.76585. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.71172/4.76286. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.71076/4.76899. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.70590/4.77246. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.71401/4.75258. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.71014/4.75395. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.70526/4.76179. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.70885/4.75450. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.70953/4.75701. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.70605/4.75700. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.71043/4.75663. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.70492/4.75437. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.70594/4.75784. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.70608/4.75754. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.70614/4.76565. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.70545/4.75347. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.70380/4.76983. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.70593/4.76048. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.70392/4.77289. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.70585/4.74443. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.70671/4.75782. Took 0.07 sec\n",
      "Epoch 70, Loss(train/val) 4.70422/4.77022. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.70738/4.73723. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.70269/4.77299. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.70932/4.77256. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.70508/4.78032. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.70201/4.76857. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.70285/4.80531. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.73156/4.73343. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.71959/4.73280. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.71659/4.74523. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.70930/4.75522. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.72821/4.73277. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.71877/4.74819. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.71839/4.76097. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.71040/4.74646. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.71024/4.76172. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.70710/4.74882. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.69940/4.78489. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.71104/4.76526. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.70371/4.77628. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.70368/4.77027. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.70284/4.76582. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.70137/4.77060. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.70245/4.75960. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.70264/4.77326. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.69927/4.76601. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.69728/4.76135. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.69796/4.76308. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.69813/4.76505. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.69866/4.76295. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.0\n",
      "Epoch 0, Loss(train/val) 4.70730/4.69491. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.70896/4.69225. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.68946/4.69758. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.69501/4.69416. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.69369/4.69126. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.69136/4.69518. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.68945/4.69852. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.68601/4.70071. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.68359/4.70450. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.68331/4.71380. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.68243/4.71877. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.68219/4.71648. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.68398/4.71283. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.68193/4.71585. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.67995/4.71368. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.67874/4.71287. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.67846/4.72143. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.67742/4.72197. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.67568/4.71497. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.67158/4.72774. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.67340/4.71672. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.67426/4.71934. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.67243/4.71512. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.67212/4.71643. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.66939/4.71426. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.66973/4.72604. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.67141/4.70797. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.67170/4.71413. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.66745/4.71359. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.66321/4.71299. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.66683/4.71131. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.66562/4.71480. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.66620/4.71443. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.66911/4.70825. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.65996/4.72366. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.66815/4.71849. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.66148/4.71678. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.67180/4.69726. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.66830/4.70325. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.66498/4.70477. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.66007/4.70620. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.66389/4.70447. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.66639/4.70797. Took 0.12 sec\n",
      "Epoch 43, Loss(train/val) 4.66618/4.70851. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.66363/4.72777. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.67050/4.73389. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.67646/4.71597. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.67426/4.72079. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.67089/4.72131. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.67028/4.70733. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.67261/4.70052. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.66443/4.71204. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.66730/4.69556. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.67086/4.70818. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.67065/4.69716. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.66703/4.70065. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.67061/4.69520. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.66632/4.71433. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.65991/4.72478. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.66122/4.70122. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.66470/4.70311. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.66542/4.71241. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.66579/4.71418. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.66222/4.71454. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.66486/4.70165. Took 0.17 sec\n",
      "Epoch 65, Loss(train/val) 4.65786/4.70233. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.66058/4.71012. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 4.66406/4.72374. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.65733/4.71528. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.65636/4.71100. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.65603/4.71506. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.65147/4.70951. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.65791/4.71252. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.66125/4.71379. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.66429/4.70361. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.66511/4.71322. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.66185/4.69315. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.65776/4.69993. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.65411/4.71852. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.66326/4.68741. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.65617/4.70349. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.66216/4.69817. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.65894/4.70606. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.66204/4.68734. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.65912/4.69708. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.65818/4.70783. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.65416/4.70504. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.65263/4.68787. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.65583/4.69090. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.65260/4.70740. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.65267/4.71090. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.65212/4.71692. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.65420/4.72410. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.65185/4.69970. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.66541/4.67634. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.65529/4.71303. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.65309/4.70542. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.65654/4.70212. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.65099/4.70237. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.65499/4.70693. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.08222643447147887\n",
      "Epoch 0, Loss(train/val) 4.99857/4.97214. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.95905/4.95479. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.95489/4.95837. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.95510/4.95782. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.95530/4.95671. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.95217/4.95929. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.95201/4.95850. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.95207/4.96004. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.95184/4.96077. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.95354/4.96369. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.94889/4.96355. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.94788/4.96501. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.95025/4.96390. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.95110/4.96487. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.95560/4.96596. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.95223/4.96654. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.95276/4.96882. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.94798/4.97206. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.94819/4.97129. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.94701/4.97003. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.94780/4.96758. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.94745/4.96891. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.94809/4.97252. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.94473/4.97426. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.94621/4.97849. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.94749/4.97651. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.94309/4.97324. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.94507/4.97620. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.94492/4.97480. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.94325/4.97982. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.94151/4.97417. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.93862/4.98101. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.93871/4.98152. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.94051/4.98464. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.94322/4.97165. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.94053/4.97784. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.93974/4.99853. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.94406/4.99507. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.94369/4.98388. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.93943/4.99047. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.93857/4.98368. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.93743/4.98247. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.93716/4.98178. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.93357/4.98781. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.93673/4.98506. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.93846/5.00664. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.93731/4.97172. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.93354/5.00290. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.93499/4.98279. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.92721/4.98140. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.93732/4.98178. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.93889/4.99985. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.93064/4.99055. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.93342/4.98725. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.93001/4.99985. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.93326/4.97725. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.92058/5.01249. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.93126/4.98181. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.92865/4.97489. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.92364/5.00966. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.93015/4.99112. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.92195/4.99559. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.92473/4.99911. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.91972/4.99089. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.92539/4.98409. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.92378/5.00284. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.92017/4.98554. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.93146/4.99994. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.93125/4.98577. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.93108/4.97975. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.92109/4.97653. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.91647/5.02013. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.92559/4.97629. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.92327/4.99354. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.92415/4.97852. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.92255/4.98126. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.92141/4.97093. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.91979/4.99305. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.93017/4.96489. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.92040/4.98647. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.92324/4.97890. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.91846/4.97981. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.93378/4.99036. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.94560/4.97805. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.93448/4.98413. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.92943/5.00688. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.93176/4.98222. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.92477/4.99231. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.93257/4.99202. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.92200/4.99320. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.92462/4.97244. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.92688/4.97748. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.92435/4.97427. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.92228/4.97446. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.92705/5.01386. Took 0.07 sec\n",
      "Epoch 95, Loss(train/val) 4.92363/4.98677. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.91265/5.02889. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.93428/4.96665. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.92976/4.98078. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.92531/4.99919. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.09416472308615167\n",
      "Epoch 0, Loss(train/val) 4.74116/4.65800. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.67470/4.66077. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.67744/4.66605. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.67683/4.66347. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.68143/4.66324. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.68098/4.66919. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.68209/4.67438. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.67630/4.66990. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.67401/4.67150. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.67372/4.67091. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.67272/4.67682. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.67469/4.67446. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.67466/4.67756. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.66891/4.68090. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.66711/4.68742. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.67030/4.68408. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.67206/4.69133. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.66450/4.69062. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.66807/4.69332. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.66450/4.69802. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.66459/4.70075. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.66199/4.70809. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.66519/4.68661. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.66832/4.69136. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.66481/4.70657. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.66228/4.71144. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.66489/4.71014. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.65901/4.70061. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.66025/4.70588. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.66017/4.72129. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.66062/4.71243. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.66048/4.71261. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.66329/4.70277. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.66156/4.70421. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.66239/4.69991. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.65888/4.70378. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.66405/4.70613. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.65909/4.71077. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.65894/4.71088. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.65853/4.71746. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.65810/4.71880. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.65782/4.72519. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.65614/4.71854. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.65701/4.72446. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.65593/4.71622. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.65728/4.72387. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.65306/4.73378. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.65710/4.72841. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.65708/4.71332. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.66007/4.70747. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.65282/4.71442. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.65679/4.71970. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.65543/4.71799. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.65604/4.71760. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.65396/4.73028. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.65615/4.72600. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.65203/4.72821. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.65308/4.73448. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.65216/4.72503. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.64942/4.70429. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.65566/4.71208. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.65653/4.72155. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.65040/4.72141. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.64782/4.73081. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.65288/4.72165. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.65601/4.71694. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.65179/4.72107. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.65045/4.71876. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.64947/4.71880. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.65272/4.73000. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.64570/4.73353. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.64890/4.73005. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.64709/4.72398. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.64908/4.71390. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.64335/4.72373. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.65318/4.70712. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.64981/4.71131. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.65028/4.71359. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.65366/4.71067. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.64719/4.71810. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.65373/4.71474. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.64592/4.72432. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.64887/4.71750. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.64753/4.71349. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.64789/4.70952. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.64602/4.72027. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.64924/4.71795. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.64402/4.72136. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.64453/4.72967. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.64082/4.73007. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.64437/4.72575. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.64143/4.72885. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.63832/4.72165. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.63970/4.70493. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.64421/4.70385. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.64370/4.71270. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.64067/4.71975. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.64183/4.72716. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.63542/4.72972. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.63955/4.72241. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.12855839970025792\n",
      "Epoch 0, Loss(train/val) 5.24181/5.19070. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.20407/5.21327. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 5.18932/5.20069. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.19516/5.18822. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.19253/5.19378. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.19420/5.19274. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.19130/5.19354. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.18835/5.19118. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.18737/5.18609. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.18888/5.19104. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.18507/5.18565. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.18561/5.18320. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.18192/5.18295. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.18405/5.18766. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.18870/5.21691. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.18991/5.21184. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 5.18770/5.21558. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.18486/5.21239. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.18256/5.21633. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.18020/5.21602. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.17948/5.21091. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.18205/5.20613. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.18206/5.20873. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.17947/5.22155. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.17929/5.22016. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.17708/5.23241. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.18229/5.21713. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.17737/5.23881. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 5.17450/5.23734. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.17804/5.23514. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.17962/5.23590. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 5.17534/5.24487. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.17225/5.25586. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.17617/5.26082. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.17226/5.25345. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.17351/5.25525. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.17184/5.25588. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.17404/5.24789. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.17050/5.25231. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.16919/5.27406. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.17182/5.25133. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.16854/5.27660. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.17163/5.25657. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.16778/5.26821. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.16981/5.24969. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.16656/5.27428. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.16835/5.24725. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.17043/5.26691. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.16592/5.24943. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.16496/5.28662. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.16091/5.26005. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 5.16615/5.27788. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.16727/5.25957. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.16341/5.25639. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.16697/5.28576. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.16899/5.26534. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.16336/5.29835. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.15942/5.29688. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.15839/5.27247. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.16389/5.29386. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.16364/5.27830. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.16409/5.30682. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.15972/5.30986. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.16659/5.29072. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.17655/5.24444. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.17091/5.25457. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.17140/5.24083. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.17072/5.28536. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.17087/5.27386. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.16778/5.28146. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.16107/5.28319. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.16809/5.29241. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.16393/5.28380. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.16634/5.27300. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.16054/5.29651. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.16720/5.30521. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.16343/5.26792. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.16474/5.29766. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.16152/5.28076. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.16183/5.30177. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.16189/5.28555. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 5.16043/5.31541. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.16294/5.29852. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.16259/5.28401. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.16143/5.30063. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.18727/5.26695. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.17828/5.24975. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.18049/5.25502. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.17666/5.25259. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.17178/5.26953. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.17070/5.27851. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 5.16899/5.28330. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.16441/5.27333. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.16850/5.26547. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.15976/5.27968. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.16572/5.27304. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.16938/5.26396. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.16812/5.26050. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.16658/5.29340. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.16810/5.30324. Took 0.08 sec\n",
      "ACC: 0.609375, MCC: 0.22877657129023765\n",
      "Epoch 0, Loss(train/val) 4.99422/4.93211. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 4.94174/4.95435. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 4.94252/4.95255. Took 0.15 sec\n",
      "Epoch 3, Loss(train/val) 4.94163/4.94873. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.93802/4.94486. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.93840/4.94847. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.93478/4.95101. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.93245/4.94722. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.93294/4.94888. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.93123/4.95250. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.93205/4.94850. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.92978/4.94940. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.92878/4.95224. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.93092/4.95232. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.92759/4.94803. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.92551/4.95761. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.93147/4.94911. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.92675/4.95453. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.92460/4.95942. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.92400/4.95500. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.92245/4.95649. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.92641/4.93803. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.91959/4.95744. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.92794/4.96189. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.93059/4.95867. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.93072/4.95863. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.92857/4.95628. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.92817/4.96317. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.92898/4.97275. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.92699/4.97125. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.92455/4.97958. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.92751/4.96353. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.92816/4.97000. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.92789/4.96600. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.92609/4.96467. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.92413/4.98126. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.92436/4.96478. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.92361/4.96838. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.92616/4.98933. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.92980/4.95819. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.92720/4.97418. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.92792/4.97449. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.92722/4.97642. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.92620/4.97841. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.92604/4.97474. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.92415/4.97975. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.92722/4.97926. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.92015/4.99203. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.92244/4.98067. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.91691/5.00327. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.92109/4.99003. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.92235/5.01193. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.92256/4.98540. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.92130/4.99223. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.92290/5.01418. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.92367/4.98508. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.91841/5.01792. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 4.91735/4.99629. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.92658/4.97564. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.91901/5.00639. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.91802/4.99546. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.92146/5.00776. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.91907/5.02721. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.91933/5.01033. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.91486/5.03965. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.91563/5.02478. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.91854/5.02843. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.92076/5.00292. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.91338/5.01514. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.92100/5.01881. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.91836/5.00218. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.91718/5.02050. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.91480/5.02767. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.91641/5.02207. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.91399/5.02576. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.91249/5.02007. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.90992/5.04910. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.91440/5.02533. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.91563/5.02150. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.91308/5.02029. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.91474/5.01980. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.90849/5.04752. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.91187/5.03367. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.91508/5.03753. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.91054/5.03501. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.90945/5.02680. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.91562/5.01915. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.91235/5.03557. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.90975/5.03584. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.90894/5.05091. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.91169/5.03808. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.90815/5.05180. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.91017/5.03407. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.90722/5.05982. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.91022/5.04256. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.90636/5.05028. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.91010/5.03797. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.90660/5.05348. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.92008/4.98414. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.90677/5.03799. Took 0.08 sec\n",
      "ACC: 0.578125, MCC: 0.1793740008335438\n",
      "Epoch 0, Loss(train/val) 5.09152/5.01216. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.99304/5.03511. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.99387/4.97936. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.97842/4.97550. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.97193/4.98314. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.97630/4.98491. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.97650/4.98372. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.97602/4.98343. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.97308/4.98477. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.97355/4.98448. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.97343/4.98952. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.97036/4.98889. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.96763/4.99366. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.96664/5.00670. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.97438/4.98889. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.96647/4.99095. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.96222/5.00816. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.96405/5.00299. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.96679/4.99958. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.96526/4.99673. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.96431/5.00741. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.96223/5.00127. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.95983/4.99985. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.96282/4.99859. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.96443/4.99914. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.95987/4.99687. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.95521/5.00150. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.95864/4.99462. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.95879/4.99888. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.95773/4.99808. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.95943/4.98733. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.95769/5.00037. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.95277/5.00057. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.95595/5.00113. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.95347/5.01500. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.95547/4.99411. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.95572/5.00533. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.95715/5.00694. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.95330/5.00893. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.94969/5.00392. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.95441/5.00127. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.95439/5.00222. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.94806/5.01339. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.95177/5.00318. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.95205/5.01751. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.94738/5.02075. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.95593/5.00583. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.95109/5.01713. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.95074/5.00989. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.94674/5.01935. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.94930/5.01303. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.95108/5.01638. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.95173/5.02023. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.94934/5.02036. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.94744/5.02902. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.94834/5.01711. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.95124/4.98876. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.95966/4.99988. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.94946/5.01115. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.95489/5.00822. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.94327/5.02791. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.95463/5.01181. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.94909/5.03040. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.94658/5.00783. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.94868/5.00657. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.94833/5.01653. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.94576/5.05422. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.95137/5.00692. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.94616/5.03891. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.94411/5.02396. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.94309/5.03290. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.94649/5.02775. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.94292/5.02457. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.94698/5.05291. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.94181/5.04171. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.94312/5.07175. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.94200/5.05387. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.93482/5.05278. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.94293/5.06418. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.94603/5.07556. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.93945/5.07183. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.93710/5.06367. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.94081/5.08148. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.93595/5.06631. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.93716/5.06757. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.92821/5.07816. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.93596/5.09792. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.92734/5.08997. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.93354/5.10354. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.93728/5.11004. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.93881/5.08531. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.92524/5.11709. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.93013/5.09350. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.92650/5.12832. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.93547/5.07268. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.92022/5.14705. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.93188/5.08043. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.92383/5.16098. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.92754/5.08452. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.92345/5.15891. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.12855839970025792\n",
      "Epoch 0, Loss(train/val) 5.03309/5.00745. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.99135/4.98438. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.98832/4.98873. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.98819/4.99230. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.98735/4.99479. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.98434/4.99502. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.98139/4.99755. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.98016/4.99400. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.97977/4.99305. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.97128/5.00447. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.97977/5.01287. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.97724/5.00906. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.97236/5.01011. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.97069/5.01785. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.97129/5.03001. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.97495/5.01901. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.97158/5.01690. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.96904/5.02747. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.96972/5.02944. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.96998/5.02946. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.97105/5.01784. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.96576/5.04089. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.96706/5.03518. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.96611/5.04876. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.96949/5.03224. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.96578/5.04493. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.96879/5.03424. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.96829/5.03628. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.96599/5.04079. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.96663/5.03427. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.96807/5.03279. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.96169/5.05267. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.96086/5.05789. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.96214/5.05447. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.96280/5.04387. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.97152/5.02638. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.96481/5.04535. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.96322/5.05319. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.96311/5.04996. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.96416/5.02576. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.96246/5.04644. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.96562/5.04114. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.96716/5.03717. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.96271/5.03545. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.96465/5.03884. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.96231/5.05497. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.96306/5.04516. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.96519/5.03320. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.95505/5.05042. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.96138/5.05551. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.96731/5.04515. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.96328/5.04849. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.95879/5.02251. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.96246/5.03007. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.96438/5.03424. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.96043/5.05850. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.95850/5.06351. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.95921/5.07472. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.96508/5.02856. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.95856/5.05957. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.95636/5.05158. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.95692/5.05487. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.96056/5.05880. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.95894/5.05475. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.96099/5.06322. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.95901/5.04408. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.95764/5.05369. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.95901/5.04872. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.96100/5.05949. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.96062/5.04106. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.96139/5.05119. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.95837/5.05375. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.95697/5.06646. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.95608/5.06341. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.95680/5.05387. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.95668/5.05626. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.96070/5.06527. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.95303/5.06080. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.96038/5.05908. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.95753/5.04729. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.95867/5.04152. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.95416/5.06310. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.96007/5.05124. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.95631/5.05231. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.95674/5.05066. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.95867/5.07781. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.95314/5.05690. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.95711/5.04056. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.95329/5.06512. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.95620/5.06815. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.95481/5.07536. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.95841/5.04061. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.96215/5.04387. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.95405/5.06443. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.95262/5.06840. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.95018/5.08201. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.95351/5.04948. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.95543/5.07446. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.96426/5.03573. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.96839/5.05041. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.0949157995752499\n",
      "Epoch 0, Loss(train/val) 4.90181/4.86373. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.88094/4.85390. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.87723/4.85381. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.87996/4.86874. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.87958/4.89083. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.87479/4.88212. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.87157/4.87125. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.87192/4.87712. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.87351/4.87614. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.87013/4.87491. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.87363/4.87721. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.87148/4.87594. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.87011/4.87462. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.86963/4.87641. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.86991/4.87693. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.86919/4.87731. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.86859/4.87929. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.87095/4.88554. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.87017/4.88830. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.86804/4.88527. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.86670/4.88639. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.86649/4.89468. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.86622/4.89615. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.86613/4.88609. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.86324/4.89609. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.86478/4.89539. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.86692/4.89798. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.86344/4.88780. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.86331/4.89306. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.86093/4.89419. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.85741/4.90086. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.86099/4.90435. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.86005/4.91166. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.86342/4.89036. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.86039/4.88737. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.86185/4.92007. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.85880/4.91032. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.85666/4.90025. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86017/4.90775. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.85616/4.90839. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.85800/4.91060. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.85226/4.88910. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.85118/4.91618. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.85567/4.92635. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.85219/4.95508. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.84762/4.91539. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.85479/4.97783. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.85376/4.92447. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.85204/4.94057. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.85093/4.91729. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.85327/4.95055. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.85414/4.99782. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.85197/4.93487. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.84671/4.92513. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.84979/4.97438. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.84937/4.97046. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.84970/4.95134. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.85578/4.87511. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.85916/4.89175. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.84613/4.89693. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.84770/4.92008. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.84740/4.92472. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.84085/4.95819. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.84457/5.01237. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.84387/4.93834. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.84427/4.99627. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.84468/4.93759. Took 0.07 sec\n",
      "Epoch 67, Loss(train/val) 4.84443/4.99945. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.84414/4.88854. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.85436/4.96402. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.84070/4.99151. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.83676/4.95590. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83990/5.00454. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.84288/4.93396. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83978/4.97867. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.84385/4.99252. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.84304/4.89836. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.84397/5.03152. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.84575/4.92530. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.83784/4.93072. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.84017/4.93091. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.83542/5.04488. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.83793/4.99875. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.84275/4.96897. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83927/4.98559. Took 0.07 sec\n",
      "Epoch 85, Loss(train/val) 4.83405/4.98822. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.84401/5.00552. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.84021/4.95055. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83644/4.98970. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.85101/4.90575. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.85917/4.91707. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.84826/4.92260. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.84150/4.96178. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.84232/4.89168. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.85115/4.93279. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.83715/4.96425. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.84064/4.91778. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.83898/4.94485. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83697/4.95380. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.83705/4.95384. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.051799406700962544\n",
      "Epoch 0, Loss(train/val) 4.90054/4.91704. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.87034/4.88537. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.87076/4.83448. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.85915/4.82672. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.85071/4.82967. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.85009/4.83217. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.84886/4.83174. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.85339/4.83174. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.85060/4.83722. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.84877/4.83649. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.84548/4.83498. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.84643/4.83386. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.84421/4.83654. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.84488/4.83365. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.84747/4.83331. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.84229/4.83846. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.84749/4.84034. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.84444/4.83112. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.84201/4.82690. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.84549/4.83015. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.84124/4.83033. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.84192/4.82469. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.83894/4.83013. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.84224/4.82758. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.83828/4.83386. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.84171/4.82522. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.84159/4.83650. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.83860/4.83119. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.83411/4.82355. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.83914/4.83491. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.84036/4.82412. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.83553/4.82928. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.83415/4.85162. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.83274/4.83901. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.83980/4.83565. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.83536/4.81724. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.83183/4.84103. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.83401/4.83883. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.82657/4.83171. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.83069/4.85323. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.83215/4.82601. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.83201/4.84789. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.82996/4.84438. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.82820/4.85204. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.83281/4.84114. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.82807/4.84776. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.82764/4.85096. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.82581/4.84815. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.82905/4.84893. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.82462/4.85319. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.82682/4.86163. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.82858/4.84465. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.82705/4.85830. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.82349/4.84862. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.83217/4.85092. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.82862/4.83886. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.82918/4.84724. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.82710/4.85024. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.82354/4.85367. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.82495/4.86272. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.82818/4.86276. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.82224/4.85935. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.82314/4.85506. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.82397/4.86085. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.82604/4.83991. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.82218/4.86650. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.82556/4.83681. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.82559/4.85261. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.82262/4.83906. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.81895/4.87577. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.82658/4.85889. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.81904/4.85962. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.81964/4.83270. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.82204/4.88470. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.81995/4.86796. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.82234/4.86796. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.82225/4.86192. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.81874/4.86320. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.82117/4.87120. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.84783/4.83371. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83473/4.83916. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.84064/4.82127. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.83869/4.82538. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.83107/4.83017. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.82796/4.81469. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83776/4.83167. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83173/4.83118. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.83506/4.82407. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83491/4.83094. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.83248/4.82684. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.83261/4.80823. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.82737/4.83887. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.82919/4.83756. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83250/4.82763. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.83346/4.83279. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.83138/4.83484. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.82689/4.84902. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.83079/4.85570. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.82954/4.83846. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.82577/4.84249. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.1111111111111111\n",
      "Epoch 0, Loss(train/val) 4.86891/4.83942. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.79468/4.78477. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.79410/4.78014. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79323/4.78161. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79304/4.78316. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79091/4.78466. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.79392/4.78540. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.79224/4.78629. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.79465/4.78925. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79246/4.79149. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79262/4.79178. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.79285/4.78948. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.78861/4.79148. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.78860/4.79217. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.78999/4.79077. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.78645/4.79067. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78706/4.79126. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.78725/4.79604. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.78368/4.80173. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.78533/4.79313. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.78771/4.79414. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78259/4.79818. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.78372/4.79770. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.78576/4.79437. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.78262/4.79590. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.78346/4.80135. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.77849/4.80067. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.78611/4.79397. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77847/4.79726. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.78350/4.79767. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.78118/4.80121. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78193/4.79941. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.78126/4.79830. Took 0.07 sec\n",
      "Epoch 33, Loss(train/val) 4.78171/4.81574. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78389/4.79785. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.77975/4.81243. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.77867/4.80534. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.77845/4.81012. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.77869/4.79195. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.77530/4.83474. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.77869/4.82113. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.77662/4.83067. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.77694/4.80423. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.77102/4.83242. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.78140/4.79204. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.77660/4.79801. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77683/4.81996. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77682/4.81795. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77363/4.78956. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.77920/4.82079. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.76753/4.81524. Took 0.07 sec\n",
      "Epoch 51, Loss(train/val) 4.77523/4.81283. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.77215/4.80496. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.77446/4.78065. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.77558/4.79263. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.77870/4.79756. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.76757/4.80328. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.77303/4.81473. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.77141/4.78051. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.78245/4.78349. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.78348/4.78092. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.77762/4.78727. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.77690/4.78887. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.77436/4.79198. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.77448/4.81203. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.77858/4.80077. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.76537/4.80972. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.77555/4.80083. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.77420/4.76485. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.77659/4.77809. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.77713/4.77832. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.77341/4.77382. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.77395/4.78830. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.76752/4.77808. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.77479/4.78622. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.77217/4.79022. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.77098/4.79523. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.76955/4.79904. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.76801/4.80579. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.77127/4.81607. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.76946/4.80882. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.75930/4.81237. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.76858/4.79132. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76402/4.79739. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.76929/4.79711. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.76036/4.80908. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.77681/4.78048. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.76709/4.76689. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.77355/4.76602. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.77045/4.76515. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.76926/4.76547. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.76770/4.75732. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.76352/4.77042. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.76856/4.76530. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.76335/4.77238. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.76165/4.75998. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.76468/4.78995. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.76184/4.77243. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.76423/4.78496. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.76221/4.77255. Took 0.09 sec\n",
      "ACC: 0.484375, MCC: -0.03779786830012622\n",
      "Epoch 0, Loss(train/val) 4.95051/4.91328. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.88993/4.88980. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.88558/4.88164. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.87860/4.88064. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.87874/4.87816. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.87495/4.87980. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.87878/4.88035. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.87906/4.88046. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.87680/4.87781. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.87501/4.87442. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.87381/4.87500. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.87576/4.87845. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.87380/4.87980. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.87317/4.87764. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.87067/4.87980. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.87170/4.88060. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.87082/4.88080. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.86701/4.87994. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.86959/4.88067. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.87001/4.87514. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.86959/4.87831. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.86702/4.88099. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.86558/4.87875. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.86646/4.88395. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.86691/4.87904. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.86292/4.87916. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.86472/4.88156. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.86173/4.88322. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.86155/4.88075. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.86314/4.88567. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.86180/4.88041. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.86054/4.87904. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.86147/4.87863. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.86115/4.88157. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.85640/4.87770. Took 0.07 sec\n",
      "Epoch 35, Loss(train/val) 4.85864/4.88190. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.85651/4.88072. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.85847/4.87742. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.85706/4.86844. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.85683/4.87390. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.85908/4.87012. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.85699/4.88159. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.86008/4.87676. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.85755/4.87980. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.85501/4.87733. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.85040/4.87196. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.85461/4.86535. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.85573/4.86007. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.85346/4.87006. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.85677/4.85925. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.84995/4.86821. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.85773/4.86948. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.85061/4.87766. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.85147/4.87082. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.85190/4.86425. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.84770/4.87169. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.85308/4.87102. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.85708/4.88239. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.84650/4.89691. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.84945/4.88822. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.84879/4.87631. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.84132/4.87461. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.85595/4.87094. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.85101/4.86051. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.85254/4.87783. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.85294/4.88205. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.85480/4.88040. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.84728/4.88601. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.85379/4.87483. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.85044/4.87847. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.84611/4.88033. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.85059/4.87452. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.84970/4.86991. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.84617/4.89057. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.84029/4.86957. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.84327/4.87088. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.84846/4.86768. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.84317/4.87116. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.84354/4.87648. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.84843/4.87044. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.84440/4.88039. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.84376/4.88045. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.84338/4.87392. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.84780/4.87889. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.84655/4.87685. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.84080/4.87731. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.85346/4.86216. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.84255/4.87718. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.84144/4.87713. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.83685/4.88370. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.84346/4.86951. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.84187/4.87593. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.84907/4.87446. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.84392/4.88494. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.84021/4.90569. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.83639/4.89908. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.84143/4.89472. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.84365/4.89935. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.84649/4.90215. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.84174/4.90752. Took 0.08 sec\n",
      "ACC: 0.3125, MCC: -0.36489784292473615\n",
      "Epoch 0, Loss(train/val) 4.95783/4.92427. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.88528/4.90868. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.88507/4.90762. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.87947/4.90583. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.88056/4.90203. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.87943/4.90233. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.87925/4.90351. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.87644/4.90490. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.87442/4.91016. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.87475/4.91484. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.87102/4.91385. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.87057/4.91806. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.87425/4.92360. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.87190/4.91711. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.86943/4.91064. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.86347/4.92508. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.86373/4.93392. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.85986/4.92519. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.85924/4.92895. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.86010/4.91742. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.85462/4.92908. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.85898/4.92683. Took 0.07 sec\n",
      "Epoch 22, Loss(train/val) 4.85762/4.91711. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.85463/4.91673. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.85581/4.91866. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.86972/4.89769. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.86402/4.90316. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.85661/4.91078. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.85779/4.91362. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.85489/4.92165. Took 0.07 sec\n",
      "Epoch 30, Loss(train/val) 4.85736/4.91122. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.85306/4.91684. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.85383/4.91538. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.85444/4.90556. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.85111/4.91654. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84870/4.91249. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84783/4.90969. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.85161/4.91522. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.85471/4.90545. Took 0.07 sec\n",
      "Epoch 39, Loss(train/val) 4.85326/4.89254. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.85216/4.88257. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.85091/4.92520. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.86236/4.89639. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.85550/4.88773. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.85431/4.88991. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.84892/4.89339. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.84850/4.89312. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.84630/4.88880. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84552/4.89052. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.84778/4.89104. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.84652/4.88571. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.84418/4.88675. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.84733/4.89019. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.84332/4.89353. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.84435/4.89418. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.84294/4.89804. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.84674/4.89216. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.84284/4.89381. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.84280/4.89087. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.84093/4.89194. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.84617/4.89358. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.84532/4.88923. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.84432/4.89687. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.84308/4.89600. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.84210/4.89430. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.83900/4.88843. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.83953/4.89824. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.84120/4.88314. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.84065/4.89819. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83654/4.89213. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.84858/4.88298. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.83403/4.89386. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83828/4.89826. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.83861/4.88397. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83296/4.90233. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83874/4.90257. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.83688/4.89775. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.83273/4.89385. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.83712/4.90521. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.84224/4.91382. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83807/4.87807. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.83768/4.89753. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.83040/4.91102. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.83212/4.89265. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83174/4.89112. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.84155/4.89549. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83904/4.90870. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.83652/4.89087. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83281/4.90495. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.83331/4.90414. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.82837/4.89201. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.82911/4.91364. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.84204/4.88210. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83171/4.88699. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.82854/4.89975. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.84465/4.89566. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.83859/4.87526. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.83021/4.88171. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83345/4.89805. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.83785/4.89281. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.08544439848099883\n",
      "Epoch 0, Loss(train/val) 4.92137/4.90069. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.86543/4.86416. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.85836/4.86357. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.85302/4.86735. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.85181/4.87341. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.85249/4.87837. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.85133/4.88320. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.85212/4.88450. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.84958/4.87815. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.84834/4.87400. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.84655/4.87303. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.84820/4.87647. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.84620/4.87509. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.84659/4.86691. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.84657/4.86753. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.84757/4.85863. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.84368/4.87100. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.84475/4.85922. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.84321/4.87569. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.84086/4.87223. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.84042/4.87247. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.84082/4.86841. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.84056/4.87554. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.83468/4.87655. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.84227/4.86958. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.83868/4.86984. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.84059/4.87313. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.83958/4.89158. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.83897/4.89144. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84567/4.87943. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.84428/4.87751. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.84879/4.88243. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84323/4.90424. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.84183/4.87215. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.83710/4.88422. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84470/4.87750. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84418/4.89989. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84159/4.89290. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84045/4.88378. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.83668/4.88065. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84162/4.87765. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.83705/4.89658. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.83613/4.92446. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.83841/4.89409. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.83431/4.89511. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.83023/4.91645. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.84079/4.86925. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.83551/4.87604. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.83308/4.90345. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.83184/4.90384. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.83845/4.87557. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.83067/4.88104. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.82997/4.89764. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.82487/4.88913. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.83212/4.89525. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.82696/4.89414. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.83445/4.89531. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.82627/4.89444. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.83184/4.87045. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.82394/4.88654. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.81912/4.91641. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.82562/4.89701. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.82809/4.92237. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.82487/4.88785. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.82871/4.90509. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.82531/4.90332. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.82697/4.93536. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.82045/4.93053. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.82379/4.90415. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.82519/4.89290. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.82256/4.90933. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.82645/4.91636. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.82747/4.90285. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.82281/4.90104. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.82113/4.91787. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.81822/4.92556. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.82584/4.92275. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.82116/4.93185. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.82188/4.93870. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.81810/4.93725. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.81949/4.93950. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.82584/4.90127. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.82004/4.91887. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.84171/4.88848. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83937/4.86956. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83817/4.87767. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83148/4.90401. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.83021/4.86724. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83101/4.88474. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.82766/4.89631. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.82848/4.90951. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.82576/4.91826. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83394/4.90946. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.82797/4.90346. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.82304/4.90757. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.82758/4.88601. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.82882/4.95009. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.82309/4.92929. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.82764/4.90929. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.82406/4.89268. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.054187192118226604\n",
      "Epoch 0, Loss(train/val) 4.84107/4.82759. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.76598/4.78701. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.76405/4.77518. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.76202/4.77443. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.75913/4.78912. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.76005/4.78769. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.76140/4.78309. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.75903/4.78214. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.75791/4.78328. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.75964/4.78765. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.75839/4.79530. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.75351/4.78830. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.75907/4.79830. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.75576/4.79081. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.75488/4.79083. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.75657/4.78592. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.75355/4.79161. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.75558/4.79032. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.75763/4.79665. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.75972/4.79300. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.75776/4.77571. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.75764/4.78054. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.75750/4.77327. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.75496/4.78271. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.75225/4.81240. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.75554/4.77411. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.75474/4.76542. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.75147/4.80305. Took 0.07 sec\n",
      "Epoch 28, Loss(train/val) 4.75477/4.79450. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.75591/4.78956. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.75369/4.78699. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.75351/4.78841. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.75342/4.80502. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.75021/4.78486. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.75003/4.80804. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.75000/4.79013. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.75145/4.82593. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.74772/4.82277. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.74989/4.80503. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.74583/4.79823. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.74882/4.79062. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.74565/4.81941. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.74806/4.78925. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.74721/4.81407. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.74403/4.81082. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.74383/4.80702. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.74520/4.79309. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.74311/4.80029. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.74183/4.81514. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.74324/4.79450. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.74862/4.82198. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.74281/4.80866. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.74369/4.81672. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.74601/4.79760. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.74604/4.79644. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.73957/4.82920. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.74195/4.80356. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.73911/4.81776. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.74489/4.79266. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.74324/4.81236. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.74029/4.81025. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.74387/4.78756. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.74284/4.82551. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.73821/4.78508. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.74896/4.78199. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.75004/4.80062. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.75005/4.78160. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.74768/4.79857. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.74044/4.78768. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.74115/4.82205. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.73464/4.79774. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.74378/4.78282. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.73960/4.81167. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.74750/4.75868. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.75789/4.78017. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.75738/4.78475. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.75319/4.78598. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.75071/4.79361. Took 0.07 sec\n",
      "Epoch 78, Loss(train/val) 4.74778/4.80414. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.74706/4.78885. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.74810/4.79690. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.74391/4.80017. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.74682/4.80507. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.74337/4.79764. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.74871/4.80233. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.74161/4.80441. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.74646/4.81467. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.73946/4.81151. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.74359/4.80956. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.73905/4.81945. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.73978/4.81187. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.74372/4.80829. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.74041/4.81485. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.74139/4.80435. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.74498/4.81201. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.73753/4.82614. Took 0.07 sec\n",
      "Epoch 96, Loss(train/val) 4.74104/4.81413. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.74477/4.81318. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.73972/4.81391. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.73846/4.80405. Took 0.08 sec\n",
      "ACC: 0.390625, MCC: -0.2057101528827944\n",
      "Epoch 0, Loss(train/val) 4.79729/4.82561. Took 0.18 sec\n",
      "Epoch 1, Loss(train/val) 4.75611/4.81346. Took 0.17 sec\n",
      "Epoch 2, Loss(train/val) 4.76068/4.80399. Took 0.12 sec\n",
      "Epoch 3, Loss(train/val) 4.77273/4.75709. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.75879/4.75615. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.74694/4.76308. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.74810/4.76902. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.74935/4.76714. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.75149/4.76635. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.74840/4.76536. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.74432/4.76995. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.74478/4.76811. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.74422/4.76739. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.74723/4.76803. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.74252/4.77663. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.74534/4.77111. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.74185/4.77606. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.74075/4.76987. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.74507/4.76990. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.74327/4.78419. Took 0.07 sec\n",
      "Epoch 20, Loss(train/val) 4.73803/4.79483. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.74194/4.76410. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.74136/4.77288. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.73595/4.78905. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.73845/4.78632. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.73936/4.78392. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.73551/4.79567. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.73295/4.79659. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.73857/4.79388. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.73686/4.80025. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.73502/4.80951. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.73462/4.80566. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.73351/4.79690. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.74066/4.77371. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.73938/4.79150. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.73282/4.81014. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.72723/4.81661. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.73385/4.81159. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.72930/4.80793. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.73203/4.81348. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.72977/4.84757. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.73335/4.82080. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.73030/4.79993. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.73426/4.78251. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.73393/4.79259. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.73241/4.79214. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.72772/4.78793. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.73321/4.79885. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.72911/4.81025. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.73297/4.78178. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.73167/4.79866. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.72893/4.80996. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.73464/4.77263. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.74559/4.74129. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.74112/4.74664. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.73992/4.75218. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.73289/4.74861. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.73479/4.74306. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.73319/4.74613. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.74235/4.76020. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.73814/4.75173. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.74722/4.75807. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.74152/4.76379. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.73803/4.76345. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.73732/4.77166. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.73584/4.77984. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.73183/4.78819. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.73642/4.77129. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.73211/4.78414. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.73401/4.79156. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.73299/4.79015. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.73463/4.79804. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.73313/4.81488. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.73572/4.78339. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.72700/4.81125. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.73049/4.81451. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.73038/4.80265. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.73130/4.81180. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.72730/4.81880. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.72651/4.82040. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.72204/4.83231. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.73358/4.82367. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.72924/4.81935. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.72530/4.81257. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.73502/4.79754. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.73267/4.78121. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.72982/4.80145. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.72917/4.79904. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.72827/4.80140. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.72625/4.78970. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.72297/4.80049. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.73544/4.78490. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.72860/4.80087. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.72904/4.82142. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.73047/4.79368. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.72290/4.80957. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.73105/4.80964. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.72629/4.81859. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.72798/4.83991. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.72306/4.81675. Took 0.08 sec\n",
      "ACC: 0.578125, MCC: 0.1463182264089704\n",
      "Epoch 0, Loss(train/val) 5.05857/5.04412. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.01245/5.03855. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.00656/5.04520. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.00531/5.03148. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.00196/5.01548. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.00123/5.01777. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.99961/5.02334. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.99980/5.02371. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.00097/5.02043. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.99687/5.01553. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.99766/5.01977. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.99699/5.01845. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.99303/5.01990. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.99632/5.02228. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.99583/5.01759. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.99326/5.01682. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.99316/5.01928. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.99579/5.02177. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.99394/5.02824. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.99201/5.02438. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.98963/5.02412. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.98868/5.02974. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.98822/5.03997. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.98906/5.03253. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.98873/5.03024. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.98970/5.03463. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.98794/5.03529. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.98685/5.03433. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.98571/5.03939. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.98950/5.03387. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.99154/5.04508. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.98924/5.03313. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.98880/5.03100. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.98642/5.04052. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.99233/5.03036. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.99191/5.02963. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.98997/5.03967. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.99041/5.04117. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.98754/5.04644. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.98892/5.04767. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.98654/5.05051. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.98783/5.05796. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.98553/5.06054. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.98391/5.06159. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.98505/5.06125. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.98392/5.05989. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.99265/5.05055. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.98762/5.05704. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.98853/5.06179. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.98338/5.07529. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.98463/5.07902. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.98280/5.06708. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.98513/5.06636. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.98356/5.06309. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.98506/5.07932. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.98178/5.04079. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.98247/5.05650. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.97924/5.05147. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.97837/5.06602. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.98721/5.04118. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.98414/5.06453. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.98298/5.05385. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.98341/5.05885. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.97589/5.06628. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.97878/5.06987. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.97822/5.07081. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.97703/5.06402. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.97528/5.07677. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.97566/5.07998. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.97573/5.07216. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.97403/5.07982. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.97298/5.08650. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.97606/5.07151. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.96902/5.07999. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.97403/5.07351. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.97593/5.07578. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.96807/5.08515. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.96886/5.09104. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.97023/5.11054. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.96446/5.09715. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.97585/5.08319. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.97442/5.07279. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.97564/5.09245. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.97113/5.08381. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.97073/5.09228. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.96977/5.07991. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.96932/5.08239. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.97040/5.10526. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.96852/5.09044. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.96738/5.12228. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.96487/5.09709. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.96739/5.11396. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.96832/5.08533. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.97110/5.08978. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.96780/5.09906. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.96703/5.10416. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.96077/5.13938. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.96734/5.10924. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.96724/5.10125. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.96202/5.11791. Took 0.08 sec\n",
      "ACC: 0.390625, MCC: -0.21885688981825285\n",
      "Epoch 0, Loss(train/val) 5.02192/4.97953. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.95217/4.95584. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.95655/4.95962. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.95736/4.96162. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.95725/4.96462. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.95888/4.97300. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.96053/4.97001. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.96138/4.95930. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.96102/4.95452. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.95210/4.95493. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.95029/4.95347. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.94960/4.95835. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.95240/4.96534. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.95384/4.95451. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.94910/4.95654. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.94775/4.96282. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.94982/4.94815. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.94891/4.95728. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.95144/4.95386. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.94847/4.96070. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.94814/4.95987. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.94397/4.95717. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.94660/4.95690. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.94364/4.96582. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.94148/4.95388. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.94125/4.96569. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.94585/4.95451. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.94483/4.95780. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.94457/4.95589. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.94373/4.96524. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.94282/4.96429. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.94539/4.97118. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.93958/4.99671. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.93744/4.97419. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.94174/4.99213. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.94066/4.99599. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.93773/4.98070. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.93405/4.98904. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.93446/4.99185. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.93619/4.99828. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.95152/4.96134. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.95093/4.96053. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.93971/4.96925. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.94330/4.97079. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.94297/4.97911. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.93663/4.99441. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.94017/4.99953. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.93809/4.98403. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.94504/4.97311. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.94211/4.98214. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.93896/4.99758. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.93579/4.98179. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.94133/4.99012. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.94019/4.98165. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.93424/5.02153. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.93753/4.97664. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.93565/5.02152. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.93806/4.97718. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.93499/5.00617. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.93412/5.00476. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.93631/5.00480. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.93302/5.01374. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.93040/5.00186. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.93568/5.00666. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.93289/5.02511. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.93082/5.01758. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.93570/5.00889. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.93984/4.98843. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.94022/5.01299. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.93422/5.00704. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.93622/5.02523. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.93625/5.02083. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.93281/5.00407. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.93206/5.03615. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.92992/5.04111. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.93598/4.99786. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.93258/5.02070. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.92760/5.00238. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.93247/5.04771. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.93175/5.01836. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.92835/5.04047. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.92588/5.04228. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.93472/4.98403. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.93659/5.01205. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.92604/5.02695. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.92681/5.04231. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.93260/5.01125. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.92765/5.02574. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.92527/5.03770. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.92442/5.03844. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.92854/5.03294. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.92641/5.03825. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.92947/5.04160. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.92556/5.04419. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.92450/5.03705. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.92495/5.02475. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.92235/5.04422. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.92425/5.03662. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.92911/5.02155. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.92781/5.02006. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.09416472308615167\n",
      "Epoch 0, Loss(train/val) 4.83836/4.84292. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.77215/4.79964. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.76449/4.79893. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.76504/4.79710. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.76533/4.79724. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.76426/4.80719. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.76505/4.80933. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.75935/4.80704. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.75994/4.80702. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.76076/4.80845. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.75852/4.81882. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.75923/4.81574. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.75529/4.81980. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.75454/4.82059. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.75323/4.83147. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.75250/4.82689. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.74852/4.83075. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.74987/4.83693. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.74706/4.83542. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.74333/4.84241. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.75616/4.82258. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.75537/4.80842. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.75352/4.79755. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.75368/4.79844. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.75019/4.80365. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.75372/4.80316. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.74952/4.79737. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.75045/4.81202. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.74856/4.81568. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.74967/4.81829. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.74651/4.82395. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.74671/4.82422. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.74432/4.82572. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.74163/4.83687. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.74013/4.83996. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.74676/4.82236. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.74372/4.82245. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.74370/4.83680. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.74059/4.84217. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.73843/4.83904. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.73797/4.85063. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.73922/4.81461. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.74468/4.82975. Took 0.07 sec\n",
      "Epoch 43, Loss(train/val) 4.74518/4.82904. Took 0.07 sec\n",
      "Epoch 44, Loss(train/val) 4.73909/4.82955. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.73868/4.83474. Took 0.07 sec\n",
      "Epoch 46, Loss(train/val) 4.73489/4.84189. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.73489/4.84496. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.73576/4.84177. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.73472/4.85893. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.73490/4.83502. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.73671/4.82712. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.73707/4.81833. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.73464/4.86487. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.73844/4.84405. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.73637/4.82240. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.73207/4.84679. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.73425/4.84483. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.73529/4.83294. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.73228/4.83457. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.73486/4.83182. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.73366/4.83392. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.73055/4.84233. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.73223/4.84171. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.73096/4.83538. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.73190/4.82444. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.73230/4.82977. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.73021/4.83261. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.73268/4.82930. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.73293/4.84344. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.73922/4.80265. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.73785/4.82277. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.73270/4.81900. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.72770/4.83798. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.73160/4.83870. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.73394/4.81892. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.72502/4.82388. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.72985/4.81910. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.73641/4.80320. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.72876/4.82054. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.73598/4.80703. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.73569/4.81288. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.73003/4.81452. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.72822/4.81728. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.72652/4.83410. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.72929/4.80970. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.72531/4.82011. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.73190/4.83337. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.72370/4.81152. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.73044/4.79921. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.72252/4.81682. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.72956/4.81333. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.72053/4.84483. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.72926/4.80420. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.72485/4.81497. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.72808/4.81648. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.72179/4.82157. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.72769/4.83922. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.72993/4.80179. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.72929/4.81333. Took 0.08 sec\n",
      "ACC: 0.578125, MCC: 0.1777495495783369\n",
      "Epoch 0, Loss(train/val) 4.80576/4.77340. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.78520/4.80102. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.77644/4.79308. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.78037/4.77699. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.77524/4.77244. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.78093/4.76988. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.77660/4.76820. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.77492/4.77506. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.76900/4.77449. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.76976/4.77730. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.77246/4.77209. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.77382/4.77461. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.76885/4.77924. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.76909/4.77847. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.76797/4.77719. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.76896/4.77797. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.76830/4.78218. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.76817/4.78087. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.76430/4.78083. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.76605/4.77872. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.76613/4.78298. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.76558/4.77975. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.76468/4.77782. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.76487/4.78511. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.76358/4.77894. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.76310/4.79156. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.75958/4.78698. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.76395/4.78086. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.76385/4.78442. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.76302/4.78555. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.75995/4.78224. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.76065/4.77531. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.75805/4.77732. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.76527/4.76719. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.76284/4.77358. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.76513/4.77317. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.75965/4.78453. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.75777/4.77431. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.75578/4.77984. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.75876/4.78189. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.75425/4.78270. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.75867/4.79492. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.75629/4.77894. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.75695/4.78452. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.75767/4.78474. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.75960/4.77923. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.75495/4.78491. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.75221/4.78894. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.75137/4.78683. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.75709/4.78008. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.75226/4.79501. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.75448/4.77507. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.75650/4.79199. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.75556/4.78041. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.75676/4.78752. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.75664/4.78089. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.75447/4.78479. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.75188/4.77376. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.75785/4.77740. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.75288/4.78714. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.75273/4.78753. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.74797/4.78749. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.75472/4.78466. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.75129/4.78201. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.75166/4.78133. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.74813/4.78434. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.75175/4.77728. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.75035/4.77391. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.75447/4.78611. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.75238/4.77281. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.74766/4.77623. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.75092/4.77951. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.74346/4.78436. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.74866/4.78074. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.74669/4.77364. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.75229/4.76747. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.74721/4.77554. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.75175/4.77705. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.75054/4.76673. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.75119/4.77109. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.75147/4.76595. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.74685/4.76667. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.74628/4.77214. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.75175/4.77461. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75221/4.77766. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.75013/4.78104. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.74958/4.77021. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.75564/4.76974. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.75937/4.77102. Took 0.07 sec\n",
      "Epoch 89, Loss(train/val) 4.75671/4.77140. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.75588/4.77796. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.75377/4.77916. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.75320/4.78260. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.75061/4.78420. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.74564/4.78074. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.74648/4.78150. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.75053/4.79772. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.74503/4.78741. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.74972/4.78604. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.74464/4.78349. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.06052275326688024\n",
      "Epoch 0, Loss(train/val) 4.65004/4.61771. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.62726/4.63883. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.63214/4.65691. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.62032/4.61441. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.60588/4.61603. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.60846/4.62298. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.61006/4.62298. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.60764/4.62098. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.60567/4.62367. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.60650/4.62217. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.60609/4.61810. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.60355/4.61850. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.60471/4.61542. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.60141/4.61650. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.60062/4.61462. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.60068/4.61771. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.59786/4.61544. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.60011/4.61655. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.60338/4.61208. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.59892/4.61074. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.59572/4.61017. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.59740/4.61258. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.59645/4.61120. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.59661/4.60363. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.59538/4.60944. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.60111/4.60494. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.59650/4.60441. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.59141/4.59819. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.59219/4.60983. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.59208/4.59895. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.58907/4.60539. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.59087/4.60874. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.58704/4.60700. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.59733/4.60212. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.59940/4.60146. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.59277/4.60830. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.58888/4.60515. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.59031/4.62112. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.58991/4.60946. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.59071/4.61024. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.58691/4.61142. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.58496/4.60841. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.58702/4.60357. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.58196/4.61385. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.58024/4.61309. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.58079/4.61610. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.58811/4.60233. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.57919/4.60815. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.57719/4.59897. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.58235/4.60717. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.57838/4.61186. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.58239/4.60266. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.58318/4.61196. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.57221/4.60887. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.57406/4.59741. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.57578/4.60647. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.57096/4.61616. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.57737/4.61298. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.57701/4.59529. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.57105/4.61780. Took 0.07 sec\n",
      "Epoch 60, Loss(train/val) 4.57600/4.60889. Took 0.07 sec\n",
      "Epoch 61, Loss(train/val) 4.60253/4.59324. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.59411/4.60278. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.59214/4.58762. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.58581/4.58242. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.58588/4.58072. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.58524/4.59016. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.58182/4.58407. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.58538/4.59945. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.58196/4.57499. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.58057/4.58116. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.57869/4.57586. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.57955/4.57174. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.57833/4.58282. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.57703/4.57470. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.57636/4.56683. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.57813/4.56386. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.57589/4.58017. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.57223/4.56996. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.57328/4.57264. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.57387/4.58589. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.57297/4.57882. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.57177/4.55678. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.57227/4.57484. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.57550/4.62197. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.60913/4.60112. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.59946/4.60861. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.59282/4.59554. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.58919/4.59101. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.58411/4.58006. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.57791/4.59452. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.58909/4.58760. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.58606/4.57789. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.57684/4.58141. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.57878/4.58135. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.57852/4.57742. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.57641/4.58904. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.57502/4.57695. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.57556/4.58460. Took 0.07 sec\n",
      "Epoch 99, Loss(train/val) 4.57670/4.60759. Took 0.08 sec\n",
      "ACC: 0.40625, MCC: -0.16834512458535864\n",
      "Epoch 0, Loss(train/val) 4.78849/4.71402. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.72258/4.70172. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.71967/4.70202. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.72392/4.70251. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.72006/4.70128. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.72078/4.69800. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.72141/4.70143. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.72264/4.70111. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.72082/4.70440. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.72329/4.70458. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.72262/4.70482. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.71964/4.70227. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.71691/4.70305. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.71312/4.70462. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.71798/4.70473. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.71410/4.70365. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.71748/4.70109. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.71266/4.70008. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.71367/4.69700. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.70956/4.69705. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.71315/4.70080. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.71154/4.70231. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.71660/4.69804. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.70894/4.69480. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.71316/4.70184. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.71117/4.70107. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.71055/4.70117. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.71894/4.69918. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.71342/4.70050. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.71246/4.69761. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.71274/4.69115. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.71369/4.69489. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.70786/4.69066. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.70629/4.69266. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.71141/4.69753. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.70925/4.69261. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.70864/4.69964. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.70990/4.70381. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.70915/4.69986. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.70773/4.70221. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.71435/4.70612. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.71280/4.70476. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.71041/4.70051. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.70456/4.69604. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.70540/4.70018. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.70513/4.69850. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.70655/4.70023. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.70386/4.71041. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.70684/4.70771. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.70734/4.69649. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.70590/4.69668. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.69990/4.70218. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.69864/4.70003. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.70661/4.70740. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.70524/4.69575. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.69881/4.69600. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.70311/4.70094. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.70148/4.69556. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.70173/4.70286. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.70591/4.70120. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.69710/4.70479. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.69486/4.70255. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.69614/4.71382. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.69846/4.70086. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.69541/4.69612. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.69601/4.70910. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.69350/4.70626. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.69381/4.70557. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.69290/4.71321. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 4.68722/4.71933. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.69626/4.70791. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.69521/4.70418. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.69527/4.70893. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.68351/4.71550. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.69063/4.70809. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.69229/4.70652. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.69354/4.70871. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.68354/4.70378. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.68819/4.71315. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.69110/4.70674. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.69059/4.69908. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.69072/4.69926. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.68188/4.70506. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.68740/4.71849. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.68442/4.71489. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.68828/4.70817. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.68092/4.71847. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.69034/4.70803. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.68884/4.71389. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.69261/4.70341. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.68027/4.70814. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.68695/4.72032. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.68734/4.70638. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.68416/4.70819. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.68928/4.72062. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.68052/4.71257. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.68406/4.70413. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.68238/4.69799. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.68017/4.70896. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.68047/4.73550. Took 0.08 sec\n",
      "ACC: 0.375, MCC: -0.24932120796616944\n",
      "Epoch 0, Loss(train/val) 5.23143/5.23238. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.22602/5.27025. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 5.22629/5.21722. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.22175/5.22265. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.22190/5.21994. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.21684/5.21962. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.22434/5.21906. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.21990/5.22150. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.22232/5.21660. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.21717/5.21908. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.21700/5.21583. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.21858/5.22214. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.21606/5.22179. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.21533/5.21783. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.21184/5.22225. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 5.21446/5.21051. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.21378/5.22073. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.21164/5.21585. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.21572/5.21343. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.21287/5.21350. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.20840/5.21047. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.20915/5.20803. Took 0.07 sec\n",
      "Epoch 22, Loss(train/val) 5.20831/5.21491. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.20564/5.21474. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.20805/5.20630. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.20916/5.21260. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.20416/5.21545. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.20327/5.22248. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.20609/5.21711. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.20904/5.22283. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.21646/5.21732. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.20806/5.22772. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.20265/5.21772. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.20418/5.21915. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.19708/5.22898. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.20642/5.21445. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.20284/5.21573. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 5.19944/5.22445. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 5.19985/5.21771. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 5.19504/5.22338. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 5.19751/5.21860. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.19920/5.21294. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 5.20122/5.21640. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.19432/5.22753. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.19182/5.22013. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.19294/5.22401. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.19768/5.23697. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.19995/5.21994. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.19583/5.22690. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.19732/5.23839. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.19550/5.24414. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.19388/5.23546. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.19329/5.24754. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 5.19341/5.22653. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.22137/5.21651. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.21337/5.21285. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.21085/5.22523. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.21047/5.22498. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.21018/5.23064. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.21045/5.23339. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 5.20640/5.24252. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.21057/5.23950. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.20961/5.24900. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.20385/5.25309. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.20395/5.26872. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 5.20604/5.23881. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.19439/5.27478. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.19706/5.23573. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.20449/5.23918. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.20254/5.25580. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.20069/5.26412. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.19420/5.25299. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.19452/5.24118. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.19302/5.24497. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.19440/5.25910. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.18912/5.25893. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.19090/5.29903. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.19717/5.24830. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.18893/5.25455. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.18509/5.27837. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.18375/5.24443. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.18643/5.26724. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.18362/5.25895. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.18994/5.26789. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.19884/5.25881. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.18318/5.28055. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.18566/5.25924. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.18824/5.24105. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.18275/5.26213. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.18141/5.27055. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.18517/5.25615. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.18076/5.28485. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.19132/5.24242. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.18141/5.26430. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.18949/5.22391. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.18852/5.23671. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.18212/5.26852. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.18176/5.25910. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.18297/5.25772. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.18810/5.26108. Took 0.08 sec\n",
      "ACC: 0.359375, MCC: -0.2965909994776427\n",
      "Epoch 0, Loss(train/val) 5.00554/4.92719. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.95642/4.93405. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.95372/4.93452. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.95232/4.93571. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.94769/4.93859. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.94738/4.93735. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.94901/4.93479. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.94860/4.93449. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.94867/4.93699. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.94478/4.93495. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.94232/4.93249. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.94548/4.93149. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.94358/4.93598. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.94357/4.93222. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.94279/4.93766. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.94174/4.93420. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.94256/4.93394. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.94210/4.93131. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.94075/4.93028. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.94053/4.93366. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.94066/4.92591. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.93801/4.93491. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.93568/4.93209. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.94142/4.93058. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.93947/4.92663. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.93358/4.93119. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.93898/4.92518. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.93510/4.92348. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.93719/4.92807. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.93251/4.92571. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.93237/4.93191. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.93721/4.92809. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.92972/4.92272. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.92876/4.93107. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.93342/4.93048. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.92692/4.93128. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.92736/4.93268. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.93023/4.93150. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.93066/4.93246. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.92784/4.93636. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.92094/4.95752. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.92730/4.93083. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.92376/4.93712. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.91856/4.95336. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.92815/4.94655. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.92326/4.95976. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.91582/4.95192. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.91975/4.97449. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.92196/4.96800. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.91602/4.97707. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.91301/4.96189. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.92177/4.94703. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.91443/4.95805. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.91778/4.96253. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.91169/4.97166. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.90944/4.99276. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.91311/4.96049. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.91556/4.94893. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.90972/4.97533. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.91034/5.00460. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.91273/4.98938. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.91079/4.96723. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.91108/4.96387. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.90589/4.97905. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.91342/4.99035. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.90923/4.97828. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.91609/4.98402. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.91073/4.96260. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.90921/4.97946. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.90327/5.01224. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.90107/4.97998. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.90588/5.00640. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.90658/4.97148. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.89662/4.99976. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.91216/5.00959. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.90486/4.98681. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.90043/5.03900. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.90414/4.98827. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.90780/4.96272. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.90035/4.99619. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.91068/4.96365. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.90061/5.00350. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.89978/4.97354. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.89954/5.01076. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.90480/4.98593. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.89584/5.02466. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.90045/5.01205. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.90259/4.99329. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.89395/4.98839. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.90878/4.97716. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.90569/4.97785. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.90234/4.99673. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.91652/4.99264. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.91194/4.97487. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.90418/4.99606. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.91237/4.98050. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.90862/4.96929. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.90443/4.97013. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.90565/4.98481. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.90340/4.96944. Took 0.08 sec\n",
      "ACC: 0.65625, MCC: 0.3110917000380287\n",
      "Epoch 0, Loss(train/val) 4.91778/4.82321. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.84447/4.82283. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.84212/4.82167. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.83826/4.82394. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.82777/4.82667. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.83013/4.82817. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.83207/4.82906. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.82711/4.82830. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.82382/4.83515. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.82175/4.83586. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.82238/4.83591. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.82434/4.83321. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.81724/4.83769. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.81785/4.84232. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.81456/4.83720. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.81424/4.84175. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.80933/4.83769. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.81464/4.82726. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.81305/4.84368. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.80965/4.84208. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.80998/4.84709. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.80921/4.84260. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.80548/4.85213. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.80040/4.86665. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.80679/4.85414. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.80272/4.87439. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.80636/4.86248. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.80686/4.85499. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.80548/4.87298. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.79637/4.88480. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.80537/4.85966. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.80428/4.87123. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.80158/4.89206. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.80314/4.87623. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.79902/4.89528. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.80789/4.84931. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.80024/4.88381. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.80114/4.86764. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.80188/4.88163. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.80142/4.90002. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.79603/4.88992. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.79652/4.89344. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.79648/4.87173. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.80157/4.89621. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.80210/4.89584. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.80120/4.89543. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.80292/4.88793. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.79569/4.90230. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.79404/4.90411. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.79372/4.91535. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.79712/4.90254. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.79591/4.90509. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.79523/4.89469. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.79315/4.90159. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.79703/4.88834. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.79019/4.93570. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.79672/4.89315. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.79173/4.92657. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.79185/4.90374. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.78547/4.93654. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.79176/4.92262. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.79093/4.92110. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.79156/4.89491. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.78709/4.93863. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.79109/4.92201. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.78763/4.91803. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.78824/4.91641. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.79424/4.88450. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.79406/4.90513. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.79395/4.90073. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.78560/4.91748. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78673/4.91295. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.79449/4.91164. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.79611/4.90744. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.79126/4.90483. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.78743/4.93086. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78945/4.89346. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78852/4.94565. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.78749/4.92759. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.78140/4.94374. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.78870/4.90309. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.78690/4.91607. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.78918/4.91526. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.79199/4.89954. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.79668/4.89158. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.79262/4.92627. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.78775/4.90010. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.79169/4.91115. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.78684/4.93384. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.78415/4.91092. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.79053/4.90265. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.78385/4.91583. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.78564/4.94352. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.78805/4.91306. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.78710/4.91544. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.77951/4.94208. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.79122/4.89582. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.77999/4.93899. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.78065/4.95622. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.77343/4.94347. Took 0.08 sec\n",
      "ACC: 0.390625, MCC: -0.2057101528827944\n",
      "Epoch 0, Loss(train/val) 4.87869/4.87533. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.83310/4.86633. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 4.83275/4.88290. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.83214/4.87811. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.83417/4.87701. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.83931/4.85513. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.84219/4.83091. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.84067/4.82868. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.83265/4.83063. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.83098/4.83239. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.83083/4.83023. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.83052/4.82766. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.82892/4.82873. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.82963/4.82723. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.82873/4.84258. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.83264/4.83109. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.83649/4.83255. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.83234/4.83336. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.83331/4.83501. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.83123/4.83534. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.82881/4.83885. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.82891/4.84253. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.82977/4.83512. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.82652/4.83991. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.82564/4.83942. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.82838/4.83530. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.82452/4.84215. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82216/4.83861. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.82563/4.83573. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.82567/4.84259. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.82841/4.83760. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.82305/4.84079. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.82430/4.84035. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.82187/4.84098. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.82524/4.84128. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.82140/4.84791. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.81995/4.83951. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.81698/4.84792. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.82111/4.83892. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.82411/4.83978. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.82073/4.84630. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.82006/4.85969. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.82265/4.85091. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.82142/4.85499. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.81954/4.84950. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.81808/4.85687. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.81593/4.86595. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.82540/4.84287. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.81850/4.84483. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.81876/4.83499. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.81562/4.85322. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.82453/4.82880. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.82493/4.83882. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.82664/4.82365. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.82603/4.83125. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.82379/4.83517. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.82142/4.84796. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.81889/4.85314. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.82199/4.84053. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.81753/4.85072. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.82095/4.84426. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.82594/4.83703. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.81991/4.85151. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.81491/4.85546. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.81931/4.83606. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.81478/4.85810. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.81588/4.84827. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.81431/4.85411. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.81762/4.84970. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.81100/4.87642. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.81427/4.85217. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.80918/4.89620. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.81781/4.86099. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.81018/4.87900. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.80766/4.86867. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.80909/4.86768. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.81104/4.88427. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.81124/4.86607. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.81198/4.87463. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.80688/4.89147. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.80820/4.87073. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.80820/4.88655. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.80640/4.88454. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.81309/4.86298. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.80659/4.87707. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.81296/4.86096. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.80525/4.87861. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.81062/4.87307. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.80602/4.88530. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.80729/4.87244. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.81221/4.91446. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.80885/4.85740. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.81360/4.87466. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.80856/4.89025. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.80532/4.89616. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.79965/4.90450. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.80441/4.91513. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.80285/4.90077. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.80239/4.88764. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.80292/4.89752. Took 0.08 sec\n",
      "ACC: 0.59375, MCC: 0.1814627428602745\n",
      "Epoch 0, Loss(train/val) 5.17934/5.07421. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.08823/5.08423. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.08922/5.10609. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.08674/5.11310. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.08591/5.10334. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.08446/5.09932. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 5.08229/5.09909. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.07744/5.10434. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.07599/5.10726. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.07374/5.11545. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.07516/5.12295. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.07496/5.10570. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.06967/5.11082. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.07093/5.11890. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.06843/5.12983. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.06971/5.12798. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.06766/5.12649. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.06637/5.11997. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.06136/5.12570. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.06005/5.12187. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.05972/5.12756. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 5.06249/5.12816. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.06035/5.14110. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.05722/5.12898. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.05652/5.14039. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.06172/5.13343. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.05496/5.13434. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.05510/5.13140. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.05761/5.13705. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.05786/5.13305. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.05457/5.14566. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.05490/5.13485. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.04890/5.14323. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.05677/5.13857. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.05058/5.12842. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.05101/5.13555. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.04963/5.14098. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.05144/5.13377. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.04937/5.13794. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.05658/5.13648. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.05286/5.13063. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.04831/5.13669. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.04486/5.15379. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.05058/5.13963. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.04558/5.12908. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.04706/5.14839. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.05024/5.12822. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.04706/5.13463. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.05245/5.13382. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.04525/5.14143. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.04941/5.13349. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.04729/5.13098. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.04171/5.15118. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.04497/5.13827. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.04482/5.13352. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.05082/5.14208. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.04195/5.15650. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.04357/5.13895. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.04829/5.13897. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.03445/5.14776. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.04327/5.14969. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.03831/5.14917. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.03932/5.13272. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.04319/5.14790. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.04234/5.15848. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.03452/5.15695. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.03823/5.15684. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.03211/5.17572. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.03814/5.15320. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.04478/5.14765. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.04348/5.15034. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.03827/5.15551. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.03687/5.16401. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.03565/5.14796. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.03876/5.14735. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.03543/5.14907. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.03765/5.16536. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.03784/5.15478. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 5.06337/5.08910. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 5.05729/5.13526. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.04503/5.13713. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.03889/5.15759. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.03520/5.16155. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.03757/5.16006. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.03659/5.16148. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.03420/5.17758. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 5.03472/5.17010. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 5.03601/5.15643. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 5.03356/5.16346. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 5.02864/5.18827. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 5.03123/5.16301. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.03595/5.16169. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.03931/5.17962. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 5.03957/5.17293. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.03282/5.16600. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.03287/5.15914. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.02403/5.16947. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.03153/5.15285. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.03052/5.18322. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.03016/5.17475. Took 0.08 sec\n",
      "ACC: 0.59375, MCC: 0.19136555680572745\n",
      "Epoch 0, Loss(train/val) 4.90027/4.85860. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.86597/4.85816. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.86360/4.86167. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.86574/4.86765. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.86378/4.86351. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.86339/4.85886. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.85919/4.85561. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.85759/4.85639. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.85933/4.85731. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.85662/4.85625. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.85830/4.85762. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.85702/4.85885. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85622/4.86028. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.85711/4.86028. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.85743/4.86135. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.85771/4.86045. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.85470/4.86094. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.85653/4.86161. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.85562/4.86222. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85805/4.84839. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.85680/4.86202. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.85669/4.85946. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.85611/4.85839. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.85486/4.85830. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.85494/4.85707. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.85352/4.85275. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.85291/4.85613. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.85322/4.86193. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.85105/4.86564. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.85157/4.87097. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.85358/4.86598. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.85163/4.87396. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84908/4.87316. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.84845/4.88010. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.84917/4.87917. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84675/4.87565. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84734/4.88701. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84966/4.87829. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84457/4.89254. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84407/4.91082. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84717/4.89900. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.84349/4.90586. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84103/4.91682. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.83824/4.93004. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.84211/4.91625. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.83890/4.93081. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.83908/4.92322. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.84126/4.92308. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84085/4.91848. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.83681/4.92031. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.83670/4.94402. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.83605/4.93683. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.84200/4.92607. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.83663/4.95687. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.84174/4.92224. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.82968/4.95270. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.83490/4.93992. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.83188/4.94903. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.83345/4.94946. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83178/4.95263. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.83130/4.95494. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83691/4.92435. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.83286/4.96705. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.82976/4.95224. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.83094/4.96051. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.82941/4.97528. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.83375/4.95462. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.82913/4.96807. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.82894/4.96704. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.82734/4.98229. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.82878/4.96448. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.82643/4.95268. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83391/4.96299. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.82579/4.97173. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83063/4.97379. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.82664/4.96281. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.83351/4.95489. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.82526/4.95589. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.82342/4.97999. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.83505/4.93093. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.82451/4.98045. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.82872/4.93765. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.82553/4.96082. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.81704/4.95574. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83044/4.96563. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.82497/4.95395. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.82424/4.94776. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.81873/4.96566. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.82316/4.97058. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.82206/4.95113. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.82145/4.97232. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.81837/4.94467. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.82487/4.95016. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.82788/4.93020. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.82220/4.98716. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.82594/4.95042. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.81968/4.95122. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.82265/4.96333. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.82090/4.95495. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.81819/4.98600. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.03126526997403612\n",
      "Epoch 0, Loss(train/val) 4.89438/4.89249. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.87659/4.85239. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.86503/4.85205. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.87226/4.85514. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.87408/4.88100. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.86783/4.89116. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.85936/4.87746. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.85677/4.87311. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.85627/4.88008. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.86061/4.88684. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.85827/4.88545. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.85543/4.88337. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85561/4.89022. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.85363/4.88579. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.85617/4.89003. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.85319/4.88454. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.85165/4.88659. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.85358/4.89547. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.85393/4.88781. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85195/4.88789. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.85037/4.89450. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.85147/4.89186. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.85035/4.88888. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.85062/4.89604. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.85053/4.88929. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.85113/4.89103. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.85011/4.88531. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.85118/4.89811. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.85238/4.88500. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.85043/4.88185. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.84937/4.88246. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.84956/4.87815. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.85214/4.88058. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.84908/4.89013. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.84971/4.88514. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84840/4.88434. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84676/4.89152. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84520/4.89346. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84698/4.89325. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84581/4.88967. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.84889/4.88716. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.84536/4.89971. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84607/4.89556. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.84718/4.89097. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.84793/4.89231. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.84113/4.88267. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.84388/4.90417. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.84669/4.89818. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84379/4.90496. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.84030/4.89137. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.84566/4.89697. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.84372/4.90712. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.84291/4.90202. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.84378/4.90148. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.84228/4.90869. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.84377/4.90209. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.84155/4.90758. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.84320/4.91902. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.84163/4.90312. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.84325/4.89370. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.84852/4.89538. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.84477/4.90742. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.84314/4.91688. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.84186/4.90451. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.84349/4.92839. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.84284/4.90496. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.83703/4.91428. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.84132/4.90836. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.84301/4.92027. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.84629/4.88155. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.84442/4.89282. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.83890/4.91505. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83855/4.91459. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.83659/4.92776. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.84091/4.90478. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83621/4.91989. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.83612/4.90914. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.84066/4.90973. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.84119/4.89541. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.83668/4.92016. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83960/4.91776. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.83483/4.92421. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.83844/4.90447. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.83426/4.92521. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83783/4.92664. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83513/4.91645. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83436/4.92591. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.83489/4.92215. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83411/4.93243. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.83734/4.92069. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.83270/4.92037. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.83885/4.89468. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83548/4.92311. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83568/4.91391. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.83865/4.92953. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.83348/4.91893. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.83218/4.93362. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.83318/4.90945. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83413/4.93357. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.83032/4.92782. Took 0.09 sec\n",
      "ACC: 0.46875, MCC: -0.040082172520323485\n",
      "Epoch 0, Loss(train/val) 4.85507/4.84081. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.83716/4.84701. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.83548/4.82190. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.84020/4.80060. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.85237/4.79992. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.84884/4.82891. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.83055/4.81463. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.83174/4.80792. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.83264/4.80686. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.83253/4.80521. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.83226/4.80346. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.83078/4.80188. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.82815/4.79924. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.82866/4.79800. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 4.82728/4.79237. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.82934/4.80090. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.82525/4.79252. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.82511/4.80458. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.82410/4.80352. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.82255/4.78991. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.82594/4.80041. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.82220/4.79579. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.82210/4.79593. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.82052/4.80285. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.82266/4.80100. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.81971/4.79215. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.82592/4.80963. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82004/4.80069. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.81532/4.79055. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.81917/4.79429. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.81860/4.80623. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.81723/4.79846. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.81874/4.80073. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.81383/4.80183. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.81413/4.80315. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.81859/4.80422. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.81594/4.79654. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.81640/4.80472. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.81178/4.79973. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.81571/4.80017. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.81439/4.80705. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.81126/4.79971. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.81691/4.80536. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.81280/4.80218. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.81176/4.79940. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.81285/4.79886. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.81113/4.80490. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.81130/4.80552. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.81185/4.80011. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.81802/4.81052. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.81043/4.79524. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.81140/4.79365. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.81356/4.80640. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.81025/4.79896. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.81134/4.79817. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.81187/4.80069. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.80942/4.79753. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.81381/4.80110. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.80924/4.80120. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.80910/4.80443. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.81243/4.80565. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.81149/4.80243. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.80848/4.80564. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.80938/4.79591. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.81342/4.80467. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.80666/4.79294. Took 0.07 sec\n",
      "Epoch 66, Loss(train/val) 4.80965/4.80492. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.80878/4.80528. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.80851/4.79520. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.81022/4.80518. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.80776/4.80319. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.80227/4.79036. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.81269/4.80209. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.80488/4.79389. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.80125/4.79188. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.80611/4.81447. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.80502/4.80012. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.80715/4.79424. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 4.80204/4.79365. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.81012/4.79757. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.80408/4.78807. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.80129/4.78528. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.80078/4.79242. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.80767/4.79813. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.80197/4.79402. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.81157/4.80902. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.80258/4.79618. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.80489/4.78523. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.80297/4.79652. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.80079/4.78692. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.80505/4.79256. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.79986/4.79466. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.80440/4.79582. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.79897/4.80890. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.79952/4.78284. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.80862/4.80056. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.80809/4.82493. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.80907/4.80769. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.80923/4.80251. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.79658/4.77734. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.02757337634153148\n",
      "Epoch 0, Loss(train/val) 4.59907/4.52863. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.55318/4.54742. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.56390/4.54611. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.56419/4.54058. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.56277/4.54792. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.55519/4.54855. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.55404/4.54332. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.55225/4.54383. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.55262/4.54892. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.55190/4.54705. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.55223/4.54583. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.54988/4.54906. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.55330/4.54564. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.54918/4.54806. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.54770/4.54997. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.54978/4.54601. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.54969/4.54812. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.54769/4.54523. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.54687/4.54844. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.54840/4.54360. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.54338/4.54543. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.54230/4.54795. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.54245/4.55091. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.54109/4.55135. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.53838/4.54994. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.54348/4.55286. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.54065/4.55263. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.53853/4.55448. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.53559/4.56123. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.54012/4.56483. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.54821/4.55162. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.53776/4.55201. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.53798/4.55631. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.54610/4.54847. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.54208/4.54740. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.53550/4.54110. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.53725/4.54579. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.54179/4.55114. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.53749/4.54780. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.53450/4.54677. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.53352/4.55576. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.53649/4.55306. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.53694/4.55017. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.53903/4.55246. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.53534/4.56036. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.53630/4.56159. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.53455/4.55444. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.53409/4.56070. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.53475/4.55583. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.53670/4.56466. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.53237/4.56155. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.53082/4.55723. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.53207/4.55547. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.53879/4.55857. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.53660/4.56175. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.53718/4.55973. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.53116/4.55929. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.53095/4.56172. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.53356/4.56074. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.52970/4.56382. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.53753/4.56230. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.53048/4.56164. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.52777/4.56075. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.53719/4.54979. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.52882/4.55482. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.53254/4.55743. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.52862/4.55489. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.53036/4.54700. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.53069/4.54911. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.53000/4.55717. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.52790/4.55130. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.53475/4.54893. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.52902/4.55795. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.52946/4.53997. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.52517/4.55761. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.52857/4.55946. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.53201/4.55217. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.52695/4.55672. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.53119/4.54713. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.53043/4.55223. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.53013/4.55074. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 4.52655/4.54681. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.52808/4.55022. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.52793/4.54848. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.52717/4.54673. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.52315/4.55727. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.52700/4.54822. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.52687/4.54557. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.52503/4.55348. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.52693/4.54912. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.52819/4.55528. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.52729/4.55741. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.52686/4.54251. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.52624/4.54484. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.52303/4.54279. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.52922/4.54944. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.52553/4.54800. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.52452/4.54792. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.52322/4.54764. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.52664/4.54907. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.0931174089068826\n",
      "Epoch 0, Loss(train/val) 4.78917/4.78317. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.78697/4.76993. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.77480/4.78202. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.78026/4.78432. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.78421/4.76859. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.77777/4.76563. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.76865/4.76241. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.76721/4.76186. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.76963/4.76114. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.77067/4.75972. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.76521/4.76256. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.76560/4.76187. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.76296/4.76666. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.76156/4.76748. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.75839/4.77468. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.76152/4.76703. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.76026/4.76069. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.76039/4.75975. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.76268/4.75975. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.75697/4.76612. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.75813/4.76240. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.76440/4.77890. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.75839/4.77076. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.76203/4.77231. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.75762/4.77144. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.75577/4.76233. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.75082/4.77033. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.75133/4.76307. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.75476/4.76600. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.75149/4.76411. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.75262/4.76521. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.74890/4.77528. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.75478/4.76260. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.74825/4.76085. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.75053/4.76569. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.74893/4.76822. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.74577/4.76843. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.74805/4.77231. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.74765/4.76506. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.74628/4.77690. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.74845/4.75011. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.74391/4.77595. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.75151/4.77241. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.74347/4.79587. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.75195/4.76975. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.74340/4.77651. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.74563/4.78965. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.74482/4.78161. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.74487/4.78120. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.73950/4.79934. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.73810/4.79111. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.74607/4.80372. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.74292/4.79685. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.74139/4.78775. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.74597/4.78327. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.73999/4.79245. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.73854/4.80070. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.74505/4.79253. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.73928/4.82693. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.74978/4.77137. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.74102/4.79520. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.74188/4.80326. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.74692/4.79442. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.73709/4.80025. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.74046/4.79346. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.73520/4.80062. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.73488/4.80358. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.73915/4.80646. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.74335/4.79041. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.73389/4.80457. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.74328/4.80839. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.73715/4.80304. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.73376/4.81246. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.73679/4.80435. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.73414/4.80531. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.73754/4.79369. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.73429/4.80847. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.73925/4.79633. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.73681/4.80431. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.73518/4.80692. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.74212/4.78267. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.72878/4.79387. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.73554/4.80031. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.73510/4.79818. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.73701/4.80296. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.73042/4.80753. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.73543/4.79935. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.73883/4.78832. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.73449/4.80238. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.73153/4.80493. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.72844/4.80558. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.73062/4.77793. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.73304/4.79162. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.72973/4.80515. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.74027/4.77868. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.72984/4.80000. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.73469/4.80110. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.73345/4.77773. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.73589/4.79610. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.73216/4.79412. Took 0.08 sec\n",
      "ACC: 0.609375, MCC: 0.19770766067180878\n",
      "Epoch 0, Loss(train/val) 4.73037/4.70977. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.70225/4.70044. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.70296/4.69899. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.70145/4.70200. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.69784/4.69815. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.69956/4.69299. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.69455/4.68756. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.69265/4.68603. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.69116/4.69042. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.69572/4.69254. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.69031/4.68923. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.69340/4.69154. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.69144/4.69389. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.68758/4.69546. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.69169/4.69492. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.69023/4.69451. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.69080/4.69156. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.68705/4.69606. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.68991/4.69346. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.68682/4.70142. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.68306/4.70093. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.68563/4.69995. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.68244/4.69464. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.68491/4.67329. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.68791/4.69071. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.68872/4.68620. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.69199/4.68553. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.68223/4.69481. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.68458/4.69112. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.68878/4.69044. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.68983/4.69240. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.68425/4.70987. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.68701/4.70504. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.68562/4.70105. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.68631/4.70069. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.67980/4.70163. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.67750/4.70126. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.67911/4.70610. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.69036/4.70179. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.68999/4.70083. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.68882/4.70385. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.68429/4.70030. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.68563/4.70334. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.68416/4.70314. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.68109/4.70808. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.68188/4.70192. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.67994/4.70421. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.68096/4.68317. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.70522/4.69530. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.69225/4.69058. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.69197/4.68643. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.69123/4.68482. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.68945/4.68535. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.69116/4.68686. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.69021/4.68777. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.69149/4.68646. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.68348/4.67905. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.68708/4.67960. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.68489/4.67735. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.68806/4.70234. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.69058/4.69731. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.69277/4.69522. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.69116/4.69347. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.68780/4.69277. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.68636/4.69620. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.68545/4.69307. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.68138/4.69303. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.68836/4.68845. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.67912/4.69237. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.68421/4.68876. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.68146/4.69701. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.67471/4.69713. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.67867/4.69180. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.67837/4.70047. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.67463/4.70170. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.67739/4.70148. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.67675/4.70200. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.66867/4.71022. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.67602/4.71317. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.67556/4.69868. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.67423/4.70564. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.68086/4.70261. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.66863/4.72129. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.67295/4.71511. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.67318/4.71460. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.66702/4.72278. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.67594/4.70770. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.67145/4.70937. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.66722/4.71530. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.67224/4.71643. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.66829/4.71946. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.66830/4.70192. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.66930/4.73369. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.68134/4.72514. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.67776/4.71190. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.68299/4.69393. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.67813/4.69933. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.67588/4.68841. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.67343/4.69592. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.67318/4.68879. Took 0.08 sec\n",
      "ACC: 0.578125, MCC: 0.1463182264089704\n",
      "Epoch 0, Loss(train/val) 4.82119/4.80474. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.80257/4.80809. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.80552/4.83554. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.80377/4.85409. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.80292/4.80805. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79424/4.79980. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.79194/4.80874. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.79428/4.81040. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.79455/4.81128. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79509/4.81010. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79172/4.80912. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.79047/4.82037. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.79027/4.82439. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.79050/4.82398. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.78927/4.82489. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.78815/4.82637. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78821/4.82482. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.78327/4.82098. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.78916/4.81854. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.78612/4.81932. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.78409/4.82096. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78518/4.81470. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.78390/4.81544. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.78352/4.81763. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.78713/4.81666. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.78352/4.82014. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.78382/4.81779. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.78171/4.83354. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.78123/4.82542. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.78302/4.82812. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.77632/4.82747. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78241/4.82489. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.78123/4.82464. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.77839/4.82145. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.77838/4.82239. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.77692/4.81412. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.77870/4.81591. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.77658/4.82187. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.77979/4.80605. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.78088/4.81411. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.77497/4.83608. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.77511/4.82542. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.77601/4.81802. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.77640/4.81648. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.77453/4.81990. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.77137/4.81483. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77422/4.82299. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77271/4.81133. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77607/4.83302. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.76968/4.82383. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77144/4.82981. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77017/4.81601. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.77190/4.82958. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.77332/4.83936. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.77033/4.83391. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.77002/4.82841. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.77124/4.82900. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.77264/4.84456. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76761/4.82668. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.76942/4.83300. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.76991/4.82636. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.76280/4.83772. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76608/4.83684. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.76477/4.83344. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.76512/4.83457. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.76918/4.82263. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.76941/4.83190. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.76073/4.84184. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.76776/4.82141. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76877/4.83569. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.76365/4.84164. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.76437/4.82702. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76370/4.82299. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.76634/4.82000. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.76207/4.83109. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.76051/4.82448. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.75915/4.84189. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.76389/4.82557. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.76160/4.81981. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.75881/4.82337. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.75875/4.82863. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.75902/4.83993. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.76244/4.80459. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76079/4.81884. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75765/4.81977. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.76280/4.80809. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.75184/4.82847. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.75910/4.81425. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.75788/4.82636. Took 0.07 sec\n",
      "Epoch 89, Loss(train/val) 4.76067/4.83584. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.75711/4.81489. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.75250/4.83009. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.76730/4.79703. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.76229/4.83516. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.75213/4.83760. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.75706/4.82790. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.76086/4.82415. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.75389/4.84183. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.75616/4.83234. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.75271/4.80784. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.08304547985373996\n",
      "Epoch 0, Loss(train/val) 4.74079/4.69700. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.72347/4.72047. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.71438/4.71237. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.71789/4.71610. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.71040/4.71706. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.71204/4.72606. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.71176/4.72239. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.70961/4.71927. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.71055/4.71772. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.70723/4.72052. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.70595/4.72487. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.70915/4.71763. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.70714/4.71956. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.70795/4.71867. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.70533/4.71697. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.71031/4.71635. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.70434/4.71598. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.70429/4.71990. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.70493/4.71953. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.70940/4.71603. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.70582/4.72093. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.70278/4.71719. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.70667/4.72332. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.70583/4.72122. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.70373/4.72925. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.69598/4.74606. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.70635/4.71231. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.70532/4.72871. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.69776/4.72586. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.70132/4.73056. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.69894/4.72333. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.69477/4.72370. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.69741/4.73605. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.69865/4.72630. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.69982/4.73983. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.68970/4.74326. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.69857/4.76324. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.69892/4.74213. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.69387/4.75872. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.69600/4.74424. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.69508/4.76585. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.69836/4.74813. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.69141/4.76237. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.69834/4.75343. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.69718/4.74306. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.70007/4.75524. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.69960/4.75318. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.69571/4.75389. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.69381/4.76403. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.69085/4.76037. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.69247/4.75702. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.69035/4.76155. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.69072/4.76081. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.69106/4.76282. Took 0.07 sec\n",
      "Epoch 54, Loss(train/val) 4.69213/4.76707. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.68977/4.74390. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.69562/4.74404. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.69240/4.74313. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.69182/4.74804. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.69206/4.74800. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.68704/4.74808. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.69254/4.74130. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.69070/4.75614. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.68925/4.75695. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.68748/4.76152. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.68745/4.76000. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.68731/4.75996. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.68611/4.75938. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.68490/4.75222. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.68860/4.76078. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.68445/4.74937. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.68868/4.75265. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.68174/4.74570. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.68281/4.75444. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.68622/4.74912. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.68760/4.75704. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.68815/4.74690. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.68405/4.76022. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.68739/4.75834. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.67908/4.76159. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.68588/4.75814. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.68537/4.76209. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.67825/4.77120. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.68669/4.77425. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.67920/4.77503. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.68537/4.77401. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.68587/4.75594. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.68746/4.76810. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.67790/4.75527. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.68172/4.76417. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.68370/4.75789. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.68291/4.76916. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.68077/4.76400. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.67904/4.75900. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.68293/4.77250. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.68640/4.77522. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.68184/4.78037. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.67199/4.77653. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.67699/4.78373. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.67959/4.75686. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.11555252264060875\n",
      "Epoch 0, Loss(train/val) 4.79631/4.73647. Took 0.18 sec\n",
      "Epoch 1, Loss(train/val) 4.74610/4.73014. Took 0.17 sec\n",
      "Epoch 2, Loss(train/val) 4.74215/4.72916. Took 0.15 sec\n",
      "Epoch 3, Loss(train/val) 4.74337/4.72855. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.73894/4.72973. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.74132/4.73800. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.74088/4.74065. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.73883/4.74500. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.73617/4.74932. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.73646/4.75225. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.73549/4.74972. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.73468/4.75213. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.73092/4.75762. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.73098/4.75246. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.73146/4.75261. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.73293/4.75179. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.73502/4.73965. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.73947/4.72682. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.73265/4.73721. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.73257/4.74875. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.73240/4.75490. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.72845/4.76096. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.73098/4.75853. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.73065/4.76382. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.72463/4.78702. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.72586/4.78305. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.73216/4.76950. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.72401/4.79787. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.73021/4.75653. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.72767/4.78484. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.72415/4.80569. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.72617/4.79237. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.71993/4.81834. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.72925/4.78031. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.72427/4.79453. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.72102/4.80708. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.72034/4.79453. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.71976/4.79494. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.72148/4.79344. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.72382/4.79266. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.72209/4.80107. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.71832/4.80174. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.72165/4.79315. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.72137/4.80412. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.71878/4.84286. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.72165/4.78188. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.71579/4.80822. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.72016/4.81086. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.71516/4.79611. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.71807/4.80336. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.71688/4.83436. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.71993/4.78870. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.71754/4.82355. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.71722/4.80469. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.71561/4.82020. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.71791/4.80274. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.71497/4.82806. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.71388/4.82342. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.71617/4.81540. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.71435/4.82702. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.71948/4.80669. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.71432/4.83546. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.71881/4.79093. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.71295/4.82976. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.71028/4.85561. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.72225/4.79569. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.71416/4.82573. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.71645/4.81352. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.71155/4.82754. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.71526/4.79966. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.71069/4.80874. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.70738/4.82907. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.71370/4.82173. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.71276/4.82710. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.71070/4.83940. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.71490/4.80609. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.71468/4.80398. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.70675/4.83444. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.70929/4.81773. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.71403/4.82917. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.70679/4.85880. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.70455/4.83427. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.71149/4.84102. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.71019/4.79929. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.71184/4.83190. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.70842/4.84965. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.71378/4.80543. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.70836/4.84802. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.70929/4.83198. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.70494/4.85124. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.70688/4.81270. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.70921/4.82953. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.70837/4.81049. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.70634/4.81894. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.71118/4.80516. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.70784/4.82593. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.70648/4.82445. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.70769/4.83763. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.70633/4.82099. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.70854/4.79906. Took 0.09 sec\n",
      "ACC: 0.375, MCC: -0.2519763153394848\n",
      "Epoch 0, Loss(train/val) 5.08709/5.04521. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.03775/5.06510. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 5.03042/5.07800. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.03248/5.07661. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.03406/5.07912. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.03314/5.07228. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.02944/5.07990. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 5.03417/5.07259. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.02863/5.07966. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.03081/5.08584. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.02768/5.08715. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.02883/5.07883. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.02816/5.08598. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.02692/5.07836. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 5.02448/5.08314. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.02815/5.08442. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.02587/5.07976. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.02592/5.07682. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.02458/5.07909. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.02120/5.08707. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.02571/5.09087. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.02444/5.09093. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 5.01965/5.08497. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.01972/5.10642. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.02277/5.08503. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.01877/5.08589. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.01530/5.10498. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.02039/5.11317. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.02550/5.07377. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.01940/5.09228. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.02297/5.08709. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.01516/5.10464. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.02060/5.10424. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.01321/5.11610. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.01727/5.10997. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.01863/5.10141. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.01257/5.10219. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.01137/5.11877. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 5.01362/5.11409. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.00976/5.10337. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.01747/5.08149. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.01319/5.09454. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.02199/5.09034. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.02131/5.07004. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.02186/5.07725. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.01611/5.08923. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.01535/5.10251. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.01704/5.08514. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.01506/5.08922. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.01325/5.10900. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.01358/5.10068. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.00581/5.12208. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.00882/5.10115. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.01317/5.10233. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.00681/5.10032. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.01357/5.11365. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.01279/5.10056. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.01079/5.09524. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.00818/5.10169. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.00581/5.11125. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.00682/5.10097. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.00180/5.12071. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.00363/5.13737. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.00990/5.09610. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.00137/5.11704. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.00250/5.09945. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.00535/5.11931. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.00449/5.10452. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.00441/5.11148. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.00605/5.09780. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.00662/5.11478. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.99653/5.11523. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.00665/5.11186. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.00068/5.11260. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.00052/5.10685. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.99723/5.12221. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.00278/5.09255. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.01044/5.08941. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.00325/5.11093. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.00654/5.11594. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.00298/5.12197. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.00013/5.11932. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 5.00078/5.13312. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.00350/5.12059. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.99096/5.12603. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.99972/5.12824. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.99476/5.15917. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.99697/5.15442. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.00102/5.12439. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.99485/5.15326. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.99708/5.12589. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.99738/5.13113. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.00499/5.10309. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.99512/5.12098. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.99890/5.15168. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.99665/5.12716. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.99730/5.12577. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.99816/5.12451. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.00239/5.11309. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.98902/5.12702. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.09202163616785992\n",
      "Epoch 0, Loss(train/val) 4.83168/4.80318. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.79803/4.77275. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.79824/4.77436. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79401/4.77382. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79484/4.77390. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79551/4.77519. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.79314/4.78030. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.79619/4.78537. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.79236/4.80070. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79269/4.80639. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.78934/4.80917. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.78702/4.80926. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.79354/4.80948. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.78415/4.81461. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.79040/4.81936. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.78628/4.82619. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78562/4.82822. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.78560/4.83947. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.78522/4.83676. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.78820/4.83314. Took 0.11 sec\n",
      "Epoch 20, Loss(train/val) 4.78581/4.83867. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78400/4.84294. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.78499/4.84558. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.78168/4.83958. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.78194/4.83844. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.78180/4.84123. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.78045/4.84528. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.78168/4.84438. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77963/4.83471. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.78088/4.83195. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.78325/4.84495. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78353/4.84862. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.78085/4.85380. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78080/4.84683. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.77991/4.85706. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.77537/4.84940. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.78012/4.85417. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.77947/4.84663. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.77941/4.85999. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.77946/4.85969. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.77891/4.86079. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.77841/4.85396. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.77603/4.87961. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.77591/4.87692. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.77746/4.87385. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.77686/4.84913. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77441/4.86806. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77793/4.85969. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77380/4.86722. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.77825/4.85783. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77781/4.87597. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77720/4.86492. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.77303/4.86297. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.77653/4.88300. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.77101/4.87137. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 4.77295/4.87858. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.77239/4.86503. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.77161/4.86755. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.77332/4.86760. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.76988/4.87625. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.77194/4.89162. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.76851/4.89944. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76299/4.89476. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.76536/4.88545. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.76646/4.87950. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.76982/4.87598. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.77421/4.87426. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.76235/4.89818. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.76130/4.88555. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76555/4.88395. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.77057/4.89947. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.77110/4.88715. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76762/4.88779. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.76478/4.89006. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.76078/4.88474. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.76693/4.88041. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.76875/4.88313. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.76498/4.90957. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.76348/4.88505. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.76426/4.88709. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.76232/4.89858. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.76144/4.90153. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.75882/4.89419. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76468/4.90332. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75933/4.89925. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.76065/4.91053. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.75794/4.89889. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.76261/4.88540. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.76202/4.88677. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.75860/4.91710. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.75756/4.94779. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.76006/4.91945. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.76074/4.91259. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.75720/4.90612. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.75882/4.91612. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.76026/4.89709. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.75240/4.93316. Took 0.07 sec\n",
      "Epoch 97, Loss(train/val) 4.75237/4.92209. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.74969/4.91042. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.75643/4.90652. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.0657951694959769\n",
      "Epoch 0, Loss(train/val) 4.67199/4.64251. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.63163/4.62331. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.62538/4.62243. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.62557/4.62382. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.62276/4.62346. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.62158/4.62303. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.62368/4.62172. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.62312/4.62221. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.62129/4.62222. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.62243/4.62184. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.62051/4.62159. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.62148/4.62240. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.61865/4.62324. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.61855/4.62377. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.61979/4.62550. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.61657/4.62813. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.61969/4.63151. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.61851/4.63111. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.61799/4.63023. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.61682/4.63401. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.61559/4.63590. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.61529/4.63644. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.61677/4.63836. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.61935/4.63704. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.61719/4.63384. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.61754/4.63716. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.61597/4.63632. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.61557/4.61655. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.61580/4.62479. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.61636/4.62669. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.61091/4.63284. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.61221/4.64077. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.61889/4.62504. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.61249/4.62905. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.61209/4.63040. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.61405/4.62955. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.61309/4.63735. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.61507/4.63149. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.61489/4.62304. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.61347/4.62610. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.61053/4.63214. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.61309/4.62817. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.60767/4.63310. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.60563/4.63343. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.60862/4.63231. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.60722/4.62770. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.61134/4.62713. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.60372/4.63744. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.60953/4.62861. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.60690/4.62634. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.61050/4.64962. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.60418/4.63201. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.60856/4.63027. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.60433/4.62697. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.60504/4.62955. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.60452/4.63296. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.60294/4.63598. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.60753/4.63807. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.60416/4.63504. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.60743/4.63133. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.60235/4.62912. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.60071/4.64560. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.60421/4.62454. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.60045/4.64686. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.59777/4.64091. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.60479/4.63240. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.60489/4.64407. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.59902/4.64289. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.59579/4.64991. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.60499/4.63796. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.59907/4.63434. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.60201/4.64458. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.59871/4.63893. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.60143/4.63746. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.60997/4.65038. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.60763/4.64879. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.60712/4.64371. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.60260/4.65278. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.60359/4.65538. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.59933/4.63982. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.59992/4.62756. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.60061/4.62514. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.59487/4.65396. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.62244/4.65485. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.61783/4.62996. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.61269/4.63038. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.61638/4.64136. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.61084/4.64280. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.61070/4.64731. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.60729/4.65733. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.60764/4.66538. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.60923/4.66483. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.60576/4.66775. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.60839/4.66122. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.60875/4.65847. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.60327/4.66866. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.60746/4.65792. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.60412/4.66823. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.60769/4.65778. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.60262/4.66113. Took 0.08 sec\n",
      "ACC: 0.640625, MCC: 0.2596919512521956\n",
      "Epoch 0, Loss(train/val) 4.77664/4.75784. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.76567/4.77405. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.77258/4.74577. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.75989/4.74978. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.74963/4.74344. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.75340/4.74410. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.75220/4.74515. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.75006/4.74860. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.75083/4.75595. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.74876/4.75739. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.74786/4.75981. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.74651/4.76647. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.74312/4.74970. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.74236/4.75853. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.74165/4.76354. Took 0.07 sec\n",
      "Epoch 15, Loss(train/val) 4.74221/4.75400. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.73926/4.76031. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.74079/4.74977. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.73717/4.75421. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.74050/4.75021. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.73484/4.76837. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.75284/4.74931. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.74995/4.74672. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.74201/4.76206. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.73976/4.75477. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.73988/4.74894. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.74243/4.77289. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.73996/4.75068. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.73500/4.76039. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.73782/4.75054. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.73569/4.75997. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.73648/4.75204. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.73561/4.75804. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.73696/4.75243. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.74638/4.75665. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.74093/4.75388. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.73508/4.75703. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.73569/4.76699. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.73553/4.76270. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.73787/4.76437. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.73574/4.76324. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.73049/4.76673. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.73128/4.76845. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.73377/4.76298. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.73027/4.77506. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.73089/4.77318. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.73239/4.75907. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.73176/4.78686. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.73659/4.74851. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.73476/4.75496. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.73743/4.78040. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.73326/4.75923. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.73238/4.78127. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.73221/4.78008. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.73261/4.77479. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.72922/4.76650. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.73002/4.77392. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.72960/4.78090. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.73108/4.77242. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.72986/4.78587. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.72759/4.78102. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.72747/4.78360. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.72928/4.78350. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.72954/4.78100. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.72859/4.75063. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.73184/4.74919. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.73330/4.75311. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.72604/4.76152. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.72977/4.79367. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.73028/4.79433. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.72334/4.79860. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.72172/4.79398. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.72672/4.79124. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.72731/4.78962. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.72770/4.79331. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.72990/4.79116. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.72872/4.78731. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.72655/4.78987. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.72567/4.78838. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.72941/4.78105. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.72053/4.79436. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.72696/4.78880. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.73027/4.77814. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.72094/4.78384. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.72167/4.79041. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.72982/4.78713. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.72674/4.79424. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.71962/4.81205. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.73108/4.78359. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.72405/4.80486. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.72454/4.79969. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.72176/4.80130. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.72170/4.80308. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.72069/4.78825. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.72286/4.79946. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.72536/4.80250. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.71912/4.80051. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.71994/4.79516. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.71661/4.81038. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.72012/4.81489. Took 0.08 sec\n",
      "ACC: 0.625, MCC: 0.2805752076422644\n",
      "Epoch 0, Loss(train/val) 4.58051/4.53012. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.49820/4.51086. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.49541/4.49845. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.49417/4.49283. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.49393/4.49009. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.49270/4.49018. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.49258/4.48884. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.49097/4.48919. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.49251/4.48815. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.49067/4.48401. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.49095/4.48755. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.48973/4.47919. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.48850/4.48148. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.49230/4.48035. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.49005/4.48159. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.48960/4.48358. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.48593/4.47976. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.48778/4.48360. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.48722/4.48674. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.48563/4.48987. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.48520/4.48009. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.48298/4.48188. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.48246/4.47866. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.48018/4.47814. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.48558/4.47580. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.48032/4.49017. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.48371/4.47940. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.47992/4.48357. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.47919/4.47750. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.47700/4.47357. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.48122/4.47870. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.47832/4.46965. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.47733/4.47237. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.47934/4.47765. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.47662/4.47100. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.47555/4.46553. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.47505/4.47549. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.47320/4.47448. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.47550/4.46352. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.47284/4.47000. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.47065/4.47364. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.47740/4.47032. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.46900/4.46702. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.47044/4.47343. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.47302/4.48098. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.47798/4.47533. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.47501/4.47090. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.46773/4.47515. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.46942/4.47219. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.47287/4.47313. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.46552/4.47548. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.46123/4.49090. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.47128/4.46946. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.46326/4.47264. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.46402/4.48814. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.46512/4.48650. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.46871/4.48602. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.46504/4.48124. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.46436/4.48777. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.46569/4.51378. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.46111/4.51276. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.46098/4.51026. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.46225/4.50683. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.46086/4.50582. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.46949/4.49623. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.49224/4.47983. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.48696/4.48285. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.48263/4.48440. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.48317/4.48397. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.48313/4.49010. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.47491/4.49133. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.47842/4.48101. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.47663/4.46488. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.47794/4.47851. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.48054/4.48033. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.47422/4.47606. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.47775/4.48841. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.47406/4.48343. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.47418/4.49614. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.47199/4.47882. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.47308/4.49393. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.47048/4.46637. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.47619/4.48346. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.46619/4.49136. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.46549/4.48485. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 4.46624/4.46865. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.46968/4.49949. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.46672/4.50000. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.46456/4.50574. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.46182/4.51588. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.46215/4.50241. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.46570/4.53103. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.46807/4.50308. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.45956/4.52028. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.46616/4.52563. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.46878/4.50600. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.45932/4.52016. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.45970/4.56928. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.46569/4.54436. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.46266/4.52513. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.0778902520624173\n",
      "Epoch 0, Loss(train/val) 4.91328/4.90053. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.87566/4.90669. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.87520/4.92034. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.88038/4.92080. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.87556/4.93456. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.87532/4.93700. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.87491/4.91817. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.87221/4.91238. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.86606/4.91381. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.86996/4.91255. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.86491/4.91610. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.86799/4.91546. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.86737/4.91507. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.86990/4.90984. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.86485/4.90847. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.86202/4.90701. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.86617/4.93592. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.87617/4.90161. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.87043/4.89459. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.86981/4.89789. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.87005/4.90084. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.86814/4.90522. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.86847/4.90749. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.86344/4.89449. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.86011/4.90843. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.86487/4.90561. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.86992/4.90575. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.86418/4.88937. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.86338/4.89387. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.86909/4.89404. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.86345/4.90751. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.86461/4.90282. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.86249/4.93413. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.86092/4.90499. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.86375/4.91659. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.86601/4.89878. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.85965/4.88974. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.85744/4.89375. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86043/4.88599. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.85968/4.89995. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.85856/4.90015. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.86411/4.87491. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.85650/4.88720. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.85510/4.89264. Took 0.07 sec\n",
      "Epoch 44, Loss(train/val) 4.85746/4.88673. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.85216/4.89180. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.85425/4.88291. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.85328/4.87719. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.85326/4.87944. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.85396/4.87337. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.84959/4.88180. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.85116/4.87762. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.85640/4.88618. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.84953/4.87923. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.84819/4.88524. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.84944/4.88075. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.85008/4.88310. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.84919/4.88056. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.84437/4.88032. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.84424/4.88346. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 4.84861/4.86259. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 4.85078/4.88285. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.84634/4.88485. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.84702/4.87809. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.84733/4.87941. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.84803/4.88202. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.84742/4.87526. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.84885/4.88187. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.84725/4.87750. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.84846/4.88455. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.84128/4.88479. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.84194/4.89567. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.84633/4.87925. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.84418/4.89084. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.84539/4.89717. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.84945/4.87926. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.84445/4.87407. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.84485/4.87067. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.84936/4.88428. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.84541/4.88188. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.84776/4.89019. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.83960/4.90154. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.84531/4.88525. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.84076/4.88026. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.84257/4.87669. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83880/4.88432. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.84563/4.87756. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.84552/4.87839. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.84043/4.88165. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.84029/4.87588. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.84374/4.88344. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.84795/4.88349. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.84258/4.87982. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83911/4.88070. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.84038/4.87900. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.84074/4.88629. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.84206/4.88316. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.84225/4.88130. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.84082/4.87428. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.84255/4.89698. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.09607689228305229\n",
      "Epoch 0, Loss(train/val) 5.11791/5.08256. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.09046/5.09836. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.09224/5.08949. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.08877/5.09087. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.08260/5.08850. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.08974/5.09019. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.09188/5.09288. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.08527/5.10999. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.08477/5.11582. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.08636/5.11728. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.08471/5.11976. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 5.08345/5.11977. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.07980/5.11804. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.08097/5.12379. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.08502/5.11359. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.08343/5.12641. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.08179/5.12239. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.07899/5.13078. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 5.08373/5.12565. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.07812/5.12251. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.07655/5.13397. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.07543/5.12140. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.07599/5.16129. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.07961/5.12101. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.07983/5.14746. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.07350/5.13202. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.07070/5.16315. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.07436/5.14332. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.07129/5.15234. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.07085/5.15693. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 5.06899/5.14387. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.06783/5.15592. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.06478/5.11743. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.07442/5.12886. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.06620/5.14367. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.07585/5.14039. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.06552/5.14611. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.06664/5.15455. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.06757/5.12852. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.06458/5.16089. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.06503/5.16162. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.06894/5.11246. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.08063/5.12815. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.08026/5.12786. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.07348/5.13956. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 5.07407/5.14190. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.07096/5.13670. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.07236/5.13978. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.07117/5.12060. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.07331/5.13847. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.06465/5.16649. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.07457/5.11370. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 5.06850/5.17401. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.06681/5.11027. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.05889/5.15839. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.06588/5.15579. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.06636/5.14986. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.05909/5.15660. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.05809/5.17440. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.06566/5.16637. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.07021/5.10992. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.07329/5.12840. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.07048/5.13326. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.06897/5.12852. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.06270/5.12950. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.07925/5.11859. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.06920/5.13160. Took 0.07 sec\n",
      "Epoch 67, Loss(train/val) 5.06889/5.14277. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.05861/5.12506. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.06125/5.16196. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.05860/5.16784. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.05493/5.15019. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.05279/5.18261. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.05748/5.16080. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.05309/5.15970. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.05570/5.15785. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.05144/5.14693. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.08196/5.12097. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.07094/5.12187. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.06508/5.10681. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.05827/5.12844. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.05771/5.13676. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.05374/5.15251. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.05020/5.12827. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.06060/5.11901. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.05059/5.10567. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.05677/5.12487. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.05243/5.14515. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.05621/5.11259. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.05187/5.14195. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 5.05726/5.13428. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.04359/5.15424. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.05956/5.13096. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.04904/5.11963. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.05030/5.17352. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.05524/5.12741. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.05481/5.12184. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.05173/5.13624. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.05404/5.12208. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.05749/5.11280. Took 0.08 sec\n",
      "ACC: 0.609375, MCC: 0.22604490834610982\n",
      "Epoch 0, Loss(train/val) 4.91175/4.90069. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.90019/4.91082. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.89908/4.93040. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.89978/4.93485. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.89546/4.90922. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.89347/4.89551. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.88657/4.89881. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.88836/4.90361. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.88782/4.90965. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.88802/4.91355. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.88837/4.91280. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.88731/4.92225. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.88794/4.92083. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.88303/4.91877. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.88026/4.93219. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.88046/4.94759. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.88023/4.93741. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.88014/4.93249. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.87419/4.94397. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.87754/4.93468. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.87658/4.93422. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.87581/4.93507. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.88198/4.91482. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.87562/4.93102. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.87524/4.95003. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.87454/4.94850. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.87922/4.93555. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.87228/4.95973. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.87544/4.94668. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.87126/4.96389. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.87372/4.95846. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.87422/4.94473. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.86823/4.96036. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.86590/4.98004. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.87085/4.97171. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.87101/4.94803. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.86503/4.98475. Took 0.07 sec\n",
      "Epoch 37, Loss(train/val) 4.87589/4.92761. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86273/4.98609. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.86953/4.95697. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.86860/4.94765. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.86785/4.95687. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.86395/4.99212. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.86807/4.95175. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.86287/4.98005. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.86821/4.94081. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.86583/4.95417. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.86761/4.93639. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.86339/4.96554. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.86115/4.98448. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.86314/4.95614. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.86211/4.98953. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.86193/4.97211. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.86007/4.97362. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.86588/4.95601. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.86320/4.95139. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.86133/4.94840. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.86188/4.96324. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.85869/4.97551. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.86030/4.99356. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.86117/4.96435. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.85987/4.98848. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.86393/4.95165. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.86302/4.94987. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.85641/4.97624. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.85642/4.96113. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.85749/4.96418. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.85711/4.96888. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.86167/4.94874. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.86281/4.98030. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.86483/4.95158. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.85638/4.99448. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.85810/4.94569. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.85240/4.95467. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.85662/4.97451. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.85615/4.95222. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.85943/4.99577. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.85686/4.97241. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.85988/4.99563. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.86036/4.96364. Took 0.07 sec\n",
      "Epoch 80, Loss(train/val) 4.85730/4.98252. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.85628/4.96521. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.85476/4.98705. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.86588/4.96371. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.85562/4.98878. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.85505/4.99992. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.85431/4.96645. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.87586/4.91029. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.87944/4.91472. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.86911/4.93969. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.86990/4.96650. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.87053/4.94646. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.85929/4.98424. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.86663/4.96498. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.86438/4.96350. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.86375/4.96239. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.86803/4.95696. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.86164/4.97601. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.86608/4.95690. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.86592/4.94041. Took 0.08 sec\n",
      "ACC: 0.40625, MCC: -0.18547738582708967\n",
      "Epoch 0, Loss(train/val) 4.93859/4.82941. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.84253/4.85290. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.83019/4.85249. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.82772/4.84520. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.82890/4.84906. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.82834/4.85168. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.82764/4.86074. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.82544/4.85851. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.82567/4.86467. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.82502/4.85425. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.82760/4.86984. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.82316/4.87226. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.82033/4.88642. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.82463/4.87720. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.82247/4.87609. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.82047/4.88680. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.81693/4.88759. Took 0.07 sec\n",
      "Epoch 17, Loss(train/val) 4.81896/4.88495. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.81737/4.88200. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.81940/4.88669. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.82136/4.88201. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.81725/4.88486. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.81240/4.89340. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.82136/4.87997. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.81662/4.89033. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.81556/4.90373. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.81495/4.90942. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.81416/4.89158. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.81337/4.90126. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.81292/4.90041. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.81409/4.89913. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.81054/4.90333. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.80851/4.90376. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.80662/4.91346. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.80513/4.91218. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.80396/4.90192. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.80583/4.91347. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.80459/4.92075. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.80721/4.90834. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.80781/4.91781. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.80292/4.92749. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.80081/4.92352. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.80190/4.94371. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.80063/4.94369. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.80549/4.93715. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.79997/4.95091. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.80244/4.91322. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.80231/4.91871. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.79762/4.92463. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.80296/4.91920. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.79758/4.93207. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.79527/4.95157. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.79224/4.94558. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.79479/4.96033. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.79662/4.93602. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.79602/4.97448. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.79496/4.95532. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.79636/4.95498. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 4.79602/4.92512. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.79992/4.96185. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.79120/4.96493. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.79348/4.95076. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.79122/4.97598. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.79219/4.94975. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.79124/4.98721. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.79279/4.97352. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.79445/4.98624. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.79339/4.99097. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.79448/4.94439. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.79390/4.92597. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.79712/4.98185. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78742/4.99774. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.79506/4.94377. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.79423/4.94812. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.78066/4.95899. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.79476/4.96764. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.79259/4.98930. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78310/4.95270. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.79771/4.91174. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.81022/4.88983. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.80777/4.89161. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.80251/4.93498. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.79742/4.95395. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.79680/4.94054. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.79111/4.95511. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.78630/4.97103. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.79016/4.96461. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.79010/4.96543. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.78521/4.98883. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.79028/4.94378. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.78802/4.93996. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.79315/4.97176. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.80115/4.90253. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.79944/4.93509. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.78592/4.98527. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.79424/4.96779. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.78698/4.98580. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.78243/4.99771. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.78519/4.99939. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.77923/4.99707. Took 0.08 sec\n",
      "ACC: 0.578125, MCC: 0.15318083468998522\n",
      "Epoch 0, Loss(train/val) 4.86476/4.84613. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.86705/4.83298. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.84342/4.83272. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.84900/4.83177. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.84772/4.83050. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.84518/4.83399. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.84164/4.83243. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.84008/4.83142. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.83888/4.83217. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.83737/4.83312. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.83683/4.83454. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.83673/4.83544. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.83755/4.83686. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.83456/4.83764. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.83533/4.83744. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.83044/4.84097. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.83407/4.83263. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.83610/4.84502. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.83101/4.84435. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.83266/4.84349. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.83298/4.84479. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.83142/4.84668. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.83232/4.84697. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.82863/4.84756. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.83340/4.84448. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.83081/4.84162. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.82722/4.84119. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82828/4.84736. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.82597/4.85280. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.83206/4.85379. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.82420/4.86181. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.82603/4.86091. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.82349/4.85770. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.82537/4.85430. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.82207/4.85936. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.82206/4.87276. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.82281/4.87084. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.81995/4.86868. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.82296/4.87081. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.82331/4.86426. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.82402/4.85845. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.81955/4.86918. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.82192/4.87205. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.81606/4.88006. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.81782/4.88005. Took 0.07 sec\n",
      "Epoch 45, Loss(train/val) 4.81912/4.88225. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.81689/4.87788. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.81760/4.87918. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.81974/4.88055. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.81388/4.88760. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.81394/4.88981. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.81265/4.88873. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.81787/4.88461. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.81601/4.89819. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.81833/4.88873. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.81696/4.88931. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.81527/4.89737. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.81742/4.88761. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.82138/4.88615. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.81448/4.90038. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.81531/4.88228. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.81043/4.89855. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.81760/4.89234. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.81786/4.89723. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.81093/4.89855. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.80900/4.89735. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.81837/4.88830. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.80640/4.91076. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.80898/4.91015. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.81371/4.91517. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.80864/4.90317. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.80846/4.91892. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.81539/4.89955. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.81104/4.90949. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.80247/4.92216. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.81092/4.91546. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.81595/4.90286. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.81159/4.89977. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.82029/4.86424. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.82418/4.85878. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.81608/4.86573. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.81790/4.88355. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.81179/4.89009. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.81312/4.88743. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.81054/4.90332. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.81495/4.89648. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.80917/4.90286. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.81264/4.90309. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.80920/4.91568. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.81051/4.90715. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.81904/4.90643. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.81799/4.90074. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.81547/4.90726. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.81294/4.90486. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.81279/4.90799. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.81003/4.91771. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.81193/4.92185. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.80801/4.92790. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.81120/4.92436. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.81131/4.90329. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.007889684472185849\n",
      "Epoch 0, Loss(train/val) 4.77094/4.79568. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.77647/4.75453. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.76211/4.79054. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.76730/4.81809. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.76751/4.76283. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.75894/4.75129. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.75145/4.76085. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.75277/4.76156. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.75510/4.76768. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.75688/4.76130. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.75066/4.76374. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.75175/4.77016. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.74725/4.78253. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.74594/4.78555. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.74816/4.78619. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.74462/4.77573. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.74336/4.79136. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.73771/4.81728. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.74262/4.80477. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.74037/4.81532. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.74559/4.78822. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.73856/4.80957. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.73947/4.80191. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.74136/4.79271. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.73984/4.79986. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.73513/4.83287. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.74163/4.79456. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.74047/4.80680. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.73482/4.81484. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.73617/4.80891. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.73894/4.80527. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.73488/4.82342. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.73661/4.80741. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.73638/4.81617. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.72968/4.82948. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.73350/4.79889. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.73386/4.80543. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.73513/4.80791. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.73034/4.83052. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.73538/4.81726. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.73152/4.80980. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.73310/4.81509. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.72702/4.82083. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.73341/4.80648. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.73001/4.83003. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.73204/4.82652. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.73264/4.79902. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.73135/4.83466. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.73263/4.80908. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.72113/4.84462. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.73315/4.80519. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.72441/4.84495. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.73130/4.79786. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.72968/4.82571. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.73012/4.82295. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.73244/4.81336. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.72316/4.86226. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.72704/4.81678. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.72140/4.84824. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.72842/4.83262. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.72191/4.84255. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.72903/4.82845. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.72098/4.84947. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.72165/4.84771. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.72036/4.85247. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.73446/4.81262. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.72606/4.82203. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.72883/4.83506. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.71832/4.85912. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.72464/4.82448. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.72525/4.85413. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.71278/4.88185. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.72351/4.84118. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.72713/4.85336. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.72629/4.83465. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.72328/4.84773. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.71764/4.87504. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.71880/4.87036. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.71470/4.88172. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.72626/4.82315. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.71876/4.86754. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.72451/4.82120. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.72096/4.87338. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.72238/4.83982. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.71873/4.88666. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.72378/4.84952. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.71598/4.86645. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.72422/4.82947. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.71484/4.87049. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.71821/4.86890. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.71853/4.89882. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.72024/4.82362. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.71768/4.84375. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.71931/4.85900. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.71190/4.86994. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.72386/4.83764. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.71530/4.86542. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.72276/4.84564. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.71793/4.86782. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.71694/4.85123. Took 0.08 sec\n",
      "ACC: 0.578125, MCC: 0.1563263498701806\n",
      "Epoch 0, Loss(train/val) 4.93801/4.90998. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.92034/4.95547. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.90963/4.93431. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.91216/4.94378. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.91033/4.94132. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.90838/4.95036. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.90689/4.94860. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.90355/4.95531. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.90308/4.95360. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.90209/4.95339. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.90132/4.98516. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.90295/4.95629. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.90386/4.97847. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.90375/4.97581. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.90605/4.95405. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.90095/4.95499. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.90480/4.96067. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.90394/4.96234. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.89893/4.97097. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.90098/4.95898. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.90126/4.96886. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.90219/4.97276. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.90464/4.96065. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.89978/4.96326. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.89866/4.97193. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.89860/4.96891. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.89597/4.96800. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.89674/4.97221. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.89806/4.97402. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.90063/4.96597. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.89949/4.97399. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.89691/4.96378. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.89513/4.97307. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.89532/4.96829. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.89596/4.97850. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.89393/4.98230. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.89784/4.96448. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.89748/4.96269. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.89535/4.97257. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.89313/4.98650. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.89829/4.96785. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.89735/4.98217. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.89685/4.97139. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.89422/4.99105. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.89620/4.97376. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.89339/4.98691. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.89648/4.96210. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.89368/4.97948. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.89225/4.97779. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.88820/5.00400. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.88887/4.97678. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.89312/4.98992. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.89148/4.96417. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.89021/4.99257. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.89050/4.97450. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.89344/4.98167. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.89407/4.98940. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.88984/4.99446. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.89154/4.96552. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.88805/4.96956. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.89515/4.95939. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89171/4.97448. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.88991/4.98129. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.88782/4.97670. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.88403/4.97455. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.88802/4.98405. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.89243/4.98727. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.88657/5.00082. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.89062/5.00045. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.89020/4.98825. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.89107/4.98944. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.88234/4.99317. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.89094/4.98213. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88781/4.98999. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.88573/4.99185. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.89307/4.97698. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88915/4.97249. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.88020/4.98935. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.88943/4.94882. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88884/4.96519. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.89140/4.95572. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.88944/4.95302. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.89179/4.97991. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.89175/4.98938. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.89132/4.97783. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.88952/4.97861. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.88331/4.98757. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.88364/5.00241. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.88936/4.96374. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88134/5.01047. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.88434/4.98514. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.88762/4.99121. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.88007/4.98405. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.87837/4.99953. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.88635/4.97538. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.88768/4.99354. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.87892/4.97746. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.88133/4.98751. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.87882/5.00692. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.88831/4.99739. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.1259881576697424\n",
      "Epoch 0, Loss(train/val) 4.76435/4.71197. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.70098/4.69789. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.69679/4.69550. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.69657/4.69746. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.69487/4.70352. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.69641/4.70391. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.69606/4.70463. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.69618/4.70956. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.69174/4.70624. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.69422/4.71061. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.69563/4.71791. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.69814/4.70847. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.69324/4.71595. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.69457/4.71591. Took 0.07 sec\n",
      "Epoch 14, Loss(train/val) 4.68887/4.72013. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.68943/4.72067. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.68758/4.72561. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.69144/4.71932. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.69246/4.71947. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.68616/4.72483. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.69001/4.73343. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.69164/4.72861. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.68454/4.74311. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.68756/4.73365. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.69188/4.72311. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.68771/4.73135. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.68678/4.73485. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.68692/4.73141. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.68470/4.73534. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.68477/4.73428. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.68723/4.73438. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.68402/4.73619. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.68300/4.73557. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.68227/4.73142. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.68343/4.72431. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.68702/4.72172. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.68608/4.72620. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.68480/4.73242. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.68413/4.72586. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.68518/4.72801. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.68332/4.73082. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.68524/4.74503. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.67668/4.75877. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.68302/4.75921. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.68201/4.75559. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.68180/4.76783. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.67999/4.75918. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.68093/4.74552. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.67535/4.75118. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.67842/4.75012. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.67979/4.75719. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.68031/4.76007. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.67546/4.76079. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.67677/4.75497. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.67850/4.76602. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.67652/4.76973. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.67602/4.78071. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.67694/4.78122. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.67533/4.77346. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.67549/4.77464. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.66596/4.79336. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.67342/4.77642. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.66815/4.77435. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.68353/4.76408. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.67776/4.75959. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.67671/4.77744. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.66891/4.77515. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.67085/4.76830. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.67482/4.77992. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.67198/4.76173. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.66639/4.79595. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.67296/4.79279. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.67297/4.76211. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.67629/4.76350. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.67156/4.76863. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.66984/4.77657. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.67546/4.77019. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.66618/4.79519. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.66793/4.77572. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.67202/4.78709. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.67389/4.75851. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.66835/4.77833. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.66580/4.78848. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.67033/4.76540. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.67013/4.77242. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.66668/4.78167. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.66819/4.78890. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.66558/4.76772. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.67054/4.76257. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.66773/4.74836. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.66833/4.75466. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.66954/4.78115. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.66532/4.78180. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.66786/4.78246. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.66484/4.78025. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.66481/4.77465. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.66717/4.78026. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.66586/4.79239. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.66627/4.76851. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.66624/4.78364. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.1014574359634967\n",
      "Epoch 0, Loss(train/val) 5.06206/5.00500. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.00662/5.00573. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.00675/5.01473. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.00763/5.01518. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.00852/5.00860. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 5.00602/5.00951. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.00573/5.00922. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.00676/5.00908. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.00420/5.00788. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.00609/5.00834. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.00487/5.01023. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.00438/5.00280. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.00219/5.00471. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.00433/4.99968. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.00057/4.99924. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.00179/4.99863. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.99790/4.99662. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.00394/5.00422. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.00435/5.00372. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.00290/5.00456. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.99864/4.99912. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.99798/4.99441. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.00001/4.99334. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.00166/4.99139. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.00144/4.99685. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.99414/4.99612. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.99864/4.99460. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.99661/4.98934. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.99851/4.99368. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.99534/4.98548. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.99691/4.98453. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.99245/4.98756. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.99457/4.97926. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.99334/4.98370. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.99691/4.98318. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.99053/4.98706. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.98827/4.98496. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.99465/4.98983. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.99449/4.98713. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.99108/4.99536. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.99283/4.99630. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.99254/4.98210. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.98888/4.98444. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.98990/4.98843. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.99087/4.98840. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.98822/4.99431. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.98973/4.99354. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.98980/4.99337. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.98649/4.99039. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.98421/4.97373. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.98961/4.99465. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.99615/5.00632. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.00020/4.98409. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.99278/4.98081. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.99667/4.98109. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.99653/4.98305. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.98694/4.97536. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.98983/4.97454. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.98572/4.99151. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.98990/4.98553. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.98610/4.97321. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.98719/4.97858. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.98953/4.97742. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 4.99013/4.98123. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.98850/4.97116. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.98838/4.98170. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.98472/4.98320. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.98564/4.99779. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.98034/4.97667. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.98720/4.98427. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.99153/4.98456. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.98678/4.99193. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.98241/4.98658. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.98021/4.98629. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.98126/4.98587. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.98799/4.97957. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.98147/4.96877. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.97998/4.97593. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.98128/4.99374. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.97590/5.00522. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.97672/4.99446. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.97873/4.98635. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.98128/4.98697. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.97938/4.98700. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.97197/4.99893. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.97815/5.00505. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.97875/4.99815. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.97906/4.99931. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.98232/4.99779. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.97080/5.01789. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.97458/5.00297. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.97886/5.01932. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.97434/5.00839. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.98094/4.98654. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.97918/5.00813. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.97946/5.00948. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.96989/5.01311. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.97627/5.03335. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.98182/4.99050. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.97763/5.00189. Took 0.08 sec\n",
      "ACC: 0.390625, MCC: -0.20959097296420087\n",
      "Epoch 0, Loss(train/val) 4.93034/4.82999. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.82753/4.81366. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.81519/4.81362. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.81324/4.81406. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.81145/4.81406. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.81163/4.81487. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.81154/4.81410. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.81226/4.81885. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.81100/4.81955. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.81156/4.81707. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.81021/4.80901. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.80976/4.80966. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.80672/4.81388. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.81170/4.81430. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.80488/4.81267. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.80670/4.81480. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.80480/4.80876. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.80690/4.80844. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.80556/4.79968. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.80605/4.79558. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.80101/4.79815. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.80473/4.79643. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.79859/4.79735. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.80300/4.79279. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.80260/4.79770. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.80135/4.79866. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.80047/4.80058. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.79738/4.80058. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.79829/4.80499. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.80082/4.80334. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.79689/4.80382. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.79349/4.80794. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.79418/4.81178. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.79582/4.80601. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.79368/4.80514. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.79649/4.81576. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.79728/4.82080. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.79725/4.81135. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.79687/4.81753. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.79318/4.82378. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.79219/4.82415. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.79257/4.82643. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.79229/4.82361. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.78933/4.82418. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.79343/4.81692. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.78897/4.81737. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.78709/4.82261. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.78787/4.82595. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.78585/4.82755. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.78804/4.82714. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.78842/4.82324. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.78722/4.83294. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.78383/4.82886. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.78118/4.83784. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.78724/4.83689. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.78355/4.83291. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.78226/4.83197. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.78518/4.83025. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.78137/4.83362. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.78298/4.84045. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.78494/4.82887. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.78638/4.82492. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.78134/4.83136. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.77845/4.83317. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.78415/4.82361. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.77940/4.82856. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.77993/4.83467. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.77688/4.83140. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.78136/4.82804. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.78190/4.83026. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.78251/4.83102. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.77595/4.82623. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.78696/4.82804. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.77841/4.82145. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.77881/4.82527. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.77945/4.84068. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.77658/4.84293. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78376/4.82735. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.77651/4.82030. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.77822/4.83245. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.78021/4.82523. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.77506/4.82993. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.77489/4.82613. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.78040/4.83279. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.78233/4.82625. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.77998/4.83673. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.77686/4.82577. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.78067/4.83079. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.77791/4.82921. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.77855/4.83565. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.77089/4.82669. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.77159/4.83784. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.77447/4.83448. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.77548/4.84008. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.76872/4.83939. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.77268/4.82788. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.77598/4.81838. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.77337/4.81859. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77495/4.82411. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.77501/4.82015. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.025861699363244256\n",
      "Epoch 0, Loss(train/val) 4.84796/4.80243. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.84538/4.79541. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.84425/4.80000. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.84489/4.80932. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.81680/4.79759. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.82118/4.79776. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.82209/4.79774. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.81994/4.79701. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.82025/4.79942. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.82283/4.79938. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.82034/4.80114. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.81729/4.80098. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.81834/4.80460. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.81532/4.80767. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.81720/4.81027. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.81519/4.81152. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.81706/4.80826. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.81125/4.81285. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.81102/4.80918. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.81242/4.81987. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.80977/4.81158. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.80823/4.81545. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.81099/4.81773. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.80780/4.82670. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.81333/4.81760. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.81049/4.82608. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.81003/4.82384. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.81130/4.83361. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.81167/4.81689. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.81094/4.82348. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.80801/4.82945. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.80584/4.82342. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.80700/4.82125. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.80523/4.81661. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.79996/4.81910. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.80186/4.84506. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.80004/4.81619. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.80226/4.81912. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.80221/4.82131. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.79816/4.82146. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.79825/4.82152. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.80206/4.81733. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.79351/4.81949. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.79027/4.80246. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.80540/4.82037. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.79233/4.82717. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.78953/4.78861. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.80151/4.83002. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.79160/4.82433. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.79316/4.81360. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.78875/4.82275. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.78932/4.80572. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.78937/4.82989. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.78755/4.80656. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.78651/4.82548. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.78363/4.82159. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.78330/4.82240. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.78993/4.82901. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.77910/4.81711. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.79430/4.81926. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.77813/4.83646. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.78834/4.81107. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.78507/4.82725. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.78337/4.81759. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.78038/4.82315. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.78092/4.83550. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.77891/4.80920. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.77705/4.83673. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.78082/4.81548. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.78136/4.81363. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.77780/4.82476. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.77828/4.78644. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.78282/4.80855. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.77616/4.81026. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.77465/4.80924. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.77975/4.80022. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78199/4.79839. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78293/4.80395. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.77971/4.81308. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.77550/4.81293. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.78399/4.80416. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.77446/4.80818. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.77305/4.81124. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.77001/4.81669. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.77763/4.80675. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.77292/4.80579. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.77149/4.81818. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.77363/4.80485. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.77630/4.80986. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.77531/4.79023. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.76708/4.77372. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.77090/4.81564. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.77980/4.80294. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.78792/4.81240. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.78878/4.81157. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.78346/4.82803. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.78321/4.80739. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.78296/4.81712. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77980/4.81861. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.77885/4.80344. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.09423188886809433\n",
      "Epoch 0, Loss(train/val) 4.72957/4.79779. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.72027/4.74771. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.71701/4.74428. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.71791/4.75382. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.71542/4.74667. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.71964/4.73055. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.71937/4.71046. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.72254/4.70633. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.71485/4.71130. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.71351/4.72578. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.71139/4.71982. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.71037/4.71465. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.71088/4.71152. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.70803/4.70936. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.70744/4.71225. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.71005/4.70474. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.70609/4.70825. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.70608/4.70806. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.70375/4.70396. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.70511/4.69936. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.70231/4.70581. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.70356/4.69870. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.70325/4.69770. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.70128/4.69573. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.70035/4.69716. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.70177/4.70101. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.70135/4.69828. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.69738/4.70179. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.69786/4.69980. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.69807/4.69956. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.69677/4.70264. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.69752/4.69883. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.69369/4.70177. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.69519/4.71028. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.69403/4.70228. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.69835/4.69503. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.69052/4.70507. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.70140/4.70611. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.69035/4.70161. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.68909/4.71422. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.68682/4.71805. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.69373/4.71544. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.69831/4.70572. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.69320/4.70843. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.69019/4.72043. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.69547/4.72111. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.68945/4.71354. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.69135/4.71856. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.68696/4.72800. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.69404/4.72037. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.68770/4.71952. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.68986/4.72677. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.68071/4.72737. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.68730/4.71593. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.68593/4.71700. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.68921/4.72133. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.68715/4.72132. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.68547/4.72750. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.68198/4.72990. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.68111/4.71372. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.68689/4.70590. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.68008/4.71876. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.68109/4.73714. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.68677/4.71321. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.68401/4.71530. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.68447/4.72564. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.68214/4.71816. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.68334/4.73034. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.69084/4.71099. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.67618/4.72575. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.68013/4.72228. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.67788/4.72405. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.68179/4.72198. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.67653/4.72816. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.67659/4.72580. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.68440/4.70144. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.68415/4.71585. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.67664/4.73172. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.67864/4.71922. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.67773/4.72090. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.67555/4.71345. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.67787/4.73348. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.67962/4.73702. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.68829/4.71755. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.67865/4.71931. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.67649/4.73302. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.67686/4.72757. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.68048/4.72698. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.68254/4.71633. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.67860/4.73956. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.67835/4.71341. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.67298/4.72287. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.67283/4.72431. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.67414/4.72994. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.67765/4.73425. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.67187/4.73061. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.67686/4.71412. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.67556/4.73260. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.67184/4.73039. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.67241/4.72957. Took 0.08 sec\n",
      "ACC: 0.40625, MCC: -0.18547738582708967\n",
      "Epoch 0, Loss(train/val) 4.88019/4.79811. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.79284/4.79534. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.79540/4.78912. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79669/4.79044. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79650/4.79062. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79402/4.79212. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.79066/4.79493. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.79455/4.79685. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.79113/4.79439. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.79061/4.79277. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.78752/4.79461. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.78873/4.79174. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.78771/4.78809. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.78883/4.78687. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.78899/4.79009. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.78618/4.79188. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78732/4.78342. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.79193/4.78196. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.78973/4.78033. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.78442/4.77809. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.78770/4.78128. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78551/4.78337. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.78822/4.78544. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.78939/4.80287. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.78940/4.79305. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.78806/4.79923. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.78802/4.80299. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.78696/4.79638. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.78618/4.80476. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.78887/4.78884. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.78657/4.79819. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78534/4.79977. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.78249/4.80032. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78479/4.80356. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78776/4.79630. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.78302/4.80001. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.78387/4.80035. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.78225/4.81302. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.78504/4.80279. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.78147/4.80100. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.77930/4.81305. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.78250/4.80763. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.77812/4.81981. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.77900/4.81365. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.78422/4.79643. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.77944/4.80254. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77563/4.82132. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77059/4.84323. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77978/4.80025. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.78335/4.79037. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77983/4.80041. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.78748/4.79800. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.78986/4.79062. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.78766/4.78254. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.78355/4.77369. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.78369/4.76860. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.78470/4.78450. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.78177/4.75367. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.78266/4.77593. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.77846/4.76503. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.77756/4.81152. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.78588/4.79257. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.77940/4.79875. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.78012/4.75662. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.78803/4.78094. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.78457/4.78342. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.78037/4.79853. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.78507/4.79592. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.78526/4.79627. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.78829/4.79686. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.78507/4.80165. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78324/4.80107. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.78221/4.79474. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.77826/4.79378. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.78050/4.79033. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.78152/4.78861. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.77728/4.79211. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.77911/4.78867. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.77989/4.78961. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.78152/4.79272. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.77611/4.80218. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.78002/4.79922. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.77448/4.79589. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.77523/4.80617. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.77969/4.79726. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.77170/4.79747. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.77160/4.79855. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.77624/4.80412. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.77629/4.82719. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.77803/4.78615. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.77601/4.79186. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.77130/4.79982. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.77178/4.79931. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.76850/4.80157. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.76966/4.80574. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.77279/4.78886. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.77900/4.78266. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.77484/4.78822. Took 0.07 sec\n",
      "Epoch 98, Loss(train/val) 4.77782/4.78580. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.77681/4.78274. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.11221803979288456\n",
      "Epoch 0, Loss(train/val) 4.66061/4.64991. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.61836/4.63576. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.61415/4.63365. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.61156/4.63491. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.61101/4.63561. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.60934/4.63583. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.60805/4.63345. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.60866/4.64558. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.60689/4.64750. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.60542/4.63611. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.60824/4.62726. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.60931/4.63207. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.60074/4.64126. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.60598/4.63467. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.60151/4.64158. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.60108/4.64963. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.60212/4.64843. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.60168/4.64008. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.60009/4.64689. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.59932/4.64655. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.60271/4.65040. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.60296/4.64520. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.60200/4.65523. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.60729/4.63977. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.59937/4.64294. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.59603/4.64808. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.59689/4.64707. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.59580/4.65384. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.59610/4.65638. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.59874/4.65387. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.59617/4.65795. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.59275/4.66656. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.59437/4.65555. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.59416/4.67383. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.59906/4.65879. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.59501/4.67657. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.59962/4.64782. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.59797/4.67463. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.59574/4.66707. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.59544/4.67400. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.59937/4.66888. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.59489/4.67513. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.59710/4.67165. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.59442/4.67590. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.59221/4.66851. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.59611/4.64678. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.60066/4.65694. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.59330/4.67671. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.59324/4.65785. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.59330/4.67844. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.59928/4.65887. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.58996/4.67456. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.59349/4.67055. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.58857/4.68060. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.59088/4.67390. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.59438/4.66999. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.59675/4.68192. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.59623/4.66135. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.59191/4.66837. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.59111/4.66963. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.58981/4.68273. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.58855/4.67260. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.59069/4.66741. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.58588/4.68718. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.58951/4.65779. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.59342/4.68381. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.58628/4.68017. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.59341/4.67051. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.58959/4.67126. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.59354/4.68047. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.59112/4.66989. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.59365/4.67110. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.58946/4.67451. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.59016/4.68265. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.58769/4.68254. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.58595/4.68912. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.58965/4.67873. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.58652/4.69593. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.58801/4.67377. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.58731/4.68131. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.59268/4.65908. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.58513/4.69491. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.58853/4.67100. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.58346/4.68392. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.58601/4.70141. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.58736/4.68742. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.58628/4.69550. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.58087/4.68626. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.58599/4.65986. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.59535/4.65167. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.59563/4.64984. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.59191/4.67742. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.58702/4.68714. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.59453/4.66011. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.58520/4.68957. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.58396/4.68465. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.58373/4.68696. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.58048/4.68104. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.58137/4.68380. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.58307/4.68148. Took 0.08 sec\n",
      "ACC: 0.5, MCC: -0.011953709238683663\n",
      "Epoch 0, Loss(train/val) 4.85340/4.79739. Took 0.19 sec\n",
      "Epoch 1, Loss(train/val) 4.79421/4.79396. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 4.78952/4.78830. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.78729/4.78590. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79058/4.78617. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79096/4.78980. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.79056/4.78732. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.79071/4.79179. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.78912/4.78664. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79289/4.78613. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.78917/4.79026. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.79081/4.79186. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.79234/4.79411. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.78591/4.79484. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.78772/4.79807. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.78589/4.80217. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78512/4.81837. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.78008/4.81609. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.78809/4.81511. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.78687/4.81570. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.78219/4.82256. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78098/4.82498. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.78125/4.81821. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.77857/4.81453. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.77861/4.83783. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.77921/4.82149. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.77764/4.83675. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.77826/4.82208. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77168/4.85409. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.77400/4.82948. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.77426/4.83276. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.77072/4.82895. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.77169/4.83590. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.77633/4.83278. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.77438/4.83396. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.76983/4.84016. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.77228/4.84989. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.78131/4.83444. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.77255/4.83341. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.76788/4.84309. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.77094/4.84830. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.76893/4.84530. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.77261/4.83831. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.77487/4.83648. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.77425/4.83755. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.77259/4.84443. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77027/4.84873. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77090/4.84040. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77059/4.83149. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.76851/4.85148. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77014/4.79791. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77260/4.84574. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.77038/4.84651. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.76609/4.85177. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.76833/4.84579. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.76884/4.86115. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.76766/4.83901. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.78169/4.79980. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.77684/4.82141. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.77563/4.84547. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.76746/4.83248. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.76847/4.83670. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76934/4.85417. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.77138/4.85315. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.77188/4.84146. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.76113/4.86567. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.76438/4.84374. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.75685/4.85324. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.76305/4.84852. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76961/4.84874. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.76344/4.83665. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.76239/4.84604. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.75984/4.85713. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.75866/4.85442. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.76338/4.85235. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.76044/4.85315. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.75367/4.85859. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.75756/4.84668. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.75909/4.85403. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.75441/4.84458. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.75416/4.86034. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.75909/4.85949. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.75788/4.85465. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.75630/4.85938. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75775/4.85601. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.75185/4.85710. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.75814/4.86903. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.75800/4.85848. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.76042/4.85925. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.75071/4.87787. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.75361/4.83745. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.75013/4.85785. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.75235/4.86304. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.76691/4.80853. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.77659/4.79649. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.76586/4.82719. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.77521/4.81670. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.76404/4.82708. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.76293/4.81746. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.76268/4.82683. Took 0.08 sec\n",
      "ACC: 0.390625, MCC: -0.21664795696917594\n",
      "Epoch 0, Loss(train/val) 4.74217/4.70035. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.69674/4.69813. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.69921/4.69965. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.70141/4.69995. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.69578/4.69945. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.69789/4.69796. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.69716/4.69827. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.69616/4.69828. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.69642/4.69423. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.69901/4.68954. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.69781/4.69311. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.69562/4.69285. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.69271/4.69728. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.69169/4.69393. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.68942/4.69549. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.69040/4.69196. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.68896/4.69525. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.68949/4.69314. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.68902/4.70267. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.69563/4.70024. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.69296/4.70573. Took 0.07 sec\n",
      "Epoch 21, Loss(train/val) 4.68981/4.69680. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.69300/4.70066. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.68935/4.71467. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.70225/4.71670. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.69463/4.71534. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.69225/4.71798. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.68921/4.72389. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.69038/4.71387. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.69266/4.70850. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.68793/4.71553. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.68830/4.71572. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.68230/4.72455. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.69269/4.70849. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.68424/4.71717. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.68586/4.71277. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.68714/4.72018. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.68479/4.71640. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.68351/4.71688. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.68702/4.71041. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.68361/4.71606. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.68749/4.72063. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.68035/4.71238. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.67974/4.72492. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.68762/4.72097. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.68401/4.72369. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.68220/4.72162. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.67989/4.72645. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.68561/4.72944. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.67953/4.74382. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.67989/4.72683. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.67828/4.74059. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.67908/4.73200. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.67404/4.74782. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.68258/4.71810. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.68048/4.73302. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.67069/4.76867. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.68183/4.72482. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.67525/4.75062. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 4.67842/4.72838. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.67473/4.74738. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.67384/4.73928. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.67503/4.74109. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.67328/4.72575. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.67740/4.73606. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.67429/4.76162. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.68019/4.75114. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.68044/4.71561. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.67543/4.74797. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.68068/4.73497. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.67896/4.74079. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.67740/4.74245. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.68205/4.72728. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.68430/4.73065. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.68171/4.72869. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.68093/4.73708. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.68379/4.75111. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.67395/4.76714. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.67568/4.77828. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.67977/4.73684. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.67806/4.77825. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.67165/4.74274. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.67893/4.70971. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.68380/4.70399. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.67879/4.74435. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.69119/4.69957. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.69641/4.69585. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.69507/4.70206. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.68865/4.70183. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.67747/4.71016. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.67802/4.70757. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.68303/4.72590. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.68620/4.72584. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.68633/4.72353. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.68340/4.73219. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.68149/4.75884. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.68099/4.72820. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.67522/4.75279. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.67787/4.73247. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.67619/4.75535. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.09593745047287525\n",
      "Epoch 0, Loss(train/val) 4.97682/4.97630. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.95137/4.94205. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.94542/4.94566. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.95023/4.94488. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.94730/4.94305. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.94494/4.94264. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.94608/4.94657. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.94539/4.95009. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.94465/4.95022. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.94514/4.94993. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.94230/4.94739. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.94261/4.94827. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.93923/4.94938. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.93886/4.95177. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.94005/4.95380. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.93901/4.95471. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.94164/4.95315. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.93521/4.95667. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.93815/4.96370. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.93552/4.96777. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.93660/4.96410. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.94034/4.95395. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.93625/4.95805. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.93230/4.96909. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.93761/4.96739. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.94153/4.96150. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.93633/4.96707. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.93211/4.99162. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.93482/4.97790. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.93510/4.97848. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.93367/4.98534. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.93133/4.98681. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.93180/4.98298. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.93052/4.97937. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.93666/4.98244. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.93049/4.98936. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.92902/4.98104. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.93494/4.96874. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.92882/4.97988. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.93237/4.97904. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.92809/4.98848. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.92787/4.99058. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.92535/4.98638. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.92524/4.99004. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.92540/4.97637. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.92487/4.99708. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.92869/4.99546. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.92587/4.97587. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.92406/5.00029. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.92817/4.99654. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.93522/4.96389. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.93090/4.98917. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.93167/4.98488. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.92692/4.98430. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.92995/4.98174. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.92520/4.99945. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.92884/4.99012. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.92836/4.99544. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.93092/4.97774. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.92555/4.99541. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.92909/4.97911. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.92525/4.97740. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.92841/4.98671. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.92104/4.98680. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.92416/4.98805. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.92080/4.97406. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.92769/4.98334. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.92157/5.00955. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.92428/4.98222. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.92714/4.98636. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.92303/5.02307. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.92064/4.96912. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.91954/4.99602. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.92321/4.98981. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.92462/4.96700. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.91887/4.99854. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.92232/5.02892. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.92279/5.00785. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.92274/4.99002. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.91786/5.01950. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.92440/4.99089. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.91811/5.01789. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.92314/5.01448. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.91797/5.00307. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.91783/4.99675. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.91826/5.01760. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.91796/4.99844. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.91545/5.03821. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.92744/4.98924. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.92028/5.01955. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.92006/5.02739. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.91873/5.02523. Took 0.07 sec\n",
      "Epoch 92, Loss(train/val) 4.91498/5.03869. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.91457/5.01707. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.91550/5.02595. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.92056/5.00816. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.91369/5.04455. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.91511/5.03303. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.92975/4.97364. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.92011/4.99879. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.0\n",
      "Epoch 0, Loss(train/val) 4.93094/4.85869. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.87405/4.88741. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.87693/4.91218. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.87902/4.91458. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.87807/4.91196. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.87734/4.91759. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.87972/4.90566. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.87779/4.88850. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.87437/4.88172. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.87452/4.88337. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.87128/4.88658. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.87260/4.88883. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.87370/4.88461. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.87278/4.88285. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.87233/4.88767. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.87253/4.88713. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.87276/4.87437. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.87138/4.87303. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.87496/4.85637. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.87197/4.85824. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.86967/4.85863. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.87069/4.85956. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.86709/4.86427. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.86727/4.86563. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.86623/4.86477. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.86958/4.86229. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.86735/4.85877. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.86598/4.86123. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.86361/4.86481. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.86577/4.86256. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.86477/4.86531. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.86682/4.86458. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.86546/4.86322. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.86388/4.87247. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.86422/4.87310. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.86175/4.87616. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.86323/4.87406. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.86246/4.87336. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86416/4.87545. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.86538/4.87530. Took 0.07 sec\n",
      "Epoch 40, Loss(train/val) 4.85948/4.88500. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.86295/4.88946. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.85736/4.89053. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.86251/4.88841. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.86665/4.87131. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.85903/4.89183. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.85571/4.90179. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.85904/4.88435. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.85472/4.88212. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.85529/4.89115. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.85550/4.88576. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.85744/4.89045. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.85811/4.88925. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.85457/4.89350. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.85346/4.90862. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.85712/4.89659. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.85672/4.88319. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.85576/4.89634. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.85221/4.89801. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.85650/4.88799. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.85441/4.90490. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.85607/4.90370. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.85715/4.89187. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.85075/4.90956. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.85439/4.89771. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.85031/4.91270. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.85333/4.90195. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.84687/4.91932. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.85593/4.89931. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.85144/4.89564. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.84639/4.90495. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.84950/4.90453. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.85403/4.87289. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.85468/4.91478. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.85475/4.88126. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.84564/4.89084. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.84448/4.90006. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.85883/4.90869. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.85748/4.91854. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.85434/4.89436. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.85378/4.90366. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.84818/4.92498. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.84100/4.92130. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.84940/4.91507. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.84779/4.89926. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.85157/4.92157. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.85474/4.89880. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.85197/4.87615. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.84337/4.92992. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.84122/4.91226. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.83932/4.95241. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.84447/4.89165. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.84502/4.91182. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.85576/4.90304. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.84923/4.94722. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.86278/4.91671. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.86111/4.90264. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.85908/4.88970. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.85584/4.92994. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.85675/4.89841. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.11383959916814836\n",
      "Epoch 0, Loss(train/val) 4.82907/4.84167. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.81056/4.85931. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.81466/4.85641. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.80765/4.86833. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.80939/4.86504. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.81667/4.84217. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.82145/4.80452. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.81633/4.80311. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.81027/4.80781. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.80862/4.80765. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.80857/4.80501. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.80967/4.80237. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.80803/4.80188. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.80875/4.80096. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.81010/4.79983. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.80928/4.80218. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.80828/4.79868. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.80864/4.79967. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.80712/4.80046. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.80568/4.79891. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.80704/4.79855. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.80712/4.79798. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.80493/4.79843. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.80337/4.80213. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.80533/4.79855. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.80436/4.79317. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.80321/4.79387. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.80109/4.79601. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.79998/4.79559. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.80141/4.79348. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.80016/4.79321. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.80137/4.79975. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.80279/4.79866. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.80187/4.79486. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.80299/4.79347. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.80025/4.79192. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.80172/4.79152. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.80326/4.78861. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.79864/4.79640. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.79748/4.79184. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.79989/4.78727. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.79817/4.79135. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.79875/4.79212. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.79589/4.79846. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.79611/4.78311. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.79960/4.78414. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.79537/4.79153. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.79609/4.78864. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.79565/4.78185. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.79144/4.78882. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.79134/4.78767. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.79295/4.78749. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.79230/4.78891. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.79550/4.79286. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.80024/4.78877. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.79751/4.79425. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.79211/4.80216. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.79487/4.78619. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.79472/4.78770. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.79370/4.80125. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.79631/4.78524. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.79898/4.78737. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.79395/4.80281. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.79388/4.78098. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.79081/4.79790. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.79145/4.78883. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.78699/4.79551. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.79250/4.79049. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.79061/4.79147. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.78857/4.79442. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.78724/4.79633. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78774/4.78973. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.78925/4.80025. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.78707/4.80445. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.78202/4.80696. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.78454/4.79950. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78575/4.79778. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78789/4.79819. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.78115/4.79289. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.78611/4.77948. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.78278/4.81011. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.78475/4.80838. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.78149/4.79990. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.78383/4.79774. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.78258/4.81013. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.78597/4.80693. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.78780/4.80859. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.77717/4.81594. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.77866/4.81065. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.79394/4.79852. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.77894/4.80910. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.77920/4.80961. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.77513/4.83767. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.77861/4.80359. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.77943/4.79885. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.77725/4.83820. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.77739/4.82602. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.78156/4.83356. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77989/4.81226. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.78471/4.82741. Took 0.08 sec\n",
      "ACC: 0.59375, MCC: 0.1746031746031746\n",
      "Epoch 0, Loss(train/val) 4.75944/4.73476. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.71532/4.72327. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.71410/4.73138. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.71700/4.72805. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.71798/4.72947. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.71616/4.72962. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.71818/4.73579. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.71829/4.74073. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.71596/4.73920. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.72072/4.73054. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.71717/4.72446. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.71566/4.72712. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.71202/4.72776. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.71170/4.72851. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.71629/4.72464. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.71233/4.72664. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.71477/4.72509. Took 0.07 sec\n",
      "Epoch 17, Loss(train/val) 4.71639/4.72770. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.71292/4.72547. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.71266/4.72208. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.70993/4.72084. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.71336/4.72352. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.71293/4.72262. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.71218/4.72171. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.70804/4.72669. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.70982/4.72988. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.71182/4.71833. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.71310/4.72404. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.70680/4.72077. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.71001/4.71756. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.70495/4.72183. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.70716/4.72295. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.70891/4.71788. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.70762/4.72103. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.71050/4.72245. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.71081/4.74232. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.71562/4.71986. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.71089/4.72731. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.70497/4.73131. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.70578/4.73043. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.71026/4.72398. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.70332/4.73552. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.71101/4.72209. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.70611/4.72284. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.70663/4.72107. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.70469/4.72576. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.70523/4.72075. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.70265/4.72852. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.70772/4.72498. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.70310/4.74854. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.70791/4.71096. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.70745/4.71855. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.70626/4.71984. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 4.70748/4.70949. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.70691/4.70593. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.70076/4.70350. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.70272/4.70252. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.70033/4.70690. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.70514/4.71144. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.70215/4.71979. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.69721/4.73452. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.70396/4.71515. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.70421/4.70993. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.70162/4.70894. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.70098/4.71545. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.70355/4.71097. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.69729/4.71123. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.69766/4.70775. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.69685/4.70590. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.70123/4.71316. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.69988/4.71777. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.69778/4.71066. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.69855/4.72858. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.70425/4.69599. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.69168/4.71781. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.69640/4.69721. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.69492/4.73189. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.70460/4.71166. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.70120/4.72192. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.70146/4.71686. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.69767/4.71921. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.69314/4.72278. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.69228/4.70501. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.69156/4.72650. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.69491/4.70911. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.69414/4.71826. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.69718/4.72135. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.69666/4.70134. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.69469/4.72848. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.69338/4.72120. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.68775/4.71196. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.69595/4.71006. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.69859/4.72415. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.68985/4.72286. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.68623/4.73084. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.68986/4.70248. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.69615/4.71553. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.68996/4.72796. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.69609/4.71643. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.68882/4.71385. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.04351259372329707\n",
      "Epoch 0, Loss(train/val) 5.08025/5.00468. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.04028/5.01232. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 5.03292/5.01664. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.02999/5.01839. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.03040/5.01938. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.02937/5.02041. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.03143/5.02541. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.03108/5.02584. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.02822/5.02706. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.02840/5.02098. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 5.02665/5.01896. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.02791/5.01762. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.02313/5.01658. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.02235/5.01502. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.02125/5.01405. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.02309/5.01114. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.01924/5.01486. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.02328/5.01875. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.02241/5.01500. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.02131/5.00326. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.02209/5.01201. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.01541/5.00794. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.01952/5.00777. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.01134/5.00564. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.01370/5.00962. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.01226/5.01385. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.01424/5.00858. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.01221/5.00836. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.00961/5.01341. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.01604/5.01561. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.01176/5.02135. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.00800/5.01953. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.00657/5.01759. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.01139/5.01550. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.00799/5.03489. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.00561/5.04239. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.00067/5.02148. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.01132/5.01246. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.00452/5.03675. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.00377/5.04335. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.99963/5.04090. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.99713/5.02780. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.99998/5.02434. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.99510/5.03195. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.99771/5.03518. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.99661/5.04184. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.99588/5.01590. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.00181/5.04141. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.00323/5.03473. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.99154/5.04340. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.00317/5.02399. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.00064/5.04145. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.00106/5.04526. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.99533/5.04226. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.99179/5.04495. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.99409/5.04995. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.99716/5.04416. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.99238/5.03450. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.99618/5.03748. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.99228/5.03202. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.99260/5.04323. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.99537/5.03298. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.99086/5.03563. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.99122/5.03571. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.99207/5.04921. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.99574/5.03214. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.98672/5.04768. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.99832/5.02889. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.01156/5.02228. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.00246/5.01494. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.99742/5.02540. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.99667/5.02425. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.99326/5.02790. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.99586/5.03034. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.99282/5.02239. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.00263/5.02231. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.99582/5.00920. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.98957/5.01649. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.99348/5.02145. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.98574/5.02920. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.99473/5.03095. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.99458/5.02397. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.98960/5.03151. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.98835/5.02600. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.99135/5.01396. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.98584/5.04095. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.99361/5.01214. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.98740/5.01589. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.98523/5.02221. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.98795/5.01753. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.98903/5.03260. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.99763/5.03396. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.98961/5.03460. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.98801/5.02221. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.98339/5.04172. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.98408/5.02550. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.97654/5.04361. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.98210/5.03373. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.98799/5.04576. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.98278/5.05801. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.08222643447147887\n",
      "Epoch 0, Loss(train/val) 4.85517/4.80817. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.81764/4.80422. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.81290/4.80925. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.81207/4.80741. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.81118/4.80958. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.81136/4.81038. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.81141/4.81195. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.81261/4.81286. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.81448/4.81129. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.81006/4.81267. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.80773/4.81543. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.80898/4.81261. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.80983/4.81569. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.80922/4.81846. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.80893/4.82200. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.80729/4.82215. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.80538/4.82345. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.80439/4.82437. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.80335/4.81761. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.80386/4.82194. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.80220/4.82613. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.80138/4.82625. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.79942/4.82789. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.80235/4.82517. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.79879/4.82357. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.79584/4.82689. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.79740/4.82387. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.79865/4.81996. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.79643/4.82287. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.79694/4.82942. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.79937/4.82567. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.79674/4.82809. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.79531/4.82347. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.79604/4.82900. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.79670/4.81676. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.79320/4.83433. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.78474/4.83013. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.79956/4.82274. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.79573/4.82686. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.79300/4.81899. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.79326/4.82358. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.79198/4.82638. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.78892/4.82892. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.79053/4.82128. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.79003/4.83135. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.79789/4.80779. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.79351/4.81334. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.79371/4.81597. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.79241/4.82498. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.79285/4.81418. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.78523/4.83141. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.78691/4.82189. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.78942/4.82556. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.78828/4.80645. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.78742/4.80733. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.78564/4.82236. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.79005/4.81243. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.78421/4.83380. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.78496/4.82683. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.78764/4.82042. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.78191/4.83031. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.78415/4.80882. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.78822/4.83660. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.78618/4.81172. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.78510/4.83634. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.78493/4.82814. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.78114/4.84057. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.78253/4.82995. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.78466/4.83428. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.78163/4.82178. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.78477/4.82236. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78735/4.82180. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.78442/4.83054. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.78571/4.81270. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.78175/4.83782. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.78245/4.84766. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78132/4.82978. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78389/4.82215. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.78344/4.83168. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.77855/4.83192. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.78353/4.82590. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.78870/4.82448. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.78045/4.82619. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.77778/4.84045. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.78906/4.81078. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.77687/4.83086. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.78008/4.82758. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.78644/4.81967. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.78232/4.82021. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.78451/4.82799. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.77943/4.83157. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.77933/4.82829. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.78069/4.83370. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.77527/4.82550. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.78203/4.82275. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.78344/4.83301. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.78074/4.82332. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.78051/4.82822. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77849/4.82751. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.77861/4.81294. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.11724137931034483\n",
      "Epoch 0, Loss(train/val) 5.01740/5.00098. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.98261/4.96774. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.98218/4.96773. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.98199/4.96876. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.97950/4.96836. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.98278/4.97184. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.97991/4.96995. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.97874/4.96896. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.97688/4.96982. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.98046/4.97084. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.97793/4.97277. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.97554/4.97362. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.97570/4.97947. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.97463/4.98866. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.97773/4.98786. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.97983/4.98969. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.97383/4.98997. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.97539/4.99120. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.97603/4.99428. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.97322/4.99755. Took 0.07 sec\n",
      "Epoch 20, Loss(train/val) 4.97068/5.00170. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.96869/5.00208. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.96716/4.99929. Took 0.07 sec\n",
      "Epoch 23, Loss(train/val) 4.96879/4.99472. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.96777/4.99251. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.97324/4.99380. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.96974/4.99595. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.96438/4.99927. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.96830/4.99672. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.96671/4.99792. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.96804/4.99850. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.96857/4.99330. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.96516/4.99146. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.96579/4.99261. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.96209/4.98811. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.96430/4.98195. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.96341/4.98779. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.96405/4.99552. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.97632/4.99004. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.97205/4.98629. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.97000/4.98603. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.97032/4.98294. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.97038/4.98994. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.97138/4.99192. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.96387/4.99613. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.97099/4.98341. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.96226/4.98651. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.96388/4.99582. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.96582/4.99434. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.96575/4.98295. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.96180/4.98639. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.96533/4.99323. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.96418/4.99695. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.96470/4.99258. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.96503/4.99446. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.96314/4.98810. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.95920/4.99167. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.95876/4.98972. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.96215/4.99622. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.96109/4.98494. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.95960/5.00265. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.96042/5.00120. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.96297/5.00021. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.95980/4.98949. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.95915/4.99696. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.97145/4.99331. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.96823/4.98207. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.96453/4.99119. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.96056/4.98822. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.95541/4.99519. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.96126/4.99454. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.96306/4.99389. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.95800/4.99491. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.95829/4.99978. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.95799/4.99981. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.95984/5.00072. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.95772/5.00618. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.95778/4.99718. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.95970/4.99710. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.95198/5.01164. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.95700/4.99964. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.94987/5.00970. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.95326/4.99397. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.95341/4.99568. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.95184/5.00665. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.95238/4.99703. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.95004/5.00727. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.95686/5.00412. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.95267/5.00241. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.96159/4.98896. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.95248/5.00347. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.95496/5.00784. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.95218/5.01561. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.95299/5.00703. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.94942/5.01196. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.95205/5.01053. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.95501/5.00948. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.95534/4.99533. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.95024/5.00119. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.94639/5.00804. Took 0.08 sec\n",
      "ACC: 0.59375, MCC: 0.1980295566502463\n",
      "Epoch 0, Loss(train/val) 4.62226/4.57182. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.56354/4.56534. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.56694/4.56393. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.56523/4.56437. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.56655/4.56361. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.56431/4.56245. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.56252/4.56085. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.56474/4.56134. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.56319/4.56470. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.56348/4.56502. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.56231/4.56134. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.56244/4.56412. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.55856/4.56091. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.55761/4.55631. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.55737/4.55332. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.56064/4.55613. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.55926/4.55989. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.56339/4.56226. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.55761/4.55842. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.55797/4.55841. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.55305/4.56438. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.55825/4.55890. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.55284/4.55957. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.55579/4.55783. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.55419/4.55756. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.55449/4.55978. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.55320/4.56010. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.55439/4.56321. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.55359/4.56394. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.54933/4.56654. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.55373/4.57426. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.55161/4.57176. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.55071/4.57874. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.54638/4.57569. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.54625/4.58085. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.55228/4.58092. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.55141/4.57332. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.54331/4.58308. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.55485/4.57682. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.55176/4.56960. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.54737/4.57290. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.54425/4.57401. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.54235/4.57935. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.54477/4.59307. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.55379/4.55268. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.56439/4.55763. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.55702/4.55852. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.56129/4.56027. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.56080/4.56381. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.55758/4.56438. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.55558/4.57266. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.54887/4.57780. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.54309/4.58462. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.55203/4.58398. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.54825/4.58180. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.55104/4.58674. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.54632/4.58472. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.54655/4.58148. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.54574/4.58088. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.53868/4.59148. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.54356/4.57853. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.54657/4.57352. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.54608/4.57045. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.54277/4.57362. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.54641/4.56967. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.54290/4.57106. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.53925/4.58484. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.54127/4.58502. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.54095/4.58803. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.53845/4.59272. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.54005/4.58245. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.54151/4.59318. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.54206/4.58524. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.53980/4.59195. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.53603/4.58204. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.53976/4.58916. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.53789/4.59731. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.53396/4.59969. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.53414/4.60095. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.55109/4.56592. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.54329/4.57247. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.54016/4.58155. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.53762/4.58476. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.53707/4.59131. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.54224/4.57533. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.53905/4.59134. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.53704/4.60427. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.53016/4.60759. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.53307/4.61359. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.53818/4.60825. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.52949/4.62954. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.53607/4.59772. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.52895/4.60334. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.53277/4.60087. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.53146/4.60618. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.53538/4.60799. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.53795/4.61488. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.53195/4.60078. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.52774/4.61568. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.52361/4.62047. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.13945994111797608\n",
      "Epoch 0, Loss(train/val) 4.91823/4.88762. Took 0.18 sec\n",
      "Epoch 1, Loss(train/val) 4.90483/4.89212. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 4.90421/4.89289. Took 0.14 sec\n",
      "Epoch 3, Loss(train/val) 4.90774/4.88398. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.89991/4.88821. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.89062/4.87983. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.89444/4.88360. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.89348/4.88165. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.89173/4.88362. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.89282/4.87652. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.89583/4.88518. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.88832/4.88314. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.89067/4.88057. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.88964/4.88211. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.88782/4.88254. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.88954/4.88148. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.88892/4.88377. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.88978/4.87726. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.88669/4.88400. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.88516/4.87613. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.88533/4.88134. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.88244/4.87622. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.88946/4.87531. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.89012/4.88049. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.89612/4.88478. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.88999/4.88354. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.88903/4.88629. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.89046/4.88464. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.88680/4.88413. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.88616/4.88093. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.89002/4.88416. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.88578/4.88006. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.88748/4.86861. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.88759/4.87221. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.88692/4.87464. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.88683/4.87292. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.88568/4.87610. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.88281/4.87701. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.88104/4.87461. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.88010/4.88065. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.87586/4.87551. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.87881/4.87652. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.87643/4.86834. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.88121/4.87068. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.87763/4.88425. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.87442/4.88720. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.87863/4.88030. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.87476/4.88193. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.87717/4.89418. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.87697/4.89222. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.87223/4.89295. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.87178/4.89073. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.87630/4.89316. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.87892/4.89204. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.87972/4.89892. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.87969/4.89255. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.87704/4.89524. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.87304/4.90721. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.87454/4.89770. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.87713/4.88844. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.87038/4.89911. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.87782/4.88507. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.87883/4.89378. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.87025/4.90552. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.87497/4.89997. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.87030/4.89993. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.87660/4.89923. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.87201/4.89109. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.87014/4.90889. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.87028/4.89784. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.86898/4.89609. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.87395/4.90520. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.86918/4.91340. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.87388/4.90473. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.87046/4.91870. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.86701/4.89175. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.86839/4.91181. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.87047/4.89190. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.86509/4.91149. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.86666/4.90060. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.86743/4.91285. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.86661/4.90913. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.87107/4.91484. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.86446/4.93000. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.85952/4.90413. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.86436/4.91125. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.86023/4.92920. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.86008/4.92762. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.85990/4.92189. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.86376/4.93972. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.86841/4.91041. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.86730/4.91976. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.86310/4.90939. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.85715/4.91522. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.86352/4.90558. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.86899/4.88862. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.86644/4.92112. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.86444/4.90785. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.86253/4.92169. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.86919/4.89759. Took 0.08 sec\n",
      "ACC: 0.375, MCC: -0.2519763153394848\n",
      "Epoch 0, Loss(train/val) 4.93446/4.89791. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.90074/4.90051. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.90235/4.90467. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.90613/4.90626. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.90756/4.90681. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.90570/4.92120. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.90061/4.91856. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.89688/4.91855. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.89595/4.92169. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.89675/4.92120. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.89693/4.92400. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.89520/4.92673. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.89267/4.92354. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.89291/4.92263. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.89929/4.91313. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.89696/4.91514. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.89716/4.91910. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.89353/4.91900. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.89331/4.92083. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.89805/4.91646. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.89355/4.91947. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.89073/4.91784. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.89591/4.91821. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.89133/4.91619. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.89039/4.92060. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.89146/4.91618. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.88966/4.92903. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.89090/4.92129. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.88963/4.92792. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.88758/4.92324. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.88407/4.93131. Took 0.07 sec\n",
      "Epoch 31, Loss(train/val) 4.88664/4.93362. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.89009/4.93071. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.88610/4.93292. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.88722/4.93225. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.88301/4.94528. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.88641/4.93937. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.88687/4.93706. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.88628/4.94296. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.88394/4.95184. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.88329/4.96386. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.88019/4.95794. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.88456/4.95808. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.87917/4.96682. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.88062/4.96929. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.88284/4.96277. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.87769/4.96892. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.87821/4.98085. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.87705/4.98717. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.87991/4.98776. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.88069/4.96196. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.87969/4.97510. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.87710/4.97355. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.87188/4.99738. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.87264/4.99107. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.87739/4.98370. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.87419/4.97832. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.87676/4.97537. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.87854/4.97319. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.87479/4.98385. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.87612/4.98200. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.87148/4.99895. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.87066/4.99991. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.87114/5.00806. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.87611/4.98629. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.88009/4.98451. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.87391/4.98855. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.87333/4.99563. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.87548/4.97383. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.88173/4.93279. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.88332/4.94608. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.87956/4.97443. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88002/4.97441. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88283/4.97652. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.87716/4.97721. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.87219/4.98161. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.87824/4.97550. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.87529/4.96530. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.87330/4.99393. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.86989/5.00487. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.86883/5.00915. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.86878/4.97587. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.87413/4.98542. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.87506/4.98249. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.86940/4.99535. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.86941/4.99499. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.86964/5.00185. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.86461/5.00338. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.87411/4.98512. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.87207/4.98008. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.86567/5.00714. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.87618/4.96797. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.86905/4.98462. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.86799/4.98929. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.86309/5.01736. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.86539/4.99450. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.86555/5.02061. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.86626/5.01828. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.86105/5.00935. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.86458/5.00560. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.0635386119257087\n",
      "Epoch 0, Loss(train/val) 5.02352/4.89502. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.89761/4.90187. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.88767/4.90818. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.89340/4.91707. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.88970/4.91848. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.89240/4.92186. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.89178/4.92379. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.89006/4.91951. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.88925/4.92705. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.88763/4.93041. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.88533/4.93232. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.88644/4.93591. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.88337/4.93992. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.88260/4.94301. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.88183/4.94001. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.88039/4.94570. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.88068/4.95022. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.88189/4.95230. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.88064/4.95070. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.87892/4.94533. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.88020/4.94303. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.87660/4.94840. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.87546/4.95373. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.87483/4.95934. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.87658/4.95172. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.87419/4.95041. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.87379/4.96042. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.87524/4.95640. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.87771/4.94085. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.87390/4.95483. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.88198/4.92990. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.88669/4.92267. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.88209/4.92685. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.87588/4.95345. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.87396/4.95364. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.87625/4.97035. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.87130/4.96642. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.87030/4.97868. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86994/4.96522. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.87024/4.96545. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.87048/4.96722. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.86951/4.95558. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.86611/4.98412. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.86576/4.96695. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.87318/4.95239. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.86840/4.97612. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.86675/4.98193. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.86873/4.97063. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.86476/4.96936. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.86377/4.98721. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.86807/4.98042. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.86543/4.96679. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.87046/4.96981. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.86575/4.99036. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.86407/4.96492. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.86078/4.98296. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.86439/4.97119. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.86168/4.96719. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.86232/4.97377. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.86515/4.97348. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.86283/4.97151. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.86313/4.96128. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.86572/4.95193. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.87706/4.94458. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.88007/4.92763. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.87317/4.96410. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.87067/4.96782. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.86460/4.96882. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.87642/4.91753. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.87575/4.95200. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.86918/4.95556. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.86964/4.94949. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.86853/4.95694. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.86659/4.96599. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.87159/4.96494. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.86728/4.97477. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.86867/4.97434. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.86517/4.97195. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.86261/5.01956. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.86729/4.98232. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.86207/4.98497. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.86255/5.00233. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.85628/5.01987. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.85638/4.99572. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.86458/4.96960. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.86195/4.98799. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.85857/4.99073. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.85266/5.01005. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.85684/5.00272. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.85972/4.98443. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.85488/4.99793. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.86080/4.97053. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.85827/4.94360. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.86183/4.98955. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.86181/4.97733. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.86168/4.98950. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.85757/4.99897. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.85579/4.96584. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.85727/5.00332. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.86308/4.99952. Took 0.08 sec\n",
      "ACC: 0.703125, MCC: 0.40644850966246954\n",
      "Epoch 0, Loss(train/val) 4.72660/4.72083. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.68091/4.70447. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.67545/4.69797. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.67466/4.70263. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.67604/4.70185. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.67343/4.69996. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.67275/4.69871. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.67237/4.69723. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.67209/4.69659. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.67151/4.69464. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.66752/4.69255. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.66678/4.69300. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.66660/4.69252. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.66848/4.69374. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.66601/4.69305. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.66482/4.70444. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.66502/4.70426. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.66358/4.70115. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.66408/4.70332. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.66117/4.70483. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.66697/4.68327. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.66415/4.68660. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.66144/4.69830. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.66361/4.70277. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.66507/4.69694. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.65969/4.70988. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.66007/4.71166. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.66230/4.71059. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.65815/4.71319. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.66108/4.71283. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.65945/4.70487. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.65987/4.71258. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.65779/4.72033. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.65501/4.72881. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.65572/4.72603. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.65330/4.73557. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.65375/4.74252. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.65496/4.73388. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.65254/4.73555. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.65526/4.72896. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.65288/4.72218. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.65753/4.71873. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.65179/4.72866. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.65104/4.73922. Took 0.07 sec\n",
      "Epoch 44, Loss(train/val) 4.64748/4.73794. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.65269/4.72586. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.64933/4.73170. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.65704/4.73006. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.65005/4.73117. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.64967/4.73583. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.65028/4.73971. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.65252/4.72798. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.64813/4.74699. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.65092/4.73389. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.65422/4.72304. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.64992/4.72983. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.64887/4.74420. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.65358/4.72494. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.64750/4.72645. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.64502/4.74140. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.64962/4.72244. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.65301/4.73214. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.64953/4.73602. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.64588/4.73626. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.64667/4.73196. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.64597/4.73862. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.65114/4.73323. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.64607/4.75258. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.64309/4.74543. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.64791/4.73645. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.64390/4.75568. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.64532/4.73181. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.64435/4.74328. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.64614/4.74811. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.64816/4.74122. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.64516/4.75122. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.64176/4.75283. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.64492/4.74815. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.64581/4.73791. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.64233/4.76217. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.64180/4.74642. Took 0.07 sec\n",
      "Epoch 81, Loss(train/val) 4.63769/4.75499. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.63789/4.77010. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.64465/4.75344. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.64321/4.74730. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.64151/4.75562. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.64334/4.74377. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.63532/4.77749. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.63812/4.76197. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.64246/4.73740. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.64736/4.73978. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.64017/4.74755. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.64321/4.73741. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.64367/4.74975. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.63766/4.76215. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.64030/4.75950. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.63532/4.73990. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.63622/4.74534. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.63615/4.76543. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.63586/4.75099. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.16511700362686718\n",
      "Epoch 0, Loss(train/val) 4.89300/4.80127. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.84596/4.81012. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.83850/4.84903. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.82707/4.84557. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.82000/4.83794. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.82121/4.83438. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.82108/4.83650. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.82113/4.83733. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.82125/4.83327. Took 0.07 sec\n",
      "Epoch 9, Loss(train/val) 4.82151/4.83794. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.81922/4.83073. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.81686/4.83225. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.81653/4.82815. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.81930/4.83538. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.81338/4.82066. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.81664/4.83526. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.81336/4.83074. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.81332/4.82986. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.80992/4.81985. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.81544/4.81971. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.81477/4.82242. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.81400/4.81997. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.81342/4.81697. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.81386/4.83324. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.81068/4.81907. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.81565/4.82975. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.80953/4.82265. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.81363/4.82625. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.81147/4.82529. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.81123/4.82561. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.80795/4.81928. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.81294/4.83252. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.80985/4.83072. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.81191/4.83738. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.80519/4.82484. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.81138/4.83303. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.80773/4.83466. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.80616/4.84294. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.80735/4.82545. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.80711/4.83493. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.80934/4.84532. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.80696/4.83349. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.80472/4.83565. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.80059/4.86162. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.80830/4.83440. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.80499/4.84450. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.80319/4.85881. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.80395/4.85488. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.80109/4.85862. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.80506/4.84057. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.79794/4.84549. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.79795/4.84805. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.79773/4.83014. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.79947/4.82979. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.80215/4.83078. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.79602/4.83481. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.79799/4.83639. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.79880/4.83483. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.79770/4.82973. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.79847/4.83964. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.79384/4.84172. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.79545/4.84277. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.79313/4.85341. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.79942/4.83920. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.79741/4.82964. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.79350/4.83961. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.79280/4.85123. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.78933/4.83478. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.78805/4.84329. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.78485/4.87815. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.81193/4.85203. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.79803/4.83042. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.79335/4.84918. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.78757/4.83477. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.78967/4.84525. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.78864/4.85201. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78859/4.86382. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78598/4.85007. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.78414/4.84460. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.78902/4.84493. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.77916/4.85567. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.78742/4.87128. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.79152/4.83382. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.78798/4.85298. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.78876/4.84184. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.78644/4.83700. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.78970/4.85194. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.78988/4.83597. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.79205/4.83225. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.79432/4.84164. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.79317/4.85376. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.79225/4.83398. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.78655/4.84687. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.79624/4.83036. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.79453/4.81807. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.78995/4.84067. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.78727/4.85060. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.78694/4.84746. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.78829/4.85152. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.78718/4.85112. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.019772733298509516\n",
      "Epoch 0, Loss(train/val) 4.92865/4.91599. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.90231/4.89654. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.90234/4.91892. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.90146/4.90958. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.89593/4.89487. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.88447/4.89777. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.88443/4.90420. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.88946/4.90970. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.88898/4.90235. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.88326/4.90305. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.88281/4.90953. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.88759/4.90160. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.88347/4.90095. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.88036/4.90155. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.88048/4.89944. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.88123/4.89775. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.87914/4.89810. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.87637/4.89630. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.87965/4.89611. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.87619/4.89751. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.87856/4.89420. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.88011/4.88288. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.88016/4.91355. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.87956/4.90622. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.88087/4.89875. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.87196/4.91845. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.87759/4.90464. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.87816/4.90333. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.87824/4.88859. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.87214/4.90284. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.87314/4.88897. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.87037/4.90269. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.87351/4.88655. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.87430/4.89263. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.87302/4.89620. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.86840/4.88756. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.87054/4.87382. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.86218/4.89633. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.87480/4.87422. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.86817/4.89100. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.86548/4.88410. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.86783/4.89080. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.87202/4.88741. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.86518/4.88225. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.86471/4.88675. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.86886/4.87394. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.87519/4.88599. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.87739/4.88391. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.87143/4.88950. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.86837/4.88901. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.87017/4.88449. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.86407/4.88803. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.86281/4.91021. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.86323/4.89419. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.86806/4.88963. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.86393/4.89432. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.86909/4.90135. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.86802/4.88840. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.86011/4.91239. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.86497/4.90162. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.86437/4.88820. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.86309/4.89130. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.86070/4.89217. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.86900/4.91209. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.86580/4.89682. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.86693/4.89972. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.86066/4.90524. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.86090/4.89627. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.86197/4.89967. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.85941/4.89717. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.85979/4.90694. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.86129/4.89962. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.86326/4.89191. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.86010/4.89788. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.85949/4.89437. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.85431/4.90153. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.85665/4.90922. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.85544/4.90964. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.85954/4.89018. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.85668/4.89826. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.86149/4.88313. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.87339/4.87431. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.86707/4.87831. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.86151/4.89029. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.85994/4.90034. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.86229/4.89423. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.86277/4.88418. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.85764/4.88428. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.86391/4.87495. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.85492/4.89162. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.86027/4.87993. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.86562/4.92909. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.87883/4.89587. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.86694/4.90037. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.86370/4.90476. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.86861/4.90342. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.86390/4.91195. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.86255/4.90179. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.86235/4.90304. Took 0.07 sec\n",
      "Epoch 99, Loss(train/val) 4.86529/4.90751. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.11383959916814836\n",
      "Epoch 0, Loss(train/val) 4.82744/4.89851. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.80359/4.75294. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.80896/4.73045. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.80112/4.76163. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79068/4.76296. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.78954/4.75195. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.79285/4.75069. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.79095/4.75330. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.79035/4.74827. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79124/4.75723. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79127/4.75473. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.78940/4.75871. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.78951/4.75955. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.78850/4.75796. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.78544/4.75708. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.78447/4.76249. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78617/4.74823. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.78627/4.74296. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.78527/4.75877. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.77947/4.73985. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.77903/4.73967. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.77537/4.75175. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.77704/4.74887. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.77497/4.73906. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.78101/4.76331. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.77829/4.74753. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.77625/4.73947. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.77764/4.74696. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77226/4.74132. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.77441/4.75347. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.77246/4.74023. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.76527/4.73749. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.77085/4.74766. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.76975/4.75246. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.77385/4.76324. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.76834/4.74886. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.76970/4.74653. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.76180/4.74335. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.76563/4.75723. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.76722/4.75412. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.76876/4.73844. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.76291/4.74453. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.76402/4.74501. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.76239/4.73727. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.76411/4.74969. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.76158/4.74217. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.76395/4.74679. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.76188/4.75526. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.76117/4.73751. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.75813/4.74994. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.76371/4.73721. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.76525/4.74789. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.75996/4.73619. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.75808/4.73505. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.76023/4.73981. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.75611/4.73901. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.75825/4.74621. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.75643/4.73471. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76291/4.73346. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.76959/4.76878. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.77564/4.73640. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.77389/4.74682. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.77293/4.74206. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.77024/4.74268. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.76430/4.75210. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.76261/4.74819. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.76320/4.74272. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.76203/4.74846. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.75796/4.74274. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.75631/4.74471. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.75870/4.73707. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.75852/4.74060. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.75971/4.74756. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.75567/4.73185. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.75751/4.74929. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.75392/4.74157. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.75641/4.73359. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.76383/4.73243. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.75849/4.74544. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.75733/4.74717. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.75444/4.74649. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.75506/4.74704. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.75713/4.74022. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.75210/4.75421. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75783/4.74282. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.75471/4.75555. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.75221/4.75475. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.75296/4.74765. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.75215/4.75895. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.75480/4.74388. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.74930/4.74609. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.75684/4.74359. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.75228/4.74988. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.75319/4.74692. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.75068/4.74740. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.74757/4.75242. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.75330/4.74356. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.75191/4.75172. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.75003/4.75251. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.75555/4.73780. Took 0.08 sec\n",
      "ACC: 0.578125, MCC: 0.14640316381507212\n",
      "Epoch 0, Loss(train/val) 4.94805/4.86091. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.84772/4.84509. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.85088/4.85707. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.85225/4.86073. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.85205/4.86071. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.85168/4.86343. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.84640/4.86630. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.84842/4.86080. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.84472/4.86381. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.84094/4.87268. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.84337/4.87171. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.84345/4.87173. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.84408/4.88201. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.83867/4.88295. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.84177/4.87866. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.83448/4.88528. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.83803/4.87955. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.83621/4.88366. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.83489/4.88491. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.83394/4.89418. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.83099/4.88308. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.83171/4.89295. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.82975/4.88329. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.83024/4.88648. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.82791/4.88464. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.82743/4.89051. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.82647/4.88574. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82535/4.89452. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.82693/4.89343. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.82249/4.89270. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.82454/4.89109. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.82640/4.89608. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.82678/4.89530. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.82105/4.88396. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.82475/4.88357. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.82276/4.87827. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.81874/4.88965. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.82113/4.89050. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.82009/4.89447. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.81932/4.87900. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.81899/4.88087. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.81880/4.89489. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.81562/4.90183. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.81662/4.90770. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.81729/4.87859. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.81497/4.89570. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.81285/4.89051. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.81619/4.88986. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 4.81537/4.86672. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.82178/4.87259. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.81952/4.87604. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.82200/4.85884. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.81123/4.88865. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.81562/4.87177. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.81117/4.86794. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.82599/4.84682. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.82398/4.84174. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.82507/4.85692. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.82060/4.85688. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.81548/4.88834. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.81989/4.86436. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.81342/4.87538. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.81047/4.85489. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.82928/4.84704. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.81988/4.84485. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.81632/4.85233. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.81421/4.86221. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.81437/4.85721. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.81767/4.86244. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.81910/4.85056. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.82037/4.83932. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.81966/4.85917. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.81555/4.82636. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.80990/4.88619. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.82457/4.86080. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.81699/4.87797. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.81442/4.88059. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.81699/4.88105. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.81194/4.86544. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.80931/4.85819. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.81093/4.86861. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.81306/4.87392. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.81318/4.87240. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.80841/4.85509. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.81236/4.86396. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.81201/4.86071. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.81384/4.86289. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.81255/4.84991. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.82806/4.84476. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.82453/4.84052. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.81927/4.84068. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.81299/4.86146. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.82086/4.84717. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.81658/4.85681. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.81747/4.85314. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.81987/4.86887. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.81843/4.85483. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.81721/4.86278. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.81109/4.85432. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.81544/4.85323. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.12267022872162016\n",
      "Epoch 0, Loss(train/val) 5.06878/4.97569. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.99885/5.08106. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.00058/5.13390. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.00611/5.12703. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.00337/5.14423. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.00009/5.14639. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.00821/5.12357. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.00606/5.07665. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.00336/5.04318. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.99950/5.02701. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.99744/5.03104. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.99495/5.02867. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.99606/5.04089. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.99380/5.04071. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.99625/5.03624. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.99474/5.04403. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.99453/5.05107. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.99513/5.03334. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.99317/5.01837. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.99709/5.02101. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.99630/5.03426. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 4.99284/5.02447. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.99320/5.01687. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.98858/5.02922. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.99137/5.02630. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.99077/5.03139. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.98868/5.02545. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.98655/5.03689. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.98908/5.03567. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.99226/5.01052. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.98666/5.02041. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.98691/5.02264. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.98769/5.00983. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.98609/5.02277. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.98655/5.02123. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.98382/5.01779. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.98508/5.02530. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.98500/5.01746. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.98177/5.00319. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.97990/5.02287. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.97963/5.04946. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.99054/4.98192. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.99893/5.04265. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.99306/5.04669. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.99031/5.01525. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.99020/5.04472. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.99058/5.06078. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.99246/5.05152. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.98888/5.02806. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.98673/5.02943. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.98991/5.00907. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.99490/5.03308. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.99025/5.01203. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.98962/5.00103. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.98854/5.02352. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.98799/5.01640. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.98649/5.00038. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.98466/5.01071. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.98283/5.00605. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.98977/5.02492. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.98724/5.03202. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.98395/5.01576. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.99953/5.00393. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.98918/5.03406. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.98442/5.01060. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.97903/5.03683. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.98447/5.00041. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.98496/4.99099. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.98961/5.01220. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.98265/5.02456. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.98070/4.99959. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.97977/5.02323. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.98350/5.02561. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.98386/5.01940. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.98046/5.03034. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.98441/5.03372. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.98392/5.03396. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.98305/4.99178. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.98175/5.02363. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.98630/5.01299. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.98492/5.01686. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.98066/5.03358. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.98771/5.01839. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.98110/5.02847. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.97766/5.03342. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.97977/5.00102. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.97481/5.01935. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.97767/5.02014. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.98469/5.00236. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.98094/5.02381. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.98216/5.04577. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.98020/5.02639. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.97591/5.03845. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.97941/5.03478. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.97753/5.04538. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.97610/5.03498. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.97531/5.03316. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.97690/5.01699. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.97207/5.06057. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.98497/5.04548. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.17004550636718183\n",
      "Epoch 0, Loss(train/val) 5.02582/4.93516. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.96485/4.94383. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.95213/4.94026. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.95222/4.94203. Took 0.07 sec\n",
      "Epoch 4, Loss(train/val) 4.95175/4.94378. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.95068/4.95069. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.95156/4.96130. Took 0.07 sec\n",
      "Epoch 7, Loss(train/val) 4.95220/4.96108. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.94622/4.96803. Took 0.07 sec\n",
      "Epoch 9, Loss(train/val) 4.94754/4.97380. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.94421/4.97695. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.94442/4.98058. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.94362/4.98884. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.94872/4.97937. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.94123/5.00702. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.94477/4.97956. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.94127/4.99497. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.93831/4.98220. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.93424/4.99594. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.94422/4.98115. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.93593/4.99294. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.93246/4.98255. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.94153/4.99733. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.93536/5.00155. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.93452/4.99212. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.93385/4.99046. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.93753/4.98975. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.93399/4.99774. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.93716/5.00798. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.93617/5.01082. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.93399/5.01086. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.93351/5.00761. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.93484/5.00490. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.92904/5.01808. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.92866/5.01302. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.92886/5.00440. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.93092/5.01006. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.92705/5.02106. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.93098/4.98264. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.93784/5.00547. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.94133/4.98099. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.93683/5.01430. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.93918/4.99310. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.93592/5.00076. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.93542/4.99876. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.93622/5.00090. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.93595/4.99891. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.93362/4.99715. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.93609/4.98753. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.93194/4.99259. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.93717/4.98913. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.93454/5.00459. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.93655/4.98900. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.93660/5.00973. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.93153/4.99965. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.93162/4.99739. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.93034/5.01771. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.93178/4.99051. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.93443/4.97836. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.93445/4.97374. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.94476/4.98614. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.93071/4.99691. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.92967/4.99439. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.93040/4.98554. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.93464/4.95574. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.95302/4.95658. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.94348/4.96317. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.94137/4.98001. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.94191/4.98667. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.93721/4.98617. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.93670/4.98462. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.93757/4.98009. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.93410/4.99135. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.93309/4.97693. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.93850/4.98493. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.93063/4.98868. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.93097/5.02247. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.93306/5.02053. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.93256/4.98424. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.93450/5.01201. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.94250/4.99770. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.94610/4.98738. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.94299/4.98871. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.93601/5.00424. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.93450/4.99850. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.93743/4.99752. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.93294/4.99549. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.93652/4.99319. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.93571/5.00019. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.93307/4.99082. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.93018/5.00700. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.92595/4.99922. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.93148/4.98326. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.93428/4.97582. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.92958/4.99914. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.92810/4.99288. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.92713/5.00154. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.92443/4.99952. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.92745/4.98539. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.92937/5.00393. Took 0.08 sec\n",
      "ACC: 0.609375, MCC: 0.2168404023533557\n",
      "Epoch 0, Loss(train/val) 4.94604/4.94527. Took 0.19 sec\n",
      "Epoch 1, Loss(train/val) 4.90574/4.86863. Took 0.17 sec\n",
      "Epoch 2, Loss(train/val) 4.89518/4.85814. Took 0.12 sec\n",
      "Epoch 3, Loss(train/val) 4.87846/4.86022. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.87922/4.86159. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.88287/4.85946. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.88158/4.85943. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.88137/4.86067. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.88044/4.85985. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.88038/4.86053. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.87663/4.86150. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.87749/4.86128. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.87744/4.85953. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.87756/4.86064. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.87370/4.86101. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.87516/4.86057. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.87488/4.86165. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.87224/4.86133. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.87285/4.86342. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.87435/4.86162. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.87397/4.87098. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.87715/4.86732. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.87308/4.86580. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.87239/4.86917. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.87281/4.87137. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.87224/4.86593. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.87218/4.86998. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.87222/4.86693. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.87164/4.86613. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.86936/4.87661. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.87187/4.87724. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.87240/4.87354. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.86701/4.87243. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.86991/4.87501. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.86885/4.88085. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.86932/4.87196. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.87622/4.87830. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.87168/4.87718. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.87124/4.88791. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.86864/4.91782. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.87420/4.88393. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.86985/4.88295. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.86807/4.88985. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.86933/4.89490. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.86959/4.89531. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.86788/4.90745. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.86419/4.92045. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.86838/4.89806. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.86920/4.91345. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.86774/4.91275. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.86549/4.91698. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.86764/4.90999. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.86218/4.94096. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.86479/4.91910. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.86796/4.91470. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.86413/4.91499. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.86355/4.91235. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.86198/4.90516. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.86706/4.89059. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.86639/4.89931. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.86166/4.92794. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.86628/4.92628. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.85839/4.92009. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.86019/4.89209. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.86551/4.92886. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.86022/4.93650. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.86603/4.89615. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.86234/4.91504. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.86703/4.90675. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.85907/4.93189. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.86464/4.90535. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.87478/4.87986. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.86727/4.89821. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.86393/4.90364. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.86723/4.90277. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.86619/4.92203. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.86835/4.88422. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.86635/4.90050. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.86300/4.90318. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.86730/4.89347. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.87370/4.86411. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.86811/4.88351. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.86693/4.88952. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.86605/4.89318. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.86581/4.90071. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.86675/4.89225. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.86514/4.90771. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.86223/4.90761. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.86598/4.90583. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.86424/4.89843. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 4.86699/4.89896. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.86396/4.90744. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.85855/4.90365. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.85856/4.92143. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.86523/4.90796. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.86420/4.90653. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.86002/4.90361. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.86428/4.91404. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.86420/4.90498. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.86398/4.93189. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.0905982365507463\n",
      "Epoch 0, Loss(train/val) 4.81879/4.82444. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.80391/4.82703. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.79724/4.82638. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79958/4.82854. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.80218/4.82757. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79914/4.82573. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.79527/4.83311. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.80262/4.82760. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.79749/4.83083. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79996/4.83415. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79960/4.82584. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.79955/4.83032. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.80222/4.83118. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.79998/4.83434. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.79510/4.83691. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.79614/4.83959. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.79565/4.83904. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.79243/4.84033. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.79261/4.84825. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.79307/4.84425. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.79306/4.84231. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78918/4.85250. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.79183/4.84441. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.79177/4.85454. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.78953/4.86358. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.79450/4.85168. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.79086/4.85365. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.78709/4.86761. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.78772/4.85806. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.78724/4.86057. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.79176/4.86608. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.79693/4.84740. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.79306/4.85346. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78774/4.86202. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.79143/4.86096. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.78821/4.86395. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.78640/4.87165. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.79260/4.85832. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.78772/4.87210. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.78651/4.87132. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.78548/4.88196. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.78708/4.86786. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.78624/4.88296. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.78511/4.87658. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.78161/4.88304. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.78238/4.88545. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.78280/4.87289. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.78377/4.87269. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.78583/4.89537. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.77998/4.89437. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.78257/4.87972. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.78065/4.89614. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.78032/4.89615. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.78130/4.89545. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.78408/4.87874. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.79309/4.85572. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.79149/4.84767. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.78461/4.86922. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.78526/4.84291. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.79041/4.84774. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.78455/4.86998. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.79033/4.86165. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.78402/4.86829. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.78579/4.88216. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.77857/4.88533. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.78598/4.88732. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.78216/4.87548. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.78048/4.87894. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.77824/4.90153. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.77955/4.86746. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.77963/4.87944. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78532/4.87302. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.78046/4.87334. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.77987/4.89091. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.77904/4.89185. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.78646/4.87620. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.79594/4.84917. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.79596/4.85297. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.78918/4.85043. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.78777/4.88443. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.77889/4.90033. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.78018/4.89962. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.77793/4.89228. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.77848/4.89142. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.77824/4.89842. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.78638/4.87093. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.78151/4.87675. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.78197/4.88272. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.77904/4.88891. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.78099/4.89765. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.77781/4.90275. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.77716/4.89629. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.77344/4.90152. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.77460/4.89088. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.77974/4.88854. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.77876/4.88988. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.77279/4.88889. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.77824/4.89207. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77573/4.88807. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.78224/4.88162. Took 0.08 sec\n",
      "ACC: 0.40625, MCC: -0.19136555680572745\n",
      "Epoch 0, Loss(train/val) 4.79783/4.82053. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.74521/4.79805. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.74208/4.78406. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.73770/4.76606. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.73354/4.76411. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.73373/4.77167. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.73597/4.77332. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.73217/4.76646. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.72875/4.77395. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.73100/4.77463. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.73419/4.77800. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.73028/4.77064. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.73047/4.77982. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.72586/4.78787. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.72791/4.79687. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.72575/4.79219. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.72426/4.79857. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.71903/4.81816. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.72243/4.79937. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.72235/4.81010. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.71936/4.80027. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.71436/4.82986. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.71694/4.78985. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.71848/4.81726. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.71930/4.80944. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.71034/4.82018. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.71368/4.80555. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.71128/4.80918. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.71192/4.82196. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.71136/4.82601. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.71504/4.79954. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.70923/4.81846. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.71446/4.80727. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.71099/4.77676. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.70783/4.81499. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.71027/4.81737. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.70715/4.82338. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.70521/4.82326. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.70903/4.80245. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.70715/4.82296. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.70841/4.81440. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.70323/4.80503. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.70441/4.82087. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.70777/4.80419. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.70643/4.82943. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.70718/4.82926. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.70330/4.83694. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.70596/4.81323. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.70116/4.82795. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.70322/4.81826. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.70328/4.83376. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.70687/4.82039. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.69852/4.83883. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.70226/4.83908. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.69985/4.84222. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.70117/4.84088. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.70276/4.85716. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.69932/4.83320. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.70134/4.86176. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.70075/4.81923. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.69527/4.84292. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.70180/4.82185. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.69919/4.83320. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.69460/4.82282. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.69946/4.83181. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.68901/4.83232. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.70124/4.83221. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.70127/4.82985. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.69968/4.83344. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.69941/4.84180. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.68993/4.85331. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.69455/4.86130. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.69407/4.85454. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.69137/4.84160. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.69419/4.86085. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.69619/4.81862. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.69352/4.82655. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.69543/4.87664. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.68877/4.84914. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.69019/4.86925. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.69012/4.88638. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.69092/4.84126. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.69106/4.87512. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.69115/4.86021. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.69601/4.83698. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.69100/4.85361. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.68931/4.85505. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.68951/4.85980. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.69113/4.85084. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.68690/4.85769. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.68841/4.85352. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.69120/4.83673. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.68862/4.88859. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.69594/4.84866. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.68893/4.87975. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.69115/4.86645. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.69969/4.86161. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.72060/4.75469. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.72028/4.75839. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.72091/4.76057. Took 0.08 sec\n",
      "ACC: 0.40625, MCC: -0.19088542889273333\n",
      "Epoch 0, Loss(train/val) 4.96451/4.98381. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.93595/4.96151. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.94010/4.91807. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.92877/4.91443. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.91928/4.91463. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.91890/4.91546. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.92070/4.91485. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.92197/4.91467. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.91798/4.91747. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.91944/4.91950. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.91548/4.92347. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.91352/4.92724. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.91576/4.92580. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.91487/4.92831. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.91310/4.92976. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.90978/4.92824. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.91377/4.92937. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.91205/4.92918. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.91223/4.93400. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.91128/4.93743. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.91103/4.93554. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.91108/4.93766. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.90667/4.94560. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.91123/4.94070. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.91054/4.94253. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.90692/4.94738. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.90838/4.94844. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.90676/4.94693. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.90854/4.94612. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.90363/4.94784. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.90532/4.95289. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.90799/4.94652. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.90948/4.94584. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.90511/4.94907. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.90594/4.95681. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.90437/4.94905. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.90531/4.95158. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.90339/4.94918. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.90623/4.94127. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.90648/4.94315. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.90304/4.94092. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.89785/4.94949. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.90105/4.95174. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.90001/4.94563. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.89997/4.94939. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.89940/4.94236. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.90001/4.94135. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.89616/4.94436. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.89852/4.94654. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.89498/4.95228. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.89787/4.94474. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.89698/4.94002. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.89522/4.95054. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.89744/4.93968. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.89388/4.94618. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.89570/4.95015. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.89391/4.93479. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.89244/4.94238. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.89867/4.94844. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.89346/4.95349. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.89014/4.95580. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89620/4.93289. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.88916/4.95129. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.89380/4.94037. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.89218/4.96412. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.89082/4.93730. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.88795/4.93827. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.88493/4.94149. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.88192/4.94990. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.88450/4.95822. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.89172/4.95061. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.88137/4.95296. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88392/4.93188. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.89319/4.94016. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.88562/4.94342. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.88879/4.94666. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88618/4.92473. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.88427/4.94386. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.88435/4.94640. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88331/4.93749. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.88518/4.93691. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.88510/4.93107. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.88432/4.92816. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.88291/4.95461. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.88158/4.93600. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.88043/4.94372. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.88162/4.93694. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.88284/4.94924. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.87963/4.92892. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88337/4.94149. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.87861/4.94199. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.88156/4.94926. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.87400/4.94575. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.87797/4.94216. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.88466/4.92547. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.87967/4.94738. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.87765/4.95677. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.87998/4.94173. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.88586/4.92842. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.89078/4.96818. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.07570682517832987\n",
      "Epoch 0, Loss(train/val) 4.90776/4.84410. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.83267/4.82053. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.82633/4.81872. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.82509/4.81898. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.82799/4.81815. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.82758/4.81900. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.82486/4.82645. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.82522/4.83000. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.82562/4.83185. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.82166/4.83570. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.81916/4.84122. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.82293/4.84247. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.81840/4.84479. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.82034/4.85180. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.81792/4.84970. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.82055/4.84735. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.81751/4.85014. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.81993/4.84379. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.81474/4.85040. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.81475/4.85750. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.81510/4.85943. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.82102/4.84665. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.82343/4.84972. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.81947/4.85665. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.82388/4.85135. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.81708/4.85542. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.81629/4.86612. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82563/4.81215. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.82899/4.81768. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.82520/4.81730. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.82378/4.81832. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.82396/4.81726. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.82044/4.81444. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.82494/4.81303. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.82089/4.81352. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.82221/4.81591. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.82126/4.81618. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.82120/4.81407. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.82004/4.81176. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.81458/4.81163. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.82055/4.81618. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.81668/4.81248. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.81907/4.81099. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.82370/4.81405. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.82123/4.80526. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.81498/4.80765. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.81478/4.81193. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.81544/4.81618. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.81694/4.81573. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.80856/4.81901. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.81832/4.81368. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.81482/4.82867. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.81375/4.81068. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.80699/4.81605. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.81041/4.82506. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.81347/4.81475. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.81872/4.81516. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.81751/4.81799. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.81392/4.82073. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.81485/4.81483. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.81320/4.81804. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.80648/4.82303. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.81050/4.81179. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.81045/4.81472. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.80635/4.81161. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.81719/4.80816. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.81112/4.83814. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.82618/4.81551. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.81796/4.81679. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.81608/4.81844. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.81151/4.82106. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.81073/4.81756. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.81622/4.81868. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.80882/4.82066. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.80577/4.82148. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.81093/4.82178. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.81269/4.82046. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.81243/4.82746. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.80941/4.82180. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.80948/4.82397. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.80811/4.82109. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.80646/4.83213. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.80492/4.82417. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.80754/4.82917. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.81066/4.82762. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.80659/4.82707. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.80675/4.82589. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.80484/4.82615. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.80392/4.82684. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.80694/4.82646. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.80475/4.82773. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.80555/4.81858. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.80413/4.82395. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.80113/4.81671. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.80116/4.82156. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.81092/4.82113. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.80194/4.81971. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.80584/4.82275. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.79882/4.81857. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.80436/4.81695. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.14285714285714285\n",
      "Epoch 0, Loss(train/val) 4.93834/4.84806. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.85061/4.84833. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.85613/4.84997. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.86447/4.85910. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.86758/4.88260. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.86095/4.88483. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.85112/4.87698. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.84927/4.86913. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.85116/4.87153. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 4.85052/4.87308. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.84917/4.87222. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.84893/4.87053. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.84549/4.86792. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.84553/4.86588. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.84495/4.87316. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.84620/4.86643. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.84295/4.86441. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.84294/4.86881. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.84011/4.86581. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.83891/4.87316. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.84139/4.87705. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.83722/4.86772. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.83614/4.85327. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.83147/4.85884. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.83463/4.86713. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.83892/4.86693. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.83393/4.86743. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.83380/4.85625. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.83368/4.85663. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.83314/4.86682. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.83281/4.87290. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.83249/4.87054. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.83346/4.86510. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.82822/4.85707. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.82881/4.86272. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.83080/4.86457. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.82793/4.86898. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.82922/4.87074. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.83328/4.85629. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.82747/4.86386. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.82553/4.87641. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.82859/4.87054. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.83116/4.89873. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.82831/4.87079. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.83101/4.88770. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.82578/4.89855. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.82997/4.88283. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.82537/4.89915. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.83117/4.86712. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.82769/4.86213. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.82388/4.89413. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.82690/4.87724. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.82363/4.87890. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.82346/4.85457. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.82713/4.87627. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.82458/4.86787. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.82564/4.90438. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.82376/4.90579. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.82202/4.86415. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.82122/4.85565. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.82524/4.91931. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83001/4.88192. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.82878/4.88367. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.82504/4.88518. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.82231/4.88375. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.82777/4.88160. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.82166/4.88429. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.82498/4.87848. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.82624/4.88252. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.82356/4.90293. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.82327/4.88062. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.81676/4.88499. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.82356/4.87927. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.82102/4.87011. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.81626/4.88692. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.82190/4.87062. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.81848/4.86785. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.81757/4.88383. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.81772/4.88568. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.81679/4.87275. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.81368/4.88844. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.81623/4.87104. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.81380/4.88496. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.81645/4.89398. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.82143/4.88563. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.81537/4.88331. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.81607/4.87818. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.81586/4.86975. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.81486/4.87220. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.82186/4.87478. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.82335/4.87860. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.81791/4.88277. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.81666/4.87792. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.81093/4.88669. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.81420/4.87041. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.81321/4.88736. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.81054/4.91577. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.81701/4.87911. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.81249/4.88836. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.81291/4.87298. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.02834008097165992\n",
      "Epoch 0, Loss(train/val) 4.92481/4.90155. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.88553/4.90493. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.88944/4.90495. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.88337/4.90675. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.88325/4.90622. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.88273/4.90543. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.87619/4.90988. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.88132/4.90757. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.88066/4.91401. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.87800/4.91189. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.87906/4.90798. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.87730/4.91000. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.87763/4.90528. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.87533/4.91113. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.87353/4.91482. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.87442/4.90835. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.87281/4.90555. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.86882/4.91097. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.88003/4.89594. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 4.87769/4.89209. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.87518/4.89399. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.87071/4.89666. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.86850/4.89974. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.87011/4.89909. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.86786/4.89486. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.87082/4.89375. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.86843/4.89181. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.86930/4.89340. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.86741/4.89755. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.86574/4.89670. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.86652/4.88652. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.86441/4.89248. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.86337/4.88908. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.86644/4.89269. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.86236/4.88941. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.86221/4.89594. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.86210/4.88687. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.86379/4.89275. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86387/4.89152. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.85756/4.89738. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.86138/4.90159. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.85551/4.89841. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.85926/4.88617. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.86208/4.89242. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.85896/4.89225. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.85758/4.88919. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.86157/4.88778. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.85778/4.89237. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.85878/4.90034. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.85994/4.89491. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.85689/4.90119. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.85971/4.88378. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.85373/4.88725. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.86143/4.87680. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.85541/4.89379. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.85721/4.89168. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.85454/4.88776. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.85293/4.89469. Took 0.07 sec\n",
      "Epoch 58, Loss(train/val) 4.85683/4.89220. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.85443/4.89956. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.85263/4.89045. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.85181/4.89655. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.85046/4.89387. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.85274/4.88178. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.85520/4.88257. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.85482/4.88674. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.85180/4.89225. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.84591/4.90920. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.84865/4.89273. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.85133/4.89449. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.84934/4.89839. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.84653/4.89049. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.84666/4.90158. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.84904/4.89355. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.84644/4.88961. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.84959/4.89366. Took 0.07 sec\n",
      "Epoch 76, Loss(train/val) 4.84654/4.89013. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.84272/4.89030. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.84814/4.87067. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.84709/4.89271. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83658/4.87236. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.84311/4.88588. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.84803/4.87864. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.87981/4.87503. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.87001/4.86639. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.86376/4.87090. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.86744/4.87882. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.85914/4.88421. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.85983/4.87776. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.85211/4.88179. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.85488/4.88733. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.85075/4.88658. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.86284/4.89076. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.85330/4.88309. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.85153/4.88976. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.84968/4.88519. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.84911/4.88223. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.85601/4.87206. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.84712/4.89019. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.84862/4.88624. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.09607689228305229\n",
      "Epoch 0, Loss(train/val) 4.97280/4.92479. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.92451/4.90537. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.92206/4.90228. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.91690/4.90260. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.91444/4.90395. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.91855/4.90424. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.91830/4.89996. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.91380/4.90200. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.91402/4.90606. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.91053/4.90908. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.91376/4.90402. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.90952/4.90578. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.91273/4.90584. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.91041/4.90731. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.90465/4.90415. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.91437/4.91324. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.90650/4.91563. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.90493/4.92811. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.90938/4.92155. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.90327/4.92130. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.90435/4.91600. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.90171/4.92900. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.90092/4.93386. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.90653/4.92523. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.89871/4.92638. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.90569/4.90840. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.90361/4.91113. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.90395/4.91099. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.90007/4.90846. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.89891/4.90853. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.89964/4.91398. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.89871/4.94454. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.90288/4.91452. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.90385/4.91746. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.89747/4.91023. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.89442/4.91184. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.89404/4.91911. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.89592/4.91121. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.89669/4.92032. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.89459/4.91934. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.89336/4.92072. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.89795/4.91796. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.89299/4.92559. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.89772/4.91049. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.88790/4.93258. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.89127/4.92423. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.89291/4.92947. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.88529/4.94730. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.89159/4.91762. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.88831/4.92687. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.88898/4.92899. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.88810/4.92816. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.88702/4.92748. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.88990/4.92479. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.88258/4.95419. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.88668/4.92435. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.88631/4.94303. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.87938/4.96176. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.89836/4.91623. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.88545/4.95252. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.89080/4.93768. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.88832/4.95217. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.88171/4.95302. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.88031/4.96725. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.87669/4.96521. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.88244/4.96533. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.88247/4.93672. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.88109/4.95335. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.88202/4.95170. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.87634/4.96462. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.87580/4.96373. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.87966/4.95661. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88233/4.96162. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.87405/4.96498. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.86970/4.99231. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.89066/4.93485. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.87332/4.96912. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.87849/4.95235. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.87892/4.96955. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.87307/4.98024. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.87906/4.96486. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.87395/4.98281. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.87080/4.97668. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.86971/4.97714. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.87446/4.96017. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.87612/4.95583. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.87012/5.00622. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.87171/4.95181. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.87718/4.97531. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.87041/4.97769. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.87520/4.95493. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.86600/4.96994. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.87411/4.98907. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.86773/4.95852. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.85425/5.03392. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.87170/4.95182. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.87501/4.95513. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.86913/4.96979. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.86859/4.98997. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.86437/4.97492. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.07192118226600985\n",
      "Epoch 0, Loss(train/val) 4.99489/4.95858. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.93511/4.93052. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.92794/4.93317. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.92913/4.93401. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.92798/4.93309. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.92035/4.93644. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.92324/4.93128. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.92437/4.93726. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.92251/4.93779. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.91989/4.94260. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.91880/4.94154. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.91601/4.94385. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.91670/4.94270. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.92089/4.93435. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.92393/4.93079. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.92399/4.93399. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.92106/4.93612. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.92004/4.93314. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.91892/4.93522. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.91739/4.93929. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.91640/4.94043. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.91507/4.94102. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.91552/4.93966. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.91091/4.95172. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.90983/4.95104. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.91063/4.95443. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.91328/4.94640. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.91064/4.95956. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.91128/4.93933. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.90741/4.94742. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.90916/4.94712. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.90579/4.95400. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.90784/4.95721. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.90403/4.95681. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.90307/4.95586. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.90170/4.95920. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.90543/4.94709. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.90201/4.96452. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.90396/4.94941. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.89976/4.96548. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.89938/4.95683. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.89613/4.96294. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.90821/4.94127. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.91290/4.94152. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.90441/4.95061. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.90260/4.96252. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.90112/4.95600. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.90443/4.96176. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.89717/4.97248. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.89706/4.95439. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.90241/4.97167. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.89549/4.97024. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.89560/4.97756. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.90217/4.95442. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.89683/4.95515. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.89302/4.95084. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.89690/4.97141. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.90253/4.95105. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.89811/4.97840. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.88930/4.96932. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.89461/4.96837. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89070/4.97310. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.89237/4.96167. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.89141/4.96284. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.89163/4.97156. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.89705/4.97757. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.88735/4.96415. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.89270/4.97355. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.89435/4.98006. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.89776/4.97656. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.88916/4.97376. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.89835/4.97280. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88866/4.99361. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88565/4.98118. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.88998/4.97570. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.88999/4.98855. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.89391/4.96982. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.88357/4.98097. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.88482/4.98512. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88469/5.00516. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.88571/4.98859. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.88580/4.98261. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.88597/5.00957. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.88398/4.98083. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.89255/4.97075. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.88839/4.98079. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.88281/4.99369. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.87753/4.99167. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.89064/4.97617. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.87775/5.02057. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.89068/4.99409. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.88417/4.98280. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.88389/5.00150. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.89150/4.99889. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.88618/5.01273. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.87938/4.98987. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.88153/4.99602. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.87782/5.02376. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.88334/4.98837. Took 0.13 sec\n",
      "Epoch 99, Loss(train/val) 4.87516/4.99546. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.04350001325672093\n",
      "Epoch 0, Loss(train/val) 4.94736/4.95436. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.91248/4.91951. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.91063/4.93684. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.90958/4.92754. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.91192/4.93673. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.90406/4.93304. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.90918/4.93434. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.90756/4.93141. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.90933/4.94484. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.90966/4.93675. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.91370/4.92672. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.91364/4.92346. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.91222/4.92212. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.91193/4.92509. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.90959/4.92317. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.90588/4.92421. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.90453/4.92534. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.90627/4.92383. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.90437/4.92361. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.90142/4.92681. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.90364/4.93715. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.90651/4.93184. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.90622/4.92064. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.89994/4.93517. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.89967/4.93865. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.89996/4.92386. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.90156/4.93345. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.90018/4.92801. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.90007/4.92233. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.89736/4.91393. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.89384/4.92801. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.89552/4.92870. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.89747/4.94699. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.89876/4.93469. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.89448/4.92392. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.89026/4.93153. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.89761/4.87637. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.91403/4.90435. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.90767/4.90898. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.90453/4.91350. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.90310/4.92006. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.90248/4.93345. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.90025/4.92766. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.89768/4.92573. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.89605/4.94348. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.89830/4.94607. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.89816/4.93464. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.89769/4.93144. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.89730/4.93402. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.89781/4.93841. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.90192/4.93551. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.89261/4.92741. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.89712/4.93405. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.89203/4.94055. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.89345/4.95071. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.89372/4.94261. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.89403/4.94604. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.89502/4.93664. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.89859/4.91998. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.89673/4.95869. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.89593/4.95238. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89859/4.93452. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.89068/4.95211. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.89436/4.96026. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.89542/4.94593. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.89389/4.95434. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.88781/4.95455. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.88874/4.95260. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.89173/4.95115. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.89317/4.94567. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.89202/4.94525. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.88909/4.94246. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88332/4.96427. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88776/4.95690. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.89111/4.96071. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.88440/4.95613. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88337/4.95337. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.88616/4.95830. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.87948/4.95912. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88446/4.95824. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.88213/4.96434. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.88440/4.96203. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.87918/4.97936. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.88090/4.95466. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.88079/4.96711. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.87719/4.97158. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.87738/4.94486. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.87694/5.00634. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.87862/4.97056. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88091/4.95445. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.87621/4.98088. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.87476/4.96260. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.87432/4.97662. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.89092/4.97928. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.88633/4.96020. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.88503/4.96539. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.88200/4.97308. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.88344/4.96632. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.87806/4.98133. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.87909/4.99333. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.03779786830012622\n",
      "Epoch 0, Loss(train/val) 5.02561/5.05608. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.00840/4.98167. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 5.00585/5.00481. Took 0.13 sec\n",
      "Epoch 3, Loss(train/val) 5.00182/5.00145. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.00089/4.99860. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.00147/5.00343. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.00388/4.99727. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.00076/4.99548. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.99878/4.99606. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.99368/5.00192. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.99749/5.00644. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.99659/5.00850. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.99530/5.00389. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.99953/4.99805. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.99351/4.99133. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.99333/4.99583. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.99714/4.98763. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.99303/4.98413. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.99415/4.98842. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.99294/4.98690. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.99526/5.00051. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.99534/5.01158. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.99144/5.01494. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.98796/5.02554. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.99003/5.01581. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.98761/5.01753. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.98886/5.00838. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.98922/5.02525. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.98737/5.02989. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.98549/5.02134. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.98367/5.02956. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.98663/5.01391. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.98604/5.01581. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.98373/5.02312. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.98447/5.02836. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.99043/5.01753. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.98308/5.02536. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.98245/5.03221. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.98332/5.02857. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.98014/5.03834. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.98225/5.03493. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.98069/5.04129. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.97730/5.04355. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.97785/5.02652. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.98055/5.03048. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.98171/5.03068. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.98066/5.02283. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.98450/5.03323. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.98084/5.03250. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.97823/5.04323. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.98121/5.01578. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.97583/5.04671. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.97723/5.02341. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.97807/5.03869. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.97946/5.02495. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.98368/5.04043. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.97873/5.03557. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.97653/5.04309. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.97682/5.03243. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.97779/5.02853. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.97229/5.03590. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.97513/5.02400. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.97682/5.02577. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.97800/5.03076. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.97366/5.01607. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.97104/5.05423. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.98004/5.02951. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.97803/5.04438. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.97103/5.04721. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.97441/5.05503. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.97195/5.05340. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.96947/5.06121. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.96692/5.03684. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.97161/5.04270. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.97543/5.04345. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.97560/5.02422. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.97959/5.04049. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.97868/5.03505. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.96944/5.03908. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.97529/5.03076. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.97475/5.05618. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.96875/5.05167. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.97475/5.04576. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.96937/5.04948. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.97015/5.03164. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.96896/5.03247. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.96766/5.04379. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.96843/5.02571. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.97530/5.04004. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.96628/5.07745. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.96317/5.06238. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.96716/5.04311. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.97147/5.02432. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.96844/5.04453. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.97515/5.04330. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.96553/5.04971. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.96711/5.05626. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.96759/5.05184. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.96779/5.04202. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.96765/5.04848. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.02757337634153148\n",
      "Epoch 0, Loss(train/val) 5.10167/5.00398. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.02571/5.00426. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 5.02157/5.00334. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.02022/5.00353. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.02083/5.00253. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.01883/5.00449. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 5.01651/5.00479. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.01703/5.00436. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.01557/5.00462. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 5.01583/5.00451. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.01430/5.00558. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.01308/5.00889. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.01114/5.01202. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.01386/5.01350. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.01241/5.00963. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.01190/5.00910. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.01184/5.01341. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.01100/5.01335. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 5.01003/5.01886. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.01085/5.02243. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.00896/5.02447. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.00551/5.02332. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.00872/5.02526. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.00924/5.00166. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.01326/5.00347. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.00767/5.00144. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.00800/5.00050. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.00648/5.00036. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.00687/5.00068. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.00882/5.01310. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.00383/5.01151. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.00476/5.01275. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.00466/5.01875. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.00507/5.02244. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.01224/5.02132. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.00558/5.02344. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 5.00338/5.02331. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.00024/5.02031. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.00005/5.02335. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.00597/5.01622. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.00133/5.02516. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.00016/5.03406. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.99982/5.03022. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.99933/5.02785. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.99738/5.05356. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.00160/5.04418. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.99469/5.04375. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.99568/5.06006. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.99684/5.04237. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.99073/5.06993. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.98703/5.07365. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.99479/5.02821. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.99784/5.05488. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.99291/5.06661. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.99070/5.08161. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.98963/5.05751. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.99470/5.07214. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.99164/5.07006. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.98607/5.09703. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.00678/5.03956. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.00885/5.02881. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.00319/5.03981. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.00130/5.04300. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.00415/5.03953. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.99803/5.04666. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.00416/5.03804. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.99714/5.04759. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.99810/5.04483. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.99485/5.05332. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.99746/5.05325. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.99890/5.06204. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.99601/5.05496. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.99769/5.07821. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.00324/5.06499. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.98784/5.10306. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.98766/5.09118. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.99616/5.09379. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.99688/5.08492. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.99108/5.09447. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.98685/5.09800. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.99223/5.10552. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.98040/5.10980. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.99095/5.09806. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.99001/5.09789. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.98718/5.10644. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.99291/5.15148. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.98836/5.06632. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.99071/5.09866. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.98771/5.09174. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.98512/5.08685. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.98716/5.11769. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.98571/5.09793. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.98325/5.12018. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.98336/5.12026. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.98406/5.08126. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.98089/5.12850. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.99043/5.11986. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.98645/5.07993. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.98156/5.13559. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.98432/5.10141. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.06158357771260997\n",
      "Epoch 0, Loss(train/val) 4.90765/4.89538. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.85013/4.88284. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.84383/4.88581. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.84352/4.88910. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.84168/4.89328. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.83657/4.89405. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.83917/4.89664. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.83584/4.89745. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.83743/4.89409. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.83749/4.90140. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.83430/4.90383. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.83806/4.89982. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.83610/4.90158. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.83829/4.90509. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.83668/4.90894. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.83443/4.91210. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.83557/4.91193. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.83395/4.91476. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.83168/4.90761. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.83080/4.92484. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.83311/4.92119. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.83069/4.91595. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.82989/4.92409. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.83104/4.91814. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.83381/4.91732. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.82911/4.92394. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.83017/4.92512. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.83406/4.93269. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.83112/4.92306. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.83113/4.92084. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.82588/4.93293. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.83042/4.92012. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.82615/4.94285. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.83042/4.92164. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.82765/4.93834. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.82997/4.92437. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.82749/4.93304. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.82697/4.93870. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.82686/4.93281. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.82430/4.95052. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.82367/4.92965. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.82718/4.93554. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.82660/4.93717. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.82582/4.93713. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.81708/4.95554. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.82268/4.93725. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.82688/4.92817. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.82360/4.94264. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.82618/4.93771. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.82224/4.93909. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.81983/4.94181. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.81823/4.95246. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.81948/4.94572. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.82037/4.93642. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.82068/4.95120. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.81939/4.95037. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.81939/4.94102. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.81537/4.95974. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.81641/4.96529. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.81569/4.95366. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.81414/4.96223. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.81912/4.94229. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.82036/4.94889. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.82113/4.96560. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.82016/4.95434. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.81452/4.96910. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.81317/4.95667. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.81975/4.96974. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.81574/4.95139. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.81589/4.96227. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.81345/4.97510. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.81482/4.94370. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.80996/4.96507. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.81422/4.97221. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.81309/4.95269. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.81395/4.95422. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.81802/4.96734. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.81346/4.95467. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.81787/4.95735. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.81146/4.97035. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.81635/4.96270. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.81157/4.98392. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.81419/4.95285. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.81084/4.98805. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.80856/4.97371. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.81141/4.96887. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.81532/4.97664. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.81045/4.99609. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.80640/5.00295. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.80508/5.00689. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.80724/4.96863. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.81398/4.97530. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.81509/4.95341. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.80852/4.97594. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.81065/4.98451. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.80596/5.01100. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.80881/4.97621. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.81303/4.99903. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.81707/4.96696. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.80695/4.99217. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.09379580992210836\n",
      "Epoch 0, Loss(train/val) 4.92521/4.84208. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.86886/4.83908. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.86740/4.83930. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.87421/4.83860. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.86665/4.84040. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.86182/4.84305. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.86462/4.84471. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.86287/4.84262. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.86327/4.84356. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.86328/4.84507. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.86166/4.84376. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.86195/4.84312. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.86263/4.84850. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.85643/4.84519. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.86119/4.84751. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.85673/4.84942. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.85710/4.84707. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.85594/4.85229. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.85171/4.86125. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85714/4.85766. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.85592/4.85494. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.85147/4.86420. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.84802/4.87046. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.84739/4.87235. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.84891/4.87382. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.84387/4.87454. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.84944/4.87280. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.84913/4.86822. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.85416/4.85678. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.85071/4.86252. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.85300/4.86654. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.85044/4.87655. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84970/4.86996. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.85185/4.86585. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.84630/4.88767. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84786/4.88001. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84867/4.88012. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84987/4.87632. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84947/4.87900. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84903/4.87299. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84666/4.88570. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.84828/4.88438. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84776/4.88867. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.84711/4.88458. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.84237/4.88427. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.84794/4.88577. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.84205/4.89996. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.84179/4.90178. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84525/4.88334. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.84001/4.89958. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.84085/4.87516. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.83670/4.88576. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.85061/4.87312. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.84060/4.89099. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.84328/4.88296. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.84317/4.89196. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.83839/4.87854. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.84288/4.88247. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.83858/4.88254. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83977/4.88753. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.83888/4.88439. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83546/4.89437. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.83576/4.88845. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.84320/4.89649. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.84874/4.88430. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.84127/4.90874. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.83844/4.87914. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.83840/4.90216. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.83625/4.88512. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.83724/4.90881. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.83406/4.87985. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.83655/4.91818. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.83935/4.87679. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.83754/4.90023. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.83098/4.88928. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83524/4.92305. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.84139/4.87394. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.83505/4.91532. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.83547/4.88919. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.82998/4.89854. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83349/4.90901. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.83798/4.89052. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.82743/4.90127. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.82975/4.88333. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83302/4.87058. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.82957/4.88105. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.82933/4.88604. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.82494/4.90499. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83504/4.87978. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.83165/4.88675. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.82778/4.90451. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.82709/4.91275. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83355/4.87267. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83187/4.90457. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.82697/4.92019. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.82914/4.92260. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.82480/4.89715. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.83087/4.87734. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83509/4.89507. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.82830/4.90368. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.05501903637039413\n",
      "Epoch 0, Loss(train/val) 4.86743/4.85728. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.85037/4.83694. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.84454/4.84360. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.84561/4.83933. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.84133/4.84415. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.84596/4.84270. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.84251/4.84593. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.84158/4.84491. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.84833/4.84540. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.84601/4.84539. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.84369/4.84892. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.84228/4.84992. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.83865/4.84834. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.83896/4.84895. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.84032/4.84576. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.83871/4.85086. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.84308/4.84885. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.84281/4.85394. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.83861/4.85227. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.83978/4.85513. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.83963/4.85591. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.83810/4.86301. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.83996/4.86866. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.83712/4.86807. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.83470/4.87569. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.83739/4.87232. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.83617/4.88133. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.83828/4.87830. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.83480/4.88035. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.83317/4.86912. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.83584/4.87427. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.83501/4.87494. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.83255/4.86506. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.83103/4.87442. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.83287/4.86668. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.83565/4.84554. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.83570/4.86724. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.83335/4.86878. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.82773/4.86537. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.82547/4.87669. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.82664/4.88291. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.82789/4.88655. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.82879/4.87081. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.82847/4.86776. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.82797/4.87496. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.82425/4.85474. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.82386/4.86692. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.82634/4.86833. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.82365/4.85002. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.82246/4.87043. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.82559/4.88129. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.82232/4.88014. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.82396/4.86601. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.82180/4.85785. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.82245/4.86331. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.82147/4.88328. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.81861/4.89015. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.81713/4.87678. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.81231/4.88156. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.82377/4.86752. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.81879/4.86610. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.81332/4.88194. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.81747/4.89237. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.81923/4.86743. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.81244/4.86477. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.81996/4.87694. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.82165/4.84372. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.81623/4.86765. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.80343/4.86517. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.81145/4.88844. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.81621/4.90165. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.81180/4.87104. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.81681/4.83768. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.80886/4.84800. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.81537/4.87391. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.80859/4.87336. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.81361/4.84836. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.80171/4.89013. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.80839/4.88082. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.80548/4.89486. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.81449/4.84039. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.80950/4.85128. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.81437/4.84862. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.80511/4.86006. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.80441/4.84758. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.80284/4.88508. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.80103/4.86732. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.79351/4.87923. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.80302/4.88060. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.80518/4.83709. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.80423/4.86062. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.80124/4.86046. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.81191/4.85577. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.80248/4.85708. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.79916/4.89191. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.81038/4.90249. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.80373/4.87002. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.80357/4.87821. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.79855/4.85663. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.80027/4.87924. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.11834526708278773\n",
      "Epoch 0, Loss(train/val) 4.81767/4.92635. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.74318/4.76733. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.73826/4.72669. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.73610/4.72386. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.73332/4.72810. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.73144/4.73501. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.73297/4.74033. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.72982/4.73975. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.73186/4.73177. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.73088/4.72351. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.72788/4.72018. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.73128/4.71370. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.72625/4.71871. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.73044/4.71525. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.72430/4.73285. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.72993/4.72311. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.72310/4.73004. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.72306/4.73141. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.72549/4.72193. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.72660/4.72452. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.72317/4.72989. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.72582/4.73828. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.71877/4.74648. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.72511/4.72242. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.72298/4.72884. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.71858/4.73380. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.72146/4.72044. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.72363/4.72282. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.71757/4.74744. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.71981/4.73304. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.72048/4.72702. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.72036/4.73981. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.72070/4.72826. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.71871/4.72748. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.71699/4.73952. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.71870/4.73972. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.71737/4.71687. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.72092/4.73199. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.71412/4.74544. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.71608/4.73722. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.71719/4.73570. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.71461/4.72833. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.71562/4.72925. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.71550/4.74027. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.72128/4.73021. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.71752/4.74522. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.71130/4.74972. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.71011/4.75243. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.71619/4.75072. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.70875/4.73553. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.71294/4.73959. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.72061/4.72594. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.71284/4.76386. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.71793/4.73909. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.71184/4.75992. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.70844/4.76344. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.71198/4.76133. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.71378/4.73468. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.71128/4.73377. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.70906/4.73939. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.71327/4.72978. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.71210/4.73854. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.71286/4.76189. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.70635/4.75857. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.70449/4.75506. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.70000/4.75812. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.70935/4.74108. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.70858/4.73208. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.70821/4.76068. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.70624/4.75328. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.70212/4.76161. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.70616/4.74294. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.70307/4.76127. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.70298/4.75952. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.70552/4.76278. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.71991/4.70384. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.71937/4.72547. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.71369/4.73008. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.70683/4.75167. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.71168/4.75079. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.70755/4.74377. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.71604/4.72772. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.71836/4.74271. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.71350/4.74775. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.71364/4.74476. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.71704/4.73217. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.71506/4.73574. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.71046/4.74109. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.71244/4.73364. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.71406/4.75731. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.70870/4.73943. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.70899/4.74232. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 4.70320/4.76162. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.70844/4.73922. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.71884/4.72187. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.70842/4.74507. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.70297/4.73925. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.70392/4.77577. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.70294/4.75734. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.70557/4.73708. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.051799406700962544\n",
      "Epoch 0, Loss(train/val) 4.97747/4.95622. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.93216/4.91851. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.92841/4.92043. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.92907/4.92375. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.92673/4.92117. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.92400/4.91985. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.92520/4.91736. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.92025/4.91745. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.92414/4.91214. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.92150/4.91381. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.91840/4.90957. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.91706/4.90751. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.91736/4.90721. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.91936/4.90481. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.91533/4.90360. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.92043/4.90735. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.92025/4.90394. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.92336/4.90562. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.92137/4.90752. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.92120/4.90832. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.92377/4.91644. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.92045/4.90298. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.91523/4.90272. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.91712/4.91533. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.91685/4.91358. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.91658/4.90987. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.91344/4.90697. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.91709/4.89447. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.92084/4.90366. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.91430/4.91310. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.91256/4.91538. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.91777/4.90807. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.91528/4.91344. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.91063/4.91423. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.91337/4.90633. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.91675/4.90629. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.91166/4.90578. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.91235/4.91224. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.91324/4.91448. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.91035/4.91756. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.91166/4.91462. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.91114/4.91190. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.90478/4.91727. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.91635/4.91252. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.91088/4.90578. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.90261/4.90268. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.90507/4.91546. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.91322/4.92644. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.90764/4.91293. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.89973/4.90831. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.90589/4.91534. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.90602/4.90810. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.90938/4.91503. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.90317/4.91741. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.90819/4.93736. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.90725/4.92565. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.90172/4.93236. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.90425/4.92243. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.90280/4.92463. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.90510/4.93033. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.90295/4.93471. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.90033/4.92769. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.90340/4.93692. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.90500/4.90009. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.92429/4.92709. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.91576/4.92608. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.91618/4.93267. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.91329/4.92325. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.90831/4.93568. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.90513/4.93080. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.90652/4.92927. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.90939/4.91684. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.90608/4.93882. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.90238/4.93608. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.89755/4.92850. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.90224/4.92347. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.90326/4.93631. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.90995/4.92278. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.90717/4.93354. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.90244/4.93208. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.90360/4.92863. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.89904/4.93226. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.90167/4.93346. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.89840/4.91714. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.91241/4.91850. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.90050/4.92198. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.90121/4.92561. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.90223/4.93659. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.90310/4.93850. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.90960/4.91314. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.89991/4.91358. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.89817/4.92260. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.90028/4.92467. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.90126/4.91432. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.89314/4.91829. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.90535/4.91251. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.90414/4.92324. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.89821/4.91583. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.90262/4.91921. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.89868/4.91064. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: -0.010188710438961876\n",
      "Epoch 0, Loss(train/val) 4.89352/4.85758. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.85197/4.83933. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.84273/4.84027. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.84407/4.83743. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.84459/4.83828. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.83830/4.84019. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.83822/4.83797. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.83526/4.83983. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.83789/4.84508. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.83655/4.84474. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.83236/4.85046. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.83448/4.84822. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.83773/4.84911. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.83358/4.84701. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.83749/4.84808. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.83292/4.85346. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.83543/4.84917. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.83476/4.85392. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.83177/4.86559. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.83200/4.86473. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.82899/4.87166. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.83312/4.86708. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.83414/4.85029. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.82873/4.85775. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.82837/4.86278. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.82831/4.86238. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.83217/4.85484. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82891/4.86796. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.82814/4.86506. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.82501/4.86867. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.82613/4.87458. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.82499/4.87779. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.82463/4.87084. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.82357/4.88294. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.82696/4.87849. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.82263/4.87876. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.82297/4.87890. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.82646/4.87917. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.82047/4.88036. Took 0.07 sec\n",
      "Epoch 39, Loss(train/val) 4.82973/4.85017. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.83656/4.84768. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.83235/4.85524. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.83172/4.86161. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.82513/4.86757. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.82859/4.86216. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.82678/4.86163. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.82982/4.84898. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.82451/4.87040. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.82794/4.86163. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.83028/4.88567. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.82300/4.87881. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.82518/4.87714. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.82619/4.86560. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.82429/4.89062. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.82514/4.87530. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.82412/4.88153. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.83387/4.82909. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.83373/4.84742. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.83084/4.85637. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83130/4.86744. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.82895/4.87315. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83077/4.86618. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.82798/4.88254. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.82581/4.87833. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.82760/4.86943. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.82547/4.88195. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.82525/4.86340. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.82470/4.88380. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.82509/4.86925. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.82271/4.87197. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.82516/4.88436. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.82418/4.86634. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.82640/4.87724. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.82403/4.87628. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.82206/4.87619. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.81722/4.87897. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.82029/4.87248. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.82103/4.88654. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.82081/4.87949. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.81924/4.88845. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.82038/4.89141. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.82172/4.89880. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.82301/4.87830. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.81800/4.89916. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.81080/4.88568. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.82319/4.88865. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.82222/4.90717. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.81584/4.88918. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.82317/4.87541. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.81140/4.89557. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.82561/4.86578. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.81282/4.89223. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.81492/4.87450. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.81916/4.86748. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.81370/4.88848. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.80888/4.88685. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.82160/4.87105. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.80605/4.90591. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.81764/4.88993. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.81274/4.88397. Took 0.08 sec\n",
      "ACC: 0.5, MCC: -0.03446966348735094\n",
      "Epoch 0, Loss(train/val) 5.00655/4.98643. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.02188/4.99799. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.99077/4.97848. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.98844/4.98501. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.98907/4.98433. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.99074/4.98061. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.98497/4.97788. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.98296/4.97665. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.98386/4.97708. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.98631/4.97411. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.98588/4.97205. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.98060/4.97036. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.98343/4.96872. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.98134/4.96850. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.98172/4.96548. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.98008/4.96330. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.98130/4.96078. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.97879/4.95865. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.98011/4.96345. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.97771/4.96226. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.97605/4.95715. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.97598/4.95778. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.97850/4.95801. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.97603/4.95743. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.97398/4.95878. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.97625/4.95937. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.97282/4.95991. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.97410/4.95986. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.97715/4.96150. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.97470/4.96034. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.97321/4.95868. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.97161/4.95961. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.97206/4.95896. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.97115/4.95966. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.97071/4.95896. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.96731/4.96243. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.97326/4.96071. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.97202/4.96575. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.96818/4.96340. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.97126/4.95914. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.97019/4.96192. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.96274/4.96280. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.96860/4.96179. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.96975/4.96595. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.96574/4.96625. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.96525/4.97116. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.96372/4.96719. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.96566/4.98014. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.96619/4.97619. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.96176/4.97432. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.96138/4.97469. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.96145/4.97342. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.96650/4.96395. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.96319/4.98021. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.96167/4.96811. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.95879/4.97731. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.96001/4.97308. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.95935/4.97166. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.95910/4.97918. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.95676/4.97751. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.96082/4.97207. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.95714/4.97920. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.95785/4.99601. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.95945/4.99774. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.95437/4.98464. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.96204/4.96873. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.95626/4.98247. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.95480/4.97707. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.95629/4.98716. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.95172/4.99875. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.95597/4.99963. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.95289/4.97538. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.95835/4.97320. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.95573/4.98010. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.94670/4.98689. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.95213/5.00471. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.95673/4.97396. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.95016/4.99025. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.95137/4.99029. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.95039/5.00157. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.94879/4.98325. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.95745/5.00693. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.94967/4.99632. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.95246/4.99525. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.95635/4.99903. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.95184/4.99217. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.95941/4.98852. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.95509/4.98485. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.95486/4.99102. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.94978/4.99306. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.95039/5.00091. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.94777/4.98143. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.95064/5.01262. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.96271/4.98684. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.96161/4.98117. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.95448/4.98255. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.95273/5.00103. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.95140/4.99983. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.95937/4.97430. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.96620/4.97162. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.059863071616150634\n",
      "Epoch 0, Loss(train/val) 4.88301/4.92002. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.84006/4.88879. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.83307/4.87777. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.83127/4.87270. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.82997/4.87031. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.82673/4.87173. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.82726/4.87538. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.82372/4.88695. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.82359/4.90119. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.82116/4.89606. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.82055/4.88596. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.81876/4.87922. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.81819/4.87452. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.81623/4.88856. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.81515/4.88027. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.81667/4.86944. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.81476/4.86948. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.81647/4.87176. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.81648/4.86620. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.81376/4.87125. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.81413/4.86149. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.81305/4.86073. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.81051/4.86933. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.81120/4.88143. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.81240/4.89822. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.81827/4.87466. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.81201/4.87862. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.81402/4.87407. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.81092/4.88518. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.81239/4.88503. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.80713/4.89247. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.80851/4.89421. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.80728/4.90285. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.81121/4.89663. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.80657/4.89914. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.80960/4.91717. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.80891/4.89981. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.80613/4.90182. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.80295/4.90010. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.80521/4.91445. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.79926/4.92160. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.80388/4.90748. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.80185/4.91615. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.80429/4.90963. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.80122/4.90044. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.79901/4.90790. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.79762/4.91884. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.79715/4.93843. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.79665/4.93452. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.79925/4.92054. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.79958/4.92722. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.79421/4.92605. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.79775/4.91164. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.80211/4.92493. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.80024/4.93819. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 4.79744/4.92419. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.79190/4.95458. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.79277/4.93509. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.79753/4.93942. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.79210/4.92875. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.79176/4.94394. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.78877/4.91035. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.79137/4.96774. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.79529/4.94054. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.79684/4.93952. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.79040/4.90611. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.79391/4.95063. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.79206/4.93272. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.78708/4.95344. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.79660/4.93647. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.79310/4.91361. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78686/4.96571. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.78734/4.95533. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.78864/4.95423. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.78372/4.94701. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.78648/4.93322. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.79002/4.98452. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.79914/4.90645. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.79744/4.92246. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.80287/4.88895. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 4.79325/4.93041. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.79331/4.94511. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.78956/4.92664. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.79285/4.90322. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.80690/4.87873. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.79003/4.93257. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.79164/4.94100. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.80213/4.88466. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.78941/4.94572. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.79002/4.93722. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.78389/4.93046. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.80252/4.91318. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.79586/4.90323. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.79709/4.92524. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.78945/4.96354. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.78726/4.94665. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.79631/4.90847. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.79614/4.91810. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.81088/4.89110. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.80373/4.89850. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.049929648757959806\n",
      "Epoch 0, Loss(train/val) 4.92543/4.90555. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.91361/4.91202. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.90448/4.90289. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.91052/4.90417. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.90219/4.90388. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.90726/4.90207. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.90470/4.90089. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.90099/4.90201. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.90366/4.90224. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.90134/4.89978. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.90468/4.90265. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.90317/4.90489. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.90290/4.91076. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.90135/4.92547. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.90148/4.92397. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.90012/4.92361. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.89893/4.92508. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.90003/4.92765. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.90031/4.92909. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.89724/4.92690. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.89662/4.92725. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.89614/4.93530. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.89736/4.93284. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.89538/4.93697. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.89370/4.94549. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.89432/4.94379. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.89211/4.94146. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.89543/4.94293. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.89292/4.94992. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.89051/4.94010. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.89273/4.94615. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.89443/4.94808. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.89862/4.95328. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.89594/4.95851. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.89490/4.95520. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.89489/4.94365. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.89076/4.95265. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.89532/4.94859. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.89145/4.95819. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.88836/4.95556. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.89462/4.94298. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.89178/4.94917. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.89051/4.96081. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.89699/4.94590. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.89618/4.95074. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.89149/4.95902. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.89181/4.95658. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.89611/4.93792. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.89577/4.93826. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.89481/4.96630. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.90232/4.94591. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.89832/4.95593. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.89633/4.94577. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.89957/4.95413. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.89618/4.95888. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.89264/4.96706. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.89571/4.96474. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.89552/4.95516. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.89396/4.97001. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.89190/4.96784. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.89479/4.96735. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.89345/4.94477. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.88962/4.97284. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.89523/4.96597. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.89421/4.96928. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.88984/4.97005. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.88927/4.97441. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.88685/4.96480. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.88651/4.93891. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.89217/4.95314. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 4.89072/4.94935. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.88961/4.96414. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88419/4.96226. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.88562/4.96648. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.88373/4.95239. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.88419/4.95273. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.90598/4.93477. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.89448/4.95606. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.89199/4.95909. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88852/4.94242. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.89075/4.96421. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.89749/4.94905. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.89080/4.96428. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.88868/4.95479. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.88843/4.97534. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.88826/4.94864. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.88425/4.96135. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 4.89917/4.94195. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.87974/4.97756. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88753/4.97818. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.88907/4.97800. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.88086/4.95400. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.88345/4.98095. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.88095/4.96253. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.88102/4.97575. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.87348/4.98233. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.88436/4.96933. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.88266/4.96931. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.88512/4.97966. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.88073/4.98518. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.019772733298509516\n",
      "Epoch 0, Loss(train/val) 4.77171/4.76070. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.73040/4.74620. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.72583/4.75137. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.72589/4.75128. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.72690/4.74964. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.72020/4.74709. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.72199/4.74473. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.72025/4.74251. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.71668/4.74405. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.72022/4.74488. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.71929/4.74536. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.71506/4.74268. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.71358/4.74786. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.71411/4.74758. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.71421/4.74662. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.70762/4.75473. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.71230/4.75473. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.71004/4.75269. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.70809/4.75563. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.70362/4.77626. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.70292/4.77789. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.70789/4.77338. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.70689/4.77723. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.70177/4.78007. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.70352/4.77730. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.69819/4.78385. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.70305/4.76765. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.70051/4.77643. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.70355/4.78348. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.69993/4.77303. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.69186/4.81178. Took 0.07 sec\n",
      "Epoch 31, Loss(train/val) 4.69767/4.81715. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.70436/4.81388. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.71175/4.77032. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.70664/4.77707. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.70892/4.77614. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.70955/4.78061. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.71366/4.79295. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.70586/4.78195. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.70178/4.80531. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.70058/4.80172. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.69961/4.80410. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.70687/4.79271. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.69792/4.79530. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.69656/4.79762. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.69568/4.80875. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.69458/4.80952. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.70362/4.78735. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.69732/4.80795. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.69402/4.80777. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.70082/4.78674. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.69290/4.81381. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.69252/4.81441. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.69398/4.82644. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.69097/4.81358. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.69416/4.80199. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.69108/4.81098. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.68807/4.82008. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.69714/4.81858. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.69870/4.78689. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.69828/4.79915. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.69089/4.80144. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.69199/4.82491. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.69451/4.80491. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.68673/4.80756. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.69047/4.82553. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.69287/4.83968. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.69012/4.81372. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.69050/4.83338. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.68776/4.84009. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.68778/4.82216. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.68870/4.84486. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.68614/4.83708. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.68192/4.84674. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.68614/4.83694. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.69395/4.82449. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.68920/4.82353. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.68796/4.82577. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.68653/4.83184. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.68060/4.82892. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.68838/4.81961. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.68518/4.83257. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.68249/4.84639. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.68182/4.81648. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.68030/4.83325. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.68337/4.85626. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.68513/4.83572. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.69114/4.84107. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.67995/4.84117. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.68475/4.84796. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.68461/4.85214. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.67925/4.85975. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.68046/4.84700. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.67426/4.85009. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.68388/4.84229. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.68312/4.81694. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.67912/4.84263. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.68337/4.82376. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.67512/4.83043. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.68077/4.85008. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.0009775171065493646\n",
      "Epoch 0, Loss(train/val) 4.94805/4.91062. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.89773/4.85812. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.88661/4.87237. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.86991/4.86579. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.86326/4.86221. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.86536/4.86628. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.86442/4.86813. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.86529/4.86968. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.86646/4.86878. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.86171/4.86947. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.86104/4.87010. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.86116/4.86937. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.86074/4.87010. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.85931/4.87023. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.85780/4.87055. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.85943/4.87106. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.85674/4.86769. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.85579/4.87476. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.85641/4.87246. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85551/4.87269. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.85470/4.87531. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.85752/4.87356. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.85293/4.87365. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.85188/4.87197. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.85470/4.87376. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.85225/4.87237. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.84955/4.87771. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.84919/4.88345. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.85184/4.87569. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.85453/4.87950. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.85230/4.87894. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.84772/4.88850. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84896/4.88826. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.84917/4.89193. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.84363/4.89740. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.85037/4.89557. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84550/4.89958. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84817/4.90051. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84607/4.90095. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84682/4.89893. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84203/4.90920. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.84446/4.90914. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84387/4.90003. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.84600/4.90887. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.84206/4.91455. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.84115/4.91962. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.83812/4.93228. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.83773/4.91649. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84649/4.89159. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.84243/4.91756. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.83981/4.92445. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.84157/4.91959. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.83879/4.92132. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.83883/4.92440. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.83748/4.93469. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.84242/4.91598. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.83976/4.94642. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.84230/4.93722. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.83549/4.93648. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.83813/4.92002. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.84179/4.91850. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.84294/4.90947. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.83962/4.91728. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.83516/4.93866. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.84073/4.92392. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.83495/4.93530. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.83702/4.92859. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.83431/4.94614. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.83673/4.92404. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83487/4.93556. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.83029/4.93361. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.83494/4.93254. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83405/4.94009. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.83505/4.93933. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83057/4.94646. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83409/4.93568. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.82884/4.93227. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.83106/4.96162. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.82979/4.93981. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.83255/4.96291. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.84010/4.92824. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.83046/4.94755. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.82972/4.94987. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.82874/4.95707. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83017/4.96028. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.82854/4.93313. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.82850/4.96234. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 4.83024/4.93824. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83523/4.92380. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.83890/4.93093. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.83400/4.94657. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.83895/4.94623. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83091/4.93952. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83372/4.94619. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.83245/4.94033. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.82847/4.93660. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.82875/4.94699. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.82950/4.93787. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83217/4.97397. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.83421/4.92012. Took 0.08 sec\n",
      "ACC: 0.578125, MCC: 0.1563263498701806\n",
      "Epoch 0, Loss(train/val) 5.06350/5.08687. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.02904/5.07397. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.02504/5.06529. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.02237/5.06083. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.02445/5.05678. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.02296/5.04044. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.01965/5.03736. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.02297/5.03399. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.01859/5.03639. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.02235/5.03917. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.01837/5.03530. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.02039/5.03374. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.01992/5.03112. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.01820/5.02818. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.01712/5.02910. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.01654/5.03532. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.01442/5.03720. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 5.01619/5.04663. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 5.01797/5.04855. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.01668/5.03887. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.01524/5.03399. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.01363/5.03928. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.01425/5.03500. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.01130/5.04461. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.01169/5.05035. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.01239/5.04142. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.01294/5.04740. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.01096/5.04094. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.00274/5.05631. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.00869/5.05534. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.00482/5.05866. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.00838/5.06093. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.00502/5.05421. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.00614/5.06222. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.00482/5.06924. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.00760/5.05274. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.00951/5.05497. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.00607/5.05943. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.00552/5.06073. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.00804/5.05453. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.00691/5.05831. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.00442/5.06139. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.00538/5.07583. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.00058/5.08953. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.99930/5.08546. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.00264/5.06334. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.00404/5.08058. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.99949/5.06798. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.00220/5.07453. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.00160/5.07328. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.00115/5.07661. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.00335/5.07720. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.00321/5.06611. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.99797/5.07777. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.99859/5.07209. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.99964/5.07394. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.00480/5.06688. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.99685/5.06775. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.00212/5.07012. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.00178/5.07498. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.99294/5.09020. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.00012/5.08132. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 5.00290/5.06784. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.99432/5.07881. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.99313/5.10413. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.99707/5.09578. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.99968/5.07307. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.99656/5.06919. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.99776/5.07695. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.99660/5.06651. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.99994/5.07121. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.00072/5.06167. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.00173/5.08267. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.99469/5.07318. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.99766/5.07915. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.99448/5.07936. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.99530/5.07918. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.99341/5.07864. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.98879/5.08439. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.99108/5.08262. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.98552/5.10788. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.99566/5.10345. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.99191/5.07984. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.98895/5.08450. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.99694/5.06705. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.98893/5.07691. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.99305/5.09214. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.98620/5.10198. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.99502/5.08013. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.98691/5.08377. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.98754/5.08519. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.99699/5.08950. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.99265/5.08384. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.98724/5.09826. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.98710/5.07502. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.99044/5.06861. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.99161/5.07318. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.98948/5.08895. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.98733/5.08158. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.98303/5.08318. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.10421364855284106\n",
      "Epoch 0, Loss(train/val) 4.96085/4.90419. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.89578/4.88629. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.89404/4.89332. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.89600/4.90137. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.89259/4.89967. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.89559/4.89591. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.89294/4.89530. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.89224/4.90005. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.89157/4.90071. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.89164/4.89977. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.88790/4.90253. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.88784/4.89781. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.89157/4.88597. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.88886/4.88964. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.88968/4.88582. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.89081/4.87892. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.88710/4.87956. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.88452/4.87984. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.88677/4.88222. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.88223/4.88964. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.87817/4.90054. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.88089/4.90247. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.87549/4.90296. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.88280/4.89130. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.87609/4.89479. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.87735/4.91336. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.87881/4.88816. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.87501/4.90538. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.87795/4.91582. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.87548/4.92631. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.87062/4.92858. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.87198/4.93693. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.86978/4.94058. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.87122/4.92537. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.87139/4.93508. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.86845/4.93516. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.86606/4.93680. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.86739/4.93875. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86487/4.93973. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.86968/4.94516. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.86847/4.93634. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.86691/4.96606. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.86123/4.94578. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.86527/4.96642. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.87040/4.92099. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.87354/4.92087. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.86547/4.91767. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.86624/4.88639. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.88424/4.91012. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.87225/4.92376. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.86839/4.91405. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.87426/4.90494. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.86957/4.94552. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.86446/4.95380. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.86169/4.92854. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.86845/4.93518. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.86825/4.94617. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.86368/4.95184. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.86015/4.96083. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.86027/4.97076. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.85672/4.94606. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.86336/4.98065. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.86145/4.97157. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.85777/4.95510. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.86507/4.92696. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.87222/4.90364. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.86461/4.94977. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.85916/4.91562. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.86787/4.95537. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.86489/4.95054. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.85885/4.97351. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.86232/4.94937. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.86074/4.96503. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.85128/4.97608. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.85864/4.95863. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.85324/4.99100. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.85692/4.95683. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.85526/4.97482. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.85642/4.97854. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.85619/4.95471. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.85434/4.96444. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.85013/4.98998. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.85738/4.97169. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.86622/4.95779. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.86568/4.94584. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.86811/4.94661. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.86207/4.95436. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.86064/4.94027. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.85787/4.97612. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.85871/4.93848. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 4.86064/4.96346. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.85549/4.99646. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.85461/4.99267. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.85211/5.00287. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.84296/5.00485. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.85311/4.92347. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.85357/5.00571. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.85509/4.96625. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.85118/4.97949. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.85057/4.97594. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.1463182264089704\n",
      "Epoch 0, Loss(train/val) 4.91630/4.88235. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.87486/4.89163. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.87238/4.89145. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.86843/4.89708. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.86300/4.90371. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.86431/4.91314. Took 0.07 sec\n",
      "Epoch 6, Loss(train/val) 4.86669/4.90220. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.86615/4.89529. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.86336/4.89729. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.86508/4.89741. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.86478/4.89668. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.86041/4.89978. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.86074/4.89894. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.86041/4.89747. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.86103/4.89782. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.85861/4.90834. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.85955/4.90328. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.85904/4.90274. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.86022/4.90239. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85810/4.90452. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.86109/4.90609. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.85869/4.90372. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.85531/4.91031. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.85453/4.90583. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.85716/4.90165. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.85582/4.90570. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.85335/4.90944. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.85377/4.90423. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.85286/4.90915. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.85185/4.90725. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.85142/4.90717. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.85201/4.90446. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.85057/4.90831. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.85322/4.90118. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.85531/4.89734. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.85394/4.90772. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.85393/4.90293. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.85303/4.90355. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84939/4.90555. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84903/4.90369. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.85072/4.90549. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.85145/4.90050. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84862/4.89872. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.85030/4.89695. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.85031/4.89934. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.84644/4.89852. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.84655/4.89838. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.84615/4.90290. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84690/4.88843. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.84703/4.89022. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.84593/4.89360. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.84696/4.89992. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.84302/4.89597. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.84395/4.88930. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.84153/4.90326. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.84712/4.88667. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.84380/4.89389. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.84154/4.88809. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.84267/4.88795. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.84442/4.89405. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.85047/4.89315. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.84428/4.90268. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.84183/4.89948. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.84415/4.89468. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.84518/4.89796. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.84596/4.89797. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.84576/4.89624. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.84088/4.89718. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.84530/4.89236. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83820/4.90522. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.83786/4.91049. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.83931/4.90468. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.84203/4.91290. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.84085/4.90490. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 4.83440/4.89795. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83492/4.90774. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.84283/4.90527. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.83790/4.89913. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.83757/4.91047. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.83830/4.89679. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83757/4.91024. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.84082/4.90022. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.84479/4.90173. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.84018/4.89490. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.83777/4.90723. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83486/4.89785. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.84080/4.90625. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.86251/4.88382. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.85595/4.89880. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.85224/4.91884. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.85360/4.91436. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.84685/4.91865. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.84578/4.91362. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.84474/4.92374. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.84545/4.90812. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 4.83847/4.92500. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.83647/4.93212. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.84076/4.91774. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83840/4.92625. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.84372/4.91341. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.0931174089068826\n",
      "Epoch 0, Loss(train/val) 5.00593/5.11380. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.98499/4.99207. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.99534/4.96978. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.00019/4.97234. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.98854/4.96869. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.97513/4.97031. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.97934/4.96910. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.97846/4.97019. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.97598/4.96897. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.97832/4.96937. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.97747/4.96688. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.97581/4.96956. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.97245/4.96733. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.97163/4.96810. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.97329/4.96709. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.97413/4.97274. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.97132/4.97475. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.96822/4.96926. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.96635/4.97828. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.96430/4.98183. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.96574/4.97917. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.96367/4.97368. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.96553/4.97523. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.95966/4.97829. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.96446/4.98802. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.96389/4.98345. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.96348/4.98500. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.96042/4.97425. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.96196/4.97725. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.96124/4.98370. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.96082/4.98434. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.95835/4.99284. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.96280/4.98246. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.95830/4.97607. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.95832/4.99961. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.96100/4.97616. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.96246/4.97639. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.96120/4.98128. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.96181/4.97829. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.95752/4.97147. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.96792/4.96005. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.96588/4.96869. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.95882/4.96960. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.95718/4.96687. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.95895/4.98015. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.95987/4.98040. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.95921/4.96357. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.95624/4.99037. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.95478/4.95117. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.95476/4.96610. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.95399/4.97747. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.95513/4.95338. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.95143/4.97649. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.95431/4.98115. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.95542/4.96637. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.95849/4.98343. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.95675/4.98385. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.95393/4.98134. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.95752/4.97645. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.95228/4.98552. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.95035/4.99272. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.95367/4.99209. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.94607/4.96833. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.94530/4.97554. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.94391/4.98249. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.95585/4.97209. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.94974/4.97975. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.94691/4.99860. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.94376/4.99862. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.94169/4.97539. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.94298/4.97049. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.94252/5.00447. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.94428/4.98074. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.96054/4.98673. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.95270/4.99148. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.94270/4.98588. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.94293/4.98061. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.95199/4.97480. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.94578/4.97913. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.94757/4.98678. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.95102/4.99326. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.95015/4.99410. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.94825/4.99273. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.94678/4.99015. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.94689/4.99696. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.94589/4.99251. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.94663/4.98791. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.94086/4.99442. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.94372/4.98420. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.95052/4.98706. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.94319/4.99513. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.94207/4.99177. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.93988/4.98814. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.94348/5.00131. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.94535/4.99590. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.94504/4.98461. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.93858/5.00032. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.94200/5.01513. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.94479/4.99344. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.93800/4.99649. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.1003113655692617\n",
      "Epoch 0, Loss(train/val) 5.12527/5.01147. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.03996/5.03379. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 5.03593/5.04119. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.03625/5.04219. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.03482/5.03816. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.03104/5.04057. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 5.03413/5.03694. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.03455/5.03436. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.03442/5.03273. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.03461/5.03573. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.02982/5.03858. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.02982/5.04505. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.02912/5.04982. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.02904/5.04909. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.02856/5.05285. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.02822/5.05409. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.03132/5.05655. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.02914/5.05660. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.02641/5.05726. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.02765/5.05696. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.02520/5.05845. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.02756/5.05852. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.02470/5.06599. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.02611/5.06836. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.02596/5.06888. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.02759/5.07092. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.02441/5.07369. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.02162/5.07482. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.02301/5.07415. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.02093/5.08256. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.02458/5.07091. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 5.01785/5.07700. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.02123/5.06919. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.02190/5.07359. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.01719/5.07689. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.01689/5.08599. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.02215/5.08014. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 5.02110/5.07151. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.01650/5.05693. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.01797/5.06917. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 5.01945/5.06247. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.01829/5.07149. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.01867/5.06905. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.01647/5.07341. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.01636/5.07210. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.01668/5.07261. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.01723/5.06741. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.01859/5.07303. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.01572/5.07585. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.01787/5.07211. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.01619/5.06469. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.01713/5.06541. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.01342/5.09014. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.01191/5.05689. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.01346/5.06177. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.01581/5.06893. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.01625/5.07870. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.01887/5.06262. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.01220/5.06526. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.00923/5.06549. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.01283/5.08080. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.01383/5.07102. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.01273/5.07843. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.01083/5.07762. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.00855/5.05153. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 5.01427/5.06525. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.00829/5.06021. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.01296/5.05759. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.01329/5.08649. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.00971/5.06234. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.01004/5.06886. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.00389/5.07337. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.00844/5.07332. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.01173/5.07947. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.00269/5.06932. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.00942/5.06064. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.00639/5.07011. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.00532/5.07395. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.00349/5.08655. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.00349/5.06094. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.00990/5.08163. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.00459/5.07485. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.00612/5.08700. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 5.00875/5.06005. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 5.00326/5.06477. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.00302/5.07912. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.00463/5.07326. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.00387/5.06384. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 5.00197/5.07909. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.00869/5.05164. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.00024/5.09179. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.00683/5.07043. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.00742/5.06041. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.00410/5.08111. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.99953/5.06599. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.00924/5.07814. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.00911/5.05890. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.00434/5.04099. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.01019/5.08380. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.01009/5.04563. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.09416472308615167\n",
      "Epoch 0, Loss(train/val) 4.78162/4.72131. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.73296/4.73892. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.73863/4.75746. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.73874/4.74760. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.73907/4.73201. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.73754/4.73041. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.73528/4.73675. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.72741/4.74597. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.72662/4.74736. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.72703/4.74771. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.72467/4.75756. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.72904/4.75405. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.72730/4.75641. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.72426/4.76541. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.72235/4.76892. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.72551/4.76367. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.72154/4.77783. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.71999/4.77005. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.71988/4.77171. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.71948/4.76131. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.72096/4.76439. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.71933/4.76056. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.71709/4.77101. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.71712/4.76336. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.71711/4.76482. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.71274/4.77069. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.71680/4.78044. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.72519/4.75901. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.72126/4.77384. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.71906/4.74632. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.71482/4.76713. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.71676/4.74770. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.71525/4.76670. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.71767/4.75402. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.71318/4.78272. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.71292/4.75695. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.71271/4.76416. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.71285/4.76561. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.71132/4.75361. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.71400/4.77660. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.71112/4.75844. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.70961/4.76573. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.71101/4.76302. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.70968/4.76707. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.71366/4.75616. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.71145/4.76358. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.71059/4.77649. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.70981/4.76313. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.70837/4.76298. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.70617/4.76128. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.70454/4.77450. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.70341/4.78261. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.70599/4.76370. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.70728/4.76795. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.70379/4.77950. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.70403/4.77490. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.70672/4.76767. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.69965/4.78974. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.70328/4.76730. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.69785/4.79015. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.69904/4.76548. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.70032/4.76863. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.70441/4.76847. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.69950/4.76599. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.69812/4.77259. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.69157/4.79263. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.69793/4.78135. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.69646/4.80792. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.71669/4.72027. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.71626/4.72691. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.70418/4.73225. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.70825/4.72234. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.71101/4.73928. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.70943/4.73402. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.70805/4.73775. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.70832/4.73657. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.70767/4.75262. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.70382/4.74692. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.70890/4.71977. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.69673/4.75622. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.70365/4.73421. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.70257/4.72652. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.69809/4.73956. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.70206/4.73202. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.70116/4.72514. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.69906/4.73468. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.70236/4.76551. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.69419/4.74198. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.69178/4.73892. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.69462/4.75994. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.70006/4.74758. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.69662/4.75693. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.70246/4.74079. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.69505/4.74624. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.69392/4.76234. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.70073/4.75111. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.69540/4.74487. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.69365/4.74333. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.69534/4.76713. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.70153/4.77344. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.0640210337986158\n",
      "Epoch 0, Loss(train/val) 4.75345/4.73277. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.71876/4.72828. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.71858/4.73138. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.71778/4.73193. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.71663/4.73135. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.71416/4.73121. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.71412/4.73649. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.71624/4.73635. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.71091/4.73637. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.70943/4.74114. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.71171/4.74310. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.70480/4.74685. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.71367/4.74907. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.71093/4.73009. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.71669/4.72468. Took 0.07 sec\n",
      "Epoch 15, Loss(train/val) 4.71880/4.71956. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.71028/4.73714. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.71278/4.73065. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.70583/4.73341. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.71550/4.72930. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.71102/4.72702. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.71079/4.72716. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.70799/4.71879. Took 0.07 sec\n",
      "Epoch 23, Loss(train/val) 4.71146/4.71882. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.71000/4.71943. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.70544/4.71114. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.70491/4.71445. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.70866/4.72624. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.71102/4.71318. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.71069/4.71415. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.70945/4.72026. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.70609/4.71804. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.71092/4.73798. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.71544/4.73630. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.71253/4.73464. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.71418/4.73461. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.71225/4.73209. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.71092/4.73113. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.70518/4.73016. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.70610/4.72748. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.70362/4.72937. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.70289/4.72347. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.70238/4.71751. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.70431/4.72031. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.70793/4.71902. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.70477/4.72414. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.70177/4.73495. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.70367/4.73616. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.69906/4.73355. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.70526/4.73452. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.70296/4.73604. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.69756/4.73834. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.70548/4.73880. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.69903/4.74252. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.70075/4.74269. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.69960/4.74067. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.69966/4.75066. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.69878/4.74627. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.69952/4.74173. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.69715/4.74742. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.69637/4.73860. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.69899/4.74867. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.69634/4.75752. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.69837/4.75719. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.69883/4.74135. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.70005/4.75058. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.69633/4.74606. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.69477/4.74699. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.69582/4.74360. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.69598/4.74720. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.69715/4.74534. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.69629/4.75085. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.69842/4.75727. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.69175/4.73585. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.69430/4.74903. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.69791/4.74253. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.69827/4.75204. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.69443/4.75795. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.69440/4.75553. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.69501/4.75775. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.69094/4.74978. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.69057/4.74033. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.69128/4.75204. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.69440/4.73514. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.69146/4.74407. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.69483/4.75369. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.69431/4.74950. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.69391/4.75763. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.68873/4.76314. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.69312/4.76226. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.68946/4.75392. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.69447/4.74423. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.68834/4.76992. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.69073/4.76748. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.69273/4.74897. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.68818/4.76777. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.68907/4.76778. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.68196/4.76397. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.68328/4.76812. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.69092/4.74787. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.0931174089068826\n",
      "Epoch 0, Loss(train/val) 4.96501/4.93449. Took 0.18 sec\n",
      "Epoch 1, Loss(train/val) 4.92699/4.92669. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 4.93185/4.93014. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.92964/4.93718. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.93371/4.93513. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.93858/4.92714. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.93265/4.93575. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.92347/4.93537. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.92324/4.94194. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.92415/4.94574. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.92449/4.94822. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.92615/4.94496. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.92028/4.95390. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.92136/4.95166. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.92288/4.95250. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.92028/4.95238. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.92118/4.95133. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.92005/4.95516. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.91743/4.96094. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.91773/4.95507. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.91496/4.97830. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.91655/4.96569. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.91810/4.97393. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.91575/4.97834. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.91431/4.96983. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.92050/4.95549. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.91325/4.96574. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.91653/4.94083. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.91636/4.93397. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.91839/4.94817. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.91629/4.95273. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.91561/4.94673. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.90999/4.96519. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.90973/4.95142. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.90955/4.95496. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.90788/4.95166. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.90740/4.98076. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.90922/4.96557. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.90591/4.95743. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.90851/4.95956. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.91566/4.92574. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.91669/4.93513. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.90829/4.95656. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.90896/4.97314. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.91492/4.95377. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.90363/4.95426. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.90797/4.96313. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.90647/4.97001. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.90566/4.95409. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.90579/4.96269. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.90685/4.97234. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.90537/4.97555. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.90121/4.97851. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.90202/4.96909. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.89940/4.96224. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.89775/4.94920. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.89724/4.96756. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.89713/4.96425. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.90080/4.97075. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.89931/4.95739. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.89843/4.96682. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89758/4.96787. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.90104/4.97663. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.89951/4.96069. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.90255/4.96696. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.89807/4.96975. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.90401/4.96573. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.89212/4.97151. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.89297/4.98086. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.89335/4.96369. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.89945/4.97562. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.89815/4.97874. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.89641/4.99211. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.89589/4.96705. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.89480/4.99435. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.89804/4.96878. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.89544/4.97596. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.89250/4.99386. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.89461/4.97612. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.89958/4.98273. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.89369/4.97938. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.88992/4.97844. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.89801/4.98167. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.88844/4.98373. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.89500/4.99124. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.89662/4.97442. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.88926/5.01415. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.89191/4.97366. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.89528/4.98450. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.89007/4.99375. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.89368/5.00184. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.89455/4.98494. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.89451/5.00508. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.88499/4.99477. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.88876/5.00204. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.89171/4.97593. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.88656/4.99604. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.88693/4.99719. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.88866/4.97737. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.89494/4.97871. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.0\n",
      "Epoch 0, Loss(train/val) 4.94328/4.88185. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.89001/4.87051. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.88506/4.86962. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.88260/4.86669. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.88334/4.86194. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.87808/4.86268. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.87944/4.86223. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.87704/4.86376. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.87641/4.86206. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.87555/4.86072. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.87655/4.86300. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.87593/4.86308. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.87304/4.86378. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.87381/4.86616. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.87551/4.86842. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.87444/4.86982. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.87324/4.87281. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.87449/4.87574. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.87268/4.87631. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.87191/4.88204. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.87341/4.88108. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.86967/4.88394. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.86961/4.88826. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.86946/4.89064. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.86954/4.89533. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.86898/4.89455. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.86459/4.90019. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.86493/4.90031. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.86820/4.90198. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.86539/4.89994. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.86655/4.89883. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.86187/4.91402. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.86737/4.89931. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.87000/4.88400. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.86871/4.89952. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.86364/4.90799. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.86600/4.91943. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.86045/4.92411. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86109/4.92222. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.85807/4.93582. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.85894/4.93349. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.85837/4.94864. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.85669/4.93453. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.86028/4.94034. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.85161/4.97017. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.85576/4.93336. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.85594/4.91934. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.86670/4.92171. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.86073/4.92355. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.85389/4.95586. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.86123/4.92106. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.85519/4.94498. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.85423/4.94613. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.85559/4.94983. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.85512/4.95202. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.85451/4.94973. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.84923/4.94110. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.85846/4.95091. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.85321/4.96108. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.85273/4.96401. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.85891/4.92003. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.86728/4.92115. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.86296/4.92397. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.85999/4.93143. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.86243/4.94893. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.85820/4.93371. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.86280/4.93576. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.86216/4.94611. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.85663/4.94639. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.85507/4.94697. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.85718/4.93810. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.85537/4.94352. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.86070/4.93840. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.85599/4.97081. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.85471/4.95099. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.85244/4.95938. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.85141/4.95395. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.85403/4.95301. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.85328/4.94328. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.85266/4.95320. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.85070/4.94964. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.85120/4.94789. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.85340/4.95732. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.84913/4.94910. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.85389/4.94841. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.85148/4.95115. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.84863/4.97095. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.85156/4.98058. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.85840/4.97204. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.85226/4.95658. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.85464/4.96408. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.85099/4.97788. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.84682/4.98699. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.85255/4.96513. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.84416/4.99008. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.85695/4.93744. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.85348/4.95783. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.84584/4.97976. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.86658/4.90367. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.86738/4.90645. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.0710915894200724\n",
      "Epoch 0, Loss(train/val) 4.77960/4.75217. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.74095/4.75672. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.73933/4.75656. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.73833/4.75692. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.73564/4.75988. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.73789/4.76357. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.73499/4.76941. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.73897/4.77933. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.74214/4.76968. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.74204/4.76046. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.73613/4.76714. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.73587/4.76234. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.73556/4.76184. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.73393/4.76892. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.73536/4.76855. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.73293/4.76627. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.73453/4.76512. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.73325/4.76942. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.73213/4.76879. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.73128/4.77300. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.73374/4.76730. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.73086/4.77442. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.73039/4.76990. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.73211/4.77077. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.72864/4.78002. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.72917/4.77798. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.72979/4.77492. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.72905/4.77084. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.72933/4.76924. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.72568/4.78232. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.73084/4.76876. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.72692/4.77419. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.72597/4.77928. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.72709/4.76847. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.72727/4.78529. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.71832/4.78936. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.72243/4.77200. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.72132/4.78876. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.72207/4.77531. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.72412/4.79129. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.72153/4.78962. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.71874/4.79277. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.72365/4.77802. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.72148/4.78605. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.71981/4.78820. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.71653/4.81531. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.72435/4.77226. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.72137/4.79129. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.71352/4.79655. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.72023/4.78901. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.71931/4.77543. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.71837/4.80226. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.71828/4.78850. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.71148/4.82108. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.72196/4.78779. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.71371/4.81439. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.70908/4.82535. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.71187/4.77605. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.71025/4.80732. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.71676/4.76959. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.71079/4.82118. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.71272/4.79183. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.71205/4.80525. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.71355/4.79637. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.71057/4.81407. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.71160/4.80860. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.71076/4.79950. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.71053/4.80724. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.71349/4.79829. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.71021/4.81285. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.70994/4.80321. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.70675/4.81569. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.70722/4.80087. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.70355/4.82386. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.71307/4.81659. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.70101/4.80841. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.70050/4.80606. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.69697/4.80902. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.71764/4.78083. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.70611/4.82029. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.71649/4.77751. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.70734/4.79957. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.70731/4.78913. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.69695/4.83906. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.71058/4.82437. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.70595/4.80243. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.70263/4.82617. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.70555/4.78609. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.70720/4.80515. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.70783/4.79664. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.70589/4.81526. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.70589/4.78648. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.70028/4.79663. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.69962/4.81355. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.70242/4.81685. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.70460/4.81698. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.70653/4.79443. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.70407/4.79931. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.70602/4.78840. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.69968/4.81235. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.15819299929208316\n",
      "Epoch 0, Loss(train/val) 4.71853/4.68899. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.67251/4.67867. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.67007/4.68601. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.67008/4.69247. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.67197/4.69322. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.67084/4.69098. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.67075/4.69039. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.66749/4.69517. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.67329/4.69449. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.66828/4.69655. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.66574/4.69927. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.66763/4.69599. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.66774/4.69517. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.66927/4.70097. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.66708/4.69805. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.66707/4.69358. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.66916/4.69330. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.66501/4.69503. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.66486/4.69488. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.66829/4.69268. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.66606/4.69393. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.66803/4.69649. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.66123/4.70318. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.66753/4.69694. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.66687/4.69351. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.66531/4.69645. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.66406/4.69993. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.66414/4.69903. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.66440/4.69983. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.66360/4.69156. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.66386/4.69647. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.66843/4.68710. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.66388/4.69350. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.66199/4.70469. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.66313/4.69336. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.66603/4.69680. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.66320/4.69142. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.66393/4.69113. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.65814/4.70614. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.65904/4.69876. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.65768/4.71175. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.65771/4.70909. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.66259/4.70464. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.65937/4.70234. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.65856/4.69730. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.65924/4.71176. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.66408/4.69146. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.66235/4.70825. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.65670/4.71150. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.65411/4.70743. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.65796/4.71570. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.65493/4.71569. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.65488/4.71401. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.65742/4.70292. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.65059/4.70262. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.65734/4.69994. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.65788/4.69759. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.65729/4.70642. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.65986/4.68507. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.65264/4.70187. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.64836/4.70689. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.65373/4.70371. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.65241/4.70812. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.65694/4.69142. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.65459/4.69284. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.65654/4.70678. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.64852/4.70003. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.64848/4.70569. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.64997/4.69333. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.65040/4.70754. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.65101/4.70264. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.64924/4.71583. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.64952/4.71421. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.64985/4.71136. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.64826/4.71099. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.64883/4.70421. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.65008/4.71287. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.64694/4.71253. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.64826/4.71353. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.65265/4.72208. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.64399/4.73049. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.64896/4.71309. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.64783/4.71870. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.64431/4.73568. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.64351/4.73616. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.64897/4.70999. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.64660/4.74715. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.63995/4.75760. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.64005/4.72426. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.64188/4.73507. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.64727/4.72621. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.64463/4.71294. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.63947/4.71838. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.64416/4.72306. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.64329/4.72111. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.63740/4.72643. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.64071/4.73359. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.64260/4.70870. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.64010/4.73442. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.63634/4.71603. Took 0.08 sec\n",
      "ACC: 0.359375, MCC: -0.26962010329298297\n",
      "Epoch 0, Loss(train/val) 4.81249/4.75192. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.74624/4.74953. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.74602/4.75163. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.74617/4.75376. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.74599/4.75315. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.74713/4.74978. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.74820/4.74838. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.74402/4.74864. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.74429/4.74917. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.74385/4.75099. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.74413/4.75173. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.74324/4.75129. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.74034/4.74972. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.74294/4.74944. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.73890/4.75374. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.74014/4.75066. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.73699/4.74951. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.73775/4.75094. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.73861/4.75453. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.73724/4.74718. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.73475/4.75400. Took 0.07 sec\n",
      "Epoch 21, Loss(train/val) 4.73657/4.75163. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.73227/4.76120. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.73427/4.74850. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.73407/4.75074. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.73346/4.75590. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.73328/4.75960. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.72578/4.77117. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.73027/4.76087. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.73215/4.74772. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.72963/4.78268. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.73062/4.74461. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.72729/4.75785. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.72736/4.77213. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.72718/4.76374. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.72504/4.75994. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.72430/4.76128. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.72017/4.76874. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.72709/4.74675. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.72403/4.76973. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.72504/4.74870. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.72306/4.76173. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.72148/4.77097. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.72002/4.75805. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.72207/4.75852. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.72711/4.76925. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.72206/4.76174. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.72218/4.76629. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.72098/4.73923. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.72372/4.74281. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.72306/4.77213. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.71939/4.76027. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.71708/4.77560. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.71839/4.76185. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.71892/4.78453. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.71559/4.77756. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.71600/4.76653. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.72169/4.77640. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.71562/4.77811. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.71660/4.76859. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.71463/4.77352. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.71837/4.76610. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.71355/4.78616. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.72888/4.74921. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.72750/4.74445. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.72674/4.75202. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.71995/4.77313. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.72063/4.76463. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.71724/4.75558. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.72788/4.74800. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.71767/4.75229. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.71647/4.74510. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.72022/4.74554. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.71729/4.74471. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.71536/4.74710. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.71858/4.74730. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.71652/4.75748. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.71232/4.77860. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.71100/4.76890. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.71562/4.75177. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.71709/4.75561. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.70990/4.77478. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.70948/4.78383. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.71634/4.76777. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.71343/4.76669. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.71060/4.74995. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 4.71274/4.75096. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.71356/4.78643. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.72127/4.75069. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.71652/4.76457. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.71318/4.74864. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.71362/4.76164. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.71183/4.77092. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.71367/4.77651. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.70813/4.76407. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.71206/4.77153. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.71238/4.77111. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.71526/4.74953. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.71852/4.76549. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.71257/4.73603. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.17004550636718183\n",
      "Epoch 0, Loss(train/val) 4.95357/4.88948. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.90436/4.89518. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.90886/4.88813. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.90598/4.89190. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.89750/4.88961. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.89167/4.88795. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.89041/4.88865. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.89181/4.88750. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.89270/4.88408. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.89136/4.88897. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.88852/4.88014. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.88373/4.88893. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.88863/4.88167. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.88767/4.88541. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.88305/4.88767. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.88724/4.88016. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.88480/4.87918. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.88343/4.88418. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.88324/4.88363. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.88274/4.88492. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.88300/4.87863. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.87844/4.89678. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.88234/4.88664. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.88730/4.87675. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.88547/4.87506. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.88392/4.87338. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.88521/4.87795. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.88437/4.87696. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.88239/4.87403. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.88333/4.87451. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.88324/4.87066. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.88045/4.87056. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.88133/4.87414. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.88245/4.87203. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.88116/4.87373. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.88353/4.87206. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.88475/4.87316. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.88034/4.87279. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.87903/4.87436. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.88205/4.87191. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.88061/4.86962. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.87906/4.87017. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.88168/4.87065. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.87978/4.87973. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.87899/4.87764. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.88270/4.86967. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.87638/4.87283. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.88207/4.87862. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.87968/4.88230. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.88538/4.87775. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.88021/4.87131. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.87772/4.87266. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.88114/4.86984. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.87976/4.87086. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.88147/4.87104. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.88063/4.87092. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.87980/4.87051. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.88033/4.86359. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.88213/4.86636. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.87830/4.86233. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.87732/4.86833. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.87437/4.87181. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.88085/4.86988. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.87692/4.87297. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.87599/4.87261. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.87762/4.87936. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.87908/4.87292. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.87236/4.89253. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.88334/4.86921. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.87684/4.86219. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.87444/4.86962. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.87914/4.88369. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88013/4.87068. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88127/4.87105. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.87573/4.87831. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.87827/4.87109. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.87712/4.87897. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.87929/4.87764. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.87755/4.87112. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.87210/4.89626. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.88035/4.86682. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.87273/4.87905. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.87697/4.87824. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.86995/4.88518. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.87662/4.86983. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.86903/4.88637. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.87266/4.89537. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.87858/4.88009. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.87236/4.87599. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.86917/4.88140. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.87196/4.87276. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.86919/4.88097. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.87178/4.87530. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.86569/4.88030. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.87593/4.87365. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.86989/4.88970. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.87514/4.87997. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.87256/4.87351. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.86840/4.88134. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.86937/4.87472. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.08952126702661474\n",
      "Epoch 0, Loss(train/val) 5.14077/5.02831. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.03148/5.03646. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.02548/5.04395. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.02449/5.04432. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.02630/5.04414. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.02705/5.04434. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.02628/5.04471. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 5.02523/5.04546. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.02431/5.04141. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.02206/5.03955. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.01888/5.04146. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.01963/5.03387. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 5.02023/5.03389. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.02520/5.04266. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.02018/5.03989. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.02050/5.03666. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.01570/5.03901. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.01649/5.04057. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.01527/5.03558. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.01232/5.03915. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 5.01552/5.03696. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.01304/5.04422. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 5.01503/5.03754. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 5.01176/5.04182. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.01481/5.02992. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.01145/5.04396. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.01355/5.03318. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.00894/5.05069. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.00881/5.04515. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.00734/5.03952. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.01056/5.03489. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 5.00673/5.04453. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 5.01351/5.02556. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 5.01562/5.03977. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 5.00765/5.04067. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.00697/5.03230. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.00761/5.03870. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.01011/5.02898. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.01032/5.04932. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.00843/5.04628. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.00449/5.04140. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.00702/5.02968. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.00326/5.03879. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.00388/5.03173. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.00311/5.04286. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.00760/5.04378. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.00785/5.04433. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 5.00679/5.07960. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.01784/5.02715. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.01264/5.03743. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.00745/5.04543. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.01408/5.03802. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.01062/5.03815. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.01005/5.03567. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.00405/5.04165. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.00722/5.03455. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.00999/5.03157. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.00569/5.03940. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.00445/5.03511. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.00532/5.04450. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.00243/5.03588. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.99954/5.03921. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.99582/5.01098. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.02488/5.03569. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.01740/5.04230. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.01436/5.05615. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.01142/5.04903. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.00939/5.04629. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.01354/5.05359. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.01107/5.04573. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.01247/5.05057. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.00417/5.04503. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.00757/5.04783. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.00588/5.04579. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.00874/5.03740. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.00289/5.03762. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.01010/5.04409. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.00787/5.02681. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 5.00364/5.04342. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.00328/5.03495. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.00406/5.03952. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.00558/5.03597. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.00181/5.04109. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 5.00429/5.03352. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.99774/5.04180. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.00077/5.03956. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.00033/5.02491. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.00209/5.01824. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.99524/5.02927. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.99933/5.02961. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.99856/5.01508. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.00126/5.04647. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.99879/5.01932. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.99328/5.01954. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.00013/5.02858. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.98857/5.02343. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.99387/5.03129. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.99609/5.05114. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.99665/5.08241. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 5.01138/5.06135. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.050256410256410255\n",
      "Epoch 0, Loss(train/val) 4.85333/4.80997. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.81370/4.81541. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.81627/4.80830. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.81625/4.80851. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.81517/4.81356. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.81001/4.81111. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.80610/4.80984. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.80291/4.81200. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.80572/4.81435. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.80522/4.81630. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.80435/4.81425. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.80127/4.81810. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.80201/4.81720. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.80176/4.82230. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.79991/4.81760. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.79748/4.81737. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.79975/4.81591. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.79741/4.82547. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.79881/4.81769. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.79997/4.82116. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.79443/4.82134. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.79859/4.82533. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.79862/4.82075. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.79762/4.83122. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.79762/4.81239. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.80348/4.82006. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.80094/4.81386. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.79564/4.81741. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.79088/4.82577. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.79940/4.81679. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.79622/4.82696. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.79374/4.82513. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.79761/4.84066. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.79597/4.83295. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.79283/4.82953. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.79582/4.83505. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.79388/4.83313. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.79168/4.83144. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.79304/4.82537. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.79507/4.83680. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.78714/4.84233. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.79110/4.83619. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.79333/4.83976. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.79072/4.83369. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.79005/4.84141. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.79060/4.84191. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.79411/4.83540. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.78754/4.84044. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.79073/4.84453. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.78854/4.84245. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.78907/4.84152. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.79082/4.84451. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.78407/4.84608. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.78897/4.84230. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.79252/4.83380. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.79024/4.84922. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.78192/4.84007. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.79183/4.84426. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.78906/4.84908. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.79035/4.83071. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.78896/4.83880. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.78341/4.84600. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.78476/4.83876. Took 0.07 sec\n",
      "Epoch 63, Loss(train/val) 4.78781/4.84038. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.78147/4.83579. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.77911/4.84483. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.78894/4.84300. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.79771/4.84241. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.78772/4.83884. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.78841/4.83526. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.78006/4.85809. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.80187/4.82645. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.80287/4.80545. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.80745/4.81208. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.79716/4.81317. Took 0.07 sec\n",
      "Epoch 75, Loss(train/val) 4.79560/4.82382. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78791/4.83148. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78815/4.83314. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.78292/4.85119. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.80111/4.82267. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.80356/4.82242. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.79192/4.85145. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.79822/4.82553. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.79250/4.83278. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.78989/4.82704. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.78809/4.83267. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.78688/4.84348. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.78276/4.84584. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.78934/4.84519. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.78347/4.83812. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.78191/4.84190. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.78567/4.85796. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.78455/4.84264. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.78021/4.83956. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.78452/4.83659. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.77582/4.84210. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.78392/4.84485. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.78117/4.85205. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.78280/4.83871. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.78387/4.83644. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.15694120514358612\n",
      "Epoch 0, Loss(train/val) 4.81019/4.76501. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.76337/4.75360. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.76047/4.75409. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.75819/4.75476. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.75805/4.75530. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.75744/4.75474. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.75689/4.75397. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.75609/4.75279. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.75572/4.75741. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.75263/4.76349. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.75507/4.75580. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.75516/4.75820. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.75170/4.76127. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.75332/4.75444. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.74961/4.77132. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.74862/4.75806. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.74947/4.76793. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.74625/4.78542. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.74574/4.75851. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.74869/4.77045. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.74439/4.76038. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.74684/4.78542. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.74148/4.77425. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.74286/4.78659. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.74334/4.76824. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.73707/4.78480. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.73780/4.80283. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.73472/4.78755. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.74244/4.80271. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.74243/4.75704. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.74507/4.78210. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.74449/4.76947. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.73891/4.77166. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.73881/4.78499. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.73618/4.78286. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.73295/4.77496. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.73601/4.78143. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.73766/4.79021. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.73466/4.77933. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.73576/4.79605. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.73797/4.78953. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.73345/4.79381. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.73324/4.79972. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.73352/4.80153. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.73360/4.79667. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.72941/4.81205. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.73446/4.80783. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.73268/4.79345. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.73223/4.79468. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.73487/4.78244. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.73552/4.80114. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.73471/4.79745. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.73617/4.79020. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.73240/4.80098. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.72908/4.81776. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.72668/4.79796. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.73333/4.80628. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.73000/4.82937. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.72632/4.82089. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.73297/4.80017. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.73443/4.79280. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.72751/4.81797. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.73006/4.79152. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.73078/4.80605. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.72987/4.79562. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.72967/4.81018. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.72971/4.79010. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.72935/4.81395. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.72459/4.82167. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.73020/4.79589. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.72492/4.81462. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.72630/4.80604. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.72473/4.80444. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.72605/4.83791. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.71935/4.81066. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.72583/4.86822. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.73120/4.80794. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.72515/4.83746. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.72438/4.81639. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.71940/4.83628. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.72332/4.83040. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.72018/4.83551. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.71961/4.84748. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.73161/4.83126. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 4.73385/4.83013. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.73045/4.84689. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.72843/4.80745. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.72558/4.80128. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.72776/4.81403. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.72665/4.81839. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.72326/4.82975. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.72260/4.81908. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.71790/4.82771. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.72430/4.79094. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.72325/4.81665. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.72775/4.80585. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.71932/4.84490. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.72001/4.83335. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.72027/4.81968. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.72706/4.81844. Took 0.09 sec\n",
      "ACC: 0.59375, MCC: 0.1980295566502463\n",
      "Epoch 0, Loss(train/val) 4.84894/4.81194. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.79306/4.88189. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.80450/4.90415. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.81039/4.85434. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.81514/4.81055. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.80148/4.80821. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.79163/4.82638. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.79625/4.82637. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.79345/4.82324. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79520/4.82027. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79378/4.81762. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.79080/4.82193. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.79289/4.82133. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.79097/4.82900. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.79118/4.82514. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.78948/4.82736. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.79134/4.82070. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.79077/4.82428. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.78784/4.82398. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.79250/4.82320. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.78866/4.83962. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78830/4.84349. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.78746/4.84947. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.78699/4.84810. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.78806/4.84460. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.78438/4.85228. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.78672/4.85179. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.78416/4.85272. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.78438/4.84493. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.78152/4.85554. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.78168/4.86480. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.78246/4.84549. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.78313/4.84680. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78243/4.84842. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78024/4.85741. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.78089/4.85005. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.77821/4.86062. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 4.77630/4.85461. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.77817/4.85286. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.78123/4.85033. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.77509/4.87003. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.77425/4.86896. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.77781/4.85931. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.77197/4.86264. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.77104/4.86848. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.77047/4.88073. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.77079/4.87933. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.77577/4.84745. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77010/4.86041. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.76684/4.87565. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 4.76957/4.87681. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.76952/4.88689. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.76729/4.87906. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.77166/4.85960. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.76411/4.87573. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.76607/4.85850. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.77292/4.85169. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.76462/4.87432. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76612/4.85982. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.76775/4.86266. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.76185/4.86955. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.77074/4.84765. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.78569/4.80650. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.78677/4.83775. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.78132/4.85696. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.77326/4.85341. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.76420/4.87567. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.76371/4.87777. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.76733/4.83967. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.76317/4.87635. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.76740/4.85021. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.76573/4.85799. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.77204/4.85269. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.76836/4.87297. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.76555/4.86267. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.76736/4.85898. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 4.75753/4.87775. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.76475/4.86348. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.76846/4.86711. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.76004/4.86253. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.75704/4.88528. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.75919/4.86446. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.76919/4.83939. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.75689/4.90120. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.76640/4.87186. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.75970/4.87826. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.75427/4.89339. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.75597/4.91445. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.76358/4.85881. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.76479/4.87080. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.75787/4.89147. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.76005/4.89597. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.75686/4.90606. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.75861/4.89194. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.75722/4.87454. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.75711/4.87455. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 4.75280/4.89757. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.76266/4.87993. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.76024/4.85568. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.77222/4.87540. Took 0.09 sec\n",
      "ACC: 0.453125, MCC: -0.11383959916814836\n",
      "Epoch 0, Loss(train/val) 4.91767/4.95853. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.88163/4.93334. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.87718/4.90151. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.86532/4.89656. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.86192/4.90111. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.85908/4.91416. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.86777/4.91711. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.86500/4.91171. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.86652/4.90917. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.86009/4.91155. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.85685/4.91432. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.85551/4.91497. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85738/4.90632. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.85000/4.92360. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.85566/4.92076. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.85364/4.92200. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.85455/4.90866. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.85730/4.91751. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.86177/4.89786. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85276/4.91168. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.85027/4.91870. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.85327/4.92984. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.85581/4.89835. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.85701/4.89565. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.85101/4.90216. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.85403/4.91297. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.85239/4.89788. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.85209/4.90843. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.84907/4.90608. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.85070/4.90285. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.84943/4.90709. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.84875/4.91912. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.85071/4.91828. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.84449/4.92099. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.84618/4.92298. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.84478/4.92550. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84641/4.93627. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.84038/4.94937. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84602/4.91845. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.84902/4.91742. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84125/4.91550. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.84082/4.90775. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84216/4.91896. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.84312/4.91343. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.84042/4.94404. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.84466/4.91307. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.84294/4.91718. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.83978/4.94186. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.84309/4.91886. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.84396/4.91832. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.84172/4.90980. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.83787/4.91986. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.83655/4.93826. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.83960/4.91451. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.84011/4.91824. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.83555/4.93907. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.84256/4.91013. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.83417/4.92506. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.83607/4.92781. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83915/4.92488. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.83883/4.92778. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.84240/4.92842. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.83851/4.92977. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.83582/4.93693. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.83402/4.91972. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.83752/4.93237. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.83390/4.96008. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.84256/4.90702. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.83185/4.94590. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83977/4.92250. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.83909/4.93605. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.83462/4.92419. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83329/4.91620. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.82926/4.95563. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83306/4.93944. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83763/4.92919. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.83388/4.94725. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.83140/4.94618. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.83807/4.92373. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.83426/4.93106. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83098/4.95005. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.82169/4.94396. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.82975/4.93101. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.83065/4.96627. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.82491/4.95928. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83890/4.91232. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83270/4.93920. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.83044/4.94624. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83186/4.95511. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.82250/4.92641. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.82250/4.93679. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.83039/4.93759. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.82859/4.94123. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.82471/4.95880. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.81645/4.96460. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.83232/4.91894. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.82169/4.94917. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.82596/4.95740. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.82822/4.92528. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.82355/4.93276. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.06052275326688024\n",
      "Epoch 0, Loss(train/val) 4.81931/4.82983. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.80630/4.81986. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.80245/4.81131. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.80213/4.78910. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79171/4.78884. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.78615/4.79164. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.78675/4.79344. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.79022/4.79308. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.78527/4.79506. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.78458/4.79602. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.78798/4.79990. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.78641/4.80043. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.78359/4.80211. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.78760/4.80095. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.78417/4.80278. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.78403/4.80383. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78258/4.81149. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.77934/4.81090. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.78701/4.80379. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.78079/4.81428. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.78083/4.80223. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78312/4.81503. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.78177/4.80452. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.77931/4.81033. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.77828/4.80481. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.77687/4.81046. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.77790/4.80828. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.77941/4.80892. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77371/4.81104. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.77184/4.82018. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.77992/4.80971. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.77572/4.80917. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.77823/4.79714. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.77980/4.80060. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78124/4.80178. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.77971/4.81107. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.77773/4.80755. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.77266/4.81307. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.77209/4.82170. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.76996/4.82997. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.77289/4.83089. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.77507/4.82174. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.77342/4.82550. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.77092/4.82437. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.77314/4.82729. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.77396/4.82311. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77424/4.82320. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77261/4.82539. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77015/4.82362. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.76373/4.84859. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77561/4.82001. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77414/4.82517. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.76616/4.83607. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.75910/4.85071. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.76751/4.84294. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.76565/4.85466. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.76153/4.88463. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.76509/4.83220. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76897/4.83401. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.76589/4.86919. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.76170/4.86205. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.76118/4.83941. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.77480/4.80724. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.76812/4.83336. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.76302/4.85121. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.76424/4.83393. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.76168/4.85124. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.76347/4.84113. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.76638/4.85355. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76537/4.86338. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.76591/4.83977. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.76215/4.85913. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.75809/4.85480. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.75275/4.85362. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.76054/4.86295. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.75660/4.87314. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.75806/4.84236. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.75778/4.87844. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.75581/4.86460. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.75421/4.88701. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.75746/4.87115. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.75083/4.88444. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.75181/4.88650. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.75253/4.89240. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75806/4.88360. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.75709/4.90330. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.75250/4.88806. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.75215/4.88498. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.75392/4.90423. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.74625/4.92538. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.74885/4.92931. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.75689/4.87833. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.74687/4.89911. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.75056/4.91534. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.75635/4.88687. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.75042/4.93458. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.75213/4.89252. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.75333/4.91895. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.74792/4.93457. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.74721/4.94396. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.09593745047287525\n",
      "Epoch 0, Loss(train/val) 5.00780/5.00410. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.97126/4.97948. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.96429/4.99298. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.96511/5.00371. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.96305/4.99650. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.96019/4.98560. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.95618/4.97582. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.95349/4.97995. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.95122/4.98649. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.95176/4.98720. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.95137/4.99296. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.95040/4.99311. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.95044/4.99277. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.95188/4.98758. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.94798/4.99872. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.95152/4.98775. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.94431/4.99965. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.94839/4.99105. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.94446/5.01490. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.94679/5.00445. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.94534/5.01761. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.94354/4.99868. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.94454/5.01124. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.94184/5.00936. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.94249/5.01953. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.94219/5.00662. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.94218/5.01770. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.94335/5.00670. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.94158/5.02073. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.94047/5.00930. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.94151/5.01188. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.94000/5.01300. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.94222/5.02946. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.93978/5.00313. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.94208/5.03349. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.93710/5.01943. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.94157/5.02177. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.94211/5.01027. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.94399/5.00481. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.94105/5.01843. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.93949/5.02328. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.93971/5.01759. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.93465/5.03050. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.94259/5.02242. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.93638/5.02799. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.94067/5.01946. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.93725/5.03368. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.93928/5.02776. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.93552/5.03272. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.93585/5.04544. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.94139/5.02742. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.93404/5.03610. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.93234/5.05686. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.93033/5.03880. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.93246/5.04071. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.93530/5.04998. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.92909/5.04290. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.93214/5.04240. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.92933/5.03935. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.93343/5.04179. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.93425/5.03270. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.92942/5.05106. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.93074/5.05169. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.93135/5.05744. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.93187/5.04318. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.92736/5.07304. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.93657/5.04822. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.93271/5.04206. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.92616/5.08095. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.93128/5.05208. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.92912/5.05661. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.92988/5.05182. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.92639/5.04845. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.92858/5.05222. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.92947/5.04675. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.92611/5.03269. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.92213/5.01946. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.94513/5.03007. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.93913/5.05377. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.93327/5.04752. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.93044/5.03286. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.92397/5.05626. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.93459/5.01328. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.93762/5.01017. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.93479/5.00980. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.93331/5.04039. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.92780/5.04816. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.92514/5.04869. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.93227/5.03283. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.93052/5.02845. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.92891/5.05185. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.92898/5.01537. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.92679/5.04560. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.93025/5.04467. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.92824/5.04488. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.92517/5.04736. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.92619/5.03249. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.93151/5.02900. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.92527/5.03991. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.92142/5.04834. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.0\n",
      "Epoch 0, Loss(train/val) 4.93641/4.88551. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.89599/4.88444. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.89055/4.88543. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.89056/4.88580. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.89078/4.88249. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.89243/4.88053. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.89007/4.87998. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.88708/4.88688. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.88535/4.88725. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.88371/4.88841. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.88245/4.89337. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.88314/4.88823. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.88371/4.89209. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.88479/4.89130. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.88284/4.90033. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.88006/4.89890. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.88118/4.89497. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.87867/4.90209. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.87839/4.90328. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.88006/4.90026. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.87396/4.90336. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.87974/4.89158. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.87794/4.89716. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.87814/4.90546. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.87668/4.90798. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.87354/4.92326. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.88502/4.90103. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.87964/4.91712. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.87784/4.90463. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.87536/4.91741. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.87176/4.91528. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.87298/4.91793. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.86833/4.93542. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.87155/4.91426. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.87401/4.91918. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.87077/4.91469. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.87325/4.92099. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.87318/4.91545. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86800/4.91986. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.87192/4.92559. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.87108/4.91848. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.86986/4.91352. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.86915/4.92504. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.86549/4.93213. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.86758/4.92986. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.86829/4.91770. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.86776/4.92844. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.86561/4.91904. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.86787/4.92868. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.86389/4.93208. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.86063/4.94029. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.86686/4.93059. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.87258/4.92185. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.86184/4.93247. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.86435/4.94570. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.86635/4.93218. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.86348/4.92162. Took 0.07 sec\n",
      "Epoch 57, Loss(train/val) 4.87158/4.91114. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.87190/4.92406. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.87749/4.91511. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.87035/4.92229. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.87051/4.93115. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.86439/4.93502. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.86861/4.93485. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.86724/4.94379. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.86865/4.91769. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.86654/4.93404. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.86650/4.92836. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.86097/4.93658. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.86872/4.93494. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.86440/4.93639. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.86843/4.93523. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.86765/4.92413. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.86737/4.92928. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.87053/4.93375. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.86594/4.95654. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.87183/4.91517. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.86138/4.96462. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.86376/4.93724. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.86022/4.94898. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.86169/4.93462. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.86245/4.93066. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.85568/4.95341. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.85871/4.92750. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.86828/4.93773. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.86732/4.92927. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.86216/4.94034. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.87362/4.93687. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.87043/4.91558. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.85971/4.93427. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.86823/4.91920. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.87865/4.91063. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.87143/4.92027. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.86641/4.91854. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.86148/4.94649. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.85738/4.94198. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.85970/4.94494. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.85685/4.94242. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.86114/4.94126. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.86144/4.92755. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.040082172520323485\n",
      "Epoch 0, Loss(train/val) 4.90743/4.87958. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.87773/4.87499. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.87623/4.87017. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.87378/4.87029. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.87218/4.87258. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.87551/4.86941. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.87325/4.86874. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.87497/4.86914. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.87375/4.86688. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.87446/4.87054. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.87440/4.86991. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.87257/4.86616. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.87163/4.86368. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.86928/4.86552. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.86706/4.87632. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.86902/4.85099. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.86783/4.86440. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.86713/4.86841. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.86652/4.86461. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.86505/4.86820. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.86590/4.85697. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.86018/4.86161. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.86223/4.85986. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.86010/4.85711. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.86402/4.85770. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.85949/4.86232. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.85958/4.87122. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.85516/4.85882. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.85846/4.86516. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.86137/4.85329. Took 0.07 sec\n",
      "Epoch 30, Loss(train/val) 4.86076/4.84664. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.86960/4.83999. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.86271/4.84561. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.85995/4.84399. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.85888/4.84540. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.85865/4.85733. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.85957/4.84713. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.85914/4.85056. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86140/4.84771. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.85686/4.83364. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.85841/4.84285. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.85854/4.84474. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.85569/4.84233. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.85316/4.83961. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.85517/4.83862. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.85138/4.83876. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.85601/4.83954. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.85285/4.85088. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84971/4.84188. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.85305/4.85835. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.85360/4.84089. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.85321/4.85885. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.85068/4.85642. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.85152/4.84599. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.85285/4.84936. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.86830/4.83360. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.86687/4.83638. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.85972/4.83636. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.85934/4.83911. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.85911/4.82462. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.85818/4.83813. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.86279/4.84722. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.85393/4.83564. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.85328/4.83900. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.85204/4.84818. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.85093/4.84038. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.85211/4.85270. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.86059/4.82927. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.85464/4.82465. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.85565/4.82672. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.85347/4.81898. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.85458/4.82545. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.85313/4.83023. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.85079/4.82299. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.85151/4.82877. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.85430/4.82222. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.85315/4.83104. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.85252/4.82688. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.84663/4.83225. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.85271/4.83403. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.84984/4.84510. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.85335/4.83153. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.85604/4.88640. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.85817/4.86463. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.85381/4.86799. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.85143/4.87101. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.84780/4.85696. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.85438/4.85055. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.84303/4.85723. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.85640/4.85097. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.84634/4.86282. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.85150/4.84897. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.84432/4.84301. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.84697/4.85092. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.85418/4.85034. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.85837/4.87067. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.85772/4.85776. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.85431/4.84074. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.85042/4.83363. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.85547/4.84291. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.035451483867683334\n",
      "Epoch 0, Loss(train/val) 4.93307/4.87443. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.85979/4.85958. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.85459/4.85325. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.85078/4.85373. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.85136/4.85624. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.85001/4.85715. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.85386/4.85640. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.85064/4.85201. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.85090/4.84937. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.84815/4.85021. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.84788/4.85006. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.84698/4.85078. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.84914/4.85002. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.84908/4.84746. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.84677/4.84496. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.84539/4.84455. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.84306/4.84618. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.84390/4.84792. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.84675/4.84368. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.84463/4.84432. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.84392/4.84689. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.84528/4.84819. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.84233/4.84556. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.84282/4.84493. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.84344/4.84507. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.83933/4.84869. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.84072/4.84722. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.83690/4.85321. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.84681/4.84522. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84119/4.84418. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.84141/4.84882. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.84144/4.84695. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.83872/4.84923. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.84519/4.84529. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.84033/4.84526. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.83906/4.84828. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84215/4.84579. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84134/4.84457. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84117/4.85256. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.83764/4.85087. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.83783/4.85672. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.83457/4.86214. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.83682/4.86281. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.83120/4.85831. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.83509/4.86152. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.83555/4.86194. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.83314/4.86555. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.83242/4.86742. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.83461/4.86073. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.82909/4.87284. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.83324/4.85921. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.83099/4.85794. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.82989/4.87518. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.83169/4.85927. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.83595/4.85820. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.82526/4.86032. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.82487/4.86292. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.83190/4.86337. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.82742/4.86198. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.82644/4.86725. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.82153/4.87123. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.82736/4.86485. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.82408/4.86097. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.82014/4.85211. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.82365/4.87453. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.81966/4.86667. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.81898/4.85965. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.82326/4.85430. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.81666/4.87231. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.82562/4.86696. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.82549/4.86152. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.81440/4.86263. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.81868/4.86004. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.81665/4.85339. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.81557/4.86152. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.81484/4.87439. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.82455/4.84971. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 4.82356/4.87074. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.82237/4.85336. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.81418/4.86341. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.82352/4.86357. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.81457/4.86511. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.81424/4.86652. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.81604/4.85529. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.80868/4.87927. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.81117/4.85586. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.81098/4.86175. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.81188/4.87038. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.81020/4.86158. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.80758/4.83065. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.81631/4.85846. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.81218/4.85635. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.80850/4.85254. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.80288/4.86826. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.80880/4.85048. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.81219/4.84879. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.80807/4.85366. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.80460/4.85608. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.80895/4.83464. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.81430/4.84426. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.051799406700962544\n",
      "Epoch 0, Loss(train/val) 5.09840/5.09551. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.07893/5.08065. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.07008/5.08645. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.07589/5.08885. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.07601/5.08941. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.07120/5.09277. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.07269/5.07827. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.07084/5.07195. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.06953/5.07126. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.07026/5.07579. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.06758/5.07533. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.06929/5.07225. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.06747/5.07265. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.06571/5.07550. Took 0.11 sec\n",
      "Epoch 14, Loss(train/val) 5.06637/5.07387. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.06656/5.07718. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.05925/5.07674. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.06212/5.08001. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.06128/5.07858. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.06361/5.07896. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.06268/5.07693. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.06223/5.07910. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 5.06467/5.07183. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 5.05824/5.08223. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.06066/5.06986. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.05910/5.07670. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.05968/5.07247. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.05811/5.07717. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 5.05800/5.07337. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 5.05768/5.07852. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 5.05863/5.06692. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.05681/5.07898. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 5.05172/5.07474. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.05615/5.07519. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.05267/5.07063. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 5.05033/5.07574. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.05412/5.06489. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.04517/5.07284. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 5.05259/5.05759. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.05109/5.06335. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.05042/5.07572. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.04559/5.08436. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.05002/5.06385. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.05088/5.08658. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.04653/5.07747. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.04837/5.06589. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.04232/5.06397. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 5.05302/5.06255. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 5.04434/5.07222. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.04939/5.05851. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.04200/5.06960. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.04339/5.07001. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.05502/5.05700. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.05203/5.08704. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.04953/5.06038. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.03718/5.05867. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.04154/5.07960. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.04178/5.06163. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.04862/5.06576. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.04233/5.07981. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.03486/5.07513. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.04652/5.05593. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 5.03958/5.06286. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 5.04293/5.05969. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.04310/5.07503. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.03906/5.06159. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 5.04135/5.08386. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.03895/5.06111. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.03680/5.08104. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.03918/5.05779. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.03379/5.09159. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.03244/5.05501. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.03843/5.04436. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.03917/5.05668. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.03923/5.04523. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.03204/5.06024. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.03709/5.06329. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.03326/5.05777. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.03158/5.07276. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.04045/5.07113. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 5.03505/5.07976. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.03947/5.05693. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.04089/5.07252. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.03311/5.07698. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.02684/5.05360. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.03150/5.06291. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.04904/5.07728. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 5.03900/5.09724. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 5.03284/5.07635. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 5.03507/5.09239. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.02861/5.08838. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.03274/5.07708. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.03274/5.05967. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.02517/5.06884. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 5.02989/5.07219. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 5.02479/5.07351. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.02647/5.08282. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 5.03465/5.07116. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 5.04563/5.07036. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.02493/5.08551. Took 0.09 sec\n",
      "ACC: 0.515625, MCC: 0.06052275326688024\n",
      "Epoch 0, Loss(train/val) 4.96953/4.90307. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.91987/4.90955. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.92204/4.89856. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.92539/4.89689. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.91940/4.89395. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.91490/4.89473. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.90947/4.89296. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.90930/4.89399. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.91031/4.89611. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.90658/4.89930. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.90594/4.90460. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.90788/4.90255. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.90483/4.90395. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.90162/4.90609. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.90031/4.90893. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.90252/4.90910. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.90393/4.90305. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.91338/4.91238. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.90574/4.89518. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.90400/4.89351. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.89661/4.89801. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.89946/4.89875. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.89749/4.90025. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.89434/4.90475. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.89305/4.91280. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.89358/4.90519. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.89519/4.90854. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.89442/4.91362. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.89506/4.92511. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.89442/4.91012. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.89125/4.91241. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.89696/4.92848. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.89374/4.91314. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.89067/4.91841. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.88734/4.91960. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.88747/4.92306. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.88985/4.91959. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.89703/4.91592. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.89363/4.91744. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.91959/4.91459. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.90576/4.90680. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.90720/4.90003. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.90646/4.90124. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.90491/4.89877. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.90321/4.89640. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.90498/4.89069. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.90413/4.89055. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.90259/4.88405. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.89987/4.88368. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.89932/4.88006. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.90082/4.88356. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.89336/4.87465. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.89976/4.88737. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.90211/4.87235. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.89646/4.88671. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.89520/4.87982. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.89258/4.88730. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.89338/4.88703. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.89039/4.89091. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.88685/4.89176. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.89115/4.88761. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89064/4.89188. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.88643/4.89436. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.88734/4.88933. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.89569/4.89537. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.88408/4.88951. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.88319/4.89208. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.87944/4.89429. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.88217/4.90486. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.87768/4.90310. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.88491/4.89915. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.88021/4.90688. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.87591/4.90849. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88222/4.90629. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.88428/4.91828. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.87795/4.91056. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88126/4.91765. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.88483/4.89617. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.87847/4.93128. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88196/4.92121. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.88287/4.93057. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.88419/4.92134. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.87922/4.92262. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.87695/4.91125. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.87701/4.93897. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.87714/4.93465. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.87531/4.93388. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.87346/4.93873. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.87908/4.93193. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88161/4.92244. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.88399/4.92869. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.88229/4.93180. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.87491/4.93587. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 4.87744/4.93257. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.87745/4.93114. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.87682/4.94102. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.87205/4.93984. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.87430/4.94644. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.87892/4.93594. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.87543/4.92602. Took 0.08 sec\n",
      "ACC: 0.390625, MCC: -0.24165949998852546\n",
      "Epoch 0, Loss(train/val) 5.05753/5.00026. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.00055/5.00655. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.00063/5.01042. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 5.00265/5.01159. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 5.00229/5.01896. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.00450/5.02757. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.00055/5.02702. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.99638/5.01812. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.99514/5.01370. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.99405/5.01534. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.99418/5.04461. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 5.00251/5.01994. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.99714/5.02074. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.99533/5.01925. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.99549/5.02311. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.99296/5.02609. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.99381/5.02675. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.99070/5.02826. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.99066/5.02210. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.99070/5.02517. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.99538/5.02111. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.99633/5.01643. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.99610/5.01683. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.99680/5.01332. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.99629/5.01093. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.99505/5.01438. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.99345/5.01608. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.99195/5.01752. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.99097/5.01952. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.99097/5.01949. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.99223/5.01942. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.99020/5.01872. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.99150/5.01664. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.98749/5.02072. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.99212/5.01503. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.99163/5.01216. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.98794/5.01368. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.98631/5.01343. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.98839/5.02009. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.99697/5.01903. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.99402/5.01266. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.99238/5.01320. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.99177/5.01629. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.98925/5.01591. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.99000/5.02594. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.99119/5.02299. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.99058/5.02285. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.98834/5.02893. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.99307/5.02797. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.98858/5.02129. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.98702/5.01757. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.98674/5.02006. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.98573/5.01590. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.98647/5.01557. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.98750/5.01874. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.98698/5.01768. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.98256/5.01710. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.98746/5.01671. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.98586/5.02273. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.98230/5.03205. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.98419/5.02139. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.98390/5.02988. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.98422/5.02149. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.98207/5.01752. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.98554/5.02099. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.98170/5.02695. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.98148/5.03539. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.97924/5.04367. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.98168/5.02975. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.97593/5.04605. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.97948/5.05053. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.98256/5.03465. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.98676/5.03361. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.98104/5.04719. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.98171/5.03724. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.97918/5.06384. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.98719/5.01094. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.98632/5.01373. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.98198/5.02414. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.98169/5.02749. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.98387/5.03306. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.98141/5.03735. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.97923/5.05562. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.98077/5.05825. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.97989/5.04989. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.97415/5.04466. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.97653/5.05544. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.97552/5.07271. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.97377/5.04629. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.97734/5.04110. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.97457/5.05102. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.97400/5.06960. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.97719/5.05168. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.97425/5.04548. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.97805/5.06389. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.97762/5.04725. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.97248/5.04960. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.97365/5.08067. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.97361/5.03773. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.97615/5.05364. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.008866995073891626\n",
      "Epoch 0, Loss(train/val) 4.53205/4.48510. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.49512/4.47890. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.50662/4.49030. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.51219/4.53828. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.49653/4.52756. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.49313/4.51689. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.49241/4.52290. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.49440/4.52593. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.49153/4.52268. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.49426/4.52831. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.48908/4.52404. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.48984/4.52048. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.49234/4.53175. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.48516/4.52401. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.49173/4.52446. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.49084/4.54067. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.48785/4.52449. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.48901/4.53551. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.48530/4.53144. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.48760/4.52489. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.48686/4.53148. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.48508/4.53416. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.48291/4.53073. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.48403/4.54437. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.48664/4.54992. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.48611/4.53696. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.48230/4.53964. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.47959/4.55213. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.47627/4.52171. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.48522/4.52849. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.48170/4.54015. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.47715/4.53717. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.47586/4.48114. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.48581/4.53120. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.47747/4.52279. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.47490/4.52718. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.47915/4.53304. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.47714/4.51993. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.47121/4.52835. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.47951/4.53200. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.47153/4.52495. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.47376/4.53624. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.47091/4.54005. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.47278/4.53140. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.48173/4.54579. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.47484/4.54362. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.47839/4.50121. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.47531/4.53992. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.47061/4.55482. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.47169/4.51583. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.47510/4.52796. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.46899/4.53831. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.47175/4.52906. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.46974/4.56267. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.47082/4.55060. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.46453/4.56305. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.47799/4.54423. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.46852/4.55308. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.46910/4.55424. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.46472/4.55913. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.46880/4.57493. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 4.46739/4.56126. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.46859/4.55117. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.46565/4.56916. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.46809/4.58321. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.46429/4.55832. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.46626/4.56554. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.47472/4.56659. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.46736/4.56124. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.46668/4.56406. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.46072/4.51117. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.46860/4.57962. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.46312/4.57682. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.46705/4.54640. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.46136/4.57484. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.46439/4.51974. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.47165/4.52926. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.46702/4.57884. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.46731/4.58247. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.46537/4.57973. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.45691/4.58344. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.45871/4.60899. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.46721/4.59517. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.45919/4.59061. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.46231/4.58840. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.46246/4.56867. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.46209/4.58087. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.46592/4.59542. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.45749/4.58711. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.45694/4.60734. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.46378/4.57956. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.46603/4.57765. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.45835/4.58787. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.46061/4.58219. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.46086/4.59629. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.45917/4.59776. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.46328/4.58269. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.45790/4.58265. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.45909/4.58432. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.46073/4.58088. Took 0.09 sec\n",
      "ACC: 0.59375, MCC: 0.19088542889273333\n",
      "Epoch 0, Loss(train/val) 4.94942/4.86614. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 4.87413/4.87085. Took 0.17 sec\n",
      "Epoch 2, Loss(train/val) 4.87478/4.86864. Took 0.15 sec\n",
      "Epoch 3, Loss(train/val) 4.86942/4.87247. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 4.86447/4.87208. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.86382/4.87386. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.86732/4.87455. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.86275/4.87851. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.86349/4.86534. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.86263/4.86673. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.85989/4.86621. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.85771/4.86896. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.85622/4.87067. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.86002/4.87281. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.85798/4.87229. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.85301/4.87224. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.85241/4.87340. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.84801/4.88257. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.84788/4.88587. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85207/4.88128. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.84944/4.88420. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.84820/4.90764. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.84763/4.89545. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.84939/4.89622. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.85219/4.90159. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.84540/4.89611. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.84312/4.89640. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.84681/4.89736. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.84829/4.90330. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.83813/4.89989. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.84397/4.90460. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.85164/4.89585. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84490/4.90690. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.83867/4.91647. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.83944/4.90226. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.84555/4.91953. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.83714/4.90725. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.84298/4.91159. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.85524/4.88917. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.84630/4.90355. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84756/4.89236. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.84364/4.91239. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84417/4.90643. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.84376/4.91965. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.84540/4.89893. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.84435/4.90538. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.84398/4.89782. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.84398/4.90202. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84492/4.90301. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.83901/4.91013. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.84320/4.91405. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.84427/4.90773. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.84270/4.91195. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.83800/4.91381. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.84123/4.91561. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.84275/4.90934. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.84405/4.90546. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.84180/4.91142. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.84002/4.91020. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.84454/4.90978. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.83966/4.90440. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.83815/4.90958. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.83662/4.90485. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.84319/4.90342. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.84107/4.90074. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.83634/4.91396. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.83988/4.91360. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.83989/4.91296. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.83265/4.92372. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.83731/4.91062. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.83606/4.91418. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.83507/4.91389. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83387/4.92502. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.83906/4.92386. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.83638/4.91411. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83455/4.94129. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.84057/4.91156. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.83826/4.91542. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.83480/4.91773. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.83571/4.91015. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.83644/4.92117. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.83211/4.92489. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.83272/4.92031. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.82993/4.91997. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83314/4.94191. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83862/4.91520. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.83331/4.90932. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.83049/4.90772. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83459/4.92644. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.83124/4.90490. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.82902/4.93266. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.82798/4.92538. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.82532/4.93750. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.82944/4.93507. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.83256/4.93020. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.82308/4.93870. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.82860/4.93281. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.82880/4.92876. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.82698/4.95719. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.82489/4.95119. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.0905982365507463\n",
      "Epoch 0, Loss(train/val) 4.82806/4.79970. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.80420/4.79002. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.79461/4.79605. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79905/4.80963. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79797/4.81116. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79586/4.80310. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.78887/4.80061. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.78954/4.80846. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.78904/4.81103. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79004/4.81154. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.78600/4.80976. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.78610/4.81192. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.78774/4.81528. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.78542/4.81287. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.78422/4.81064. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.78023/4.81210. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78246/4.81945. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.78320/4.81875. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.78184/4.82452. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.78256/4.81844. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.78160/4.82852. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78637/4.81856. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.78214/4.81102. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.78088/4.81939. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.77922/4.83163. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.78038/4.82761. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.78009/4.82558. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.77879/4.82785. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77996/4.82857. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.77793/4.83379. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.77907/4.83101. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.77733/4.83694. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.77499/4.83783. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.77877/4.83492. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.77863/4.83608. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.77342/4.84928. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.77413/4.84448. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.77648/4.84659. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.77417/4.85203. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.77581/4.84383. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.77479/4.84977. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.77092/4.87889. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.76980/4.87999. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.77739/4.82621. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.77661/4.83570. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.77152/4.84021. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77337/4.84811. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.76665/4.87237. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.76914/4.85065. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.77033/4.86045. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.76709/4.85329. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.77105/4.85649. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.76689/4.86174. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.76747/4.89080. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.76750/4.86348. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.77062/4.84611. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.76828/4.86409. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.76448/4.86932. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76412/4.91670. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.77228/4.83896. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.78067/4.83247. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.77147/4.87404. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.77323/4.86272. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.76693/4.86118. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.76490/4.91259. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.77478/4.84085. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.76790/4.88629. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.76762/4.89793. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.77174/4.85741. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76083/4.89875. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.76788/4.88870. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.76508/4.88648. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76430/4.87774. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.76048/4.90729. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.76541/4.89546. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.75939/4.89330. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.76680/4.88238. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.76373/4.89431. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.76028/4.93336. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.76194/4.90560. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.76289/4.89458. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.75801/4.92501. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.76560/4.89013. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76114/4.90689. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75985/4.92243. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.76317/4.85688. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.75494/4.94759. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.75559/4.89125. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.75685/4.91494. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.75999/4.91069. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.76121/4.89662. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.75718/4.90273. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.74975/4.96647. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.76014/4.88900. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.75992/4.91683. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.76044/4.89030. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.76158/4.93150. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.75355/4.91818. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.74650/4.96605. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.75513/4.90576. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.09847634407689815\n",
      "Epoch 0, Loss(train/val) 4.86875/4.84348. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.86059/4.87241. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.85371/4.86053. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.85036/4.84532. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.85279/4.83763. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.85179/4.83744. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.84913/4.84057. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.84394/4.84348. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.84800/4.84753. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.84701/4.84995. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.84541/4.85560. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.84460/4.86349. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.84348/4.86631. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.84532/4.85337. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.84502/4.85888. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.84108/4.85689. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.83859/4.86893. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.84014/4.86824. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.83824/4.87051. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.83804/4.87861. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.83760/4.87315. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.83278/4.87594. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.85283/4.84422. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.84920/4.84342. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 4.84613/4.84452. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.84568/4.84363. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.84458/4.84260. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.84294/4.84545. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.84451/4.84414. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84380/4.84745. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.84069/4.85347. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.84365/4.84966. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.83851/4.85218. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.83760/4.86125. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.84536/4.84202. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84538/4.84658. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84236/4.85693. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.84075/4.85952. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.83908/4.86506. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.83855/4.86705. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.83934/4.87013. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.83708/4.87445. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.83494/4.88892. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.83700/4.87476. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.83415/4.87761. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.82934/4.88225. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.83618/4.87110. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.83334/4.87072. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.83484/4.89180. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.83470/4.87342. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.83316/4.89153. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.83204/4.88059. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.83204/4.88122. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.82833/4.89485. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.82935/4.88426. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.82979/4.89051. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.82972/4.87290. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.83268/4.87326. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.82767/4.88369. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83495/4.87903. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.82991/4.89713. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83175/4.89529. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.83346/4.88564. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.83234/4.87837. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.83054/4.87616. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.83053/4.88051. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.82752/4.89422. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.82578/4.89447. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.82450/4.89930. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.82887/4.87075. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.83042/4.86068. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.82687/4.88785. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.82934/4.86585. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.84342/4.86507. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83961/4.86907. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83629/4.87733. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.83695/4.87774. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.83056/4.88244. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.82989/4.89022. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.82993/4.88043. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83812/4.87142. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.83076/4.87571. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.82822/4.87636. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.82874/4.87518. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.82710/4.88369. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83714/4.87394. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83759/4.87398. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.83325/4.89215. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83649/4.88722. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.83627/4.88897. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.83083/4.89070. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.83437/4.89426. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83269/4.88892. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83188/4.89707. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.82564/4.89648. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.82619/4.90210. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.82663/4.90559. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.83060/4.88969. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.82539/4.89957. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.82582/4.88154. Took 0.08 sec\n",
      "ACC: 0.59375, MCC: 0.1889822365046136\n",
      "Epoch 0, Loss(train/val) 4.82482/4.83519. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.80825/4.81119. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.80084/4.78226. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.80434/4.78036. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.80118/4.77924. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79137/4.77303. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.78728/4.77083. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.79222/4.77074. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.79222/4.76926. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.78822/4.76787. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79103/4.76842. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.78798/4.76717. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.78920/4.76782. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.78888/4.76849. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.78603/4.76738. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.78815/4.76812. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78688/4.76882. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.78636/4.76889. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.78477/4.76991. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.78531/4.76927. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.78823/4.76874. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78561/4.76898. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.78510/4.77064. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.78369/4.76792. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.78517/4.76874. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.78215/4.77040. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.78378/4.77221. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.78398/4.77347. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.78374/4.77461. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.78263/4.77870. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.78096/4.78450. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78225/4.78737. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.78054/4.78600. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.77790/4.78921. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78017/4.80671. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.79066/4.80002. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.78545/4.80509. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.78201/4.81126. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.78290/4.81091. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.78470/4.80948. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.78347/4.80961. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.78212/4.81399. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.78193/4.81812. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.78257/4.81847. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.78125/4.81729. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.77808/4.82449. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.78027/4.82452. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.77982/4.81365. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77746/4.81820. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.77918/4.81520. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77529/4.83257. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77923/4.81784. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.77748/4.82063. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.77694/4.82887. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.77580/4.82151. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.77667/4.81437. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.77475/4.82393. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.77378/4.82552. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.77673/4.82671. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.77459/4.82881. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.77491/4.82974. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.77687/4.83492. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.77573/4.81785. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.77409/4.83797. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.77769/4.82390. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.77383/4.83158. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.78269/4.80855. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.78239/4.80374. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.77905/4.81677. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.77716/4.82418. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.77907/4.81871. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.77568/4.82620. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.77337/4.83167. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.77046/4.84557. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.77268/4.83927. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.77227/4.83286. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.77460/4.81573. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.77316/4.82839. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.77087/4.84064. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.76990/4.82400. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.77385/4.81915. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.77140/4.82998. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.77458/4.82252. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.77120/4.84059. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.77398/4.82373. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.76938/4.83244. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.77202/4.81830. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.76956/4.81666. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.77374/4.81357. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.76835/4.83731. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.77229/4.81276. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.77324/4.82927. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.77070/4.82874. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.76986/4.83175. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.76567/4.81759. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.76965/4.82107. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.76562/4.83360. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.76739/4.82975. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77125/4.83769. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.77264/4.82026. Took 0.09 sec\n",
      "ACC: 0.53125, MCC: 0.07570682517832987\n",
      "Epoch 0, Loss(train/val) 5.00819/4.91640. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.91937/4.91522. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.92154/4.92619. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.92070/4.93288. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.91875/4.92558. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.91545/4.92356. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.91091/4.93013. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.91158/4.93207. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.91197/4.93191. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.91224/4.92945. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.90853/4.93037. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.91032/4.93022. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.90685/4.93081. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.90774/4.93175. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.90676/4.92399. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.90438/4.93684. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.90549/4.93269. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.90687/4.91257. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.90393/4.91781. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.90340/4.91311. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.90376/4.92508. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.90140/4.92349. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.90618/4.90846. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.90123/4.92220. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.90069/4.92134. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.89949/4.91784. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.89899/4.92514. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.90149/4.91337. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.89699/4.92864. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.89874/4.91716. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.89860/4.91573. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.89352/4.92942. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.89766/4.91767. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.90047/4.91317. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.89364/4.92010. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.89432/4.91109. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.89203/4.91582. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.89101/4.91424. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.89489/4.91286. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.88947/4.92500. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.88967/5.00450. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.92022/4.92238. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.91238/4.91694. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.90654/4.91734. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.89778/4.92308. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.89348/4.92193. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.89069/4.94086. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.89158/4.92723. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.89389/4.91501. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.88708/4.91436. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.88814/4.90973. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.88560/4.91119. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.88827/4.91310. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.88819/4.91130. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.88632/4.91238. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.88505/4.92081. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.88710/4.91435. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.88524/4.91024. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.88793/4.90947. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.89063/4.90699. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.88033/4.91903. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.87740/4.92635. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.90169/4.91800. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.90372/4.91469. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.89383/4.92600. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.88927/4.93964. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.88512/4.94022. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.88643/4.93010. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.88109/4.94352. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.88592/4.93064. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.87995/4.96033. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.87839/4.95145. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88987/4.90912. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.89886/4.92693. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.89703/4.93289. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.89501/4.92637. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.89406/4.92860. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.89257/4.92785. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.89591/4.93107. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88705/4.93287. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.88923/4.93441. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.88515/4.94113. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.88460/4.93491. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.88036/4.95588. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.88921/4.94805. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.88603/4.93752. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.87916/4.96905. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.88695/4.94630. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.88272/4.96599. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88031/4.95070. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.88187/4.97954. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.88294/4.96865. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.88101/4.95577. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.88381/4.94919. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.88935/4.97761. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.88092/4.93056. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.88188/4.91995. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.87874/4.95342. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.88004/4.95952. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.88006/4.95580. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.055227791305300936\n",
      "Epoch 0, Loss(train/val) 4.98759/4.99420. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.97716/4.97806. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.98513/4.99137. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.00280/4.97422. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.99320/5.00661. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.97212/4.98212. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.97711/4.98987. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.97418/4.98824. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.97433/4.98948. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.97668/4.98981. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.97433/4.99116. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.97579/4.99005. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.97652/4.99072. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.97423/4.99302. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.97454/4.99389. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.97371/4.99312. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.97428/4.99394. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.97330/4.99534. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.97302/4.99254. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.97146/4.99429. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.97064/4.99548. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.97302/4.99468. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.97116/5.00058. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.96929/4.99505. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.97186/4.99753. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.97087/4.99625. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.96970/4.99976. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.96898/4.99669. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.97006/4.99671. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.97017/4.99811. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.96906/4.99687. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.96886/4.99808. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.96534/5.00614. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.96695/5.00135. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.96658/5.00155. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.96532/5.00877. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.96573/5.01385. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.96482/5.00770. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.96165/5.01074. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.96698/5.00932. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.95985/5.01435. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.96473/5.01099. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.96182/5.00838. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.96136/5.01930. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.96126/5.00930. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.96026/5.01665. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.95780/5.01354. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.95874/5.00907. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.96279/5.01220. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.95743/5.02010. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.95809/5.00802. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.95844/5.00621. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.96266/5.00795. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.95466/5.01677. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.95759/4.99928. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.95690/5.00797. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.95560/4.99825. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.96268/5.00765. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.95718/5.01306. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.95476/5.01228. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.95225/5.02562. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.95844/5.01086. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.95525/5.00584. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.95779/5.00942. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.95014/5.00329. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.95333/4.99188. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.95381/5.00961. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.94653/5.00871. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.94980/5.00817. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.95792/5.00645. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.95435/5.02137. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.95826/5.00411. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.95839/5.01059. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.94816/5.01556. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.95106/5.01594. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.95094/5.01307. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.94681/5.00131. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.95114/5.01220. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.94928/5.00438. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.95194/5.00615. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.95010/4.97883. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.95201/4.97880. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.94826/4.99718. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.94938/4.99997. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.95153/4.99925. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.94992/5.00600. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.95060/5.00397. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.95324/5.00046. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.94903/5.00713. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.94744/5.01235. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.95298/5.01267. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.94646/5.00985. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.94427/4.99489. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.94481/5.00121. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.95243/5.00699. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.94705/4.99567. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.94648/4.99801. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.94633/4.99795. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.94341/5.00384. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.94611/4.99766. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.12725695259515554\n",
      "Epoch 0, Loss(train/val) 5.07976/5.03437. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.02099/5.03403. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.02286/5.03539. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.02081/5.02037. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 5.01715/5.01322. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.01346/5.01230. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.01019/5.01550. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.01162/5.01487. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.00934/5.01583. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.01176/5.01336. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.01145/5.01427. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.00676/5.01304. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.00679/5.01334. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.00825/5.01210. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.00750/5.01513. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.00627/5.01621. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.00290/5.02036. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.00268/5.02147. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.00546/5.02517. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 5.00428/5.02587. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.00623/5.01975. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.00241/5.01997. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.00011/5.02015. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.00140/5.01921. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.00137/5.02089. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.99966/5.02208. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.00150/5.02107. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.99855/5.02271. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.99929/5.02111. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.99863/5.02212. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.99230/5.02804. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 4.99864/5.02888. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.99248/5.03075. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.99635/5.03421. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.99555/5.02459. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.98757/5.03326. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.98910/5.02942. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.99310/5.03193. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.99326/5.02811. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.99312/5.02317. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.99081/5.01778. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.98466/5.02675. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.98756/5.01196. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.98515/5.01708. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.99739/5.02566. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.98957/5.02218. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.98284/5.02107. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.98550/5.03096. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.99147/5.02642. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.98338/5.02679. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.98527/5.00628. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.99181/5.01150. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.98077/5.02133. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.99203/5.01415. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.98079/5.01173. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.97967/5.02731. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.98037/5.01059. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.97627/5.01989. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.97460/5.02640. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.97136/5.02852. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.98031/5.01582. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.97500/5.02272. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.97752/5.01805. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.99434/5.01493. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.99350/5.00339. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.98583/5.01405. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.97490/5.00964. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.99199/5.00004. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.98015/5.01053. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.97687/5.02166. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.97561/5.01769. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.97566/5.01474. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.98156/5.00710. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.96963/5.03756. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.97074/5.02748. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.97593/5.00563. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.97258/5.01884. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.96226/5.04396. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.97496/4.99480. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.97326/4.99939. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.97270/5.01299. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.96778/5.01461. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.96825/5.02471. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.96539/4.99837. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.96960/5.00517. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.97404/5.03123. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.96781/5.01399. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.97480/5.02726. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.96939/5.02422. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.96308/5.01641. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.97357/5.01689. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.97567/5.03455. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.95816/5.06375. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.97061/5.02411. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.96533/5.01735. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.96597/4.99166. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.96546/5.01808. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.95331/5.03107. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.96708/5.04031. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.96189/5.05230. Took 0.09 sec\n",
      "ACC: 0.328125, MCC: -0.36701067093367395\n",
      "Epoch 0, Loss(train/val) 4.73681/4.69848. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.70737/4.69295. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.69963/4.69649. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.69678/4.69843. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.70016/4.69802. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.69980/4.69465. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.70700/4.69059. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.70722/4.70884. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.70111/4.70360. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.69432/4.70136. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.69754/4.70370. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.69827/4.70857. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.69621/4.70375. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.69557/4.70588. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.69537/4.70732. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.69539/4.70844. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.69519/4.71213. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.69307/4.70805. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.69482/4.70906. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.69392/4.70910. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.69133/4.71089. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.69441/4.71662. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.69198/4.71211. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.69369/4.71674. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.68983/4.71609. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.69271/4.72491. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.68950/4.72491. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.69001/4.72452. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.68670/4.72494. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.68715/4.73034. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.68852/4.74063. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.68944/4.73506. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.68478/4.73327. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.68558/4.72956. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.68749/4.74478. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.68929/4.73245. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.68075/4.73880. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.68406/4.72572. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.68571/4.72911. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.68525/4.73595. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.68050/4.74783. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.68917/4.73040. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.67556/4.76875. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.68892/4.72386. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.68093/4.76449. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.68019/4.73566. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.67746/4.74374. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.67857/4.74404. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.68049/4.75363. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.68649/4.73168. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.67738/4.74955. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.67530/4.73660. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.68591/4.74398. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.67705/4.74391. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.67441/4.74478. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.67560/4.76283. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.68367/4.72400. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.67718/4.76365. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.68180/4.74315. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.67313/4.75538. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 4.67404/4.75334. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.67389/4.74888. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.67294/4.74888. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.67637/4.75522. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.67745/4.75713. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.67551/4.74817. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.67066/4.77418. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.67570/4.73773. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.67151/4.77408. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.68249/4.74016. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.67924/4.74295. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.67356/4.75166. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.67653/4.74301. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.67277/4.76552. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.67576/4.74112. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.67421/4.76450. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.67083/4.77398. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.67581/4.73441. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.67173/4.76854. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.67533/4.74314. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.67386/4.77415. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.66838/4.75630. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.66645/4.74458. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.67023/4.77537. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.66971/4.75639. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.66854/4.75277. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.66483/4.76743. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.66419/4.76317. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.67036/4.74637. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.66609/4.77120. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.67096/4.74119. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.66594/4.76359. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.67050/4.75570. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.67106/4.76557. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.66775/4.75881. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.66701/4.75405. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.66220/4.76291. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.67135/4.73128. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.66711/4.76782. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.66377/4.76341. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.03734655007457467\n",
      "Epoch 0, Loss(train/val) 4.90047/4.83866. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.85503/4.84548. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.85549/4.84569. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.85085/4.84391. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.84983/4.83183. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.85089/4.83902. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.85000/4.84095. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.84921/4.85479. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.85423/4.86962. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.85451/4.86811. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.85082/4.85178. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.84620/4.85181. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.84150/4.86087. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.84069/4.86416. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.84056/4.87499. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.84245/4.87298. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.84187/4.87099. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.83735/4.86757. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.83573/4.88225. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.83705/4.87637. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.83183/4.88380. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.82872/4.89114. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.83101/4.88441. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.82929/4.89025. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.83228/4.87884. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.82562/4.90309. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.83027/4.88366. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82570/4.90667. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.82855/4.87938. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.82818/4.89762. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.82870/4.88532. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.83041/4.88242. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.82155/4.89653. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.82498/4.87909. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.82547/4.89877. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.82659/4.89265. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.82320/4.89321. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.82530/4.89122. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.82670/4.91736. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.83421/4.88873. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.83318/4.87853. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.82429/4.89253. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.81936/4.89701. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.82376/4.90484. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.82313/4.90220. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.82134/4.91443. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.81933/4.91373. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.83656/4.87923. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.82877/4.87294. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.82487/4.89393. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.82410/4.88678. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.82423/4.88386. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.82511/4.88449. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.82068/4.88628. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.81814/4.89041. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.81778/4.90534. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.81871/4.90139. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.81813/4.91184. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.82134/4.89961. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.81491/4.90003. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.81803/4.90097. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.82032/4.87479. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.81326/4.92497. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.82279/4.88902. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.81945/4.90556. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.81831/4.88578. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.81177/4.92733. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.81878/4.89011. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.81709/4.90940. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.81108/4.93429. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.81685/4.90331. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.81579/4.89345. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.81837/4.88887. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.81570/4.92274. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.81272/4.90018. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.81283/4.90623. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.80420/4.91869. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.81450/4.90621. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.81839/4.91174. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.81580/4.91360. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.80932/4.92245. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.80933/4.92144. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.81680/4.90163. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.80912/4.93392. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.81548/4.90551. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.80785/4.90662. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.81330/4.91258. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.81299/4.91008. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.80947/4.92542. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.81093/4.90664. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.80792/4.88992. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.80739/4.92300. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.81273/4.92131. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.81097/4.91382. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.80503/4.96900. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.82198/4.87199. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.80881/4.93724. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.80253/4.92195. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.80545/4.91677. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.80784/4.91962. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.013980301652228746\n",
      "Epoch 0, Loss(train/val) 4.86249/4.82659. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.81812/4.81165. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.81584/4.81249. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.81361/4.81479. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.81272/4.81397. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.81259/4.81728. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.81032/4.82125. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.81092/4.82328. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.80897/4.82423. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.80755/4.82605. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.80804/4.82494. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.80687/4.82583. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.80643/4.82624. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.80567/4.82865. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.80448/4.82957. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.80585/4.82811. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.80221/4.83415. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.80380/4.83801. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.80177/4.83943. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.80100/4.84820. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.80429/4.83677. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.79508/4.85387. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.80027/4.85109. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.78885/4.86178. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.79604/4.85271. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.79302/4.85736. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.79229/4.86483. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.79086/4.85369. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.79277/4.86283. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.79367/4.85784. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.79331/4.86907. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.79270/4.86710. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.78804/4.88602. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.79015/4.87856. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78410/4.87901. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.78948/4.88301. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.78890/4.88311. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.78393/4.88536. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.78141/4.88883. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.78726/4.90305. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.78010/4.91604. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.78424/4.89060. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.77739/4.94570. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.78109/4.90157. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.77662/4.92730. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.77501/4.91130. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77963/4.91162. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77197/4.92987. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.78022/4.90377. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.78499/4.91075. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77801/4.91163. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77798/4.91125. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.78099/4.89345. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.79150/4.86555. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.78006/4.90663. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.78293/4.88830. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.77865/4.90405. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.77504/4.89199. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.77752/4.94074. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.78302/4.94990. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.79807/4.88385. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.78156/4.91292. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.77954/4.90453. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.78339/4.89257. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.77950/4.92284. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.78130/4.92608. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.77692/4.90733. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.77198/4.93394. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.77307/4.91590. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.77383/4.92248. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.77100/4.91588. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78012/4.89795. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.77768/4.94568. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.78441/4.89141. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.77006/4.94456. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.77285/4.95768. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.77461/4.90241. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.77326/4.92840. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.77437/4.95600. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.76410/4.94148. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.77270/4.87302. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.76916/4.93406. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.77605/4.90629. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76768/4.93201. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.76466/4.90422. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.76558/4.97902. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.77557/4.95684. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.76607/4.98275. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.76413/4.98279. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.77610/4.95276. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.76741/4.94538. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.76800/4.95093. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.76212/4.96088. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.76598/4.97265. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.76463/4.94364. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.76379/4.97064. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.76692/4.96246. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.76972/4.95729. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.76500/4.91256. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.76399/4.93117. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.11555252264060875\n",
      "Epoch 0, Loss(train/val) 4.77652/4.72712. Took 0.18 sec\n",
      "Epoch 1, Loss(train/val) 4.70633/4.67795. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 4.70056/4.67192. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.70079/4.67002. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.69981/4.66996. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.70022/4.66871. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.69676/4.66962. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.69413/4.66734. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.69574/4.66582. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.69675/4.66475. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.69593/4.66487. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.69634/4.66548. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.69276/4.66584. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.69469/4.66635. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.69150/4.66540. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.69112/4.66715. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.69430/4.67716. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.69063/4.67708. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.68821/4.68071. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.68907/4.68297. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.68874/4.68265. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.68571/4.68539. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.68355/4.68915. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.68767/4.69155. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.68680/4.68988. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.68602/4.69526. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.68533/4.70000. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.68635/4.69423. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.68621/4.70870. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.68259/4.70434. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.68229/4.69613. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.68347/4.70408. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.68165/4.70289. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.67998/4.70778. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.68230/4.69780. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.67981/4.71826. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.68427/4.70449. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.67862/4.70736. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.68080/4.70857. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.68004/4.70947. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.67845/4.72056. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.68001/4.70402. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.68012/4.71359. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.68004/4.71609. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.67595/4.71915. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.67535/4.72228. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.67418/4.73974. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.67690/4.71883. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.67953/4.70715. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.67292/4.71602. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.67723/4.73707. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.67478/4.71548. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.67541/4.73939. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.67973/4.69172. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.67375/4.71358. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.67614/4.70944. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.67321/4.72588. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.67501/4.69917. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 4.66941/4.71849. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.66848/4.72546. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.66661/4.74005. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.67437/4.71279. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.66910/4.73627. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.67348/4.70405. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.66861/4.71733. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.67329/4.70057. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.67602/4.68543. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.67815/4.69525. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.68798/4.68594. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.67796/4.69159. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.67274/4.70676. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.67643/4.70940. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.67922/4.70521. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.67132/4.69526. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.67624/4.71885. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.67928/4.69390. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.67477/4.70313. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.67872/4.70219. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.68175/4.69647. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.66780/4.72374. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.67482/4.71436. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.67051/4.73459. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.67646/4.69976. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.66982/4.72755. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.66994/4.71610. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.66988/4.75184. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.66988/4.71770. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.66792/4.71934. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.67023/4.69996. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.66908/4.70725. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.67050/4.71116. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.66963/4.73204. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.67137/4.72074. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.67210/4.71657. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.66480/4.71918. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.66451/4.72661. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.66587/4.72270. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.66765/4.71106. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.66564/4.73274. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.65911/4.74218. Took 0.08 sec\n",
      "ACC: 0.375, MCC: -0.26035958758213296\n",
      "Epoch 0, Loss(train/val) 5.22837/5.23062. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.16630/5.19312. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.15990/5.17998. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 5.15489/5.18186. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.15319/5.18576. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.15140/5.18496. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.15070/5.18556. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.15385/5.18358. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.15039/5.18303. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.15012/5.18054. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.15024/5.17852. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.15057/5.18053. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 5.14808/5.18132. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.15095/5.18017. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.14897/5.17651. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.15149/5.17721. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.14806/5.17751. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.14672/5.16148. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.14359/5.16568. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.14441/5.16335. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.14193/5.17478. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.14485/5.17647. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 5.14110/5.17808. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.14101/5.16988. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.14944/5.18291. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.14781/5.17078. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.14342/5.16222. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.14438/5.16479. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.14047/5.16101. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.14129/5.17162. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.13852/5.17563. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.14086/5.17059. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.14020/5.18152. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 5.14060/5.17777. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.14058/5.17485. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.13247/5.19111. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.14516/5.16270. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.14362/5.16516. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.14116/5.17468. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.14059/5.16557. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.13133/5.18249. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.13940/5.17129. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.13615/5.18667. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.13677/5.20696. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 5.13648/5.17609. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.13952/5.18252. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.13508/5.17822. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.13990/5.18321. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.13336/5.18169. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 5.13206/5.18583. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.13079/5.18331. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.13127/5.16913. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.13630/5.18617. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.13826/5.18664. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.13967/5.17655. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.13695/5.16236. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.13280/5.18366. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.13218/5.18596. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.13612/5.17868. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.13417/5.17532. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.13616/5.17682. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.13804/5.17733. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.12882/5.19579. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.13132/5.19673. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.12646/5.20172. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.12723/5.19874. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.13193/5.17844. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.13207/5.17102. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.12941/5.19576. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.12800/5.18376. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.12529/5.20299. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.13101/5.18832. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 5.12766/5.20281. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.12946/5.20280. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.12716/5.20083. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.12521/5.21274. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.12825/5.22240. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.12646/5.19365. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.12107/5.22910. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 5.12307/5.20976. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.12520/5.21729. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.12435/5.22636. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.12825/5.21812. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.12557/5.20575. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.12483/5.25043. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 5.12565/5.21248. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.12301/5.20962. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.12479/5.21266. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.12597/5.22906. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 5.12059/5.22015. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.12112/5.22478. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.11942/5.22038. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.11974/5.23217. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.12088/5.21832. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 5.12488/5.22366. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.11881/5.20175. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 5.11435/5.24922. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.12051/5.23379. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.11754/5.24232. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.11683/5.21436. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.02757337634153148\n",
      "Epoch 0, Loss(train/val) 4.90099/4.85780. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.88653/4.85591. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.87126/4.85791. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.86189/4.86324. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.85468/4.86704. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.85494/4.87016. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.85455/4.87509. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.85940/4.87280. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.85667/4.86762. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.85263/4.87110. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.85116/4.87367. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.85182/4.87213. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85336/4.87373. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.85068/4.87592. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.84739/4.87794. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.84735/4.88294. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.84991/4.88007. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.84879/4.87496. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.85038/4.88560. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.84817/4.88338. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.84847/4.88372. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.84667/4.88548. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.84537/4.88836. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.84632/4.88672. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.84484/4.88818. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.84218/4.89455. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.84250/4.89494. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.84074/4.89649. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.84391/4.89246. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84203/4.89375. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.84086/4.90411. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.83824/4.88996. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84004/4.90020. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.83953/4.90116. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.84327/4.89580. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.83793/4.90342. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.83752/4.90568. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.83802/4.89941. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.83627/4.92179. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.84378/4.88691. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84158/4.91422. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.83965/4.89782. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.83749/4.91035. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.83634/4.91165. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.83688/4.91435. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.83590/4.90681. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.83399/4.92390. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.83315/4.90215. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.83527/4.91442. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.83503/4.90799. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.83240/4.91488. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.83342/4.91475. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.83264/4.91078. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.83457/4.91334. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.83509/4.93040. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.83261/4.90915. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.83818/4.92473. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.84142/4.88440. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.83633/4.90325. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83672/4.90787. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.83433/4.91448. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.82615/4.91982. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.83759/4.90165. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.83212/4.93151. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.83307/4.94252. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.82797/4.93072. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.82982/4.93131. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.83116/4.92863. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.82873/4.93248. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83138/4.92875. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.82606/4.93383. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.83151/4.91644. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83162/4.90799. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.82922/4.91583. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.82983/4.92625. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.82730/4.93465. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.83675/4.91412. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.82584/4.94177. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.82862/4.93515. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.82956/4.94765. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.82682/4.92906. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.82261/4.94549. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.82952/4.93723. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.82670/4.94446. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.82297/4.95092. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.82083/4.94732. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 4.82417/4.93218. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.82757/4.93179. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.82760/4.95758. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.82140/4.96389. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.83595/4.90203. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.83278/4.91376. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83174/4.91646. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.82806/4.94054. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.82566/4.94688. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.82275/4.93057. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.82488/4.95417. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.82433/4.94316. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.82201/4.95305. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.81817/4.95560. Took 0.08 sec\n",
      "ACC: 0.578125, MCC: 0.1563263498701806\n",
      "Epoch 0, Loss(train/val) 4.86352/4.81995. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.79815/4.81238. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 4.79773/4.80867. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79913/4.80487. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79549/4.80331. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79867/4.80717. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.79364/4.82789. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.79284/4.83445. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.79391/4.84192. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79260/4.85168. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79541/4.84192. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.79577/4.84055. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.79341/4.83115. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.79175/4.82790. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.79126/4.83274. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.79070/4.83160. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.79121/4.83202. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.79013/4.82890. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.79051/4.81241. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.79168/4.82825. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.79122/4.83847. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78944/4.83170. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.78735/4.83387. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.79003/4.83829. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.79011/4.84012. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.79075/4.83449. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.78758/4.82506. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.78749/4.82916. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.78873/4.83698. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.78826/4.84059. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.78804/4.83944. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78628/4.84775. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.78382/4.85782. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78315/4.85066. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78432/4.87178. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.78695/4.83876. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.78080/4.83689. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.77938/4.85944. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.78150/4.86937. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.78425/4.85710. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.78295/4.84937. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.77670/4.85230. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.78008/4.85864. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.77921/4.85181. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.77902/4.84320. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.78213/4.84558. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77822/4.86247. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77867/4.85906. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77679/4.86701. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.76941/4.87971. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77646/4.86581. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77641/4.87194. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.77327/4.87645. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.77237/4.86632. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.76910/4.88175. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.77221/4.88132. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.77321/4.86647. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.77222/4.88267. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.77395/4.86380. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.77089/4.86588. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.77151/4.87076. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.77256/4.86406. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.77224/4.85762. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.77176/4.86668. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.77654/4.87515. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.77147/4.87620. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.77146/4.86853. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.77372/4.86730. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.77295/4.87686. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76995/4.88221. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.76889/4.86341. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.76940/4.86456. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76436/4.88013. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.77386/4.86669. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.76991/4.85594. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.76706/4.87020. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.76854/4.88191. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.76895/4.88233. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.76830/4.85713. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.76307/4.88504. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.76335/4.88711. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.76383/4.90485. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.76638/4.88688. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.76587/4.87090. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.76642/4.87653. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.77407/4.86799. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.77094/4.85716. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.76875/4.88154. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.77395/4.85928. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.76251/4.86900. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.76867/4.85773. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.76488/4.85549. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.75885/4.90418. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.76191/4.88248. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.76350/4.87599. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.76383/4.87443. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.76234/4.86692. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.76378/4.89850. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.76308/4.89050. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.76123/4.89451. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.0778902520624173\n",
      "Epoch 0, Loss(train/val) 4.95106/4.90655. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.92434/4.90720. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.92165/4.90741. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.92325/4.90088. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.91206/4.90397. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.90931/4.90193. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.91245/4.90199. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.91066/4.90323. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.91023/4.90349. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.91181/4.90295. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 4.90849/4.90218. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.91136/4.90186. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.90942/4.90349. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.90918/4.90258. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.90450/4.90141. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.90700/4.90458. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.90750/4.89834. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.90668/4.90608. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.90466/4.90583. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.90224/4.91959. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.90474/4.91035. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.90732/4.90990. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.90122/4.90634. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.89628/4.90402. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.90449/4.90619. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.90074/4.90943. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.90111/4.90500. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.90070/4.90601. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.89756/4.90976. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.89978/4.90277. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.89939/4.90371. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.89472/4.90719. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.89311/4.90516. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.89640/4.90323. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.89669/4.90224. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.89825/4.91530. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.89347/4.90878. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.89578/4.90627. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.89240/4.90662. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.89679/4.91477. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.89203/4.91901. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.89318/4.89900. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.89504/4.89868. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.89241/4.89170. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.89065/4.90331. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.89172/4.89677. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.89164/4.90593. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.89391/4.91692. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.89335/4.90458. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.89280/4.88976. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.89159/4.89283. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.88787/4.90234. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.89381/4.89745. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.88538/4.89345. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.89108/4.89902. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.88818/4.90703. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.88859/4.89849. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.88511/4.90539. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.88769/4.91152. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.88694/4.89969. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.88372/4.90583. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89057/4.89965. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.89286/4.89713. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.88701/4.89855. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.88410/4.90569. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.88075/4.89328. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.88076/4.90927. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.88638/4.90286. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.89076/4.91101. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.88618/4.90267. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.88372/4.90447. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.88044/4.90399. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88056/4.90145. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.88714/4.89625. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.87639/4.90661. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.88267/4.90298. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.87818/4.90828. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.88180/4.92684. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.88092/4.92222. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88267/4.91276. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.88176/4.90241. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.87897/4.90327. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.87675/4.92369. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.88374/4.90165. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.87909/4.90677. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.87938/4.89307. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.88128/4.90184. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.87353/4.90687. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.87792/4.90519. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.87713/4.92464. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.87516/4.92671. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.88478/4.91016. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.87475/4.92071. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.87345/4.92583. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.87617/4.91600. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.87798/4.92202. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.87679/4.91137. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.87486/4.89840. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.88014/4.90638. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.87306/4.90014. Took 0.08 sec\n",
      "ACC: 0.59375, MCC: 0.2164437316097468\n",
      "Epoch 0, Loss(train/val) 5.13972/5.05728. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.05506/5.01522. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 5.03486/5.01809. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.02971/5.01446. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.03095/5.00959. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.03313/5.00936. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.03366/5.01836. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.03330/5.01712. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.02961/5.01274. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 5.03052/5.00708. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 5.02727/5.01157. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.02652/5.01419. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 5.02383/5.00990. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.03949/5.00908. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.03321/5.01239. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.03276/5.01108. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.03030/5.01109. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.03160/5.00980. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 5.02740/5.00666. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.02823/5.00731. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.02637/5.00682. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.02544/5.00585. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.02552/5.00605. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.02794/5.00286. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.02672/5.01261. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.02814/5.00579. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.02476/5.00975. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.02298/5.01249. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.02517/5.01273. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.02568/5.01613. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.01979/5.01454. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.02383/5.02453. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 5.02320/5.01918. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 5.02223/5.01783. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.02170/5.01640. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.02020/5.01674. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.01744/5.01813. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.01911/5.02523. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.01512/5.02534. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.02122/5.02868. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.01732/5.02884. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.01715/5.02973. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.01450/5.03999. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 5.01732/5.03161. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.01343/5.03327. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.01607/5.03301. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.00980/5.04614. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.00808/5.04536. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.02838/5.03676. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.02854/5.02349. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.02595/5.02804. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.01885/5.03315. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.01989/5.02368. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.01679/5.03572. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 5.01864/5.02105. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.01737/5.02427. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.01517/5.03396. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.02120/5.02171. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.01384/5.02030. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.01537/5.02148. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 5.01035/5.03533. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.01445/5.03049. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 5.00626/5.03301. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.01465/5.02099. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.02601/5.00484. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.02658/5.02024. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.01960/5.02853. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.01980/5.01719. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.02096/5.01788. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.01466/5.01184. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.01220/5.00788. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 5.02293/4.99403. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.01926/4.99566. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.01672/4.99989. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.01117/5.00948. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 5.01263/5.01401. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 5.01344/5.01219. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.00956/5.01177. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.01293/5.00500. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.00709/5.00843. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.00813/5.01126. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.00834/5.01004. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.00632/5.01027. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.00934/5.01090. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.01270/5.01494. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.00882/5.01345. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 5.00644/5.01506. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.00635/5.01308. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.00788/5.01580. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.00734/5.02277. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.00978/5.01238. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.00669/5.01285. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.00539/5.02583. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.00621/5.02780. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.00759/5.02774. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 5.00617/5.01949. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.00249/5.01958. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.00310/5.02707. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.01052/5.03424. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.00760/5.02027. Took 0.08 sec\n",
      "ACC: 0.578125, MCC: 0.191246010612082\n",
      "Epoch 0, Loss(train/val) 4.75392/4.71416. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.71531/4.73086. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.71266/4.73554. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.70505/4.73513. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.70399/4.74420. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.70606/4.74630. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.70336/4.74371. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.70197/4.74250. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.70459/4.73657. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.70265/4.72516. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.70281/4.71823. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.69999/4.71644. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.69898/4.71907. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.69745/4.71720. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.69941/4.71906. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.69913/4.71469. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.69736/4.71319. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.69715/4.71145. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.69589/4.71626. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.69502/4.71419. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.69322/4.72076. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.69562/4.72226. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.69687/4.72270. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.69603/4.71934. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.69348/4.72193. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.69449/4.72110. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.69218/4.72148. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.69276/4.72880. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.69396/4.72412. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.68896/4.72359. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.69047/4.72392. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.68937/4.71977. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.69165/4.71580. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.69062/4.71810. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.68751/4.72493. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.69028/4.72286. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.68903/4.73218. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.69015/4.72394. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.68607/4.72814. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.69030/4.71020. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.68602/4.72458. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.68908/4.72367. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.68838/4.72805. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.68931/4.72838. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.68535/4.73005. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.69028/4.72414. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.68643/4.72752. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.68419/4.73920. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.68227/4.72929. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.68592/4.73947. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.68800/4.72032. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.68575/4.72761. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.68311/4.72933. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.67913/4.73430. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.68465/4.72787. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.68276/4.72643. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.68413/4.72428. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.68321/4.71777. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.67777/4.72623. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.68160/4.73660. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.67866/4.73571. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.68817/4.71340. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.67979/4.71731. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.68172/4.73364. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.68259/4.72509. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.68136/4.72367. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.67965/4.72458. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.69173/4.72026. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.68925/4.73518. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.68893/4.72006. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.68903/4.70772. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.69428/4.69950. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.68826/4.70624. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.68381/4.70929. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.68296/4.70939. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.68221/4.71855. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.68470/4.72339. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.68876/4.72728. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.68852/4.72487. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.68857/4.71977. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.67795/4.71907. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.68306/4.71600. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.67885/4.71551. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.68468/4.71019. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.67771/4.71579. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.67942/4.71327. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.67796/4.71520. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.67517/4.71270. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.68149/4.71121. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.67123/4.72904. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.68438/4.71330. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.68331/4.72443. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.68194/4.73436. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.68269/4.71878. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.68177/4.72453. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.68010/4.72297. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.68081/4.72396. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.67363/4.70771. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.67874/4.71094. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.67625/4.71066. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.10755941926431617\n",
      "Epoch 0, Loss(train/val) 4.84815/4.76851. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.78586/4.75786. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.78089/4.75413. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.78047/4.75305. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.77904/4.75429. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.78216/4.75636. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.77973/4.75850. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.77886/4.75770. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.77851/4.75995. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.77829/4.76004. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.77560/4.75792. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.77482/4.75887. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.77610/4.75930. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.77608/4.75764. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.77364/4.75919. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.77509/4.75899. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.77429/4.75723. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.77365/4.75660. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.77349/4.75726. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 4.77105/4.75726. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.77453/4.75744. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.77215/4.75678. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.77127/4.75883. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.77017/4.76092. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.77069/4.75973. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.77119/4.76268. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.77195/4.80234. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.77762/4.76574. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77526/4.76059. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.77518/4.75789. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.77555/4.75713. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.77447/4.75773. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.77468/4.76058. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.77136/4.76136. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.77791/4.76923. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.77432/4.76500. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.77465/4.76116. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.77366/4.75710. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.77046/4.76317. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.77134/4.75975. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.76866/4.76772. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.76996/4.76852. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.76696/4.76875. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.76413/4.76762. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.76518/4.77222. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.76950/4.76779. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77352/4.75717. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.76752/4.75752. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.76976/4.76493. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.76690/4.76539. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.76341/4.77078. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.76721/4.76829. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.76565/4.76890. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.76721/4.76953. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.76245/4.75951. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.77140/4.76217. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.77156/4.75340. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.77165/4.74908. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76686/4.75508. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.76861/4.75770. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.77146/4.76110. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.76645/4.75818. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76588/4.75938. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.77033/4.75369. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.77378/4.74903. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.76984/4.74992. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.76756/4.74797. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.76069/4.74568. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.76388/4.75995. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76711/4.75645. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.76445/4.75633. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.76629/4.76034. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76643/4.74495. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.76493/4.74963. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.76427/4.74797. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.75699/4.74439. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.75681/4.75082. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.76210/4.77575. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.76858/4.76042. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.76341/4.75166. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.76440/4.74441. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.76123/4.76689. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.75920/4.77245. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76295/4.76312. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75930/4.76583. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.76082/4.75779. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.76066/4.76040. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.76120/4.75817. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.75826/4.74932. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.76312/4.76486. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.75525/4.75887. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.75617/4.76360. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.75767/4.76724. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.75412/4.76341. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.75079/4.76460. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.75987/4.75672. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.75308/4.76463. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.75284/4.76166. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.75501/4.77238. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.75687/4.76985. Took 0.09 sec\n",
      "ACC: 0.625, MCC: 0.2482845429325592\n",
      "Epoch 0, Loss(train/val) 4.94989/4.88505. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.89212/4.88176. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.88686/4.87856. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.88110/4.88074. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.88341/4.88275. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.88125/4.88699. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.87957/4.88874. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.87920/4.88691. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.87632/4.89563. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 4.87697/4.89515. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 4.87738/4.89313. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.87304/4.89926. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.87269/4.89836. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.87317/4.90122. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.87155/4.89735. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.87347/4.89459. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.87109/4.89827. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.87034/4.90495. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.86829/4.90388. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.86856/4.90714. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.86922/4.91057. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.86974/4.90571. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.87137/4.91525. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.86842/4.91352. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.86617/4.92128. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.86702/4.91458. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.86828/4.92154. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.86605/4.91506. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.86238/4.92297. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.86967/4.91155. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.86274/4.92378. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.86061/4.90438. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.86953/4.92395. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.86181/4.94974. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.86447/4.92970. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.86545/4.91679. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.85837/4.92662. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.86651/4.91192. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86060/4.92940. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.86437/4.91390. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.86079/4.93857. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.86412/4.94118. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.85652/4.92614. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.85833/4.92640. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.86200/4.90224. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.86263/4.91195. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.86269/4.91711. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.86363/4.92937. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.86241/4.92127. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.86080/4.92578. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.85967/4.91727. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.85714/4.93157. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.85366/4.92981. Took 0.12 sec\n",
      "Epoch 53, Loss(train/val) 4.85753/4.94820. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.86498/4.89500. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.86607/4.92523. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.86330/4.92018. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.85651/4.92333. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.85833/4.92685. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.85752/4.91075. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.85661/4.93823. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.85775/4.91528. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.85856/4.93655. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.86327/4.90044. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.86044/4.93776. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.85744/4.92548. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.85421/4.91251. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.85267/4.92692. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.85520/4.94695. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.85481/4.91782. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.85596/4.91656. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.85336/4.92608. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.85228/4.91256. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.85218/4.93156. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.85414/4.92396. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.85646/4.92691. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.85531/4.94333. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.85175/4.93257. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.84859/4.94016. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.85138/4.90419. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.85440/4.93026. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.85388/4.90840. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.85201/4.95798. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.85527/4.91454. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.85207/4.93011. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.85529/4.92744. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.85026/4.94294. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.85409/4.92476. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.85022/4.93483. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.85038/4.91082. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.85129/4.92357. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.85089/4.92896. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.85471/4.92122. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.84904/4.92410. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.84471/4.93234. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 4.84075/4.94172. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.85069/4.92742. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.84938/4.92973. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.85076/4.92078. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.84467/4.93856. Took 0.08 sec\n",
      "ACC: 0.6875, MCC: 0.37389885714349824\n",
      "Epoch 0, Loss(train/val) 4.76666/4.72915. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.73662/4.75391. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.73345/4.75451. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.73410/4.74767. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.73685/4.73442. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.73901/4.73892. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.73473/4.74909. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.73129/4.75516. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.72303/4.75715. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.72433/4.75930. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.72336/4.76135. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.72316/4.75745. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.72226/4.76176. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.72339/4.75730. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.71789/4.77002. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.71982/4.76073. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.71944/4.77106. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.71830/4.77061. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.72115/4.75646. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.71780/4.77294. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.71784/4.75968. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.71858/4.76954. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.71890/4.75947. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.71717/4.76245. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.71436/4.77255. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.71757/4.76282. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.72109/4.76226. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.71629/4.75445. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.71336/4.77003. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.71352/4.75594. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.71140/4.77727. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.71575/4.76465. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.71329/4.76063. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.71406/4.76229. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.71475/4.76057. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.71131/4.76450. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.71029/4.77825. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.71227/4.77176. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.70876/4.76492. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.71260/4.75944. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.70925/4.77233. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.70991/4.76983. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.71202/4.76213. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.71046/4.75772. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.70993/4.78195. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.71000/4.75765. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.70852/4.78441. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.70699/4.75544. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.70550/4.76957. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.70843/4.78157. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.71122/4.75592. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.70572/4.78707. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.70265/4.77363. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.71068/4.76728. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.70657/4.77132. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.70541/4.78622. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.70273/4.77128. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.70578/4.78437. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.70581/4.76033. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.69888/4.78992. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.70334/4.76119. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.70092/4.79122. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.70532/4.80295. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.70266/4.78775. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.70194/4.78991. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.70551/4.80100. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.70584/4.77257. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.69841/4.78033. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.69705/4.77792. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.69836/4.79811. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.70271/4.80230. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.69714/4.78867. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.69759/4.79627. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.69741/4.79174. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.69102/4.83542. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.69870/4.82168. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.70527/4.78117. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.69464/4.81797. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.69782/4.83482. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.69907/4.81396. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.69781/4.79353. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.69089/4.81315. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.69418/4.80425. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.69606/4.84257. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.69855/4.77595. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.69464/4.79868. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.69970/4.79710. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.68660/4.83687. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.69861/4.80461. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.69146/4.84253. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.69287/4.79576. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.69439/4.80260. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.69088/4.81593. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.70178/4.78627. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.70063/4.79170. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.69554/4.80908. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.69087/4.80474. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.68996/4.78076. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.68966/4.84397. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.68732/4.80304. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.06362847629757777\n",
      "Epoch 0, Loss(train/val) 4.93219/4.87071. Took 0.19 sec\n",
      "Epoch 1, Loss(train/val) 4.86496/4.86217. Took 0.17 sec\n",
      "Epoch 2, Loss(train/val) 4.86495/4.86529. Took 0.14 sec\n",
      "Epoch 3, Loss(train/val) 4.86542/4.86054. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.86315/4.86371. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.86100/4.86135. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.86067/4.86377. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.85789/4.86351. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.86335/4.86216. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.86041/4.86362. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.85969/4.85896. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.86048/4.86105. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85747/4.86183. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.85584/4.86188. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.85834/4.86216. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.85112/4.86598. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.85117/4.86937. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.85225/4.87205. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.85634/4.87597. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85383/4.88084. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.85392/4.88296. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.84983/4.88938. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.85130/4.89512. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.85084/4.88271. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.85017/4.87292. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.85064/4.87996. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.85083/4.87976. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.84884/4.88767. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.84893/4.88762. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.85308/4.88767. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.84932/4.88930. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.84584/4.90112. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84978/4.88731. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.85065/4.88514. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.84627/4.89626. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84948/4.88673. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.85002/4.88628. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84580/4.89170. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84653/4.89605. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84593/4.89844. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.84451/4.88871. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.84667/4.90441. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84961/4.89893. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.84592/4.89423. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.85153/4.88486. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.84609/4.89517. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.84456/4.89987. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.84452/4.89585. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84791/4.89826. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.84678/4.90392. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.84703/4.88859. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.84308/4.89564. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.84504/4.91495. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.84774/4.88361. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.84437/4.89274. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.84549/4.89997. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.84145/4.89584. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.84454/4.88570. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.84532/4.89545. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.84427/4.87754. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.84719/4.87747. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.84385/4.88578. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.84225/4.89230. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.84296/4.89083. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.84468/4.88537. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.84145/4.89402. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.84312/4.89515. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.84306/4.88706. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.83925/4.89442. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.84226/4.88851. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.83894/4.89738. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.84054/4.89105. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83894/4.89035. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.84132/4.89592. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.84147/4.89525. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83733/4.89779. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.84402/4.89605. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.83915/4.89236. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.83932/4.90427. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.83804/4.90187. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83906/4.89213. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.83567/4.89404. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.83806/4.89848. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.83784/4.89636. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.84360/4.90410. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83883/4.89617. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83662/4.89699. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.84247/4.89014. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83808/4.90403. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.84166/4.88567. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.83753/4.89461. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.83703/4.89204. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83379/4.89991. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.84521/4.90369. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.83339/4.88922. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.83531/4.90764. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.84144/4.89859. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.83169/4.90584. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83898/4.90003. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.82943/4.89637. Took 0.08 sec\n",
      "ACC: 0.390625, MCC: -0.19770766067180878\n",
      "Epoch 0, Loss(train/val) 4.71899/4.64330. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.64265/4.63632. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.64024/4.63517. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.63842/4.63340. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.63594/4.63632. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.63614/4.63723. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.63390/4.63728. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.63869/4.63552. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.63644/4.63762. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.63765/4.63596. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.63585/4.63727. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.63442/4.64035. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.63584/4.64098. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.63710/4.64024. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.63514/4.63979. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.63442/4.64292. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.63571/4.64395. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.63466/4.64587. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.63289/4.64892. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.63145/4.64873. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.63211/4.65624. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.63127/4.65021. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.63063/4.64821. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.62909/4.65502. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.63031/4.66092. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.63198/4.65409. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.63134/4.65431. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.62970/4.66052. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.62841/4.66056. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.63399/4.64410. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.63030/4.65384. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.62686/4.66082. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.63002/4.65515. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.62909/4.65685. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.62958/4.65186. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.62752/4.64859. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.62811/4.65873. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.62813/4.65887. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.62818/4.65486. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.62609/4.67470. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.62443/4.66621. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.63018/4.65765. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.62281/4.67129. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.62154/4.66796. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.62644/4.65204. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.62390/4.66574. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.62334/4.66728. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.61994/4.68566. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.62803/4.65172. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.62013/4.68063. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.62843/4.65889. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.62581/4.66630. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.62277/4.67236. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.62384/4.66058. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.61842/4.68939. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.62330/4.65925. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.62475/4.66391. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.62035/4.66767. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.62237/4.66502. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.62382/4.65382. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.61871/4.67760. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.61152/4.70420. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.62037/4.65672. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.61790/4.66496. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.61839/4.67541. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.61762/4.67066. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.61065/4.64971. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.62119/4.68480. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.61506/4.69916. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.61956/4.67518. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.61938/4.66652. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.60895/4.69227. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.60830/4.67587. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.62036/4.66248. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.61986/4.66900. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.61574/4.68240. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.61050/4.66634. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.61376/4.68181. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.60884/4.69713. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.60960/4.68557. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.60753/4.68776. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.60585/4.68416. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.61310/4.67170. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.62056/4.64539. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.61057/4.68638. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.61281/4.66528. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.60920/4.69880. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.60591/4.68479. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.60379/4.70949. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.61159/4.67859. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.60554/4.67046. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.60787/4.69549. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.60767/4.68289. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.60293/4.68871. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.60976/4.67094. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.60868/4.70478. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.61505/4.70495. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.62512/4.66467. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.62050/4.66867. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.61936/4.67579. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.13948480638371882\n",
      "Epoch 0, Loss(train/val) 4.91499/4.88442. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.87726/4.86859. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.87710/4.87047. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.87586/4.88573. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.87633/4.89672. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.87397/4.88987. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.87168/4.87679. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.86925/4.87911. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.86781/4.88142. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.86808/4.88231. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.86570/4.88589. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.86565/4.88906. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.86322/4.89974. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.86594/4.89747. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.86804/4.88853. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.86324/4.88679. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.86191/4.89311. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.86034/4.90153. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.86237/4.90090. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.86039/4.90815. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.85940/4.90665. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.85593/4.92473. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.85678/4.91496. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.85620/4.92152. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.85436/4.92828. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.85889/4.90888. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.85140/4.94042. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.85256/4.91710. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.85303/4.92589. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.85012/4.92157. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.85079/4.92373. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.84796/4.93516. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84729/4.93745. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.85493/4.91280. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.85438/4.92016. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.85004/4.91977. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84990/4.93179. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84993/4.91745. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.85060/4.93456. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84885/4.93395. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84816/4.93253. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.84686/4.92787. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84510/4.94848. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.84788/4.93684. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.84444/4.94406. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.84301/4.95161. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.84185/4.94920. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.84293/4.94794. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84216/4.96353. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.84138/4.96003. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.84262/4.95643. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.84068/4.96773. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.84248/4.96869. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.83826/4.97616. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.84539/4.94360. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.84512/4.94001. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.83995/4.96477. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.83506/4.97518. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.84550/4.94860. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83852/4.97223. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.83864/4.96316. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83559/4.99438. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.84144/4.96392. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.83977/4.97144. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.83684/4.99696. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.83415/4.99163. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.83755/4.98889. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.83837/4.96714. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.83515/4.98526. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83794/4.96242. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.83745/4.96916. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.83716/4.96731. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83562/4.96223. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.84809/4.90960. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.84703/4.93989. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.85023/4.95265. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.84697/4.96713. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.84293/4.97681. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.84945/4.95757. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.84094/4.97035. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.84069/5.02231. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.84533/4.96935. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.84338/4.96818. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.84032/4.98035. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.84390/4.98082. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83764/4.98689. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.84184/4.97562. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.83828/4.99570. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.84028/4.97673. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.83881/4.98983. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.83789/5.00289. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.84132/4.98281. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83511/5.01100. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83820/4.99925. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.83853/5.01558. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.83316/5.05262. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.84108/5.01820. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.83815/5.00392. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83413/5.02435. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.84181/4.98945. Took 0.09 sec\n",
      "ACC: 0.546875, MCC: 0.09379580992210836\n",
      "Epoch 0, Loss(train/val) 4.77738/4.73407. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.73827/4.73940. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.73791/4.73593. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.73727/4.73459. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.73439/4.73610. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.73631/4.73434. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.73737/4.73860. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.73420/4.73697. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.73412/4.73351. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.72947/4.73966. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.72620/4.73832. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.73037/4.74623. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.73071/4.74689. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.73074/4.74719. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.72671/4.75519. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.72653/4.75731. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.72964/4.75350. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.72702/4.76324. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.72445/4.76184. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.72880/4.77669. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.72870/4.76902. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.72748/4.78035. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.72598/4.76598. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.72946/4.76251. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.72419/4.76066. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.72651/4.77042. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.72541/4.76115. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.72491/4.76313. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.72220/4.76732. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.72634/4.76681. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.72127/4.76316. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.72319/4.76224. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.72432/4.77689. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.72142/4.77160. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.72248/4.76448. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.72310/4.77669. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.72147/4.76483. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.72030/4.76720. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.71917/4.77610. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.71838/4.76913. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.71754/4.77551. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.71846/4.77978. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.71879/4.78296. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.71866/4.77819. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.71873/4.77302. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.71811/4.78218. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.71797/4.77350. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.71862/4.77373. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.71982/4.78357. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.71560/4.78914. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.72181/4.77733. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.71503/4.78560. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.71629/4.79061. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.72082/4.77048. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.71637/4.78072. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.71511/4.78132. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.71048/4.78805. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.71231/4.77883. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.71276/4.77720. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.71037/4.79089. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.71161/4.78947. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.71922/4.78171. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.71735/4.78026. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.71213/4.79422. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.70972/4.79680. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.70725/4.79455. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.71115/4.78603. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.71547/4.79627. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.71288/4.78812. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.70972/4.78383. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.71309/4.79421. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.71713/4.80406. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.71059/4.79920. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.70757/4.78705. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.70853/4.80408. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.71142/4.80236. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.70756/4.79855. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.71027/4.78816. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.70542/4.81315. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.70901/4.78529. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.71190/4.77889. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.70758/4.78456. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.70292/4.80438. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.70896/4.77608. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.71264/4.79077. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.70787/4.78559. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.70767/4.81866. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.70436/4.79696. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.70712/4.78907. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.70505/4.83001. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.70163/4.83151. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.70703/4.80274. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.70552/4.80876. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.70699/4.82387. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.70673/4.80345. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.70914/4.83436. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.70928/4.77951. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.70058/4.83447. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.71154/4.79037. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.70591/4.78794. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.14060420405767388\n",
      "Epoch 0, Loss(train/val) 4.93602/4.89882. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.89791/4.90192. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.89879/4.89995. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.89498/4.90031. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.89383/4.90024. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.89792/4.89878. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.89480/4.89525. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.89233/4.89542. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.89529/4.89328. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.89292/4.89417. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.88750/4.89733. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.89161/4.89627. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.89326/4.89529. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.88937/4.89780. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.88857/4.90190. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.88919/4.90541. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.88986/4.90902. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.88746/4.90565. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.88681/4.91898. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.88407/4.91379. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.88434/4.92098. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.88381/4.90861. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.87931/4.90940. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.89236/4.89436. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.89102/4.90321. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.88601/4.90186. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.88130/4.91617. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.88816/4.89679. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.88504/4.90128. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.88080/4.90814. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.88009/4.90247. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.87901/4.92075. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.88069/4.90292. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.87599/4.91746. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.87941/4.89616. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.87790/4.91066. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.88049/4.89266. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.87936/4.91131. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.87507/4.90366. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.87736/4.90036. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.87892/4.89944. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.87342/4.90587. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.87696/4.90141. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.87167/4.91181. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.87666/4.88831. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.88410/4.90659. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.88241/4.90692. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.88243/4.89675. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.88269/4.90093. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.88042/4.91605. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.88439/4.89537. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.87608/4.91491. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.88234/4.90076. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.87596/4.90126. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.87792/4.91118. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.88332/4.88198. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.88183/4.90429. Took 0.07 sec\n",
      "Epoch 57, Loss(train/val) 4.88199/4.89347. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.87889/4.89807. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.87700/4.90831. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.87770/4.90700. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.88223/4.89006. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.87787/4.90108. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.87713/4.90758. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.87497/4.90751. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.88014/4.89542. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.88036/4.88931. Took 0.07 sec\n",
      "Epoch 67, Loss(train/val) 4.87631/4.90446. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.87685/4.90106. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.87801/4.90085. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.87342/4.89957. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.87611/4.89330. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.87263/4.91260. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88023/4.89722. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.87209/4.90315. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.87429/4.90947. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.87391/4.89894. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.87676/4.89545. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.88217/4.91058. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.87544/4.88769. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.87263/4.92300. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.87464/4.91193. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.87572/4.89893. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.87122/4.91487. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.87590/4.91968. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.87267/4.90146. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.87093/4.91997. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.87070/4.91427. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.87222/4.91591. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.86802/4.93296. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.86827/4.91923. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.87214/4.91097. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.87125/4.92773. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.87287/4.89425. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.86780/4.90459. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.87260/4.90551. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.86828/4.92816. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.87164/4.91772. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.87054/4.92294. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.86628/4.92502. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.007889684472185849\n",
      "Epoch 0, Loss(train/val) 4.93317/4.93114. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.88617/4.91402. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.87537/4.89501. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.87131/4.87837. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.86833/4.87325. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.86503/4.88117. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.86755/4.88598. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.86797/4.88222. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.86579/4.87342. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.86400/4.87062. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.86490/4.87402. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.86244/4.87150. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.86540/4.86602. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.86219/4.86697. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.86375/4.87057. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.86176/4.86934. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.86309/4.86339. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.86333/4.86151. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.86086/4.86320. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.86085/4.86752. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.85831/4.86148. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.86056/4.85530. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.86389/4.85662. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.85902/4.85920. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.86122/4.86018. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.85858/4.85645. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.85830/4.86007. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.85388/4.86196. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.85953/4.86250. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.85570/4.85273. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.85380/4.85624. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.85240/4.86318. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.85315/4.86744. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.85446/4.87336. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.85268/4.85966. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.85490/4.85930. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84899/4.85659. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.85081/4.86539. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.85019/4.86098. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84899/4.86041. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84912/4.86380. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.84668/4.86174. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84797/4.86315. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.84635/4.86717. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.84928/4.86612. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.84894/4.86013. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.84967/4.85834. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.84509/4.86726. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84698/4.86293. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.84821/4.86019. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.84097/4.86220. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.84321/4.87648. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.84512/4.86792. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.84466/4.85961. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.84607/4.86263. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.84682/4.85363. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.84294/4.85585. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.84192/4.86634. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.84473/4.85416. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.84894/4.85616. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.84107/4.85758. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.84598/4.86168. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.84061/4.86600. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.84107/4.85636. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.84024/4.85856. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.84033/4.86075. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.83333/4.84929. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.84613/4.86297. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.84580/4.86659. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.84388/4.86431. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.83702/4.85744. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.84121/4.86916. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83783/4.86262. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.84405/4.86411. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83609/4.88959. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.84206/4.86119. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.83823/4.86015. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.84086/4.87284. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.85451/4.85825. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.84625/4.85552. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.84765/4.85702. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.84821/4.85360. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.84492/4.85614. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.85008/4.85895. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.84925/4.84934. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.84491/4.83809. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.84196/4.84783. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.84639/4.85701. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.84484/4.85203. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.84201/4.85330. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.84095/4.85447. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.84082/4.86692. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.84964/4.86069. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.84367/4.85158. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.83992/4.85728. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.84545/4.85357. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.84272/4.85219. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.84277/4.85553. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.84393/4.86273. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.83686/4.85542. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.03779786830012622\n",
      "Epoch 0, Loss(train/val) 4.95608/4.93577. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.94549/4.92905. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.93252/4.92877. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.93176/4.92954. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.93776/4.92866. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.93467/4.92727. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.93746/4.93082. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.94020/4.94107. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.93249/4.93579. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.92942/4.93156. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.93028/4.92975. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.93065/4.93064. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.92934/4.92885. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.92815/4.92489. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.92478/4.92448. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.92477/4.92265. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.92860/4.92359. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.92790/4.91989. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.92723/4.92207. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.92451/4.91544. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.92492/4.91758. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.92197/4.91853. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.92517/4.91942. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.91950/4.91529. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.91875/4.91543. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.91740/4.91271. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.91995/4.90890. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.92466/4.92507. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.91853/4.91536. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.91682/4.91597. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.91232/4.91609. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.91783/4.92011. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.91535/4.91432. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.91800/4.92905. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.91430/4.92277. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.91277/4.91302. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.91159/4.91455. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.91116/4.91958. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.91291/4.91987. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.91096/4.92462. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.91047/4.91825. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.90768/4.92564. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.90871/4.93115. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.91123/4.91553. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.91161/4.92701. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.90583/4.91513. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.91139/4.91964. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.90626/4.92478. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.90502/4.91843. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.90641/4.92562. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.90839/4.92577. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.90391/4.93095. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.91041/4.90761. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.90955/4.90837. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.90984/4.91237. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.90299/4.90660. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.91130/4.91391. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.90727/4.92073. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.90815/4.92383. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.90311/4.92120. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.90362/4.92685. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.90652/4.92596. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.90361/4.92591. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.90249/4.92169. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.89894/4.92427. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.90641/4.91553. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.91654/4.93047. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.90823/4.93474. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.90982/4.93087. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.90939/4.94664. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.90831/4.93617. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.90061/4.91982. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.90696/4.93837. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.91161/4.91458. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.91679/4.93376. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.90703/4.94730. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.90535/4.93113. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.90747/4.92987. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.91208/4.92808. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.90873/4.92903. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.90575/4.92664. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.91036/4.93572. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.90394/4.92565. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.89794/4.94134. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.90439/4.93171. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.90293/4.92204. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.90426/4.92936. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.90446/4.92671. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.89415/4.94038. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.90199/4.92372. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.90338/4.92342. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.90025/4.92601. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.89828/4.91826. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.89558/4.93134. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.90090/4.92105. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.90124/4.92889. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.90200/4.92364. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.89991/4.92971. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.90836/4.92790. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.90339/4.93705. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.08222643447147887\n",
      "Epoch 0, Loss(train/val) 5.02253/4.93410. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.96562/4.99107. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.95066/4.96407. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.93708/4.94169. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.93327/4.95291. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.93742/4.96247. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.94010/4.95908. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.93352/4.95406. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.93499/4.95472. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.93421/4.94990. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.93038/4.94961. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.93138/4.94648. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.92898/4.94958. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.92794/4.94208. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.92120/4.94491. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.92174/4.94804. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.92171/4.93912. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.92000/4.94334. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.91773/4.93826. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.91867/4.94521. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.91957/4.94165. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.91547/4.93544. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.91938/4.93448. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.91800/4.93937. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.91251/4.93942. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.91187/4.94718. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.91437/4.95243. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.91836/4.95527. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.91274/4.94689. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.90771/4.94980. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.91143/4.95080. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.91565/4.94782. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.90848/4.95478. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.91132/4.95860. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.90815/4.95522. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.91211/4.95461. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.90688/4.96423. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.91077/4.95480. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.90671/4.94109. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.91345/4.95004. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.90905/4.95280. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.90579/4.96605. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.90057/4.95897. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.90425/4.96317. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.90485/4.96671. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.90801/4.95317. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.90412/4.98167. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.90123/4.96894. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.90688/4.97468. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.90645/4.95957. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.90433/4.96449. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.89999/5.00160. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.90182/4.96492. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.89876/4.98367. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.89730/4.97980. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.90332/4.96919. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.90245/4.97731. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.89877/4.97501. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.89609/4.98300. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.89784/4.99323. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.89918/4.99636. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89635/4.97730. Took 0.12 sec\n",
      "Epoch 62, Loss(train/val) 4.89665/4.98658. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.89354/4.98418. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.89406/4.97332. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.89570/4.99914. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.89267/4.95538. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 4.90290/4.97280. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.89234/4.97381. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.89219/4.98220. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.90070/4.96887. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.89461/4.97673. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88829/5.00077. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.89139/4.97487. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.89387/4.98112. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.89340/4.98526. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88404/5.01727. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.89055/4.98192. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.89001/4.99826. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88671/4.98869. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.89492/4.99565. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.88815/5.00291. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.88746/5.00228. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.88855/5.01043. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 4.89520/4.97029. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 4.88723/5.02410. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.89252/4.97042. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.89041/4.99094. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.88479/5.01523. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.88742/4.97410. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.88735/4.99587. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.88767/5.01776. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.90202/4.93954. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.90146/4.99693. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.88965/4.98460. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.88328/4.99566. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.88612/4.98190. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.88348/4.97214. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.88638/4.97507. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.88889/4.97754. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.03138824102871723\n",
      "Epoch 0, Loss(train/val) 4.82656/4.82976. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.78843/4.78304. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.78382/4.78394. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.78041/4.78427. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.77908/4.78676. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.77517/4.79613. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.77692/4.79577. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.77193/4.79589. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.77065/4.79682. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.77219/4.79332. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.76976/4.79658. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.77377/4.79845. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.77525/4.79764. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.77324/4.80056. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.77118/4.80283. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.77006/4.80857. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.77059/4.80747. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.77097/4.80444. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.76826/4.81215. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.77109/4.80694. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.76849/4.81029. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.76590/4.80821. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.76755/4.80680. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 4.76814/4.81477. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 4.76824/4.81153. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.76685/4.81431. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 4.76685/4.81520. Took 0.12 sec\n",
      "Epoch 27, Loss(train/val) 4.76650/4.80706. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.76408/4.80872. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.76107/4.80534. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.76275/4.80255. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.76505/4.79999. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.76252/4.81784. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.75657/4.81400. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.76780/4.80503. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.75972/4.81155. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.76409/4.80602. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.76248/4.80411. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.76186/4.80459. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.75898/4.80110. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.76131/4.79663. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 4.76089/4.80857. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.76395/4.80388. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.75992/4.80258. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.76002/4.79583. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.75977/4.79716. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.76250/4.79719. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.75908/4.80061. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.75843/4.80401. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.76097/4.79417. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.75970/4.78686. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.75854/4.78536. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.75952/4.79719. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.75687/4.79575. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.75834/4.79182. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.75239/4.77784. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.75797/4.78621. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.76015/4.78667. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.75483/4.78960. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.75962/4.79076. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.75274/4.78990. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.75594/4.78550. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.75114/4.78542. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.75708/4.78026. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.75268/4.78712. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.75021/4.78400. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.75352/4.77538. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.75226/4.79037. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.75253/4.79053. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.75090/4.77998. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.75131/4.78277. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.74764/4.79028. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.75145/4.78343. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.74860/4.79586. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.75511/4.79129. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.74762/4.79553. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.74351/4.77841. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.74872/4.77682. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.75052/4.79876. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.75089/4.79685. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.75243/4.79492. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.74668/4.78779. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.75342/4.77973. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.74655/4.78939. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.74582/4.78050. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.74775/4.77246. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.74670/4.78194. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.74674/4.79927. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.75748/4.78184. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.75325/4.78579. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.75036/4.78354. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.75173/4.78212. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.75064/4.79275. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.74035/4.79149. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.74392/4.78955. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.75064/4.80039. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.74270/4.78276. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.74303/4.77853. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.74700/4.77850. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.74324/4.78182. Took 0.08 sec\n",
      "ACC: 0.40625, MCC: -0.18547738582708967\n",
      "Epoch 0, Loss(train/val) 4.74341/4.78292. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.72546/4.79776. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.74219/4.72486. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.73645/4.72023. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.71204/4.71504. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.71096/4.71554. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.71424/4.71277. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.71267/4.71318. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.70907/4.71263. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.70814/4.71072. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.71360/4.70981. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.71072/4.71157. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.70669/4.71358. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.70285/4.71063. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.70562/4.71738. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.70574/4.72387. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.70181/4.72003. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.70116/4.72374. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.70126/4.72803. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.69832/4.72717. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.70140/4.73399. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.69738/4.72519. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.70236/4.72666. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.69694/4.72451. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.69834/4.73608. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.69371/4.72675. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.69745/4.73587. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.70171/4.73059. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.69967/4.73087. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.69992/4.71752. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.69506/4.73284. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.69652/4.72926. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.69253/4.74012. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.69826/4.73387. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.69832/4.72489. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.69992/4.73836. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.69541/4.73862. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.69601/4.73037. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.69154/4.74247. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.69605/4.73255. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.69044/4.73836. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.69184/4.73241. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.69808/4.72912. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.69708/4.73555. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.68903/4.72964. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.68975/4.73787. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.69216/4.73002. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.69158/4.74066. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.68765/4.75337. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.69173/4.73323. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.69032/4.72929. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.68838/4.75728. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.68869/4.73716. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.68894/4.77940. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.70088/4.73705. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.69434/4.74629. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.68680/4.74968. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.68878/4.75679. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.68983/4.76011. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.69374/4.75850. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.68348/4.75147. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.68476/4.75086. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.68983/4.76708. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.68497/4.75708. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.68895/4.79560. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.68850/4.74595. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.67941/4.79448. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.68950/4.72289. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.68382/4.77879. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.68900/4.75496. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.68803/4.77073. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.68646/4.78555. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.68220/4.76217. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.68038/4.79990. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.68419/4.76064. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.68793/4.78294. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.67633/4.78265. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.67703/4.74920. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.68488/4.75996. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.67890/4.79030. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.68378/4.76182. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.68200/4.75796. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.67929/4.79614. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.68052/4.75706. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.67713/4.80309. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.68803/4.74771. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.68520/4.76699. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.68316/4.76218. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.67740/4.76746. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.67653/4.79781. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.68501/4.77021. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.67891/4.79500. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.68002/4.76685. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.68239/4.78902. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.67964/4.77987. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.68300/4.78604. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.67530/4.80617. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.68374/4.77143. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.68228/4.78826. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.67448/4.79895. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.013980301652228746\n",
      "Epoch 0, Loss(train/val) 5.00969/4.97217. Took 0.18 sec\n",
      "Epoch 1, Loss(train/val) 4.96965/5.00306. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 4.96920/5.00356. Took 0.14 sec\n",
      "Epoch 3, Loss(train/val) 4.97190/5.01088. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.97622/5.00026. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.98117/4.97303. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.96928/4.97185. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.96136/4.97452. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.96418/4.97990. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.96458/4.98050. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.96372/4.98297. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.96099/4.98916. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.96466/4.99419. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.96261/4.99711. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.96097/4.99863. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.96139/4.99738. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.95779/4.99901. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.96107/4.98118. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.96429/4.98785. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.95983/4.98783. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.95900/4.98851. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.95933/4.98893. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.95910/4.98979. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.95568/4.99157. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.95726/4.98988. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.95606/4.99191. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.95783/4.98695. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.96014/4.98717. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.95638/4.99731. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.95617/4.99790. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.95882/4.99627. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.95343/4.99472. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.94967/4.99503. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.95351/5.00083. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.95544/5.00087. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.95379/5.00796. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.95499/5.00325. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.95913/4.99855. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.95604/4.99712. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.95164/5.00241. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.95723/4.99814. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.95162/5.00560. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.95354/5.00626. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.95086/5.00723. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.94877/5.01826. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.95087/4.99982. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.95533/4.98992. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.95034/5.00365. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.95046/4.99683. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.96767/4.98706. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.95851/4.98873. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.95704/4.99544. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.95428/4.98704. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.95614/4.98698. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.95572/5.00410. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.95429/5.01040. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.95293/5.00305. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.95209/5.00609. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.95048/5.02164. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.95525/4.99794. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.95038/5.00773. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.94894/4.99920. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.94627/5.01484. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.94763/5.02901. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.94933/5.01836. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.94468/5.01414. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.95471/4.98670. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.95792/4.99600. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.95276/5.01066. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.95226/5.00749. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.95384/5.01508. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.94968/5.01901. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.94381/5.02574. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.94922/5.01326. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.94980/5.02685. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.94499/5.01611. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.94221/4.99249. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.95354/5.00936. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.94725/5.00212. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.94687/5.02328. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.95026/5.00464. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.94445/4.99126. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.94496/5.01043. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.94741/5.00222. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.94680/5.00665. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.94853/5.00185. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.94088/5.00431. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.94514/5.01161. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.94656/5.02176. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.94439/5.00977. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.94071/5.02027. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.94374/5.00844. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.93895/5.02118. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.93728/5.01853. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.94199/5.02491. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.94216/5.01565. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.93795/5.03446. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.94001/5.01585. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.93853/5.02958. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.94301/5.03163. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.07100716024967263\n",
      "Epoch 0, Loss(train/val) 4.81992/4.74582. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.74157/4.74409. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.73576/4.74490. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.73797/4.74590. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.73910/4.74542. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.73851/4.74554. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.73865/4.74358. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.73524/4.74488. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.73364/4.74281. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.73791/4.74278. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.73740/4.74334. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.73389/4.74078. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.74229/4.73226. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.73865/4.73353. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.73571/4.73296. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.74131/4.73470. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.73577/4.73501. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.73430/4.73881. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.73206/4.74621. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.73345/4.75086. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.73839/4.75538. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.73128/4.75995. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.73351/4.75577. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.73232/4.75875. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.72564/4.76871. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.73239/4.76609. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.72725/4.77705. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.72930/4.77609. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.72386/4.78712. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.72701/4.77296. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.72568/4.77998. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.72039/4.78885. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.72468/4.78006. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.72644/4.76488. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.72434/4.78432. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.71796/4.78825. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.72321/4.78260. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.71932/4.78885. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.71905/4.79633. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.71930/4.79200. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 4.71584/4.78861. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.71549/4.78793. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.71526/4.79813. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.71892/4.78030. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.71289/4.80606. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.72325/4.77168. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.71783/4.78779. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.71801/4.78003. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.71500/4.81229. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.71298/4.79462. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.71387/4.78594. Took 0.14 sec\n",
      "Epoch 51, Loss(train/val) 4.71607/4.80134. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.70966/4.79514. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 4.70747/4.81052. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.71841/4.77578. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.70918/4.80329. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 4.71017/4.79244. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 4.71436/4.79094. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.70731/4.81073. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.70841/4.80348. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.70936/4.80446. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.70350/4.84324. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.71059/4.80055. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.70776/4.81299. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.70287/4.81243. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.70719/4.81433. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.70154/4.83230. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.70745/4.81428. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.71159/4.82572. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.71127/4.79167. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.70012/4.81713. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.70554/4.81999. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.70204/4.81218. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.69617/4.83330. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.71428/4.80615. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.70107/4.83573. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.70943/4.81289. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.70635/4.79756. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.69749/4.85365. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.69811/4.83641. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.70620/4.79256. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.70423/4.79293. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.69454/4.87392. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.70687/4.82953. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.70100/4.82140. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.69419/4.83346. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.69814/4.84593. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.70316/4.79933. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.69953/4.80711. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.70270/4.79927. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.69856/4.80530. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.69471/4.82818. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.70516/4.80243. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.69982/4.83064. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.69171/4.82326. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.70243/4.81161. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.69908/4.79395. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.70060/4.82255. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.69341/4.84685. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.69596/4.81396. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.06158357771260997\n",
      "Epoch 0, Loss(train/val) 4.99644/4.93249. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.92123/4.93002. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.92988/4.92225. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.93593/4.91791. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.93465/4.91815. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.92937/4.91628. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.92481/4.91719. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.92253/4.91741. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.92161/4.92054. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.91653/4.92466. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.92118/4.92391. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.91812/4.92488. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.91687/4.93386. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.91475/4.93373. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.91119/4.94688. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.90916/4.93799. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.90776/4.94965. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.91037/4.94453. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.91163/4.94433. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.90808/4.95134. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.90718/4.95729. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.90986/4.94648. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.90608/4.96442. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.90499/4.94151. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.90630/4.95528. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.90628/4.95373. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.90691/4.95104. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.90454/4.95887. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.90116/4.97052. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.90363/4.96040. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.90916/4.95150. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.90503/4.97049. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.90228/4.96114. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.90094/4.97056. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.90214/4.96344. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.89862/4.97845. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.89679/4.97976. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.90546/4.95270. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.90140/4.99123. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.90055/4.97862. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.89650/4.98756. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.90100/4.96733. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.89545/4.99970. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.89627/4.98788. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.89854/4.98384. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.89735/4.97929. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.89860/4.98874. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.89558/4.98535. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.89487/4.97652. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.90214/4.96470. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.89075/5.01132. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.89484/4.98076. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.89315/4.99309. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.89620/4.98130. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.91131/4.96286. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.90827/4.97416. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.90423/4.97470. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.90064/4.97786. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.90391/4.95728. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.90346/4.98073. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.89490/4.99090. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89716/4.97900. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.89368/4.99738. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.90323/4.96521. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.90166/4.97816. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.89339/4.99127. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.89412/4.98876. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.89711/5.00704. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.89477/4.99063. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.90143/4.97872. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.89379/5.00489. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.89650/4.99392. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88861/5.00621. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.89265/5.00637. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.88888/5.01748. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.89386/4.99339. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 4.88975/5.00392. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.89577/5.00472. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.89311/5.00461. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88635/5.00378. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.89077/5.00034. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.89180/5.01374. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.89153/5.00008. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.89250/5.02132. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.89136/5.00614. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.89037/5.00437. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.88962/5.00181. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.89082/4.99525. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.88901/4.99509. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88463/5.00827. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.88809/5.00840. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.88936/4.98393. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.89121/5.00447. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.90321/4.96381. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.89714/4.99609. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.89416/5.00510. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.88641/5.01833. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.88502/5.01366. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.88944/5.00996. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.88922/5.01615. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.03138824102871723\n",
      "Epoch 0, Loss(train/val) 4.85919/4.83570. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.79480/4.80810. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.79280/4.80605. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79097/4.80810. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.78971/4.80819. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79304/4.81019. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.78987/4.81154. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.78759/4.81010. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.78783/4.80793. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.78659/4.80698. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.78833/4.80462. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.78881/4.80231. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.78518/4.80687. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.78610/4.81107. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.78364/4.81189. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.78175/4.81138. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78744/4.80422. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.78698/4.80592. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.78194/4.81183. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.78630/4.81117. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.78566/4.80547. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78573/4.80530. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.78188/4.81448. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.78206/4.81319. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.78290/4.81518. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.78496/4.81101. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.78034/4.81243. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.77902/4.81113. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.78320/4.81292. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.78371/4.81557. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.78126/4.81875. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78221/4.81115. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.78436/4.81548. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78237/4.81697. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78172/4.81564. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.77949/4.82830. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.77948/4.82488. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.77590/4.83759. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.77250/4.83740. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.77605/4.84277. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.77492/4.83869. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.77400/4.84240. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.77742/4.83276. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.77562/4.83800. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.77505/4.83669. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.77436/4.85087. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.77346/4.84873. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77270/4.85346. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77010/4.85201. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.77447/4.84553. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77460/4.84514. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77419/4.85956. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.77539/4.84038. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.77086/4.84483. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.76747/4.86590. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.77106/4.86535. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.77284/4.86067. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.76818/4.85485. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76939/4.86075. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.77443/4.85231. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.76768/4.86386. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.76713/4.88092. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76816/4.86447. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.76289/4.87795. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.76707/4.85997. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.76918/4.87094. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.76729/4.85704. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.76453/4.87817. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.76705/4.86514. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76805/4.86828. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.76651/4.85372. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.76950/4.85734. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76590/4.87614. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.76301/4.87468. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.76546/4.85435. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.76576/4.86949. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.76855/4.85996. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.77001/4.85974. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.76154/4.87773. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.76762/4.85047. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.76930/4.85836. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.76284/4.86112. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.76660/4.86644. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76394/4.86758. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.76230/4.86330. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.76870/4.85596. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.76488/4.87929. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.77001/4.85846. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.76313/4.87060. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.77235/4.81737. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.77821/4.81539. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.77504/4.81642. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.77615/4.83038. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.77189/4.82492. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.77061/4.82947. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.77032/4.84542. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.76484/4.87150. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.76735/4.85626. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.76390/4.85181. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.76849/4.84411. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.08952126702661474\n",
      "Epoch 0, Loss(train/val) 5.04810/5.02374. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.02517/5.02223. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.02992/4.99058. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.01307/4.99264. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.00075/4.98714. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.00399/4.98660. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 5.00713/4.98792. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.00340/4.98713. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.00473/4.98671. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 5.00309/4.98770. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.00033/4.98852. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.00007/4.98758. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.00146/4.98722. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.00172/4.99192. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.00017/4.99172. Took 0.16 sec\n",
      "Epoch 15, Loss(train/val) 4.99802/4.99033. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.00096/4.98491. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.99699/4.99379. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 4.99467/4.98629. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.99613/4.99442. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.99638/4.98463. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.99780/4.99301. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.99400/4.98450. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.99635/4.99152. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.99219/4.98631. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.99052/4.98971. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.99152/4.99457. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.98921/4.99498. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.99111/4.99114. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.98719/4.99517. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.99532/4.98601. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.98915/4.99207. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.98709/4.99735. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.98816/5.00020. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.98612/5.00947. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.98625/4.99761. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.98627/5.00667. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.98341/5.01364. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.98378/5.00249. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.98683/5.01508. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.98178/5.02318. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.97963/5.00789. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.98311/5.02383. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.98856/5.00837. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.98219/5.00874. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.98257/5.00138. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.98201/5.00121. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.97428/5.03340. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.98129/5.01354. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.98227/5.00966. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.97937/5.01589. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.97587/5.00754. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.98325/5.00240. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.98150/5.01933. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.97629/5.00954. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.97608/5.00301. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.97693/5.01333. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.97407/5.01439. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.98224/5.00651. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.98338/5.01433. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.97638/5.01902. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.97175/5.02820. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.97256/5.01799. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.97714/5.00894. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.97653/5.02333. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.97339/5.04242. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.98091/5.00406. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.97544/5.02480. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.97880/5.02046. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.97066/5.01435. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.97600/5.04665. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.96992/5.05062. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.97489/5.01335. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.97531/5.03542. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.97359/5.02580. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.96876/5.01487. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.97352/5.03233. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.97564/4.99585. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.96996/5.00691. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.96973/5.01962. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.96668/5.02992. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.96609/5.02108. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.97126/5.02717. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.97220/5.00439. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.96663/5.02695. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.97344/5.01576. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.96236/5.01675. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.97141/5.01778. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.96734/5.02036. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.96500/5.02343. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.96680/5.02194. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.96806/5.01857. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.96963/5.03462. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.96107/5.02626. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.96826/5.02676. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.95950/5.02400. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.96282/5.03502. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.97192/5.04283. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.96097/5.02634. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.96323/5.01754. Took 0.08 sec\n",
      "ACC: 0.40625, MCC: -0.1889822365046136\n",
      "Epoch 0, Loss(train/val) 4.81305/4.80802. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.79596/4.80159. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.79943/4.82449. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79838/4.81239. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.80052/4.79565. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.80507/4.78643. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.79849/4.78625. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.79126/4.78522. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.79393/4.78361. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79293/4.78181. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79496/4.78111. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.79298/4.77894. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.79390/4.77555. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.79093/4.77130. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.79117/4.77140. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.79044/4.76758. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78895/4.76484. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.79089/4.76741. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.78851/4.76461. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.78989/4.76003. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.79084/4.76815. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78891/4.76066. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.78841/4.75970. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.78504/4.75775. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.78954/4.75688. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.78942/4.75849. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.78736/4.75883. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.78513/4.75481. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.78594/4.75639. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.78252/4.75494. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.78452/4.75840. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78525/4.74917. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.78338/4.76509. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78300/4.76440. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78387/4.75122. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.78080/4.76063. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.77904/4.76122. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.78253/4.75966. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.77813/4.76371. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.78017/4.75521. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.78575/4.76630. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.78055/4.76367. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.77490/4.76896. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.78023/4.77683. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.78281/4.76930. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.77983/4.77760. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.78001/4.76244. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77446/4.77172. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77893/4.77118. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.77503/4.77363. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77841/4.77653. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77097/4.76420. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.78165/4.77245. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.77435/4.77420. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.77469/4.77505. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.77328/4.77618. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.77477/4.78544. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.77636/4.77299. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.77085/4.77529. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.77281/4.77969. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.77430/4.78731. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.77248/4.78036. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76364/4.78105. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.77614/4.78936. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.77964/4.77895. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.77333/4.78072. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.77322/4.79281. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.76920/4.79238. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.77239/4.78411. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.77396/4.78000. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.76898/4.79187. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.76878/4.79218. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.77512/4.77951. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.77059/4.78078. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.76859/4.77594. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.77392/4.78491. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.77367/4.79927. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.77107/4.80844. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.78465/4.77789. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.78056/4.77641. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.77880/4.78982. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.77649/4.77726. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.77533/4.78510. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.77390/4.78615. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.77448/4.78696. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.77022/4.78527. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.77193/4.78423. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.76677/4.78829. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.77431/4.77968. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.76741/4.77908. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.77486/4.78419. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.77131/4.78229. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.76921/4.79333. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.77445/4.78353. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.77174/4.79214. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.77210/4.77633. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.76955/4.76661. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.77077/4.77746. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77070/4.77739. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.77009/4.78824. Took 0.08 sec\n",
      "ACC: 0.59375, MCC: 0.19088542889273333\n",
      "Epoch 0, Loss(train/val) 4.94640/4.91218. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.88568/4.86803. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.88667/4.86754. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.88956/4.87203. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.88419/4.85290. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.88725/4.85234. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.88826/4.83940. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.88635/4.84736. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.88252/4.85481. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.87378/4.85596. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.87686/4.86883. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.87553/4.86692. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.88280/4.86627. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.87752/4.86355. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.87275/4.84758. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.87147/4.85161. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.87390/4.84782. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.86782/4.86788. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.86807/4.86416. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.86768/4.85838. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.86808/4.84758. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.86881/4.86222. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.86710/4.86084. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.86368/4.85040. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.86578/4.84775. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.86010/4.86109. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.86402/4.86720. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.86055/4.86828. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.86114/4.85400. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.86449/4.86066. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.86242/4.85826. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.86515/4.87142. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.85683/4.87234. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.85818/4.86908. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.86489/4.88908. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.86954/4.86845. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.87043/4.85491. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.86835/4.87331. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86692/4.86263. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.86918/4.86974. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.86330/4.88054. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.86052/4.86827. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.86077/4.86785. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.85808/4.85054. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.86065/4.87963. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.86002/4.86282. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.85781/4.86377. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.85965/4.87948. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.86400/4.86056. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.86307/4.86646. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.85730/4.85933. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.85280/4.87730. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.85548/4.87628. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.85867/4.86638. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.85884/4.87830. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.86187/4.88469. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.87591/4.88090. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.86935/4.88118. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.86653/4.86749. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.86939/4.89103. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.86476/4.86623. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.86729/4.88557. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.86473/4.87826. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.86637/4.87566. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.86018/4.87656. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.86575/4.86027. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.85884/4.88410. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.85942/4.87584. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 4.85669/4.87688. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.86030/4.87188. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.85700/4.87914. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.85204/4.88091. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.85989/4.86943. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.85721/4.84993. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.86296/4.88275. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.85739/4.87046. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.85647/4.87304. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.85428/4.88985. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.85714/4.87955. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.85731/4.87849. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.85393/4.88031. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.85742/4.88668. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.85439/4.86571. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.85870/4.88181. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.85547/4.87911. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.85280/4.88200. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.85295/4.87786. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.85823/4.88770. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.84973/4.88455. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.85551/4.88307. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.84743/4.88168. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.85488/4.87941. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.85198/4.89690. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.85510/4.86730. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.84715/4.89080. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.84688/4.88556. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.84138/4.88319. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.86021/4.87555. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.85500/4.87922. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.84478/4.88491. Took 0.08 sec\n",
      "ACC: 0.640625, MCC: 0.2506422767984621\n",
      "Epoch 0, Loss(train/val) 4.76988/4.78513. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.72686/4.67345. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.68661/4.67401. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.67821/4.67754. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.68129/4.67570. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.68244/4.67548. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.68243/4.67573. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.68122/4.67620. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.68421/4.67352. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.68041/4.67263. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.68127/4.67202. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.67733/4.67122. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.67817/4.67456. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.67996/4.67477. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.67791/4.67610. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.67815/4.67461. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.67439/4.67644. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.67598/4.67706. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.67527/4.68008. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.67595/4.67619. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.67520/4.67681. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.67378/4.67995. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.67497/4.68530. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.67349/4.68290. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.67182/4.68491. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.67230/4.69246. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.67246/4.68866. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.67073/4.69018. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.67419/4.69560. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.67282/4.69534. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.66873/4.69140. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.66779/4.69707. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.66883/4.69324. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.66990/4.70001. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.66601/4.70099. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.66962/4.70710. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.66723/4.70326. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.66633/4.70355. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.66089/4.71020. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.66240/4.71548. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.66732/4.71216. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.66749/4.69926. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.66956/4.69119. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.66832/4.68937. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.67061/4.68739. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.66833/4.69128. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.66839/4.70654. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.66089/4.71389. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.66826/4.70433. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.66028/4.70524. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.66545/4.69406. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.66183/4.70756. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.66578/4.69473. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.66191/4.69227. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.66202/4.69869. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.66015/4.69680. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.66147/4.70956. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.66087/4.69842. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.65607/4.69985. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.66132/4.70552. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.66009/4.69683. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.65740/4.71194. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.65935/4.70286. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.65957/4.69408. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.65994/4.69493. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.65754/4.71151. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 4.65640/4.71696. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.65903/4.70055. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.65289/4.70455. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.65673/4.70840. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.65497/4.72424. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.65751/4.72310. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.65445/4.72901. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.65457/4.70973. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.66092/4.70457. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.65157/4.72077. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.65830/4.70962. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.65577/4.70080. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.65348/4.70390. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.65421/4.72385. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.65072/4.74160. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.65605/4.69955. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.65652/4.70354. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.65196/4.70574. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.65460/4.69956. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.65176/4.71383. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.64912/4.71439. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.65310/4.71885. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.64883/4.70804. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.64691/4.71504. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.66738/4.69110. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.66969/4.69268. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.66254/4.69045. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.66337/4.71095. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.65547/4.71190. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.65322/4.71606. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.65049/4.72864. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.66278/4.71726. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.64959/4.71239. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.65634/4.70970. Took 0.09 sec\n",
      "ACC: 0.5625, MCC: 0.12267022872162016\n",
      "Epoch 0, Loss(train/val) 4.65018/4.65657. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.62422/4.62758. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.61703/4.63812. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.61594/4.63719. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.61776/4.63776. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.61742/4.63486. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.61651/4.62910. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.61400/4.63653. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.61420/4.64357. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.61070/4.63731. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.61360/4.64637. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.61541/4.62756. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.61243/4.63431. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.61085/4.63146. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.61501/4.63263. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.61069/4.63042. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.61021/4.63819. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.61066/4.63589. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.61226/4.63000. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.60752/4.64438. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.61366/4.63776. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.61093/4.63808. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.61142/4.64725. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.61092/4.64032. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.61247/4.64538. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.61578/4.64238. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.61253/4.64706. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.61318/4.64057. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.61081/4.64171. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.61363/4.64353. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.60910/4.64648. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.61307/4.64448. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.60795/4.64937. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.60621/4.65087. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.61033/4.64727. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.60412/4.65949. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.60447/4.66098. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.60419/4.64897. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.60427/4.64880. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.60086/4.65941. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.60084/4.65905. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.60076/4.64601. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.60349/4.65059. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.60205/4.66282. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.60280/4.65708. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.59842/4.66103. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.59847/4.66424. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.60522/4.63992. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.59885/4.66078. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.59861/4.64653. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.59588/4.66902. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.59260/4.67066. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.59887/4.64319. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.59446/4.64693. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.59729/4.64325. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.59672/4.66910. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.60749/4.64246. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.60576/4.66445. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.59726/4.65031. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.59366/4.67608. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.59319/4.66794. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.58998/4.68510. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.59606/4.66292. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.59554/4.64453. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.59385/4.66022. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.58862/4.65068. Took 0.07 sec\n",
      "Epoch 66, Loss(train/val) 4.59536/4.64931. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.58862/4.65320. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.59187/4.64615. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.58882/4.67186. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.59252/4.65640. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.58587/4.66902. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.59247/4.65990. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.58756/4.66159. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.58422/4.68022. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.58725/4.67141. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.58265/4.68379. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.59338/4.66382. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.58840/4.66893. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.58620/4.65096. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.58249/4.65266. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.58768/4.66875. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.58486/4.66533. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.58149/4.66931. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.58561/4.65037. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.58087/4.67779. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.58308/4.67841. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.58199/4.67139. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.57814/4.66837. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.58861/4.64940. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.57934/4.64003. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.58267/4.64393. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.58045/4.66519. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.58512/4.66010. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.58578/4.66192. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.58255/4.64705. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.57764/4.67792. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.58466/4.65890. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.57858/4.65870. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.56984/4.69034. Took 0.08 sec\n",
      "ACC: 0.625, MCC: 0.2482845429325592\n",
      "Epoch 0, Loss(train/val) 4.97754/4.95667. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.95515/4.96443. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.95848/4.96085. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.95731/4.94255. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.93229/4.94075. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.93533/4.94158. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.93528/4.94183. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.93387/4.94220. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.93415/4.93907. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.93272/4.94577. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.93038/4.94461. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.93175/4.94459. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.93113/4.94323. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.92766/4.95262. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.92709/4.94170. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.92934/4.95133. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.92616/4.95664. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.92779/4.95213. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.92308/4.96224. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.92532/4.95103. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.92624/4.96303. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.92345/4.95995. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.92224/4.95950. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.92377/4.96585. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.92395/4.96195. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.92410/4.94814. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.92047/4.96113. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.92273/4.96316. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.91907/4.97117. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.91844/4.95436. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.92116/4.97380. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.92018/4.96791. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.91557/4.95344. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.92021/4.96997. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.92362/4.95982. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.91743/4.96163. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.91995/4.95640. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.92074/4.97190. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.91730/4.97136. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.92387/4.97479. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.91736/4.97099. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.91753/4.97211. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.91175/4.98135. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.91914/4.98432. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.91704/4.97511. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.91052/4.98351. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.91642/4.97652. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.91096/4.98598. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.91099/4.99068. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.91334/4.98306. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.91231/4.98686. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.91113/5.00522. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.90924/4.99739. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.90859/4.98410. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.91195/4.95334. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.92081/4.98814. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.91632/4.98456. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.91154/4.97334. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.91363/4.98078. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.91601/4.96211. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.92095/4.97653. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.91596/4.98757. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.92019/4.99472. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.91325/5.00855. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.91115/4.99973. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.91253/5.00592. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.90605/5.02727. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.90908/4.99134. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.91127/5.01516. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.90743/5.00236. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.90829/5.01222. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.90790/5.01954. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.90245/5.04382. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.90427/5.03004. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.90716/5.02753. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.90148/5.01181. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.90751/4.99774. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.90940/5.01834. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.91251/5.01346. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.90440/5.00233. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.90408/5.01267. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.90255/4.99439. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.90971/5.01344. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.90334/5.01749. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.90798/5.02479. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.89402/5.03962. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.89775/5.02444. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.90817/5.01753. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.89679/5.01787. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.90314/5.03130. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.90121/5.01904. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.90295/5.01647. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.89971/5.02710. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.90334/5.01149. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.90315/5.03467. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.89673/5.02962. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.89572/5.02184. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.90229/5.00364. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.90402/5.02706. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.89786/5.02426. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.13945994111797608\n",
      "Epoch 0, Loss(train/val) 5.06048/5.02246. Took 0.17 sec\n",
      "Epoch 1, Loss(train/val) 5.02136/5.04535. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 5.01341/5.04289. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 5.01281/5.03974. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.01164/5.03781. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.01443/5.04113. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.01192/5.03672. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.01038/5.04030. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.01132/5.04125. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 5.01377/5.04295. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.00859/5.04963. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.01288/5.04739. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.00932/5.04537. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.00944/5.04176. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.00989/5.03680. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.00862/5.03580. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.01262/5.02754. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.00977/5.02573. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.00681/5.02678. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 5.00664/5.03921. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.00487/5.02796. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 5.00549/5.02876. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.00277/5.02850. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.00486/5.03367. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 5.00445/5.04029. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.00690/5.03542. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.01156/5.03432. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.00833/5.04014. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.00907/5.03998. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.00689/5.03825. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 5.00614/5.04845. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.00439/5.05374. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.00865/5.04671. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.00588/5.04998. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.00364/5.05843. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.00695/5.05125. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.00169/5.06172. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.99945/5.06432. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.00653/5.05665. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.99835/5.07184. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.00349/5.05588. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.99694/5.06710. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 5.00099/5.05258. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.99450/5.07326. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.99989/5.04759. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.99588/5.07651. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.00465/5.05321. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.00229/5.04146. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.00101/5.04303. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.00015/5.03566. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.99723/5.03656. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.00045/5.04446. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.99600/5.05941. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.00651/5.02958. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.00782/5.03451. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.99897/5.05320. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.99862/5.05269. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.99979/5.04662. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.99442/5.06139. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.99965/5.04026. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.99928/5.05215. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.99678/5.06155. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.99008/5.06323. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.99380/5.07223. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.99300/5.06095. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.99337/5.06857. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.99421/5.03383. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.99458/5.04256. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.99708/5.04559. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.99723/5.04637. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.99054/5.05745. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.99473/5.05375. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.99176/5.07221. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.99424/5.05991. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.98646/5.06412. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.99099/5.05496. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.98527/5.07173. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.99626/5.04050. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.99582/5.04974. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.99586/5.04060. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.99808/5.04745. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.00057/5.04666. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.99048/5.05645. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.99291/5.07528. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.99115/5.06417. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.99308/5.07568. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.98843/5.07700. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.99043/5.06596. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.98943/5.05530. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.98842/5.07197. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.98746/5.05974. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.98761/5.07564. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.98963/5.07274. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.98342/5.09306. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.98690/5.08702. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.98422/5.07861. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.98774/5.07273. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.98390/5.08311. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.98045/5.08771. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.98621/5.07316. Took 0.09 sec\n",
      "ACC: 0.640625, MCC: 0.27967281717839076\n",
      "Epoch 0, Loss(train/val) 4.95026/4.85872. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.85188/4.86842. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.84806/4.87064. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.84911/4.87025. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.84496/4.86922. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.84209/4.87196. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.84491/4.86302. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.84594/4.86892. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.84323/4.86972. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.84111/4.86901. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.84270/4.86816. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.83707/4.87218. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.84055/4.87330. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.84233/4.87098. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.83722/4.87448. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.83365/4.87836. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.83861/4.87677. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.83247/4.88584. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.83364/4.88340. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.83294/4.88303. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.82791/4.89765. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.82821/4.89527. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.82986/4.88847. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.82577/4.90076. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.82480/4.89612. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.82542/4.90907. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.82488/4.89724. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82681/4.90709. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.82416/4.90474. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.82849/4.91857. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.82727/4.91311. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.82176/4.92085. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.81892/4.92858. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.82311/4.91384. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.81928/4.93453. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.81887/4.95181. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.82120/4.91269. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.82078/4.91946. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.81711/4.93600. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.81786/4.92758. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.82024/4.92225. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.81502/4.93694. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.81108/4.94148. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.81942/4.93743. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.82179/4.93024. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.81574/4.91337. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.81558/4.94465. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.81288/4.95652. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.81485/4.93792. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.81584/4.96045. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.80850/4.94814. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.81358/4.94706. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.81411/4.93800. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.81180/4.92162. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.80685/4.96727. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.81649/4.93452. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.80942/4.94312. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.80994/4.94984. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.81158/4.94189. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.80908/4.95665. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.80977/4.95152. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.80142/4.97227. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.80967/4.94483. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.81197/4.93755. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.81679/4.93962. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.80763/4.94435. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.80803/4.94497. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.80368/4.94355. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.81305/4.92662. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.80394/4.98118. Took 0.07 sec\n",
      "Epoch 70, Loss(train/val) 4.80889/4.93829. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.80951/5.00380. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.80480/4.96266. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.81068/4.96991. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.80625/4.96687. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.80037/4.95996. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.80149/5.01115. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.81299/4.92364. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.80872/4.99748. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.80657/4.94391. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.80457/4.96453. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.80761/4.95618. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.80451/4.95542. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.80588/4.98104. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.80899/4.97038. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.80246/4.96900. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.80313/4.96953. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.79779/4.98731. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.80693/4.98999. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.80479/4.95794. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.80025/4.99834. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 4.80652/4.98737. Took 0.12 sec\n",
      "Epoch 92, Loss(train/val) 4.80003/4.99061. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 4.80118/4.97172. Took 0.12 sec\n",
      "Epoch 94, Loss(train/val) 4.79832/4.98568. Took 0.11 sec\n",
      "Epoch 95, Loss(train/val) 4.79988/4.99077. Took 0.13 sec\n",
      "Epoch 96, Loss(train/val) 4.80358/4.98498. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 4.79917/4.98590. Took 0.11 sec\n",
      "Epoch 98, Loss(train/val) 4.79994/4.98949. Took 0.12 sec\n",
      "Epoch 99, Loss(train/val) 4.79492/5.00050. Took 0.14 sec\n",
      "ACC: 0.484375, MCC: -0.03328442159263018\n",
      "Epoch 0, Loss(train/val) 4.97430/4.93240. Took 0.13 sec\n",
      "Epoch 1, Loss(train/val) 4.93211/4.93265. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 4.92419/4.94935. Took 0.15 sec\n",
      "Epoch 3, Loss(train/val) 4.92169/4.95435. Took 0.13 sec\n",
      "Epoch 4, Loss(train/val) 4.92195/4.94303. Took 0.14 sec\n",
      "Epoch 5, Loss(train/val) 4.92108/4.95032. Took 0.13 sec\n",
      "Epoch 6, Loss(train/val) 4.92573/4.94873. Took 0.14 sec\n",
      "Epoch 7, Loss(train/val) 4.92364/4.94897. Took 0.13 sec\n",
      "Epoch 8, Loss(train/val) 4.91845/4.95195. Took 0.14 sec\n",
      "Epoch 9, Loss(train/val) 4.92548/4.94782. Took 0.13 sec\n",
      "Epoch 10, Loss(train/val) 4.91963/4.94817. Took 0.14 sec\n",
      "Epoch 11, Loss(train/val) 4.91790/4.94314. Took 0.13 sec\n",
      "Epoch 12, Loss(train/val) 4.92387/4.94178. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 4.92479/4.94162. Took 0.14 sec\n",
      "Epoch 14, Loss(train/val) 4.91714/4.94674. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 4.92021/4.94569. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.91764/4.94689. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.91611/4.95718. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.91896/4.95788. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.91514/4.96348. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.91316/4.96581. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.91674/4.97638. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.91904/4.96389. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.91983/4.96367. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.91544/4.96505. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.91395/4.97500. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.91449/4.95978. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.91335/4.96676. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.91673/4.97341. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.91282/4.97398. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.91197/4.98085. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.90896/4.98052. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.91534/4.97613. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.91277/4.97812. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.90975/4.97873. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.90800/4.99446. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.91241/4.98611. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.91027/4.97123. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.90633/4.99125. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.90901/4.99101. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.91030/4.99688. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.90665/4.99446. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.90963/4.98347. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.90983/4.99475. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.90527/4.99186. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.90463/4.99853. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.90923/4.99983. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.91023/4.99001. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.90548/4.99220. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.89992/5.01292. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.90349/5.00415. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.90040/4.98686. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.90338/4.98453. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.90269/4.98596. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.90153/5.01588. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.89612/4.99744. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.90355/4.98772. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.91107/4.96104. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.91851/4.96814. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.90609/4.97567. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.90930/4.98467. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.90457/4.98600. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.89893/5.01626. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.90229/4.98989. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.90030/4.97901. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.89509/5.00383. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.90049/4.97773. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.89774/4.98806. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.89531/5.00777. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.88872/4.97284. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.89364/5.00819. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.89887/4.98987. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.89119/4.98715. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.89153/5.03120. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.90081/4.99653. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.89797/4.99806. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.89523/5.00781. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.89167/4.99265. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.88904/4.97859. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.89097/4.96413. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.88717/5.00096. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.89343/4.99740. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.89127/4.99325. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.88874/4.97574. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.89617/4.97410. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.88982/4.99393. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.88933/4.96355. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.89259/4.98627. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.88265/4.99098. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.89225/4.99315. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.88713/4.97269. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.88869/4.97260. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.88676/4.99893. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.88773/4.94121. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.88808/4.98592. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.88331/5.00589. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.88793/4.97704. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.88372/5.01236. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.89043/4.99760. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.89071/4.97209. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.0\n",
      "Epoch 0, Loss(train/val) 4.81817/4.78784. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.76537/4.77826. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.75890/4.76816. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.75097/4.76599. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.74900/4.76697. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.75218/4.76827. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.75135/4.77008. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.75386/4.77280. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.75488/4.77079. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.74847/4.76799. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.74863/4.76753. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.74706/4.77355. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.74676/4.77634. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.74439/4.77452. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.74743/4.77050. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.74659/4.77542. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.74381/4.77274. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.74055/4.77623. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.74207/4.77541. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.73804/4.77891. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.74845/4.76008. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.73890/4.76739. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.74111/4.76811. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.73837/4.77040. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.73833/4.76869. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.73774/4.76532. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.74174/4.76625. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.73607/4.76574. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.73781/4.76902. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.73979/4.76213. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.73521/4.76621. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.73876/4.76762. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.73387/4.76663. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.73643/4.76153. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.73456/4.76359. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.73791/4.75932. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.73648/4.75920. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.73492/4.76458. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.73103/4.76819. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.73029/4.76642. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.73448/4.76416. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.73077/4.76202. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.73420/4.76826. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.73071/4.76675. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.72640/4.78029. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.72578/4.76941. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.72631/4.79095. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.73493/4.75716. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.72830/4.77512. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.72677/4.76510. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.72249/4.77329. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.72194/4.78266. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.72519/4.77473. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.72922/4.77259. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.72117/4.78448. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.72186/4.77242. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.71903/4.79461. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.71963/4.78244. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.71982/4.79214. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.72143/4.77523. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.71483/4.79196. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.71923/4.78700. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.72533/4.77717. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.71860/4.81407. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.71941/4.79545. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.71322/4.82372. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.72338/4.78024. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.71262/4.83582. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.71436/4.79488. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.71122/4.80915. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.71472/4.81512. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.71057/4.83955. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.71786/4.78337. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.71758/4.80690. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.71494/4.78659. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.70950/4.80150. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.71308/4.81316. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.71294/4.81716. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.70810/4.82359. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.70946/4.81813. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.70407/4.82139. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.71646/4.81984. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.70980/4.81876. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.70867/4.80645. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.70798/4.81945. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.70822/4.82166. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.70646/4.82651. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.71038/4.84491. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.70854/4.83597. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.71269/4.82134. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.70475/4.83022. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.69926/4.84537. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.70942/4.81507. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.70230/4.85497. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.70942/4.82249. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.70349/4.83062. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.71274/4.83023. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.70385/4.88446. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.70392/4.85356. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.71015/4.84233. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.0\n",
      "Epoch 0, Loss(train/val) 4.87102/4.86173. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.86525/4.85342. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.85354/4.87287. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.87000/4.85351. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.86813/4.84072. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.85359/4.84020. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.84861/4.83931. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.85283/4.83804. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.84967/4.83750. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.84880/4.83731. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.84921/4.83687. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.84927/4.83681. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.84929/4.83215. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.84632/4.82053. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.84632/4.81862. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.84445/4.81732. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.84730/4.81877. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.84538/4.81741. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.84657/4.82233. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.84611/4.82276. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.84171/4.82030. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.84369/4.82012. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.84224/4.82902. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.84403/4.82828. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.84059/4.82000. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.84152/4.81755. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.84118/4.82257. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.83659/4.82018. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.83919/4.82020. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84033/4.82086. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.83856/4.82023. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.83913/4.82362. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.83886/4.83238. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.83990/4.82292. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.83819/4.82021. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.83833/4.81896. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.83746/4.80980. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.83605/4.80815. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.83444/4.81367. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.83749/4.82406. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.83385/4.82145. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.83864/4.82025. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.83766/4.82292. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.83722/4.82146. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.83123/4.80840. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.83029/4.81309. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.83353/4.81768. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.83546/4.82666. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.83426/4.82671. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.83207/4.82117. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.83077/4.81795. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.83587/4.82722. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.82790/4.82223. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.83063/4.82582. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.82857/4.82528. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.82814/4.82929. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.83001/4.82485. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.83288/4.81942. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.82867/4.80613. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83299/4.80726. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.83097/4.81920. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83224/4.81977. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.82637/4.82088. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.82935/4.82647. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 4.82678/4.81798. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.82403/4.82346. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.82451/4.81444. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.82381/4.81541. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.83155/4.82513. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83032/4.82231. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.82723/4.82602. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.82433/4.82807. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.82493/4.81398. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.82399/4.82520. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.82745/4.82443. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.82669/4.82086. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.82235/4.82348. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.82356/4.82060. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.82432/4.81897. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.81987/4.82613. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.81442/4.81777. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.81999/4.81707. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.81722/4.82904. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.82693/4.83633. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.81917/4.83229. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.82440/4.81550. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.81599/4.82363. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.82224/4.80592. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.82210/4.81945. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.82145/4.81313. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.82283/4.82554. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.81970/4.83022. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.82043/4.82369. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.81600/4.82534. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.81690/4.83062. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.81793/4.80864. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.82361/4.81112. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.82053/4.81721. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.81887/4.81959. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.81792/4.81365. Took 0.08 sec\n",
      "ACC: 0.59375, MCC: 0.1972421118046462\n",
      "Epoch 0, Loss(train/val) 4.90394/4.85840. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.87142/4.84459. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.86417/4.83843. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.86687/4.83725. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.86469/4.83690. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.86179/4.83818. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.86160/4.84256. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.85990/4.84261. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.85863/4.83966. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.85533/4.84221. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.85333/4.84617. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.85626/4.83521. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85578/4.85284. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.84954/4.85161. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.85535/4.84686. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.85620/4.85118. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.84914/4.85057. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.85084/4.85207. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.84754/4.85104. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.84614/4.85066. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.85001/4.85009. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.85186/4.85096. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.84863/4.85077. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.84475/4.84976. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.84600/4.85017. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.84622/4.85191. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.84133/4.85203. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.84570/4.85368. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.84338/4.85464. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84235/4.85885. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.84204/4.86156. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.84325/4.85763. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.83499/4.85964. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.83963/4.85797. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.83451/4.85407. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84263/4.85963. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84261/4.85764. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84000/4.86156. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.83784/4.86343. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.84477/4.84979. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84462/4.85713. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.83747/4.85799. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.83470/4.87167. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.83885/4.86919. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.83678/4.85943. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.83902/4.86602. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.83073/4.86828. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.83524/4.86766. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.83450/4.86714. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.83765/4.86847. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.83660/4.86523. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.83736/4.85576. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.83344/4.86565. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.82838/4.86129. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.83508/4.86299. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.83166/4.87402. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.84060/4.84937. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.83590/4.87732. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.83249/4.86048. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.85579/4.83823. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.84463/4.84798. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83820/4.87860. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.83691/4.88009. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.83200/4.86858. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.83482/4.86884. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.83536/4.86609. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.83302/4.85620. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 4.83292/4.85108. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.83734/4.84372. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83252/4.86882. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.83199/4.86516. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.83014/4.85823. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83334/4.85097. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.83035/4.85723. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83579/4.85498. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.82546/4.85711. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.83284/4.85066. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.83283/4.84964. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.83057/4.85915. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.82629/4.86717. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.82786/4.85991. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.82785/4.86489. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.83081/4.86332. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.83195/4.85767. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83209/4.85045. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.82906/4.85147. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83102/4.85714. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.83137/4.84880. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.82828/4.85058. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.82760/4.86091. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.82829/4.85602. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.83019/4.85855. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.82907/4.84801. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.82444/4.85067. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.83115/4.85693. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.82853/4.84678. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.82905/4.85683. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.82518/4.85033. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.82653/4.85910. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.82443/4.86803. Took 0.08 sec\n",
      "ACC: 0.578125, MCC: 0.15318083468998522\n",
      "Epoch 0, Loss(train/val) 4.92179/4.86253. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.85819/4.84094. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.85523/4.84044. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.85480/4.83527. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.85132/4.83407. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.84928/4.82986. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.84564/4.83283. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.85161/4.83912. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.84983/4.83342. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.84525/4.83931. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.84211/4.84221. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.84761/4.84708. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.84100/4.84297. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.84383/4.85090. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.84206/4.84237. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.84287/4.83718. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.83713/4.83645. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.83812/4.83000. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.84026/4.82431. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.83819/4.81948. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.83308/4.81828. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.83549/4.82407. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.83530/4.82609. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.83947/4.82895. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.83788/4.84018. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.83764/4.84858. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.84053/4.84094. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.84220/4.83321. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.83534/4.82809. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.82880/4.81854. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.83136/4.82527. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.83317/4.84303. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.83264/4.82377. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.83251/4.82868. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.82600/4.83144. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.83132/4.83914. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84183/4.85368. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84020/4.84679. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.83778/4.85973. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84249/4.85197. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84651/4.84104. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.84479/4.83893. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84014/4.84862. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.83915/4.84186. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.83883/4.84075. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.83963/4.84497. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.83766/4.84861. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.83659/4.85017. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84266/4.85357. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.84200/4.85725. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.83983/4.85577. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.83535/4.85417. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.83491/4.85327. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.84099/4.85121. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.83417/4.84807. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.83007/4.84905. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.83223/4.85208. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.83624/4.84268. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.83538/4.84248. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83849/4.84861. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.83647/4.83985. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83255/4.84096. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.82973/4.84876. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.83252/4.83719. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.83214/4.84656. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.82971/4.84717. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.82660/4.84987. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.82742/4.85897. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.82811/4.84937. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83325/4.84662. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.82414/4.85515. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.83093/4.85206. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.82407/4.86329. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.82942/4.84463. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.82410/4.86436. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83222/4.85765. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.83561/4.85278. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.83220/4.84846. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.83341/4.84984. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.83592/4.84774. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.82678/4.85249. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.83125/4.82229. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.83701/4.83613. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.83530/4.86836. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83253/4.85567. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.82838/4.85480. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.82645/4.85463. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.82505/4.86128. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.82109/4.86479. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.82627/4.86444. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.82226/4.85861. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.82527/4.86090. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.82463/4.86678. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83037/4.82902. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.84094/4.83019. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.82804/4.84676. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.82373/4.83895. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.81795/4.84937. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.82355/4.85993. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.82378/4.85054. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.043224813349211175\n",
      "Epoch 0, Loss(train/val) 4.58736/4.55490. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.57289/4.55790. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.57258/4.57230. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.57610/4.59917. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.57185/4.57866. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.56917/4.57071. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.56341/4.58279. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.56718/4.58072. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.56465/4.57678. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.56221/4.57961. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.56354/4.57743. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.56181/4.58034. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.56310/4.57653. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.56136/4.57313. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.56222/4.57446. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.55833/4.57559. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.56602/4.57380. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.55676/4.56676. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.55941/4.57271. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.55605/4.57289. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.55815/4.57397. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.55576/4.56863. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.55580/4.58344. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.55796/4.56856. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.55334/4.57334. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.55364/4.56759. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.54986/4.57675. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.55041/4.56519. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.56213/4.57860. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.55801/4.56128. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.55071/4.57588. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.55342/4.57872. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.55053/4.57396. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.55264/4.57143. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.54777/4.56875. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.54741/4.57114. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.54443/4.57387. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.55181/4.57233. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.55132/4.56270. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.54592/4.56242. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.55341/4.58887. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.55096/4.56005. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.55162/4.56342. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.54853/4.56484. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.54763/4.57841. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.54669/4.56749. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.54523/4.57525. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.54827/4.57021. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.54701/4.57215. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.54417/4.57426. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.54931/4.57606. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.54603/4.58016. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.54531/4.57467. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.54339/4.58029. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.54778/4.57511. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.54563/4.57804. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.54466/4.57252. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.54095/4.57348. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.54030/4.58052. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.54710/4.57757. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.54652/4.57989. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.54356/4.57031. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.54458/4.57834. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.54450/4.56975. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.54590/4.56898. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.54378/4.57686. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.54319/4.57055. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.53973/4.57317. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.54251/4.57358. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.54171/4.56949. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.54049/4.57434. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.53983/4.58301. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.54039/4.57424. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.53763/4.57666. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.54018/4.57187. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.54029/4.59489. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.54465/4.58764. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.54405/4.58836. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.54327/4.58488. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.53787/4.57253. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.54761/4.57726. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.54564/4.58479. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.54770/4.57514. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.54240/4.56569. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.53613/4.59369. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.54087/4.58343. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.54139/4.58552. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.55074/4.59065. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.55036/4.56937. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.54380/4.58089. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.54837/4.57954. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.54011/4.58530. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.54237/4.58056. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.53230/4.59061. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.54203/4.57945. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.53664/4.57576. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.53450/4.58625. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.53740/4.58838. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.54205/4.59108. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.53829/4.58358. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.0148961999725068\n",
      "Epoch 0, Loss(train/val) 5.06510/5.02046. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.02071/5.02168. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.01544/5.02187. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.01444/5.02235. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.01289/5.01971. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 5.01166/5.01971. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.01332/5.01942. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.01015/5.01807. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 5.01012/5.01859. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.01154/5.01330. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.00399/5.01384. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.01169/5.00303. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.00315/5.02405. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 5.00369/5.01324. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.99869/5.01757. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.00315/5.02171. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.99892/5.01728. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.00427/5.01529. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.00190/5.01733. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.99835/5.02539. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.99478/5.02634. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.99818/5.03027. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.99706/5.02141. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.99943/5.02761. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.99512/5.03792. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.00064/5.03096. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.99680/5.02853. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.99752/5.03014. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.99571/5.01829. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.99783/5.01920. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.99392/5.03261. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.99331/5.02814. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.99492/5.02439. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.99414/5.02106. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.99828/5.01957. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.99311/5.01762. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.99216/5.02314. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.99762/5.01949. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.98680/5.03765. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.99680/5.03265. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.99206/5.02885. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.98936/5.03119. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.98721/5.03720. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.99620/5.02924. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.99357/5.03312. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.99698/5.02487. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.99348/5.04594. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.99095/5.03350. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.99125/5.04044. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.98970/5.03861. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.98997/5.03851. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.98722/5.04281. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.99121/5.03545. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.98765/5.04097. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.98828/5.04580. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.98725/5.05116. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.99143/5.03695. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.98600/5.03797. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.99188/5.04654. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.98754/5.04050. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.98404/5.04160. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.98338/5.05028. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.98062/5.06552. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.98487/5.05866. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.98364/5.05241. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.98580/5.05226. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.98323/5.05431. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.97923/5.07199. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.99070/5.05139. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.98437/5.06069. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.98081/5.06170. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.98776/5.06693. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.97768/5.06482. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.98475/5.06425. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.98029/5.06241. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.98018/5.06483. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.98117/5.07039. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.98136/5.07454. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.98048/5.06994. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.97781/5.07641. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.98280/5.07303. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.98518/5.05323. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.97519/5.07931. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.97331/5.08127. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.97717/5.08013. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.97851/5.07526. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.97383/5.07338. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.97806/5.08286. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.99033/5.05415. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.98195/5.06304. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.97902/5.07006. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.96875/5.09384. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.97845/5.07426. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.97951/5.08926. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.97958/5.06959. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.97209/5.11636. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.98070/5.07239. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.97085/5.09994. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.96979/5.10246. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.97309/5.09540. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.13010726223589664\n",
      "Epoch 0, Loss(train/val) 4.99757/4.93467. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.93142/4.91211. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.92580/4.90760. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.92427/4.90973. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.92155/4.91235. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.92144/4.91391. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.91854/4.92477. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.92037/4.92965. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.92167/4.92050. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.91819/4.92079. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.92164/4.91949. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.91935/4.92525. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.91677/4.92299. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.91826/4.92394. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.91494/4.92469. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.91481/4.92314. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.91446/4.92729. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.91477/4.92497. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.91306/4.93543. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.91417/4.93343. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.90937/4.94413. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.91371/4.94019. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.90983/4.93509. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.91060/4.94478. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.91228/4.92198. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.91074/4.93010. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.91028/4.92696. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.91326/4.92867. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.90795/4.94188. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.90949/4.93699. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.90675/4.93252. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.90428/4.94341. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.90791/4.94062. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.89993/4.95825. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.90300/4.94492. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.90843/4.91353. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.91160/4.91960. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.91062/4.93388. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.91382/4.91906. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.91937/4.92454. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.91143/4.93036. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.91059/4.92877. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.90874/4.94065. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.91311/4.93069. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.91392/4.93071. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.91046/4.92904. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.90759/4.94243. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.90570/4.95782. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.90196/4.93857. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.90776/4.94746. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.90399/4.94603. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.90806/4.95118. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.90674/4.93964. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.90132/4.96082. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.89935/4.96971. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.90017/4.97684. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.89450/4.96834. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.90451/4.96715. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.90369/4.96088. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.90278/4.94968. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.90104/4.93864. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.90192/4.94694. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.90055/4.95753. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.90073/4.95613. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.89703/4.97224. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.89459/4.98921. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.89226/4.94696. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.89699/4.96315. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.89607/4.95763. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.89315/4.98961. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.89810/4.96461. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.89182/4.97311. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.89481/4.95126. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.90473/4.96837. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.90079/4.97360. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.89763/4.97083. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.89826/4.97161. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.89263/4.98018. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.89354/5.00026. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.89017/4.99660. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.89350/4.98676. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.90037/4.99223. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.89660/4.97536. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.88876/5.01642. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.89554/4.99443. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.89124/5.01831. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.89388/4.99893. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.89418/4.98800. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.89595/4.98992. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.89061/5.01047. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.89936/4.97063. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.89291/4.98516. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.89096/5.00307. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.89247/5.01352. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.88942/5.01416. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.89218/5.01770. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.89020/5.00971. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.90048/4.98334. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.89475/4.98370. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.89984/4.97457. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.06362847629757777\n",
      "Epoch 0, Loss(train/val) 4.75239/4.69403. Took 0.18 sec\n",
      "Epoch 1, Loss(train/val) 4.69796/4.68948. Took 0.17 sec\n",
      "Epoch 2, Loss(train/val) 4.69802/4.69988. Took 0.13 sec\n",
      "Epoch 3, Loss(train/val) 4.69690/4.69801. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.69894/4.69860. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.69719/4.70272. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.69606/4.69843. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.69315/4.69651. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.69170/4.70011. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.69467/4.70429. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.69322/4.70616. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.69130/4.70645. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.68778/4.71510. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.68784/4.72023. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.69063/4.70844. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.69182/4.71679. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.68782/4.71694. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.68849/4.71336. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.68810/4.71368. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.68871/4.70654. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.69055/4.70459. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.68797/4.71214. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.68715/4.71102. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.68568/4.71190. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.68523/4.71266. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.68442/4.71141. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.68261/4.72492. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.68267/4.71595. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.67857/4.71995. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.68696/4.71838. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.67894/4.74602. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.68148/4.72276. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.67718/4.74315. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.68128/4.72827. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.68618/4.70132. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.68299/4.70252. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.68046/4.70996. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.68109/4.71950. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.67889/4.71928. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.67673/4.73009. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.67340/4.74089. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.67884/4.73632. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.67871/4.73529. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.67742/4.72761. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.67670/4.74325. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.67520/4.74847. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.67611/4.72774. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.67465/4.72435. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.67312/4.74742. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.67435/4.74109. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.67420/4.73971. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.66958/4.74515. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.67115/4.74393. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.67120/4.75103. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.66907/4.75168. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.66910/4.75596. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.66967/4.74374. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.67696/4.73176. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.66947/4.75520. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.67068/4.76379. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.67317/4.74424. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.67246/4.75074. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.66444/4.76842. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.66867/4.74638. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.66790/4.74559. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.66636/4.77282. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.67061/4.73766. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.66909/4.75531. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.66840/4.74906. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.66821/4.75995. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.66512/4.75327. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 4.66749/4.75671. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.66854/4.73413. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.66449/4.77663. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.66486/4.77813. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.67178/4.76514. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.66140/4.79393. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.66593/4.75762. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.67501/4.74566. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.67294/4.75312. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.67217/4.74194. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.67394/4.74925. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.66686/4.78325. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.66610/4.77788. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.66024/4.76846. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.65519/4.79660. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.66577/4.78851. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.66857/4.77446. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.66430/4.77303. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.66000/4.80117. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.65714/4.79252. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.66293/4.79901. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.65475/4.79097. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.66550/4.75809. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.66211/4.76360. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.66480/4.78767. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.65587/4.78506. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.67329/4.74630. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.67452/4.75673. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.67252/4.78128. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.11834526708278773\n",
      "Epoch 0, Loss(train/val) 4.62238/4.61563. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.61732/4.60982. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.60745/4.59837. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.60198/4.60191. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.60021/4.59889. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.59787/4.59866. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.59852/4.59847. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.59844/4.59816. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.59921/4.59831. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.59646/4.59894. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.59644/4.60144. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.59415/4.60299. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.59769/4.60053. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.59320/4.59968. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.59140/4.60084. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.59246/4.60001. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.59072/4.59956. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.59234/4.60169. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.59128/4.60135. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.58792/4.59725. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.59121/4.59324. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.58943/4.60331. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.59511/4.61629. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.59412/4.60940. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.59144/4.60785. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.59130/4.60260. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.58933/4.61514. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.58986/4.60572. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.59012/4.61187. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.58772/4.62077. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.58695/4.60784. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.58542/4.62414. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.58500/4.62257. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.58560/4.63059. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.58405/4.63168. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.58470/4.62033. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.57993/4.63465. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.58532/4.62019. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.58188/4.62889. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.58368/4.59480. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.59286/4.61265. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.58393/4.64639. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.58163/4.63359. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.57832/4.64284. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.57848/4.63972. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.58782/4.60392. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.58411/4.62993. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.58308/4.63648. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.57427/4.63223. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.58050/4.63320. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.57865/4.64324. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.57464/4.64052. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.57928/4.63239. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.57465/4.64963. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.57355/4.64771. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.58017/4.63515. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.57303/4.64455. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.56980/4.66264. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.56956/4.67381. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.58889/4.58728. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.59736/4.58310. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.59410/4.58445. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.58950/4.58671. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.58902/4.59850. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.58359/4.63799. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.58219/4.62892. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.57297/4.66056. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.58469/4.65323. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.57284/4.66591. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.57564/4.67425. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.57733/4.67357. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.57151/4.67651. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.57513/4.65362. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.57702/4.66748. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.57467/4.65185. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.57251/4.68003. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.56656/4.66480. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.57473/4.66055. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.57453/4.67771. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.57220/4.66366. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.57192/4.67331. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.57422/4.68549. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.56367/4.69096. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.57363/4.67994. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.57258/4.67371. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.56502/4.68204. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.57039/4.69430. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.56904/4.69506. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.57418/4.67262. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.56759/4.67269. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.56632/4.68890. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.56925/4.68432. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.57237/4.69004. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.56738/4.68722. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.57134/4.63864. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.58300/4.63335. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.57921/4.63948. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.57488/4.64736. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.57343/4.67734. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.57343/4.68128. Took 0.08 sec\n",
      "ACC: 0.59375, MCC: 0.19925670837456913\n",
      "Epoch 0, Loss(train/val) 4.95535/4.99198. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.87413/4.84957. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.86983/4.84466. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.86078/4.85139. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.85603/4.85288. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.85743/4.84878. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.85590/4.84930. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.85799/4.84856. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.85889/4.84992. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.85343/4.85195. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.85528/4.85109. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.85347/4.85411. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85370/4.85399. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.85458/4.85668. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.85276/4.85731. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.84900/4.86166. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.84936/4.85849. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.85150/4.85718. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.84966/4.86449. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.84396/4.86048. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.84719/4.85908. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.84771/4.86478. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.84354/4.86587. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.84650/4.85748. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.84570/4.86251. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.84546/4.87049. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.84609/4.86873. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.84722/4.86959. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.84405/4.87333. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84373/4.86366. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.84557/4.87470. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.84276/4.87417. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84176/4.87581. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.85300/4.83587. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.85367/4.84342. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84901/4.85111. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84672/4.85333. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84628/4.85288. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84605/4.85527. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84583/4.85459. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84696/4.85514. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.84312/4.85846. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84492/4.85600. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.84432/4.85604. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.84216/4.86456. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.83979/4.85468. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.84046/4.86079. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.84149/4.86194. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84152/4.85806. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.83908/4.86551. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.84056/4.85765. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.84247/4.85731. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.83870/4.86556. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.83719/4.86399. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.83861/4.86696. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.83781/4.86641. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.83909/4.85853. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.83700/4.86361. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.83671/4.86255. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83798/4.86176. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.84153/4.87088. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83603/4.86270. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.83550/4.86344. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.83551/4.85905. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.83612/4.86130. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.83674/4.86728. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.83420/4.86721. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.83572/4.86568. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.83536/4.88312. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.83613/4.86515. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.83238/4.87182. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.83428/4.87376. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83151/4.86929. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.83868/4.86413. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83154/4.87197. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83259/4.87774. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.83110/4.86703. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.83322/4.86631. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.83204/4.86509. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.83433/4.87812. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.82939/4.87743. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.83523/4.87219. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.83180/4.87654. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.83484/4.88201. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83078/4.87980. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.82985/4.89076. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83021/4.87312. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.82723/4.87773. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83401/4.88315. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.82961/4.87928. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.83646/4.86946. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.83047/4.88546. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.82835/4.87990. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83266/4.88933. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.83028/4.89074. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.82837/4.89128. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.83275/4.87492. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.82636/4.90394. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83363/4.88474. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.82930/4.88060. Took 0.08 sec\n",
      "ACC: 0.6875, MCC: 0.3757345746510897\n",
      "Epoch 0, Loss(train/val) 5.18708/5.10742. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.11335/5.11954. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.11555/5.13618. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.12188/5.12699. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 5.11746/5.12780. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.11962/5.12163. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.12598/5.09759. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.11846/5.08873. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.11222/5.08778. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.11005/5.09849. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.11082/5.10011. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.11337/5.10009. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.11055/5.09960. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.11221/5.09738. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 5.11165/5.09798. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.10560/5.09752. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.10901/5.10056. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.10964/5.10125. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.10669/5.10138. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.10749/5.10320. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.10701/5.10504. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.10498/5.10632. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 5.10547/5.10665. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 5.10585/5.10895. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.10762/5.10952. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 5.10592/5.11042. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.10230/5.11540. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 5.10470/5.11407. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.10558/5.11368. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 5.10661/5.11423. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.10259/5.11725. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.10045/5.11587. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.10087/5.11729. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.10224/5.11774. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.10124/5.12029. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.10463/5.11514. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.10023/5.11350. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.10229/5.11676. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.10218/5.12171. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.09970/5.11541. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.10272/5.11776. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.09853/5.12082. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 5.09802/5.12526. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.10248/5.12122. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 5.10259/5.11837. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.09761/5.12631. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.10125/5.12172. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.09733/5.12267. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.09315/5.12484. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.09813/5.12507. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.09812/5.12576. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.10116/5.11647. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.09715/5.12420. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.09724/5.11421. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.09396/5.11967. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.09776/5.11875. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.09666/5.11619. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.09359/5.11629. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.09521/5.12596. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 5.09662/5.11987. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.09644/5.11847. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.09190/5.12403. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 5.09307/5.12542. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.09519/5.12543. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.09433/5.12225. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 5.09217/5.13019. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.09283/5.12614. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.08783/5.13532. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.09523/5.12750. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.09449/5.11893. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.09251/5.12042. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.09159/5.12927. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.08577/5.13226. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.08713/5.13447. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.09452/5.11691. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.10259/5.11185. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.10066/5.12167. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.10053/5.12288. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.09086/5.13860. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 5.09322/5.12390. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.09407/5.13271. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 5.08916/5.13199. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.08696/5.12129. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 5.08410/5.13418. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.09132/5.13775. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.09188/5.14515. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.08415/5.15397. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 5.08536/5.14537. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.08938/5.12581. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.08784/5.12828. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.08748/5.12313. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.08542/5.13182. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.08804/5.13782. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 5.08844/5.13326. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.08727/5.12905. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 5.08979/5.12856. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.08679/5.12610. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 5.08601/5.12627. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.08646/5.13957. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 5.09172/5.12934. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.03779786830012622\n",
      "Epoch 0, Loss(train/val) 4.85893/4.84253. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.82349/4.80658. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.82489/4.84020. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.82375/4.84448. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.81753/4.84594. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.82136/4.84225. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.82067/4.84376. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.81849/4.85490. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.81901/4.85420. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.82008/4.85694. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.81859/4.82963. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.81955/4.80448. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.81553/4.79693. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.81445/4.80371. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.81490/4.80837. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.81578/4.80953. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.81523/4.80583. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.80953/4.80840. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.81296/4.81340. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.81658/4.81141. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.81350/4.80516. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.81040/4.80743. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.81158/4.80931. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.81056/4.80650. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.81267/4.80152. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.80851/4.81351. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.80793/4.81706. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.80874/4.81475. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.80996/4.80997. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.80853/4.81871. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.81104/4.80481. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.80748/4.79853. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.80662/4.80496. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.80475/4.81063. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.79951/4.82775. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.81302/4.82337. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.81003/4.79689. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.80300/4.80352. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.79983/4.81421. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.81003/4.81723. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.80480/4.79793. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.80088/4.80515. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.80077/4.80639. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.80009/4.81022. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.80066/4.81655. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.79874/4.82457. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.81165/4.81918. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.80572/4.79534. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 4.80700/4.80468. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.80422/4.80521. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.80081/4.79714. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.80472/4.80398. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.80307/4.79654. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.80081/4.80156. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.80023/4.82449. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.79858/4.81648. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.79636/4.82111. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.80262/4.80778. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.79880/4.78447. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.79858/4.80091. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.79633/4.79883. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.79849/4.81854. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.79697/4.80008. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.78985/4.81520. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.80000/4.80494. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.79367/4.80157. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.79560/4.79900. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.79246/4.81188. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.79300/4.80668. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.79281/4.79835. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.78939/4.80019. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78928/4.81729. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.79147/4.82045. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.79253/4.79783. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.78938/4.80492. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.78802/4.82485. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78986/4.80502. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78481/4.81149. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.78686/4.81645. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.78315/4.81525. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.79351/4.80619. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.78305/4.81369. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.78898/4.80452. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.78868/4.80474. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.79009/4.78407. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.79009/4.82067. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.78853/4.82312. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.77971/4.80956. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.78497/4.81891. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.78493/4.82045. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.78180/4.80834. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.78244/4.79477. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.78327/4.81734. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.78497/4.80822. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.78701/4.78843. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.77857/4.78756. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.77931/4.80753. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.78129/4.81298. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77692/4.83085. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.78013/4.79339. Took 0.08 sec\n",
      "ACC: 0.5, MCC: -0.015873015873015872\n",
      "Epoch 0, Loss(train/val) 5.08902/4.96281. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.97919/4.95487. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.97109/4.95782. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.97163/4.96197. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.97061/4.96642. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.97109/4.96731. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.96912/4.97156. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.96650/4.97243. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.96826/4.96793. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.96853/4.97495. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.96811/4.97282. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.97070/4.97089. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.96656/4.97337. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.96560/4.97218. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.96700/4.97470. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.96729/4.97523. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.96231/4.97773. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.96186/4.97535. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.96508/4.97678. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.96384/4.98084. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.96093/4.98338. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.95919/4.98336. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.96306/4.97790. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.96650/4.97576. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.96550/4.97164. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.96073/4.97686. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.96123/4.98263. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.95560/4.99966. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.96004/5.02216. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.97045/4.98519. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.96666/4.97977. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.96475/4.98274. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.96574/4.98187. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.96282/4.98149. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.96118/4.98687. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.96502/4.98165. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.95939/4.98886. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.95885/4.98628. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.95581/4.99255. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.95951/4.98951. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.95518/4.98332. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.95714/4.98001. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.95409/4.98793. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.95484/4.98727. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.95126/4.99198. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.95514/4.99451. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.95127/4.98880. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.95504/4.98659. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.95172/5.00075. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.94844/4.99453. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.95816/4.99655. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.95337/4.98414. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.94887/4.98863. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.95201/4.99035. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.95179/4.99039. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.95116/4.99150. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.94844/4.99408. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.95204/4.98412. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.94952/4.99999. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.95124/4.98682. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.95056/4.98074. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.95058/4.99501. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.94273/4.98432. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.95291/4.98661. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.95073/4.98364. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.94341/4.97983. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.94572/4.98164. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.94467/4.99193. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.94866/4.97984. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.95097/4.97857. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.95128/4.98013. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.94941/4.98177. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.95742/5.00493. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.96411/4.97727. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.96190/4.97990. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.96503/4.97974. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.95526/4.97943. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.95239/4.97461. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.95309/4.97736. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.94955/4.96497. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.95392/4.96761. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.95244/4.97479. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.95327/4.96621. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.94416/4.97364. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.94492/4.97663. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.94761/4.97143. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.93930/4.98451. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.93974/4.98208. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.94467/4.98267. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.94066/4.98079. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.94306/4.97527. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.94578/4.97095. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.93918/4.97485. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.93976/4.97559. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.94235/4.98406. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.93860/4.98478. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.94161/4.96978. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.94127/4.98634. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.93584/4.98116. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.94765/4.96816. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.03643724696356275\n",
      "Epoch 0, Loss(train/val) 4.89077/4.80693. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.83544/4.81044. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.83064/4.80488. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.82962/4.80028. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.83232/4.79823. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.82574/4.80187. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.82495/4.80344. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.82430/4.80269. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.82275/4.80396. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.82163/4.80522. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.82194/4.80471. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.82169/4.80695. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.82289/4.80290. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.82314/4.80228. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.82329/4.80371. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.82018/4.80551. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.82242/4.80280. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.81924/4.80449. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.81751/4.79920. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.81770/4.81172. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.82432/4.80528. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.82154/4.80347. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.82101/4.81045. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.81830/4.81492. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.81579/4.80678. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.81807/4.79688. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.81700/4.80036. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.81507/4.79819. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.81232/4.78974. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.81362/4.78751. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.81197/4.78567. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.81023/4.78721. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.80888/4.78431. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.81296/4.78815. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.80455/4.78051. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.80595/4.78251. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.80240/4.78654. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.80673/4.78929. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.80922/4.78577. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.80500/4.79118. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.80293/4.78170. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.80592/4.78128. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.79947/4.78046. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.80254/4.77552. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.79610/4.78055. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.79796/4.78135. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.79638/4.79090. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.79789/4.78611. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.80098/4.79251. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.79141/4.80129. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.80010/4.78014. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.79518/4.79598. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.79552/4.79618. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.79573/4.78576. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.79279/4.78197. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.79477/4.79282. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.78904/4.79296. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.79040/4.77969. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.79533/4.77884. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.79769/4.80199. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.78568/4.79878. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.79525/4.81312. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.78788/4.81593. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.78856/4.81562. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.78850/4.80045. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.78585/4.78620. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.78713/4.79757. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.78635/4.80626. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.78303/4.79919. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.78885/4.81634. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.78631/4.80733. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78196/4.81353. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.78535/4.82053. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.78810/4.80928. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.77870/4.81690. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.78131/4.82013. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78312/4.82561. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.77731/4.84267. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.78778/4.84558. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.78082/4.86247. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.78062/4.83596. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.78203/4.82153. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.77857/4.81643. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.78181/4.82044. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.77847/4.81918. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.77534/4.82726. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.77793/4.83208. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.78416/4.83235. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.76952/4.83842. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.78361/4.82429. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.77798/4.84058. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.77777/4.84273. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.77689/4.82311. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.78621/4.81936. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.78080/4.83358. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.77663/4.86227. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.77550/4.81598. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.77305/4.84091. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77046/4.84583. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.77707/4.84236. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.018049705127885604\n",
      "Epoch 0, Loss(train/val) 4.73585/4.80767. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.71663/4.76751. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.71010/4.74307. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.70890/4.74830. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.70945/4.75281. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.70596/4.75769. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.70844/4.76693. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.70671/4.77582. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.70960/4.78917. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.70665/4.79241. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.70754/4.79072. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.70850/4.78178. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.70843/4.76910. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.70658/4.76828. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.70627/4.76972. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.70327/4.76815. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.70296/4.78164. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.70298/4.78584. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.70618/4.77229. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.70279/4.76827. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.70258/4.76909. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.70449/4.76326. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.70605/4.75975. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.69753/4.77831. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.70272/4.76539. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.69696/4.76095. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.69990/4.75380. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.68958/4.76417. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.69128/4.76313. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.69375/4.77281. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.69298/4.75950. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.69045/4.76274. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.69002/4.76384. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.69316/4.75027. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.69318/4.76301. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.69318/4.76548. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.68906/4.76117. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.68603/4.77737. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.68886/4.77118. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.68975/4.75948. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.68440/4.75731. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.69016/4.76356. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.68433/4.75241. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.68623/4.76958. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.68559/4.75712. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.68516/4.75948. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.68346/4.76632. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.67969/4.79181. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.68923/4.76470. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.68473/4.77719. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.68045/4.79107. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.68578/4.76704. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.68155/4.77464. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.67483/4.77787. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.68500/4.79206. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.68297/4.75724. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.68254/4.74167. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.68051/4.75957. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.67520/4.81639. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.68407/4.74320. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.67541/4.75960. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.68081/4.75981. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.67826/4.75345. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.67041/4.77161. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.67339/4.74587. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.67547/4.77520. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.67105/4.77763. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.67454/4.81001. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.66836/4.77749. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.67282/4.73684. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.66803/4.76705. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.67731/4.76578. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.66910/4.77360. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.67646/4.75596. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.67054/4.75868. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.66739/4.76679. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.67074/4.78998. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.66894/4.77046. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.67252/4.77237. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.66458/4.75977. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.66898/4.75450. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.67120/4.78166. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.66349/4.78498. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.68217/4.75268. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.67763/4.76153. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.67530/4.79312. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.67344/4.75524. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.66907/4.77340. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.66951/4.75803. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.67466/4.77278. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.67576/4.76720. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.67297/4.76555. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.67930/4.76985. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.69014/4.75399. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.68589/4.76077. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.68312/4.76706. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.68135/4.78477. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.67887/4.77741. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.67798/4.77719. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.67546/4.77552. Took 0.08 sec\n",
      "ACC: 0.390625, MCC: -0.21425126095413888\n",
      "Epoch 0, Loss(train/val) 5.06477/5.02651. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.03352/5.08050. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.02452/5.08822. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.02862/5.08762. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.02946/5.06419. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 5.02627/5.04358. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.02320/5.03862. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.01847/5.04969. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.01868/5.04604. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.01513/5.05434. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 5.01775/5.04758. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.01690/5.04477. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.01492/5.04712. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.01230/5.06379. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.01106/5.06421. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.01225/5.05498. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.01182/5.06922. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.01273/5.05865. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.01115/5.05655. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.01070/5.06501. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.01162/5.05829. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.00481/5.06616. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.00722/5.07110. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.00987/5.07188. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.00513/5.07465. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.00709/5.06483. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 5.00589/5.06384. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.00391/5.06174. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.00761/5.06261. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.00112/5.08169. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.01394/5.00668. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.02603/5.01018. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 5.02077/5.00991. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.01385/5.04649. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 5.00723/5.04835. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.00522/5.03766. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.00881/5.01860. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.01603/5.01934. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.00715/5.02227. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.00976/5.04321. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.00276/5.06878. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.00368/5.06089. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.99926/5.06358. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.99945/5.06190. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.00360/5.05856. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.00284/5.05121. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.99723/5.07624. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.99975/5.06436. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.99962/5.06868. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.00281/5.05786. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.99539/5.06961. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 5.00213/5.04949. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.99748/5.04561. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.99664/5.06612. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.99737/5.08376. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.99779/5.08817. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.00022/5.05844. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.99840/5.07369. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.99734/5.07093. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.00202/5.06344. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.00510/5.06322. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.00378/5.06294. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.00280/5.07337. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.99937/5.08558. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.99953/5.08371. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.99886/5.07271. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.99659/5.08087. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.99965/5.05543. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.99864/5.08123. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.00359/5.04922. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.99652/5.06428. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.99602/5.07029. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.99612/5.06039. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.99852/5.05771. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.99662/5.04591. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.99201/5.05585. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.99168/5.06513. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.99597/5.06858. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.99992/5.05981. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.00229/5.05723. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.99055/5.06989. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.99162/5.07985. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.98979/5.08128. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.99424/5.05748. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.99103/5.07523. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.99361/5.08703. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.98946/5.08133. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.99093/5.06083. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.98657/5.06374. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.99476/5.06182. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.99680/5.05148. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.99292/5.12766. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.00411/5.09501. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 5.00027/5.06521. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.99504/5.06657. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.98917/5.08099. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.99416/5.05620. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.99005/5.07337. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.99273/5.07538. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.98819/5.13933. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.11221803979288456\n",
      "Epoch 0, Loss(train/val) 4.54688/4.53123. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.50565/4.51896. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.49891/4.50436. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.49158/4.50312. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.49226/4.50661. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.49131/4.51177. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.49391/4.51469. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.49224/4.50809. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.49073/4.51034. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.48904/4.51358. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.49018/4.51970. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.48708/4.51970. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.48878/4.52001. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.48575/4.51351. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.48746/4.53154. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.49046/4.52030. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.48700/4.53253. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.48528/4.52927. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.47984/4.53905. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.47948/4.53374. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.47777/4.53475. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.47566/4.53789. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.47539/4.53103. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.47465/4.53945. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.47861/4.53765. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.47569/4.52969. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.48044/4.51665. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.48078/4.50648. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.47586/4.51663. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.47356/4.52801. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.47383/4.53092. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.48120/4.51417. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.48536/4.51341. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.47941/4.51800. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.47443/4.51731. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.47652/4.53083. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.47505/4.54952. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.47533/4.52812. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.47189/4.52999. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.46966/4.53333. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.47419/4.52579. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.47624/4.51506. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.47262/4.52068. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.46466/4.52705. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.47109/4.52910. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.48101/4.53907. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.47067/4.54346. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.47184/4.53772. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.46684/4.54327. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.47162/4.54861. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.46862/4.55439. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.46730/4.56914. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.46823/4.51532. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.46911/4.53445. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.47087/4.52088. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.47160/4.53908. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.46561/4.54771. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.46674/4.54496. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.47371/4.54401. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.47777/4.51487. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.48338/4.49691. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.47943/4.49797. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.48021/4.49485. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.47982/4.48957. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.47557/4.49694. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.47061/4.51352. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.47798/4.51131. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.47328/4.51748. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.47221/4.49330. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.47429/4.49841. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.47192/4.52750. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.47159/4.53567. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.47900/4.53170. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.46676/4.55060. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.48122/4.50166. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.46937/4.49816. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.47275/4.51375. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.46382/4.52281. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.47138/4.51716. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.46946/4.52571. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.46946/4.54141. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.46391/4.52299. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.46674/4.53809. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.46613/4.53063. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.47071/4.52070. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.46003/4.52724. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.45878/4.55366. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.46886/4.51911. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.46447/4.54033. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.46352/4.54048. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.46199/4.54196. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.46609/4.53480. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.46461/4.53564. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.46154/4.54884. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.46386/4.55360. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.46598/4.54924. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.46418/4.53901. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.46138/4.54626. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.46343/4.54959. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.46322/4.54128. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.02404930351219409\n",
      "Epoch 0, Loss(train/val) 4.82447/4.83818. Took 0.18 sec\n",
      "Epoch 1, Loss(train/val) 4.81101/4.80032. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 4.79511/4.80071. Took 0.11 sec\n",
      "Epoch 3, Loss(train/val) 4.79949/4.80145. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79897/4.80009. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.80182/4.79700. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.80197/4.79876. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.79998/4.80325. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.79686/4.80116. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79600/4.80029. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79629/4.79845. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.79484/4.79815. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.79696/4.79734. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.79253/4.79472. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.79395/4.79388. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.79397/4.79339. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.79162/4.79453. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.79216/4.79260. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.79060/4.79028. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.79108/4.79330. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.79037/4.79241. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.78837/4.79190. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.78878/4.79031. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.78847/4.78734. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.78922/4.79358. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.78967/4.79731. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.78071/4.79969. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.78604/4.78658. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.78589/4.79340. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.78337/4.79214. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.78863/4.79738. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78725/4.79636. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.78825/4.79662. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78530/4.79973. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78436/4.80063. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.78697/4.79660. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.78013/4.80473. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.77978/4.81578. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.78691/4.79579. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.78303/4.80577. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.78614/4.80431. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.78263/4.79363. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.78056/4.80546. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.77947/4.80823. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.78409/4.79927. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.77607/4.82022. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.78012/4.79916. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77999/4.80936. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.77566/4.80816. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.77738/4.80483. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77768/4.80593. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77279/4.82570. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.77272/4.82294. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.76826/4.83138. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.77111/4.80947. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.77561/4.81872. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.78006/4.81192. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.76872/4.81565. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76787/4.82995. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.77483/4.80330. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.77168/4.82776. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.77178/4.81463. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76634/4.81730. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.77140/4.81947. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.77413/4.82911. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.76804/4.82645. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.76838/4.82586. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.76870/4.83040. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.76912/4.81576. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76478/4.82562. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.76578/4.83217. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.76523/4.83645. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76547/4.82315. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.76551/4.82474. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.77274/4.81285. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.76747/4.81512. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.77273/4.84541. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.77195/4.81906. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.76056/4.82619. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.76689/4.83434. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.76583/4.84680. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.76148/4.84262. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.76969/4.83658. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.75968/4.83544. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.76189/4.83477. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.76686/4.83493. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.76769/4.84921. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.76294/4.84009. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.76071/4.83897. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.75772/4.87273. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.75866/4.86282. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.75870/4.87746. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.76075/4.84417. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.76297/4.86121. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.76031/4.83737. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.75904/4.83619. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.76251/4.84694. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.75500/4.85542. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.76086/4.83832. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.75245/4.87448. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.1259881576697424\n",
      "Epoch 0, Loss(train/val) 5.05985/5.00042. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.99440/4.99210. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.98104/5.00376. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.97823/5.00129. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.97527/4.99999. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.97503/5.00672. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.97441/5.00593. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.97354/5.01336. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.97457/5.00662. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.97286/5.01745. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.96995/5.01771. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.96727/5.02028. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.96595/5.02865. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.96792/5.02975. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.96519/5.04952. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.96536/5.05132. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.96694/5.04466. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.95909/5.05732. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.96057/5.05533. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.96896/5.02710. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.96986/4.99976. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.96744/4.99678. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.97022/5.01205. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.96521/5.00838. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.96604/5.01735. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.96295/5.02493. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.95923/5.02643. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.96119/5.01218. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.96139/5.01487. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.95950/5.02769. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.95775/5.02582. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.96218/5.04314. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.96443/5.00304. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.96137/5.01227. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.95755/5.04632. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.96017/5.06729. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.96259/5.02418. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.95654/5.02011. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.96366/4.99257. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.96768/4.99097. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.96521/4.99428. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.96105/4.99527. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.96705/4.99347. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.96131/5.00424. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.96931/4.99940. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.96324/5.03041. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.97518/4.97896. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.97277/4.97816. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.97059/4.98258. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.97182/4.98566. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.96929/4.98639. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.96913/4.98873. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.96927/4.99106. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.97016/4.99202. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.96919/4.99827. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.96605/4.99868. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.96570/4.99968. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.96477/5.00483. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.96767/5.00187. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.96718/4.99974. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.96606/5.00118. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.96840/5.00323. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.96624/5.00468. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.96467/5.00253. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.95970/5.00973. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.96535/5.00216. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.96484/4.98893. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.96784/4.98836. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.96355/5.01502. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.96870/5.01040. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.96397/5.00709. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.95797/5.00590. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.96245/5.00780. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.95297/5.01953. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.96884/4.98268. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.96409/4.99109. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.96436/5.00749. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.96001/5.00433. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.95800/5.00260. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.95546/5.03494. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.96226/5.01149. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.95847/5.00645. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.96366/5.00042. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.96564/4.99115. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.95702/5.00013. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.95767/4.99776. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.95091/5.03039. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.95760/5.01745. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.95594/5.00833. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.95279/5.01861. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.94940/5.02280. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.95424/5.02059. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.95154/5.02777. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.96838/4.98982. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.96569/4.99134. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.95332/4.99458. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.96023/5.00545. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.95174/5.00703. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.96707/5.00523. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.96133/5.01671. Took 0.08 sec\n",
      "ACC: 0.5, MCC: -0.0009775171065493646\n",
      "Epoch 0, Loss(train/val) 4.90099/4.79810. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.81862/4.81939. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.81484/4.80428. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.80650/4.80333. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.80384/4.80900. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.80199/4.81395. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.80575/4.81531. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.80685/4.81419. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.80656/4.81517. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.80094/4.81789. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.80080/4.82018. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.80030/4.82732. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.79997/4.82384. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.80040/4.82827. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.79729/4.82391. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.79500/4.83683. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.79466/4.83725. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.80072/4.82025. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.80028/4.82536. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.79499/4.83806. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.79104/4.84590. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.79232/4.85020. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.79038/4.84386. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.79749/4.84423. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.78802/4.85329. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.78625/4.86205. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.78943/4.83881. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.79017/4.85886. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.78484/4.87054. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.78662/4.87058. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.78773/4.87062. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78532/4.86928. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.78638/4.86185. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78275/4.88309. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78071/4.87540. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.78363/4.87724. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.78638/4.86762. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.78257/4.87373. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.78323/4.87877. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.77983/4.89362. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.78586/4.86591. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.77924/4.89930. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.77911/4.89268. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.77967/4.87050. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.78138/4.88094. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.78038/4.88052. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77608/4.89064. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.78447/4.86144. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.79050/4.85798. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.78223/4.88146. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77739/4.87665. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77876/4.89250. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.77647/4.88979. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.78072/4.88632. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.78084/4.88255. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.78653/4.86248. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.78091/4.86822. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.77911/4.88449. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.77270/4.88519. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.77765/4.87646. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.77882/4.88740. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.78288/4.87599. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.77321/4.91819. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.77508/4.89619. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.77942/4.89794. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.77377/4.89953. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.77386/4.88041. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.77428/4.90240. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.77226/4.91730. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.77251/4.90316. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.77436/4.89979. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.77010/4.89961. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.77559/4.89551. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.76987/4.90965. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.77622/4.90731. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.76388/4.91644. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.77165/4.92375. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.76996/4.91011. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.77338/4.87937. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.77963/4.88179. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.77703/4.89783. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.77224/4.89660. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.77483/4.90111. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76706/4.90792. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.76823/4.89824. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.77474/4.90458. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.77364/4.89219. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.77423/4.90957. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.76923/4.91961. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.76706/4.90201. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.76421/4.92234. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.76904/4.90417. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.78173/4.87447. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.78044/4.89038. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.77398/4.90325. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.77130/4.90245. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.76946/4.89331. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.76976/4.90857. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.76491/4.93663. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.76650/4.93647. Took 0.08 sec\n",
      "ACC: 0.609375, MCC: 0.21971768720102058\n",
      "Epoch 0, Loss(train/val) 4.85409/4.84147. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.81139/4.80987. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.81179/4.81699. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.81017/4.82163. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.80577/4.81515. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.80578/4.80822. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.80172/4.80803. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.80343/4.80931. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.80336/4.81063. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.80194/4.80907. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.80137/4.80948. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.80225/4.80983. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.80117/4.80851. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.80102/4.80759. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.79892/4.80891. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.79979/4.80892. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.79828/4.80774. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.80142/4.80869. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.79962/4.80884. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.79906/4.80861. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.79858/4.80818. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.80079/4.80842. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.79843/4.80460. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.79703/4.80696. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.79789/4.80659. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.79656/4.81008. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.79853/4.81048. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.79554/4.80993. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.79432/4.81991. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.79385/4.82481. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.79424/4.81854. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.79258/4.82587. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.79383/4.82402. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.79482/4.82098. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78870/4.82579. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.79080/4.82980. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.79098/4.83020. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.79277/4.82021. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.79012/4.81851. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.78964/4.83659. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.79812/4.81146. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.79025/4.81354. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.78894/4.82440. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.78943/4.83448. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.78846/4.83659. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.79124/4.83411. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.79423/4.81580. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.79263/4.81071. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.79601/4.81337. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.79596/4.81024. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.79444/4.80728. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.79048/4.81018. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.79151/4.81644. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.79323/4.81454. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.79245/4.80988. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.79032/4.81674. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.78880/4.82064. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.78860/4.82467. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.78611/4.82261. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.78778/4.82625. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.79019/4.82420. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.78519/4.82698. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.79120/4.82306. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.78579/4.82009. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.78017/4.83360. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.78809/4.82956. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.78187/4.82852. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.78356/4.83191. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.78297/4.83753. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.78295/4.82840. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.77918/4.85907. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78521/4.83785. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.77919/4.84474. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.78222/4.84043. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.77810/4.83502. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.78124/4.83690. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78146/4.83885. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.78244/4.83179. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.77952/4.84196. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.77611/4.83797. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.77652/4.83811. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.79175/4.83337. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.78605/4.83000. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.78016/4.84611. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.77665/4.85652. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.78006/4.83691. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.78048/4.82022. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.78300/4.82078. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.78074/4.82885. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.78344/4.84298. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.78231/4.83046. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.77806/4.84679. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.78173/4.83920. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.77824/4.83924. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.77818/4.84216. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.77284/4.86571. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.78456/4.83798. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.78173/4.83651. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77423/4.85888. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.77726/4.86644. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.0778902520624173\n",
      "Epoch 0, Loss(train/val) 4.98787/4.97729. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.97935/4.96827. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.96393/4.96457. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.96760/4.96438. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.96265/4.96323. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.96661/4.96263. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.96518/4.96233. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.96254/4.96299. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.96372/4.96365. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.96588/4.96434. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.96317/4.96672. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.96259/4.96613. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.96433/4.96443. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.95948/4.96453. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.95991/4.96356. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.95674/4.96084. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.96007/4.96189. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.95555/4.96408. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.95555/4.96557. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.95584/4.96153. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.95564/4.95893. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.95569/4.95729. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.94862/4.95556. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.95344/4.95457. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.95115/4.95487. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.95210/4.96002. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.95120/4.96003. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.95084/4.96483. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.95592/4.95458. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.95078/4.94796. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.94891/4.95118. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.94500/4.95292. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.94739/4.95465. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.94682/4.95031. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.94946/4.95409. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.94618/4.95164. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.94767/4.95176. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.94238/4.94876. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.94724/4.95689. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.94045/4.95190. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.94610/4.95346. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.94137/4.95127. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.94281/4.94735. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.94455/4.95377. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.94066/4.94820. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.94024/4.95620. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.94309/4.95675. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.94172/4.95615. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.94695/4.95912. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.93960/4.97005. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.94288/4.96091. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.94352/4.96149. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.93905/4.96251. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.93431/4.96548. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.94522/4.95781. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.94028/4.96150. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.93672/4.96045. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.94091/4.95957. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.93338/4.96146. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.94548/4.95892. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.93688/4.96115. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.93610/4.96257. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.93689/4.96516. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.94098/4.95501. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.93268/4.96827. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.93585/4.95349. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.93571/4.96775. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.93527/4.96614. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.93438/4.97435. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.93962/4.96039. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.93415/4.96007. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.93753/4.96118. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.94000/4.96647. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.93605/4.97322. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.93297/4.95692. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.93135/4.95263. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.93363/4.95580. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.93915/4.96300. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.93579/4.97714. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.93510/4.96446. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.93260/4.96898. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.93145/4.96420. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.92704/4.95995. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.93598/4.96315. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.93293/4.96561. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.93287/4.96345. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.92965/4.96444. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.93022/4.97417. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.93575/4.95902. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.93143/4.98257. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.92417/4.98258. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.93393/4.97053. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.93236/4.96950. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.92483/4.97382. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.92360/4.97015. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.93046/4.96656. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.93328/4.97534. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.92516/4.96748. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.93006/4.96984. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.92322/4.97009. Took 0.08 sec\n",
      "ACC: 0.578125, MCC: 0.16150120428611295\n",
      "Epoch 0, Loss(train/val) 4.89641/4.84414. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.86053/4.84566. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.85280/4.84126. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.84793/4.84100. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.84545/4.84247. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.84601/4.84662. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.84439/4.84924. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.84491/4.85443. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.84443/4.85807. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.84230/4.85860. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.84247/4.86190. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.83931/4.87168. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.84587/4.85830. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.83881/4.85526. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.83993/4.86180. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.83577/4.85792. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.83632/4.85775. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.83504/4.85129. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.83508/4.85969. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.83075/4.85457. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.83426/4.85811. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.83325/4.86219. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.84114/4.89286. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.84319/4.83718. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.84059/4.83894. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.83810/4.83687. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.83632/4.83590. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.83575/4.83608. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.83631/4.84448. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.83594/4.84103. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.83151/4.84382. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.83305/4.84039. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.83319/4.84293. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.82823/4.84173. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.82647/4.84140. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.82707/4.84524. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.82831/4.83787. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.83016/4.84794. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.82223/4.85054. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.82506/4.85004. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.82626/4.85041. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.82594/4.84014. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.82623/4.84809. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.82330/4.85018. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.82617/4.84458. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.82168/4.85370. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.82292/4.84505. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.82374/4.84276. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.82084/4.85195. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.82181/4.85561. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.82377/4.85375. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.82050/4.84782. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.82445/4.85327. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.81890/4.84835. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.82225/4.85114. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.82298/4.86061. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.81848/4.85030. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.82554/4.84723. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.82240/4.83873. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.82063/4.84962. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.81486/4.85142. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.82206/4.84476. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.81813/4.84929. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.81969/4.83542. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.82170/4.82950. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.81751/4.84143. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.82290/4.84617. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.81596/4.84215. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.81354/4.83995. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.81483/4.83711. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.81381/4.83427. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.81551/4.84043. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.81870/4.83502. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.81508/4.83593. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.81447/4.83161. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.82289/4.82952. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.81387/4.84049. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.81041/4.84375. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.81283/4.82788. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.81141/4.83160. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.81349/4.84502. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.80785/4.84112. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.81730/4.84090. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.81117/4.83453. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.81428/4.85261. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.81406/4.84392. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.80852/4.85224. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.81168/4.83110. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.80923/4.83874. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.81990/4.83968. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.80892/4.86164. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.81628/4.84168. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.80998/4.84222. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.80924/4.83747. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.81178/4.84589. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.81318/4.84938. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.80834/4.84907. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.80666/4.86155. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.81346/4.83085. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 4.81036/4.84276. Took 0.10 sec\n",
      "ACC: 0.484375, MCC: -0.0020131159011798102\n",
      "Epoch 0, Loss(train/val) 4.73730/4.79493. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.71150/4.70965. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.70841/4.67875. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.71166/4.67206. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.70630/4.67610. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.70513/4.68426. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.70139/4.68135. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.70120/4.68040. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.69978/4.67762. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.70092/4.67384. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.69824/4.67715. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.69836/4.67862. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.69488/4.67870. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.69768/4.67224. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.69666/4.67212. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.69665/4.67192. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.69623/4.66467. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.69168/4.66483. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.68953/4.66678. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.69619/4.65379. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.70167/4.67791. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.69508/4.67988. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.69523/4.67396. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.69322/4.67836. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.69745/4.67425. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.69386/4.67848. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.69517/4.67897. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.69614/4.66663. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.69756/4.66783. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.69645/4.66754. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.69660/4.67345. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.69620/4.67585. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.69243/4.66764. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.69372/4.67269. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.69386/4.67539. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.69178/4.67404. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.68939/4.67204. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.69037/4.67352. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.69033/4.67700. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.68827/4.67599. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.68476/4.68697. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.68571/4.67652. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.68573/4.66867. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.68498/4.66642. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.68454/4.67588. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.68070/4.67924. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.68388/4.67728. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.68141/4.67197. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.68462/4.67345. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.67855/4.67242. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.67598/4.65934. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.67942/4.66872. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.67644/4.67858. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.67801/4.66973. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.67138/4.67205. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.68668/4.68278. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.67729/4.67792. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.67610/4.67689. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.67044/4.67730. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.67039/4.68100. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.67915/4.67298. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.67480/4.67438. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.67447/4.67330. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.67202/4.67096. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.67765/4.67147. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.67586/4.67253. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.66811/4.67328. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.67045/4.66375. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.67478/4.67869. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 4.66631/4.68217. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.66508/4.68835. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.66935/4.67961. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.66528/4.68648. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.67586/4.67987. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.66781/4.66521. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.66787/4.68938. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.66428/4.68450. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.65986/4.69998. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.67386/4.69528. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.66999/4.67490. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.66721/4.68854. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.66527/4.70290. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.67165/4.67696. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.66372/4.67658. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.66279/4.69901. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.66650/4.69087. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.66226/4.67709. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.66441/4.68790. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.66450/4.68756. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.65832/4.72511. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.66626/4.68607. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.66602/4.68125. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.66369/4.68558. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.66233/4.70855. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.65714/4.70371. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.66109/4.71775. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.66132/4.71222. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.66082/4.68751. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.66249/4.70814. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.65852/4.71826. Took 0.09 sec\n",
      "ACC: 0.421875, MCC: -0.13315005718713654\n",
      "Epoch 0, Loss(train/val) 4.83629/4.79995. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.78588/4.79327. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.78077/4.78321. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.77979/4.78107. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.77675/4.78265. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.77632/4.78602. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.77634/4.78896. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.77635/4.79089. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.77650/4.79103. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.77341/4.79133. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.77480/4.78774. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.77282/4.78735. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.77186/4.78469. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.77192/4.78989. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.76949/4.79176. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.77116/4.79316. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.77039/4.79026. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.77255/4.79103. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.76599/4.79795. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.77038/4.79395. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.76895/4.79140. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.76684/4.79684. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.76641/4.79775. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.76377/4.79961. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.76463/4.79938. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.76652/4.79627. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.76210/4.79968. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.76638/4.79317. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.76013/4.79445. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.75983/4.78711. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.75993/4.78471. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.75821/4.80132. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.75883/4.78188. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.75783/4.79847. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.75769/4.80893. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.76153/4.78746. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.76179/4.79224. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.75272/4.79514. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.75881/4.78941. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.75205/4.80189. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.75038/4.78493. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.75239/4.78025. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.74955/4.78944. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.74476/4.79252. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.74769/4.81125. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.75274/4.79341. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.75421/4.79575. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.74381/4.78949. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.75454/4.79362. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.75103/4.79347. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.74496/4.81875. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.74101/4.79978. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.74951/4.76831. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.74765/4.78931. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.73795/4.78588. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.74435/4.78200. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.73946/4.77756. Took 0.07 sec\n",
      "Epoch 57, Loss(train/val) 4.74343/4.79338. Took 0.07 sec\n",
      "Epoch 58, Loss(train/val) 4.73926/4.78972. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.73428/4.77825. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.73682/4.78245. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.73968/4.77453. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.73284/4.81016. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.73822/4.79185. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.74588/4.78955. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.73692/4.80550. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.73929/4.80174. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.73257/4.81138. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.73735/4.80711. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.73892/4.77748. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.73502/4.80759. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.73617/4.78260. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.73357/4.79238. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.73239/4.78575. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.74174/4.77293. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.73038/4.79267. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.73323/4.78931. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.73088/4.78627. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.72665/4.82586. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.73758/4.79612. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.73313/4.79483. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.73385/4.78707. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.73421/4.78544. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.72359/4.79902. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.72230/4.80913. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.73795/4.79073. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.72918/4.79978. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.73049/4.78725. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.72753/4.78344. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.73180/4.78236. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.72724/4.83935. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.72904/4.79202. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.72410/4.79574. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.72485/4.81539. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.72868/4.80530. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.72553/4.76677. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.73241/4.79754. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.73306/4.77046. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.73005/4.79392. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.71559/4.81766. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.11221803979288456\n",
      "Epoch 0, Loss(train/val) 4.98268/4.92976. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.95964/4.93274. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.95170/4.93978. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.94855/4.94242. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.94768/4.94236. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.94692/4.94791. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.95056/4.94791. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.94557/4.94620. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.94532/4.94149. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.94517/4.93630. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.94227/4.93750. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.94311/4.94084. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.94425/4.94793. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.94468/4.94988. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.94285/4.95470. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.93931/4.96043. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.93984/4.96371. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.93950/4.96620. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.93862/4.96678. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.93964/4.96665. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.93765/4.96851. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.93728/4.96867. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.93600/4.97388. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.93511/4.97209. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.93737/4.96118. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.93929/4.95890. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.93550/4.96096. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.93647/4.95841. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.93575/4.93858. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.93563/4.93184. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.93836/4.94768. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.93750/4.93837. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.93772/4.94233. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.93473/4.94386. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.93287/4.94254. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.93512/4.94048. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.93439/4.94038. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.93691/4.95196. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.93702/4.94594. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.93410/4.96556. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.93097/4.97104. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.93211/4.97319. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.93006/4.96854. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.93449/4.96853. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.93095/4.98300. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.93125/4.96129. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.92718/4.97874. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.92809/4.97462. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.93438/4.96741. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.92489/4.97750. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.92472/4.98257. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.92503/4.97049. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.92226/4.98369. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.92971/4.96415. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.92671/4.96631. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.92617/4.99274. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.92977/4.97001. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.92307/4.98906. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.92896/4.96519. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.92178/4.97741. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.92922/4.96954. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.92577/4.98020. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.92297/4.97133. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.92199/4.95818. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.92273/4.98467. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.92533/4.96810. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.92234/4.98217. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.92969/4.98319. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.92183/4.97892. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.92407/4.97518. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.92284/4.98346. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.92871/4.98151. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.91576/4.98953. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.92025/4.97960. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.92292/4.98129. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.92042/4.97979. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.91861/4.98845. Took 0.11 sec\n",
      "Epoch 77, Loss(train/val) 4.91917/4.98762. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.91704/4.99165. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.92186/4.98242. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.91779/4.99093. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.91736/4.98536. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.91969/4.97276. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.92096/4.97919. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.91966/4.99520. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.91901/5.01026. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.92064/5.01411. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.91578/4.98609. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.92106/5.00361. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.92242/5.00376. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.92258/4.97279. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.92287/5.01134. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.92421/4.98049. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.92056/5.01178. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.91812/5.00815. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.91548/5.02867. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.92243/4.97926. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.92029/4.99868. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.91502/5.00533. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.91774/5.01909. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.08845235543978211\n",
      "Epoch 0, Loss(train/val) 4.74476/4.70999. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.70896/4.71112. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.70715/4.71802. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.70449/4.71786. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.69900/4.72168. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.69937/4.72283. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.69812/4.72232. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.70238/4.72445. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.69523/4.72535. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.69491/4.73142. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.69361/4.73809. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.69644/4.74428. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.69553/4.73471. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.69206/4.73673. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.69214/4.73970. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.69535/4.75139. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.68781/4.75147. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.68959/4.74372. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.69083/4.74323. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.68510/4.75061. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.69197/4.74413. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.69137/4.74718. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.68785/4.75209. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.68976/4.75045. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.68783/4.75961. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.68805/4.76097. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.68817/4.76558. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.69152/4.75779. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.68872/4.78173. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.68700/4.76405. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.68506/4.76624. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.68683/4.77957. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.68837/4.76775. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.68688/4.76910. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.68531/4.76250. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.68613/4.78345. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.68641/4.77611. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.68432/4.77165. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.68144/4.78293. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.68615/4.78105. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.68272/4.77314. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.68573/4.76637. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.68205/4.77495. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.68232/4.77899. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.68282/4.78311. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.68124/4.77932. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.68211/4.77171. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.67684/4.79708. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.67896/4.79043. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.67876/4.80922. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.67968/4.78382. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.68301/4.79609. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.68002/4.78458. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.67902/4.79021. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.67947/4.79324. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.67903/4.79594. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.67854/4.80466. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.67859/4.80920. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.68312/4.77862. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.68084/4.79606. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.67856/4.79530. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.68146/4.76842. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.67706/4.77110. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.67916/4.76405. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.67358/4.79632. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.67445/4.77704. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.68309/4.78362. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.68043/4.76518. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.67769/4.76463. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.67464/4.78482. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.67217/4.78686. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.67417/4.77707. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.67465/4.79220. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.67528/4.77630. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.67449/4.79365. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.67820/4.79375. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.67024/4.80514. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.67346/4.81422. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.67487/4.79428. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.67174/4.79621. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.67478/4.77217. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.67311/4.80480. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.67387/4.80933. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.67531/4.79496. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.67607/4.79456. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.67190/4.81840. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.67539/4.79010. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.67121/4.79319. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.66648/4.79867. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.67609/4.75842. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.66822/4.80511. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.67457/4.79579. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.67767/4.79212. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.66795/4.77054. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.67039/4.78658. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.67329/4.78235. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.67278/4.78994. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.67118/4.78499. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.67299/4.79771. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.67292/4.82031. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.15318083468998522\n",
      "Epoch 0, Loss(train/val) 5.07005/5.02306. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.97900/4.98011. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.97197/4.97490. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.97038/4.97538. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.97323/4.97365. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.96927/4.97263. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.97144/4.96893. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.97165/4.96975. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.96863/4.96914. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.97161/4.96885. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.96821/4.96646. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.97091/4.96762. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.96788/4.96505. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.96820/4.96369. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.96460/4.96584. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.96605/4.97003. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.96846/4.97173. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.96436/4.96956. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.96821/4.98058. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.96865/4.97811. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.96678/4.97540. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.96634/4.97547. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.96589/4.97337. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.96738/4.97836. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.96577/4.98045. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.96184/4.97527. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.96265/4.97456. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.96356/4.97787. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.96234/4.97883. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.95978/4.97612. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.96409/4.97591. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.95844/4.98266. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.95831/4.97857. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.95834/4.97323. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.96080/4.97481. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 4.96134/4.97446. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.95391/4.98823. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.95806/4.98806. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.95973/4.98926. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.95649/4.97996. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.95744/4.98831. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.95992/4.97708. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.95594/4.99414. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.95453/4.99659. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.95292/4.98661. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.94911/5.00200. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.95041/5.00211. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.94933/5.00180. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.94925/4.99642. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.95153/4.98863. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.94830/4.99424. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.95210/4.98846. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.95141/4.99956. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.94511/5.01695. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.94953/4.99417. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.94882/4.99403. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.95026/5.00006. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.95066/4.99627. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.94754/5.01128. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.94369/5.00589. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.94700/4.98474. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.94444/5.00909. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.94407/4.98896. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.95277/4.99916. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.94645/4.99696. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.94084/4.99841. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.94524/5.00245. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.94155/4.99808. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.94173/5.00795. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.93638/4.99933. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.94675/4.98907. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.93952/5.01169. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.94754/5.01106. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.94041/5.01888. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.94478/5.02530. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.93834/5.03328. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.93873/4.99846. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.94164/5.01378. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.94154/5.01388. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.94411/4.99622. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.95004/4.98658. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.94744/4.99969. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.94157/5.03418. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.94067/5.02238. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.93900/5.04911. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.93819/5.03483. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.93882/5.02363. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.93537/5.05626. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.93718/5.02182. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.93465/5.03167. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.93917/5.06950. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.93955/5.01455. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.93874/5.02352. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.93381/5.04991. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.94131/5.00289. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.92656/5.05277. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.93672/5.02399. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.92447/5.07819. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.93937/5.02092. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.93328/5.05656. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.13951311175942296\n",
      "Epoch 0, Loss(train/val) 5.01082/5.01135. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.96871/5.02390. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.95897/5.03701. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.96653/5.01374. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.96483/4.98208. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.95290/4.97657. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.94951/4.98917. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.95272/4.99683. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.95576/4.97780. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.95724/4.97826. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.95248/4.97901. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.95079/4.98366. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.95154/4.98521. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.95155/4.98846. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.95231/4.98478. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.94803/4.98889. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.94503/4.98368. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.94490/4.99003. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.94246/5.01493. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.94447/5.00886. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.94664/5.01350. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.94939/4.98732. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.94705/4.98900. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.93984/5.00657. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.94033/5.00622. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.94027/5.01928. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.94352/5.00612. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.93987/5.01549. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.94016/5.00405. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.93803/5.00671. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.93479/5.01909. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.93702/5.01949. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.94052/5.00610. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.93748/5.00232. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.93456/5.00987. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.93436/5.04115. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.93780/5.01749. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.93310/5.00955. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.92572/5.01934. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.93712/5.02036. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.93335/5.00004. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.92813/5.01479. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.92976/5.00156. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.92948/5.01267. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.93223/5.01078. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.92915/5.01121. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.92735/5.01151. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 4.92696/5.00646. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.92593/5.02674. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.93299/5.00969. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.92937/5.00320. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.92763/5.00386. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.92098/4.99846. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.92396/5.01321. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.93215/4.99651. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.92503/5.01318. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.92602/4.99880. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.92306/5.00415. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.93016/5.00725. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.92222/5.00311. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.91889/5.02739. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.92495/5.01788. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.92769/4.99882. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.92112/5.01605. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.92303/5.00003. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.93256/4.98763. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.94030/4.97009. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.93383/5.00150. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.93021/5.01701. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.93203/5.00450. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.92765/5.01396. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.93010/5.02641. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.92792/5.00050. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.92497/5.01953. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.92504/5.01261. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.92396/5.01818. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.92571/5.01700. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.92252/4.99889. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.92268/5.03417. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.92062/5.01650. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.92209/5.00872. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.92324/5.00287. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.92623/5.00087. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.92612/5.00724. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.92020/5.03270. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.92158/5.00851. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.92311/5.02669. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.91702/5.03970. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.92090/5.03765. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.92348/4.99874. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.92508/5.01559. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.91949/5.02291. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.92324/5.00103. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.92266/5.02645. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.92076/5.02441. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.92161/5.01737. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.91697/5.01521. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.92020/5.04081. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.91655/5.04155. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.92144/5.01055. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.03328442159263018\n",
      "Epoch 0, Loss(train/val) 4.67842/4.62427. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.63660/4.64508. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.62889/4.66235. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.63078/4.67202. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.62995/4.68323. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.62971/4.68443. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.62656/4.68816. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.62940/4.69013. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.62557/4.70014. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.62607/4.70609. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.62818/4.70383. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.62687/4.69726. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.62451/4.69762. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.62106/4.69085. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.61940/4.69665. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.62008/4.70467. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.62034/4.69539. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.62042/4.68569. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.61872/4.67706. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.61772/4.67562. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.61606/4.68125. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.62133/4.68404. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.61858/4.68746. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.61621/4.66600. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.62064/4.69207. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.61706/4.70659. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.61528/4.70691. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.61985/4.69196. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.61387/4.69640. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.61126/4.71115. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.61037/4.72623. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.61162/4.72450. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.61416/4.71503. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.61141/4.72236. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.61128/4.71510. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.60763/4.72768. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.60729/4.71819. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.60674/4.71124. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.60552/4.71427. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.60781/4.71067. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.61156/4.72008. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.60815/4.69569. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.60522/4.72087. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.60504/4.73735. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.60662/4.70734. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.60391/4.72611. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.60425/4.71274. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.61640/4.68900. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.61221/4.70706. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.60802/4.70827. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.61052/4.70311. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.60770/4.70885. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.60645/4.73082. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.60558/4.71591. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.60171/4.74917. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.60458/4.71775. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.60899/4.72111. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.60244/4.74138. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.60429/4.72828. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.60113/4.72102. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.60591/4.73776. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.60690/4.71749. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.60578/4.70561. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.60379/4.73374. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.60095/4.73653. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.59825/4.73659. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.60386/4.72439. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.60208/4.72078. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.59464/4.75434. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.60175/4.71879. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.60109/4.71910. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.60393/4.72587. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.59622/4.74121. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.59926/4.72880. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.59960/4.73959. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.59711/4.74068. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.59960/4.75667. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.59779/4.73365. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.60022/4.73079. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.60003/4.74172. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.59371/4.73486. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.59729/4.74332. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.59507/4.72840. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.59482/4.74596. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.59259/4.74932. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.59416/4.74023. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.59889/4.73860. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.60215/4.71470. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.59872/4.72738. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.59731/4.71995. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.60190/4.69765. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.59622/4.73292. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.60082/4.71962. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.59552/4.72117. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.58691/4.75301. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.59534/4.73706. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.59550/4.73080. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.59270/4.75054. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.58986/4.77078. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.58552/4.74803. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.12725695259515554\n",
      "Epoch 0, Loss(train/val) 4.76861/4.71254. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.72829/4.73160. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.72379/4.73545. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.72626/4.73552. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.72332/4.73661. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.72615/4.73061. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.72319/4.72340. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 4.72550/4.71699. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.72401/4.71317. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.72316/4.71052. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.72148/4.71112. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.72061/4.71037. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.72025/4.71226. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.72079/4.70913. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.71758/4.70770. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.72001/4.70790. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.72012/4.71087. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.71497/4.70995. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.71754/4.71372. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.71917/4.71205. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.71758/4.71478. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.71458/4.71093. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.71231/4.71470. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.71235/4.71827. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.71332/4.71672. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.71393/4.71682. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.71518/4.72131. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.71300/4.71896. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.71364/4.71940. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.71271/4.71840. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.71479/4.71968. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.71174/4.72121. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.71217/4.71989. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.71220/4.72088. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.71102/4.72552. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.70977/4.72680. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.71542/4.72563. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.70931/4.72530. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.71119/4.72512. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.71000/4.72443. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.71081/4.72705. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.70792/4.73111. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.70689/4.73843. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.70986/4.73614. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.70809/4.73318. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.71167/4.73402. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.70700/4.73560. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.70378/4.74036. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.70751/4.74161. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.71103/4.74220. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.70621/4.74191. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.70820/4.74211. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.70521/4.74879. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.70539/4.73838. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.70369/4.74232. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.70405/4.75094. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.70459/4.75256. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.70577/4.74240. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.70385/4.75188. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.70386/4.75147. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.70665/4.75195. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.70672/4.74472. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.70379/4.74363. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.70135/4.75416. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.70157/4.74561. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.70518/4.74628. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.70851/4.73675. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.70248/4.74240. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.70151/4.75339. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.70280/4.73948. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.70341/4.74091. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.70241/4.74589. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.70284/4.75475. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.70580/4.74219. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.70899/4.74101. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.69837/4.75747. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.70649/4.74228. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.70320/4.74422. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.70435/4.76226. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.70376/4.75870. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.70429/4.74750. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.69651/4.76393. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.69630/4.76437. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.70120/4.75339. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.70255/4.74676. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.70897/4.73970. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.70357/4.73295. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.70732/4.73429. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.70721/4.73345. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.70541/4.72968. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.70181/4.73834. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.70086/4.75199. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.70330/4.74344. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.70396/4.74663. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.69767/4.75250. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.70171/4.74597. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.69834/4.77351. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.69769/4.75268. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.69994/4.75118. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.69528/4.78099. Took 0.08 sec\n",
      "ACC: 0.59375, MCC: 0.15789473684210525\n",
      "Epoch 0, Loss(train/val) 5.10167/5.10027. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.03708/5.04495. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.03365/5.03044. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.03297/5.02884. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.03364/5.02631. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.03125/5.02541. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.02667/5.02600. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.02822/5.02678. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.02744/5.02570. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.02796/5.02584. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.02473/5.02814. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.02254/5.02679. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.02331/5.02846. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.02443/5.02950. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.02065/5.03006. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.02135/5.03237. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.02576/5.03121. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.02379/5.03145. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.02064/5.03180. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.02217/5.03543. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.01852/5.03653. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.02071/5.03548. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.01872/5.02996. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.02346/5.04990. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.02140/5.03582. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.01621/5.04766. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.01844/5.04609. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.01451/5.04562. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.01611/5.04926. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.01775/5.05786. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.01159/5.06433. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.01076/5.05838. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.01518/5.07869. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.02521/5.03547. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 5.01582/5.04344. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.01867/5.04471. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.01359/5.03528. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.01258/5.03685. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.01081/5.04258. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.01613/5.04063. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 5.01283/5.05094. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.01404/5.06550. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.01152/5.05713. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.01074/5.06134. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.01090/5.04761. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.01122/5.06770. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.00840/5.07878. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.01091/5.07425. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.00458/5.06746. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.01159/5.04397. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.01063/5.06167. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.00919/5.05980. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.01137/5.05226. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.00965/5.07551. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 5.00926/5.05047. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.00571/5.07529. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.01000/5.04955. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 5.00469/5.07031. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.00840/5.06196. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.00540/5.05937. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.00689/5.06431. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.00440/5.06820. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.00135/5.06938. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.00162/5.05845. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.00068/5.07445. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.00297/5.05460. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 5.00517/5.05602. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.00022/5.08625. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.00705/5.05561. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.00177/5.07997. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.99898/5.07661. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.00560/5.06603. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.00462/5.06102. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.99897/5.08305. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.99786/5.08221. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.00649/5.07244. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.00247/5.05145. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.99952/5.08124. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.99847/5.09344. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.00349/5.07479. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.99842/5.07943. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.99124/5.08292. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.99963/5.07080. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.00556/5.04942. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.99775/5.06058. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.99782/5.05284. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.99813/5.06856. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.99791/5.06968. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.99791/5.07306. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.99862/5.05124. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.99708/5.06143. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.99572/5.05901. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.99931/5.07793. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.99608/5.05708. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.99327/5.08331. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.99299/5.06259. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.99582/5.07599. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.99933/5.07377. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.99638/5.07800. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.99455/5.05037. Took 0.08 sec\n",
      "ACC: 0.390625, MCC: -0.20959097296420087\n",
      "Epoch 0, Loss(train/val) 5.08568/5.04740. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.05023/5.02525. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.04094/5.02435. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 5.03183/5.01683. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.02443/5.01489. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.02320/5.01447. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 5.02455/5.01656. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.02253/5.01508. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 5.02471/5.01558. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.02296/5.01476. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.01887/5.01456. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.01891/5.01496. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.01768/5.01652. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.01903/5.01504. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.01829/5.01180. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 5.01870/5.01354. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.01524/5.01312. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 5.01745/5.01344. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 5.01596/5.01323. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.01361/5.01275. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.01126/5.00552. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 5.01249/5.00682. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.01138/5.00904. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.01247/5.00470. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.01005/5.00432. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.00674/5.00170. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.00868/5.00863. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 5.00444/5.00541. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.00610/5.00173. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.00510/5.01109. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.00540/5.00701. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.00088/5.00967. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 5.00657/5.00800. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 5.00410/5.00289. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.99871/5.00244. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.00758/4.99536. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.00704/4.99058. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.00452/4.99165. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.00281/4.98988. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.99927/4.99351. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.00077/4.98977. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.00472/4.99297. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.00398/4.98770. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.00421/4.98372. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.99460/4.98447. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.00137/4.98271. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.00031/4.98903. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.99909/4.98712. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.00054/4.97618. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.99489/4.99369. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.99881/4.97914. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.00011/4.98314. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.99222/4.99097. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.99817/4.99015. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.99339/4.97964. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.99562/4.97238. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.99328/4.98422. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.99571/4.98188. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.99372/4.98359. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.99825/4.98314. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.98854/4.98675. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.99598/4.99093. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.99042/4.98522. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.99406/4.97751. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.99272/4.98702. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.99710/4.98241. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.99313/4.97737. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.99048/4.97995. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.98536/4.98824. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.98852/4.98881. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.98621/4.99460. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.99056/4.98444. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.99649/4.98970. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.99317/5.00872. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.98543/4.99768. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.99022/4.99041. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.99482/4.99678. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.99095/5.00154. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.98926/4.99025. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.98510/4.99901. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.99025/4.99413. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.99134/4.99083. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.98384/4.99429. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.98838/4.99885. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.98714/5.00063. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.98619/5.00213. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.98692/5.00557. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.98871/4.99707. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.99146/5.01613. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.98802/4.99950. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.98296/4.99983. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.99284/5.00213. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.99234/4.98567. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.98533/4.99660. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.98457/4.99297. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.99229/4.99961. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.99055/4.99269. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.98446/5.00163. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.98214/5.00067. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.98621/5.01045. Took 0.08 sec\n",
      "ACC: 0.40625, MCC: -0.16834512458535864\n",
      "Epoch 0, Loss(train/val) 4.96413/4.90725. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.91470/4.89812. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.91270/4.89921. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.91091/4.89814. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.91212/4.89556. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.90952/4.89517. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.90568/4.89386. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.90637/4.89406. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.90753/4.89369. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.90535/4.89283. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.90382/4.89157. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.90400/4.89237. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.90410/4.89155. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.90317/4.89158. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.90220/4.89035. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.90373/4.89205. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.90176/4.89071. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.90149/4.89082. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.89918/4.89151. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.89907/4.89037. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.89905/4.88992. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.89667/4.89498. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.90008/4.89363. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.89637/4.89604. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.89565/4.89216. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.89295/4.89603. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.89593/4.89472. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.88963/4.89550. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.89317/4.89364. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.89648/4.89555. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.89106/4.88925. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.89005/4.89431. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.88910/4.89157. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.88918/4.89641. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.90049/4.89246. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.88801/4.88882. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.88627/4.89027. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.88826/4.89069. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.89141/4.89155. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.88708/4.89416. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.88790/4.89035. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.88380/4.89188. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.88526/4.88637. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.88534/4.89323. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.88348/4.88807. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.87789/4.89819. Took 0.13 sec\n",
      "Epoch 46, Loss(train/val) 4.88624/4.89358. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.88381/4.88533. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.88394/4.89341. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.87674/4.88539. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.87627/4.88797. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.87556/4.88260. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.87916/4.88309. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.88035/4.87887. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.87008/4.90099. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.87192/4.88193. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.88180/4.89741. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.87539/4.89418. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.87501/4.89663. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.86991/4.89022. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.87587/4.89977. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.87111/4.89957. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.87074/4.88466. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.86478/4.87500. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.87571/4.91526. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.87541/4.89496. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.87361/4.88634. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.86328/4.90817. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.85900/4.90926. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.86764/4.90590. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.87549/4.88794. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.86777/4.90701. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.86360/4.91040. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.87623/4.90593. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.86979/4.89121. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.86008/4.91842. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.86250/4.89288. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.87243/4.89533. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.86323/4.88855. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.86737/4.89151. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.86533/4.90221. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.85714/4.94229. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.87276/4.92145. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.86850/4.90661. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.86061/4.89779. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.86701/4.90496. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.86385/4.91755. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.86041/4.92156. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.85856/4.89824. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.85559/4.90655. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.85565/4.91834. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.85799/4.89207. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.84877/4.91163. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.86693/4.89787. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.86346/4.91507. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.86160/4.91338. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.86177/4.90421. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.86988/4.92197. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.86714/4.90193. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.88727/4.89752. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.18963796023622476\n",
      "Epoch 0, Loss(train/val) 4.99257/4.97491. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.92318/4.93680. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.91698/4.92437. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.91702/4.91866. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.91005/4.91471. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.91033/4.91224. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.90871/4.90419. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.90422/4.90446. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.90943/4.90110. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.90720/4.90160. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 4.90892/4.90925. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.90654/4.90307. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.90582/4.89904. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.90396/4.90639. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.90158/4.90103. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.90695/4.90905. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.90291/4.90950. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.90331/4.91193. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.90337/4.91573. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.89946/4.90920. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.91103/4.90967. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.89887/4.90920. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.90271/4.91623. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.90224/4.91296. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.89647/4.91615. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.89907/4.91880. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.90050/4.91562. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.89982/4.91568. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.89870/4.91577. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.90012/4.91378. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.89551/4.91661. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.89574/4.91276. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.89527/4.92305. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.89657/4.92264. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.89583/4.92439. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.89511/4.92793. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.89510/4.93215. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.89462/4.92285. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.89331/4.91726. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.89528/4.92017. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.89634/4.92584. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.89410/4.93280. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.89417/4.91937. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.89265/4.92850. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.89507/4.91753. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.89005/4.92815. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.89632/4.92721. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.89064/4.93531. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.89048/4.92184. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.88871/4.92298. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.89626/4.92682. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.88880/4.92061. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.88545/4.92498. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.89358/4.92265. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.88846/4.92100. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.89477/4.92917. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.89051/4.93452. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.89003/4.92895. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.88692/4.92605. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.88787/4.93195. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.88293/4.94609. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89134/4.93319. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.88062/4.94122. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.88419/4.95360. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.88661/4.94384. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.88896/4.94757. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.88962/4.92113. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.89471/4.93768. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.89623/4.93333. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.89348/4.93650. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.89049/4.94163. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.88780/4.94617. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88923/4.94326. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88910/4.94791. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.89146/4.93988. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.88989/4.95030. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88647/4.95314. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.89033/4.94403. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.88789/4.94276. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.89080/4.93908. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.88400/4.94470. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.89017/4.94343. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.88816/4.94262. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.88564/4.94657. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.88201/4.95636. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.88559/4.95383. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.88461/4.94968. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.88569/4.94886. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.88075/4.95832. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88142/4.94672. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.89586/4.91134. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.89583/4.90691. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.88798/4.94904. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.88172/4.95322. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.88288/4.95256. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.88368/4.94220. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.89833/4.92140. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.89554/4.90812. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.89202/4.92229. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.88828/4.92727. Took 0.09 sec\n",
      "ACC: 0.53125, MCC: 0.059863071616150634\n",
      "Epoch 0, Loss(train/val) 4.77779/4.70806. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.71542/4.70517. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.71184/4.70529. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.71566/4.70082. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.71382/4.69896. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.71331/4.70079. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.71035/4.70343. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.71316/4.70542. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.71409/4.70709. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.71019/4.70665. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.71087/4.70404. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.71094/4.70107. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.71102/4.70334. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.70598/4.70656. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.70750/4.71275. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.70908/4.70879. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.70332/4.71290. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.70833/4.70939. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.70584/4.70930. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.70255/4.71303. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.70606/4.71795. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.70547/4.71143. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.70505/4.71324. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.70491/4.71435. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.70424/4.71521. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.70351/4.72131. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.70169/4.71856. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.70028/4.71172. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.70223/4.71515. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.70116/4.72146. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.70023/4.72397. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.69915/4.72801. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.69830/4.72348. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.69469/4.73350. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.69726/4.72351. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.69797/4.73357. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.69795/4.73034. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.69789/4.73125. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.69709/4.73098. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.69783/4.73033. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.69604/4.73164. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.69516/4.73012. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 4.69907/4.71760. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.69176/4.72721. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.69087/4.72788. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.69632/4.72656. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.69021/4.73489. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.68791/4.73606. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.69416/4.72721. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.69101/4.73865. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.69205/4.73858. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.68802/4.73169. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.69338/4.72263. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.68968/4.71821. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.68829/4.72265. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.68637/4.72436. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.67881/4.73475. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.68494/4.73478. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.70665/4.74295. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.69715/4.74220. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.68809/4.73494. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.69177/4.70076. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.68219/4.71491. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.68377/4.72185. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.68049/4.73446. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.67798/4.74170. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.67720/4.73989. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.67702/4.73215. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.67214/4.75567. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.67946/4.74843. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.67586/4.75123. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.67708/4.75466. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.67541/4.74689. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.67200/4.75077. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.67413/4.73708. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.67325/4.73933. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.67038/4.73037. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.69975/4.73500. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.68970/4.72723. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.68073/4.74928. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.67555/4.77496. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.67423/4.78431. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.67685/4.76369. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.67443/4.75060. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.66672/4.78127. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.67460/4.76645. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.67592/4.75661. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.67738/4.74932. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.67100/4.74939. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.66831/4.76586. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.67509/4.76363. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.67498/4.74968. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.67171/4.73702. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.66138/4.78207. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.66992/4.77342. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.66690/4.77206. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.66554/4.77494. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.66972/4.75802. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.67291/4.78372. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.68620/4.75532. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.11383959916814836\n",
      "Epoch 0, Loss(train/val) 4.78177/4.74136. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.73109/4.73995. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.72937/4.73663. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.72749/4.73541. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.72982/4.73803. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.72564/4.74169. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.72504/4.74105. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.72374/4.74561. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.72187/4.74996. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.72700/4.74965. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.72297/4.74365. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.72109/4.74575. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.72152/4.75488. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.72027/4.76469. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.71473/4.75843. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.71784/4.75648. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.71714/4.75656. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.71152/4.76629. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.71736/4.75320. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.71626/4.76390. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.71301/4.76350. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.71429/4.75557. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.71169/4.77112. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.71166/4.77140. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.70936/4.76963. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.70962/4.77729. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.70827/4.77785. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.70500/4.78524. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.71045/4.76952. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.70912/4.77297. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.70806/4.78754. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.71162/4.76928. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.71012/4.78369. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.70872/4.77928. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.70798/4.78279. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.70896/4.77906. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.70717/4.77368. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.70416/4.79999. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.71163/4.78190. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.69955/4.82246. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.70773/4.77051. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.70875/4.78038. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.70581/4.78624. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.70334/4.78676. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.71003/4.76911. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.70509/4.78890. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.70522/4.77657. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.69783/4.80310. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.70771/4.77871. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.70211/4.77683. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.69872/4.79665. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.69806/4.79907. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.70408/4.78980. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.70169/4.80515. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.70985/4.76968. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.70588/4.79535. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.70719/4.78187. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.71163/4.74696. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.71442/4.76812. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.71023/4.77527. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.70408/4.78592. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.70380/4.80017. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.70139/4.81012. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.70563/4.79788. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.70305/4.79661. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.70401/4.80736. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.69682/4.83272. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.70162/4.82485. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.70234/4.81358. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.69513/4.81856. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.69912/4.83312. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.69631/4.81606. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.69870/4.81398. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.69861/4.82546. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.69215/4.79406. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.69560/4.80945. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.69995/4.82597. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.69603/4.81568. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.69697/4.80163. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.70016/4.78944. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.69867/4.83032. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.69406/4.83619. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.69267/4.84464. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.69508/4.80514. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.69574/4.81653. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.69259/4.85927. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.69005/4.82176. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.68693/4.82169. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.69598/4.80377. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.71304/4.77396. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.70277/4.83532. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.70113/4.80144. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.69946/4.80071. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.69569/4.80410. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.69412/4.86036. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.70504/4.81779. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.70620/4.81923. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.69814/4.80893. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.69768/4.83414. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.70016/4.80544. Took 0.09 sec\n",
      "ACC: 0.609375, MCC: 0.2168404023533557\n",
      "Epoch 0, Loss(train/val) 4.77042/4.71330. Took 0.19 sec\n",
      "Epoch 1, Loss(train/val) 4.70413/4.71363. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 4.70370/4.70587. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.69980/4.69745. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.69755/4.69344. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.69229/4.69674. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.69085/4.70007. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.69174/4.69913. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.69091/4.69659. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.68987/4.69817. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.68734/4.70359. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.68541/4.71177. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.68718/4.71367. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.68479/4.71231. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.68504/4.71182. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.68044/4.71505. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.68290/4.70268. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.68168/4.69892. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.67795/4.71635. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.68148/4.69564. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.68051/4.70736. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.67941/4.70904. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.67879/4.69352. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.67853/4.70374. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.67840/4.70367. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.67793/4.70306. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.67997/4.70606. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.67564/4.70593. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.67499/4.69734. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.67202/4.71338. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.68021/4.68727. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.69122/4.67868. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.68427/4.68471. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.68114/4.68008. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.68215/4.69448. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.68261/4.69003. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.68022/4.68863. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.68236/4.68804. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.68231/4.68279. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.68568/4.70334. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.68630/4.70010. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.68159/4.70240. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.67947/4.69521. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.67868/4.68246. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.68121/4.68709. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.68062/4.68502. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.67921/4.69684. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.67481/4.70118. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.68233/4.70288. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.67549/4.69468. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.67954/4.70274. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.67801/4.69024. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.67636/4.71277. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.68073/4.71610. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.68066/4.71009. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.67758/4.70783. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.67521/4.70792. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.67697/4.70559. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.67623/4.71351. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.67696/4.70763. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.67787/4.71033. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.67868/4.70242. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.67658/4.70998. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.67391/4.70304. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.67664/4.66865. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.68567/4.67759. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.68398/4.67560. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.67922/4.67286. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.68162/4.67538. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.67915/4.68974. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.67992/4.68787. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.68361/4.68721. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.67737/4.71492. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.67882/4.69378. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.68055/4.68465. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.67895/4.69221. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.68125/4.69485. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.67768/4.68710. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.67793/4.69602. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.67143/4.72584. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.67227/4.71815. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.67358/4.71646. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.67232/4.70429. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.66918/4.71533. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.67525/4.70096. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.67559/4.71047. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.66988/4.71490. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.67237/4.69381. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.66927/4.71535. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.67291/4.68728. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.67285/4.69890. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.67042/4.71176. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.67153/4.69442. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.66742/4.70902. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.67354/4.70640. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.66913/4.70293. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.66847/4.70149. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.66810/4.69980. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.67404/4.70284. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.66670/4.71217. Took 0.08 sec\n",
      "ACC: 0.390625, MCC: -0.21664795696917594\n",
      "Epoch 0, Loss(train/val) 5.25552/5.18520. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.18669/5.20689. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 5.19012/5.22313. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.19047/5.22082. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.19490/5.20355. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.18931/5.18797. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.18579/5.18912. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.18541/5.19050. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.18309/5.18827. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.18290/5.19268. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 5.18410/5.19228. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.18195/5.19433. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.18201/5.19426. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.18283/5.19209. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.18139/5.19401. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.18113/5.19788. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.17752/5.19728. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.17835/5.19476. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.18058/5.19406. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 5.17621/5.19868. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.17940/5.20002. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.17541/5.20698. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.17672/5.20990. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.17779/5.21837. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.17577/5.21317. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.17925/5.19469. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.17690/5.20039. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.17361/5.20457. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.17252/5.21962. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.17215/5.21308. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.17289/5.21393. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.16991/5.21881. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.16983/5.22632. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.17282/5.22797. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.17029/5.23152. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.16871/5.22917. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.16690/5.22960. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 5.16747/5.22403. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.16328/5.23236. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.16967/5.22462. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.16587/5.22895. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.16674/5.22125. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.16617/5.22485. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.16546/5.22990. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.16140/5.22488. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.16606/5.23146. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.16417/5.23641. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.16412/5.23307. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.16236/5.22663. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.15932/5.23506. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.16490/5.22842. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.16307/5.23567. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.15855/5.24325. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.16080/5.23847. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.16630/5.22719. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 5.16096/5.23467. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.15601/5.24869. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.15802/5.24817. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 5.15284/5.25809. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.15485/5.25526. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.15444/5.25502. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.16026/5.23862. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.15139/5.26163. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.15565/5.26410. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 5.15142/5.24870. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.15684/5.24591. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.15588/5.25467. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.15024/5.26725. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 5.16014/5.23813. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.15433/5.26556. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 5.16588/5.25796. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.16486/5.23063. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.16316/5.23991. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.15425/5.26150. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.15592/5.24424. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.15987/5.24439. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.15579/5.23721. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 5.15233/5.24185. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.15592/5.25899. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.16112/5.23956. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.15283/5.23452. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.15019/5.24923. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.15465/5.25268. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.14996/5.24157. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.15611/5.22919. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 5.14921/5.24670. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.15075/5.25576. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.15373/5.23738. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 5.15369/5.24144. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.15172/5.22965. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.15589/5.24015. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.15207/5.24465. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.15285/5.24580. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.14676/5.24314. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.15458/5.23259. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.16323/5.21535. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.15662/5.22216. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.15185/5.25129. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.15380/5.24682. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.15142/5.24390. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.12167029203938934\n",
      "Epoch 0, Loss(train/val) 4.64966/4.60053. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.60521/4.59029. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.59515/4.59025. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.59500/4.59164. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.58899/4.59503. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.59128/4.59929. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.58730/4.60548. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.58768/4.60648. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.58736/4.60851. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.58985/4.60608. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.58585/4.60471. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.58749/4.60181. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.58504/4.59586. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.58540/4.59816. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.58451/4.60313. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.58302/4.60485. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.58068/4.60934. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.58107/4.61373. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.57424/4.62334. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.57428/4.63303. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.58240/4.62625. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.57620/4.62892. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.57629/4.63024. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.57793/4.63054. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.58148/4.62159. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.57946/4.63107. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.57767/4.63284. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.57949/4.62724. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.57624/4.63300. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.57623/4.63808. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.57347/4.64013. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.57632/4.63248. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.57170/4.64277. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.57457/4.63950. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.57492/4.65117. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.57795/4.64240. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.56480/4.65886. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.57541/4.64828. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.57525/4.63592. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.56817/4.64872. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.57142/4.65094. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.57198/4.64575. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.57131/4.64599. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.57061/4.65950. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.57328/4.65141. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.56978/4.65237. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.56751/4.65946. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.57285/4.65161. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.57164/4.65409. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.56863/4.65298. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.56769/4.65643. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.56889/4.65632. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.56682/4.65760. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.56735/4.65522. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.57724/4.64196. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.56906/4.65937. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.56935/4.63858. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.57056/4.64398. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.56970/4.64252. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.56778/4.65332. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.56782/4.65391. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.57000/4.65369. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.56850/4.65027. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.56903/4.64945. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.56726/4.65714. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.56544/4.65101. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.56654/4.65872. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.56151/4.66778. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.57184/4.65489. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.56744/4.65879. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.56555/4.66960. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.56764/4.66257. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.55952/4.67102. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.56705/4.66343. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.55992/4.66818. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.56618/4.66607. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.56500/4.67153. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.56376/4.68113. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.56903/4.67178. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.56909/4.65450. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.56726/4.65839. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.56767/4.66518. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.55988/4.67344. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.56548/4.67541. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.56775/4.67579. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.55965/4.67071. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.56582/4.67317. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.56459/4.67362. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.56447/4.68777. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.57188/4.66154. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.57018/4.65100. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.56939/4.65152. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.56963/4.65244. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.56911/4.67206. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.56036/4.68106. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.56215/4.67941. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.56673/4.66498. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.56137/4.67687. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.56530/4.66488. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.56512/4.67252. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.06362847629757777\n",
      "Epoch 0, Loss(train/val) 4.62401/4.55759. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.56832/4.58540. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.58475/4.56976. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.58477/4.55613. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.57173/4.55721. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.56278/4.56106. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.56428/4.56299. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.56572/4.56271. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.56192/4.56610. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.56283/4.56227. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.56114/4.56640. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.55876/4.56336. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.56213/4.56283. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.55619/4.56756. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.55735/4.56799. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.55609/4.56660. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.55728/4.56618. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.55296/4.57102. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.55102/4.58014. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.55343/4.58501. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.54976/4.58918. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.55367/4.58740. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.55378/4.58663. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.54917/4.58824. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.54958/4.58415. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.54850/4.58615. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.54646/4.59162. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.54887/4.59767. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.54419/4.60474. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.54719/4.60423. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.54828/4.59618. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.54870/4.59008. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.54679/4.58925. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.54616/4.59819. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.54551/4.60116. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.54628/4.59043. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.54591/4.59766. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.54374/4.60802. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.54517/4.58482. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.54523/4.60487. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.54100/4.61635. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.54356/4.61396. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.54359/4.59863. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.54332/4.63528. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.54201/4.60348. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.54341/4.60271. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.54417/4.59111. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.54031/4.61189. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.54261/4.60190. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.54337/4.59546. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.53918/4.61119. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.54147/4.61241. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.53632/4.61492. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.53878/4.60899. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.54322/4.61407. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.53374/4.60794. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.54017/4.60759. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.54339/4.60150. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.53963/4.61042. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.54195/4.59277. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.53978/4.62123. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.53584/4.61154. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.53621/4.59911. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.53927/4.59715. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.53759/4.60034. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.54024/4.59875. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.54082/4.61090. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.53851/4.59707. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.53786/4.61536. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.54048/4.61146. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.53803/4.57838. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.54233/4.59913. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.54373/4.60278. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.54289/4.59685. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.54055/4.61607. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.54325/4.61754. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.53497/4.62507. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.53987/4.61457. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.53618/4.63418. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.53123/4.63477. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.53241/4.63543. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.53821/4.62814. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.53181/4.61617. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.53064/4.61770. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.53890/4.61801. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.53444/4.60833. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.53573/4.62005. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.53333/4.60773. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.53386/4.64119. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.53058/4.63619. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.53600/4.62695. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.53146/4.64084. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.52800/4.62595. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.53231/4.60771. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.53136/4.62969. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.53180/4.64178. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.52731/4.63241. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.53204/4.62641. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.54264/4.60996. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.55576/4.57449. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.06643282473893375\n",
      "Epoch 0, Loss(train/val) 4.98321/4.86305. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.88404/4.87388. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.87451/4.86577. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.87167/4.86018. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.87049/4.85628. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.87060/4.85608. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.87262/4.85449. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.87129/4.85328. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.87016/4.85384. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.86840/4.85381. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.87324/4.85661. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 4.87052/4.85425. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.87188/4.85211. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.86717/4.85563. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.86710/4.85633. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.86546/4.85688. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.86676/4.85743. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.86457/4.85662. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 4.86509/4.86002. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.86439/4.85661. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.86169/4.85774. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.86105/4.85752. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.86278/4.85920. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.86175/4.85920. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.86329/4.86141. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.86474/4.86018. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.86127/4.85869. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.86031/4.85991. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.86205/4.85739. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.85705/4.85802. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.86107/4.86073. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.85973/4.86195. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.85902/4.86235. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.85817/4.87881. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.87081/4.86645. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.86215/4.86068. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.86265/4.87200. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.85831/4.87054. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86400/4.87469. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.85664/4.87330. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.85979/4.88069. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.85760/4.88624. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.85750/4.87973. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.85762/4.87016. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.85870/4.86455. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.85691/4.86810. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.85280/4.87886. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.85774/4.87080. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.85507/4.86864. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.85440/4.87737. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.85520/4.87682. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.85686/4.87420. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.84744/4.87271. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.85413/4.87231. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.84924/4.87156. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.85112/4.86221. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.84874/4.86110. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.87450/4.87101. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.87374/4.86779. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.87098/4.86877. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.86659/4.86730. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.86554/4.86551. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.86550/4.86703. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.86583/4.85057. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.86355/4.85885. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.86339/4.86687. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.86359/4.87488. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.86326/4.86424. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.85791/4.86829. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.86188/4.86896. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.85884/4.86460. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.86257/4.86257. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.86100/4.86664. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.86042/4.87193. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.85479/4.86354. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.85696/4.86483. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.85955/4.86937. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.86062/4.86637. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.85449/4.85803. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.85540/4.86506. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.85882/4.86938. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.85714/4.86468. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.85591/4.86742. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.86779/4.86498. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.86494/4.87098. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.86495/4.87118. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.86057/4.87770. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.85775/4.87887. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.85988/4.87784. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.85725/4.88463. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.85825/4.88252. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.86704/4.86944. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.85956/4.87148. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.86255/4.87575. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.86084/4.87938. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.85977/4.87853. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.86077/4.88209. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.86030/4.88443. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.86442/4.86162. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.86420/4.86651. Took 0.08 sec\n",
      "ACC: 0.59375, MCC: 0.1889822365046136\n",
      "Epoch 0, Loss(train/val) 4.73894/4.72906. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.72732/4.71276. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.71482/4.71098. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.71709/4.71446. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.71468/4.71313. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.71315/4.71107. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.70902/4.70477. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.71037/4.69824. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.70919/4.69728. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.71126/4.69747. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.70979/4.69593. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.71374/4.70302. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.70812/4.69749. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.70760/4.69598. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.70967/4.69778. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.70702/4.69301. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.70935/4.70167. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.70497/4.70386. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.70842/4.70512. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.70990/4.70887. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.70602/4.69701. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.70745/4.70034. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.70525/4.69182. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.70891/4.69684. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.71003/4.69836. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.70639/4.69271. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.70877/4.69753. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.70433/4.69432. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.71071/4.69437. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.70331/4.71522. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.70323/4.69828. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.70447/4.70708. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.70358/4.70139. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.70156/4.70044. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.70356/4.70314. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.70389/4.70138. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.69949/4.70789. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.70198/4.70022. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.70137/4.70424. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.70027/4.70005. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.70260/4.70177. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.70148/4.70571. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.69919/4.70833. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.70225/4.70026. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.69902/4.70401. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.70166/4.70509. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.70317/4.70917. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.70298/4.70691. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.70069/4.71023. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.70213/4.70243. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.69868/4.70395. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.70078/4.70522. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.69919/4.70337. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.69797/4.69935. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.69960/4.70434. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.69801/4.70642. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.69750/4.69945. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.69990/4.69940. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.69804/4.70275. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.69834/4.70679. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.70056/4.70293. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.69546/4.70130. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.69189/4.69828. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.69866/4.71016. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.69709/4.69795. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.69511/4.70175. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.70108/4.70046. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.69493/4.69970. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.69884/4.70034. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.69423/4.70550. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.68939/4.70696. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.69642/4.69119. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.69449/4.70177. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.69803/4.70251. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.69292/4.71239. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.69403/4.70193. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.69305/4.70756. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.69171/4.71167. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.69313/4.70943. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.69684/4.71100. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.69251/4.71252. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.68975/4.70025. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.69447/4.70863. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.69191/4.70374. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.69117/4.70267. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.69133/4.71252. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.68255/4.71246. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.68671/4.71283. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.69088/4.71512. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.68858/4.70579. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.69169/4.71470. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.68961/4.70158. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.68807/4.70751. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.68588/4.70940. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.68944/4.69602. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.69136/4.70046. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.68321/4.69655. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.69654/4.70274. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.68721/4.69129. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.68717/4.70593. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.07570682517832987\n",
      "Epoch 0, Loss(train/val) 4.94310/4.90689. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.88577/4.86832. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.88155/4.86061. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.88002/4.86579. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.88271/4.87208. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.87938/4.87082. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.87993/4.87023. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.87817/4.86956. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.87816/4.86973. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.87595/4.87123. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.87727/4.87216. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.87639/4.87001. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.87576/4.86357. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.87250/4.86465. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.87324/4.86378. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.87305/4.87001. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.87339/4.86682. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.86789/4.86591. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.86814/4.86579. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.86846/4.87370. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.86733/4.87764. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.86745/4.86054. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.86459/4.86776. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.86314/4.87993. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.86114/4.86970. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.86904/4.84347. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.86931/4.86714. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.86785/4.86267. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.86400/4.85629. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.86624/4.86090. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.86629/4.85412. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.86127/4.84433. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 4.85754/4.86372. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.85926/4.86621. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.85456/4.87332. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.85601/4.85890. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.85596/4.87071. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.86062/4.86369. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.85299/4.87371. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.87564/4.86008. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.86473/4.87554. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.85830/4.88600. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.85758/4.87181. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.85206/4.86735. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.84578/4.85509. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.85831/4.86040. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.85217/4.88370. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.85096/4.86750. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.85235/4.86383. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.85120/4.86128. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.85033/4.86702. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.85475/4.85891. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.84241/4.86858. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.85071/4.85340. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.85246/4.87195. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.85406/4.86057. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.84264/4.89162. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.85023/4.86688. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.84370/4.85781. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.84946/4.86162. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.84619/4.85507. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.84741/4.85706. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.84516/4.85331. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.84235/4.87019. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.84473/4.86259. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.84429/4.86466. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.85293/4.85407. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.83951/4.87233. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.84646/4.87794. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.84583/4.88222. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.84199/4.87118. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.84400/4.85756. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.84744/4.87203. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.84106/4.85072. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.84252/4.86465. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83783/4.86334. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.84149/4.86948. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.84096/4.89025. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.84048/4.87438. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.84194/4.85239. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83326/4.86826. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.84836/4.86319. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.84107/4.85065. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.84598/4.84928. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.82907/4.87867. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.84749/4.87551. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83483/4.87954. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.84261/4.86271. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83459/4.85518. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.83371/4.84396. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.84182/4.86247. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.84327/4.84764. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83784/4.84848. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.84117/4.86311. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.83640/4.86124. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.83768/4.84928. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.83683/4.85256. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.83392/4.85307. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.84316/4.85474. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.83577/4.88172. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.02938413738897565\n",
      "Epoch 0, Loss(train/val) 5.13421/5.11378. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.05350/5.08502. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 5.04874/5.08768. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.04705/5.08358. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.04402/5.07895. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.04598/5.07636. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.04560/5.07825. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.04180/5.07272. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.04121/5.07227. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 5.04137/5.07248. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.03680/5.06432. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.03759/5.06258. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.03513/5.06360. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.03509/5.07123. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.03364/5.07056. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.03156/5.05522. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.03100/5.05762. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.02762/5.06454. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.03212/5.06086. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.03096/5.04058. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 5.03259/5.04783. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.02711/5.04848. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.02856/5.05302. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.02475/5.05243. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.02261/5.04491. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.02916/5.06711. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.02596/5.06321. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.02700/5.06238. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.02337/5.05420. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.02492/5.05684. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.02774/5.05390. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.02004/5.04300. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.02518/5.04908. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.02324/5.05105. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.01874/5.05288. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.02382/5.04517. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.01861/5.04775. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.02308/5.05237. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 5.01758/5.04829. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.01924/5.04594. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.02003/5.05912. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.01985/5.04498. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 5.02002/5.04500. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.02048/5.03974. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.01971/5.05182. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.01854/5.03991. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.02032/5.04718. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.01789/5.05476. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.01761/5.03602. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.02052/5.04933. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.01695/5.05238. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.01317/5.05212. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.01819/5.05862. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 5.01987/5.03616. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.01496/5.04150. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.01253/5.03805. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.01759/5.04629. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.01339/5.03145. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.01593/5.04374. Took 0.07 sec\n",
      "Epoch 59, Loss(train/val) 5.01055/5.02847. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.01762/5.03371. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.01442/5.02726. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.01044/5.05231. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.01267/5.02328. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.01626/5.03779. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.01597/5.03619. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.00808/5.03516. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.01016/5.02999. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.00524/5.03417. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.01696/5.05280. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.01502/5.02363. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.00931/5.03510. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.00625/5.05513. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 5.00687/5.04515. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.00869/5.03750. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.01174/5.02482. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.00801/5.02905. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.01024/5.04073. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.02192/5.04911. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.01334/5.03730. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.01090/5.04747. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.00654/5.03019. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.00812/5.04721. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.99916/5.04490. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.00463/5.03067. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.00472/5.02820. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.00165/5.03358. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.00038/5.02732. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.00952/5.03828. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.00282/5.02990. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.00612/5.04127. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.00865/5.01907. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.00120/5.03636. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.99831/5.03056. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.00546/5.04607. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.00584/5.02436. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.00586/5.04971. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.00890/5.04828. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.00036/5.06576. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.99593/5.01994. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.12267022872162016\n",
      "Epoch 0, Loss(train/val) 4.87175/4.83922. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.80199/4.81826. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.79976/4.81069. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79977/4.80430. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79710/4.80966. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79903/4.80588. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.79635/4.80589. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.79820/4.81633. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.79304/4.80935. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.79887/4.81079. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79666/4.80897. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.79437/4.81035. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.79365/4.81022. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.79526/4.81042. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.79357/4.81255. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.79197/4.81240. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.79152/4.81612. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.79385/4.81034. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.79258/4.81502. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.79457/4.81197. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.79561/4.80867. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.79189/4.81466. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.78932/4.81632. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.78902/4.81530. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.79051/4.81687. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.78836/4.81444. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.79100/4.81605. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.79293/4.81597. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.78673/4.81931. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.78614/4.82360. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.79098/4.81330. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78848/4.81379. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.78411/4.82246. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78505/4.81753. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78725/4.81886. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.78994/4.82082. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.78652/4.82365. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.78581/4.82367. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.77898/4.82677. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.77816/4.83050. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.78368/4.81446. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.78430/4.82402. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.78212/4.82245. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.77699/4.82398. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.77850/4.82366. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.78186/4.81860. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.78242/4.80286. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77820/4.82143. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77418/4.83210. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.77833/4.81163. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.78005/4.82064. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.78370/4.81531. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.77749/4.82129. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.77418/4.82544. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.77682/4.83044. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.78184/4.81406. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.78012/4.84272. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.78188/4.82797. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.77199/4.83510. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.77802/4.82812. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.78081/4.81401. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.77268/4.82330. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.78027/4.81652. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.77921/4.80770. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.77480/4.82681. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.77563/4.82090. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.77412/4.81634. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.77588/4.81899. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.77125/4.82052. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.77494/4.82286. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.77315/4.82839. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.77207/4.83027. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.77535/4.81627. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.77279/4.81245. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.76659/4.83129. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.77086/4.81994. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.77958/4.81320. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.77411/4.80912. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.77073/4.82032. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 4.77134/4.81100. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.76547/4.83079. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.77173/4.82544. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.77285/4.81819. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76615/4.82605. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.76702/4.82050. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.76556/4.81891. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.76916/4.81671. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.77050/4.80752. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.76482/4.81931. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.76706/4.83009. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.76507/4.81796. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.76933/4.79953. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.76724/4.83334. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.77099/4.81509. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.76739/4.81463. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.76674/4.84110. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.76690/4.82389. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.76533/4.82044. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.75624/4.82169. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.76186/4.80484. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.12855839970025792\n",
      "Epoch 0, Loss(train/val) 5.12269/5.07394. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.05328/5.06652. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.05918/5.06460. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.06072/5.05917. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 5.05804/5.06018. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.06057/5.05966. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 5.05497/5.05790. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.05235/5.06097. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.04614/5.06645. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.04883/5.07216. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 5.05221/5.05923. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 5.04862/5.05919. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 5.05111/5.06811. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.04835/5.06487. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 5.04609/5.05977. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 5.04701/5.06932. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.04219/5.06414. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.04948/5.04773. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 5.04924/5.05951. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 5.04715/5.06816. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.04855/5.05791. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.04776/5.05284. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 5.04947/5.05176. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.05009/5.05395. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 5.04595/5.05672. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.04590/5.05575. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.04723/5.05312. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.04804/5.05467. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.04513/5.05713. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 5.04709/5.05877. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.04412/5.06009. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 5.04232/5.06422. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.04212/5.07491. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 5.04387/5.07183. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 5.04112/5.07456. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 5.03983/5.08180. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.03970/5.07874. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 5.04040/5.08365. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 5.04394/5.06727. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 5.04453/5.06052. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.04383/5.06526. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.03976/5.07476. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 5.03527/5.08007. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.03336/5.08850. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 5.03492/5.07867. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.03381/5.09377. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.03600/5.08295. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.03317/5.09656. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.02905/5.09338. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.03701/5.08941. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 5.02657/5.10297. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.03025/5.09090. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.03015/5.10131. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 5.03539/5.08818. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.02848/5.09596. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.02608/5.09750. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.02973/5.10731. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.03016/5.06203. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.04816/5.05483. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.04646/5.05243. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.04163/5.05833. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.04070/5.06265. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 5.03608/5.07915. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.04169/5.07316. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.03687/5.07313. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 5.03522/5.07690. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.03692/5.07947. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.03495/5.08371. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.03176/5.09079. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.03009/5.09952. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.02964/5.10329. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.03000/5.08692. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.03166/5.09317. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.02278/5.08794. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.03148/5.07516. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.02873/5.07896. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.02988/5.07533. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.02652/5.09765. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.02604/5.07405. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.02971/5.08269. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 5.03043/5.09021. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.02512/5.09035. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.02100/5.09261. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.03023/5.05683. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.02952/5.05096. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.03754/5.06023. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.03666/5.06801. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.03074/5.06968. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.03284/5.06837. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.02655/5.06692. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.02476/5.07599. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.03011/5.07408. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.02319/5.08631. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.02852/5.09629. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.02681/5.10246. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 5.02781/5.07472. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.02341/5.10120. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.02466/5.11495. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 5.02908/5.09273. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.02511/5.11142. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.07539645724831788\n",
      "Epoch 0, Loss(train/val) 4.69594/4.66586. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 4.66782/4.66540. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.66777/4.66261. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.66353/4.66229. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.66314/4.66399. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.66256/4.66867. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.66475/4.67075. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.66657/4.67055. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.66176/4.67021. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.66370/4.66751. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.65900/4.66823. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.65964/4.66704. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.66009/4.66556. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.65738/4.66733. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.65781/4.67106. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.65875/4.66667. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.65333/4.66730. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.65657/4.66511. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.65748/4.66439. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.65404/4.67110. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.65355/4.67611. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.65784/4.67318. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.65547/4.68266. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.65449/4.67168. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.65544/4.67019. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.64708/4.69246. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.66065/4.67303. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.64690/4.69362. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.64730/4.69943. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.64989/4.69205. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.64982/4.68836. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.64604/4.68986. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.64769/4.69014. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.64756/4.68913. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.64540/4.68112. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 4.66249/4.67056. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.65225/4.68412. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.64919/4.69599. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.65451/4.67694. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.65983/4.66859. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.65322/4.67590. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.64914/4.67785. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.65266/4.67023. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.65063/4.68051. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.65161/4.67376. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.65056/4.67678. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.65103/4.67084. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.64531/4.69013. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.64795/4.68583. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.64923/4.69463. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.64718/4.69630. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.64624/4.68492. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.64660/4.68480. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.64779/4.68773. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.64503/4.68953. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.65175/4.67224. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.64843/4.67908. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.63999/4.69072. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.64518/4.68544. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.63918/4.68703. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.64457/4.67673. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.63969/4.68293. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.64074/4.68243. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.63793/4.69962. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.63801/4.69167. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.63953/4.69741. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.63713/4.70254. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.63177/4.70682. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.63374/4.70490. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 4.63509/4.71532. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.64105/4.68274. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.63221/4.73266. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.63321/4.71329. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.63789/4.68887. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.63879/4.70569. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.63660/4.68422. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.64350/4.68258. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.63646/4.70527. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.63401/4.70582. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.63635/4.68541. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.63322/4.72860. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.63556/4.70269. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.63277/4.71993. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.63044/4.74958. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.63235/4.71233. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.62584/4.73177. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.64349/4.69565. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.64557/4.71196. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.64402/4.67855. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.63747/4.71199. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.63565/4.70127. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.62695/4.73803. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.63192/4.68726. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.63555/4.69664. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.62097/4.73760. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.63430/4.72244. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.63034/4.71272. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.62493/4.75997. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.63067/4.71789. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.62709/4.73121. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.07100716024967263\n",
      "Epoch 0, Loss(train/val) 4.96011/4.85829. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.87600/4.86772. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.87450/4.87485. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.87283/4.87199. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.87264/4.87343. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.87124/4.87348. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.87277/4.86783. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.87563/4.86276. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.87107/4.86412. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.87116/4.86659. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.86915/4.86510. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.86905/4.86541. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.86618/4.86758. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.86412/4.86883. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.86491/4.87057. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.86346/4.86885. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.86483/4.87042. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.86180/4.86911. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.86362/4.86802. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.86265/4.87213. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.86262/4.87314. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.85800/4.87719. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.85854/4.87227. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.85912/4.87501. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.85462/4.88085. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.85731/4.88295. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.85456/4.88009. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.85804/4.87537. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.85689/4.87862. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.85278/4.88285. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.85224/4.88349. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.85094/4.89095. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.85112/4.88419. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.85025/4.87937. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.84942/4.90087. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84625/4.87719. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84587/4.89237. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.85253/4.89033. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.84997/4.89233. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84984/4.89577. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84449/4.90087. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.84436/4.90402. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84730/4.89563. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.84544/4.91136. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.85444/4.89245. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.84850/4.88603. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.85327/4.88653. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.84535/4.90210. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84598/4.90473. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.84033/4.90967. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.83985/4.92831. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.84398/4.88675. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.84060/4.93453. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.84696/4.89540. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.83812/4.91698. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.83959/4.91896. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.84040/4.89675. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.84011/4.89554. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.83934/4.90268. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83596/4.88969. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.83545/4.91005. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83596/4.89729. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.83196/4.91316. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.83425/4.88949. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.84157/4.89423. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.84193/4.91152. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.84202/4.91470. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.83765/4.91049. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.82605/4.93929. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83378/4.92228. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.83643/4.91687. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.83453/4.92931. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83415/4.92382. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.83118/4.93258. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83705/4.90470. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83074/4.92430. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.82550/4.92886. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.82612/4.94051. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.81937/4.94085. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.83724/4.93260. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83510/4.92755. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.83430/4.92742. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.82478/4.92656. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.83506/4.92900. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.82805/4.91004. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.82559/4.93788. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.82697/4.90440. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.82721/4.93924. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.82677/4.92281. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.82694/4.92595. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.83094/4.92592. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.82177/4.92700. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.81895/4.92954. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.82136/4.94385. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 4.82170/4.94284. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.82010/4.91839. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.82447/4.94336. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.82811/4.95070. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83190/4.91948. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.82654/4.92823. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.058362172604259924\n",
      "Epoch 0, Loss(train/val) 4.81262/4.73085. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.72310/4.72606. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.72503/4.72392. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.72024/4.72387. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.71978/4.72381. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.72017/4.72390. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.72039/4.72148. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.71908/4.72148. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.71755/4.72291. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.71508/4.72191. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.71670/4.71989. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.71922/4.72109. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.71799/4.72508. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.71607/4.72276. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.70988/4.72320. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.71686/4.73315. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.71689/4.72095. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.71556/4.71913. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.71420/4.72483. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.71002/4.72624. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.71708/4.72355. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.70672/4.72798. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.71051/4.74070. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.71404/4.73532. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.71256/4.74166. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.70162/4.74173. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.71508/4.73264. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.70662/4.73415. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.70670/4.73524. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.70544/4.75914. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.70892/4.75127. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.70036/4.76173. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.69936/4.75523. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.70686/4.76667. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.70173/4.76097. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.70663/4.76212. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.70321/4.76469. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.69867/4.76298. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.70019/4.77501. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.70949/4.77040. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.69804/4.79240. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.70101/4.76845. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.70270/4.77995. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.69339/4.80346. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.69488/4.78953. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.70033/4.78552. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.69778/4.81108. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.69421/4.79198. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.69549/4.81039. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.70340/4.77300. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.69155/4.80300. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.68844/4.79761. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.70115/4.77432. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.69462/4.80193. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.69521/4.78863. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.69552/4.79534. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.69441/4.80315. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.69575/4.78210. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.69291/4.79409. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.69028/4.80099. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.69331/4.79851. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.69957/4.80150. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.69757/4.80260. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.70138/4.79699. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.69572/4.82073. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.69023/4.79598. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.69695/4.79072. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.68988/4.81688. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.69309/4.80459. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.69329/4.82640. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.69085/4.80637. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.69311/4.81192. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.69175/4.78921. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.69311/4.80384. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.69377/4.78935. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.68994/4.81296. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.69120/4.81298. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.69160/4.81136. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.68814/4.82191. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.69307/4.81716. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.69327/4.79939. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.69005/4.81684. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.68961/4.81603. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.68977/4.81565. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.68717/4.82052. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.69522/4.80640. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.68467/4.81928. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 4.68185/4.82227. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.68823/4.79004. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.69522/4.80180. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.68388/4.80419. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.68899/4.80846. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.68969/4.80456. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.68434/4.81042. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.69862/4.73579. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.70814/4.75022. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.69925/4.74301. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 4.70263/4.76070. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.69072/4.80639. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.69435/4.79767. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.06262242910851495\n",
      "Epoch 0, Loss(train/val) 5.03754/5.06780. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.03010/5.02077. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 5.01290/5.02293. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.01345/5.02681. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 5.01553/5.03078. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.01518/5.02596. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 5.01932/5.02009. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.01446/5.01644. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.01590/5.01552. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 5.01209/5.01665. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.01371/5.01675. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 5.01322/5.01677. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.01168/5.01749. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.00922/5.01948. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 5.01084/5.01970. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.01221/5.02029. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.01009/5.02094. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.01019/5.02092. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 5.01073/5.02317. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.00926/5.02353. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 5.00860/5.02663. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.00830/5.02981. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.01109/5.02950. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.00741/5.03023. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.00851/5.03383. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.00882/5.03046. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.00580/5.03686. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.00842/5.03322. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.00644/5.03494. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.00550/5.03871. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 5.00328/5.04390. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.00351/5.04360. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.00310/5.03582. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.00027/5.04406. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.00417/5.04864. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 5.00226/5.05187. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.00138/5.05219. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.99929/5.04580. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 5.00673/5.04078. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.00228/5.04986. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.99873/5.04753. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.00283/5.04064. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.99850/5.05703. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.99901/5.04287. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 5.00187/5.05078. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.99487/5.06664. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.99920/5.06666. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.00201/5.05303. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.99906/5.05039. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.99548/5.05586. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.00051/5.04990. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.99777/5.05873. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.99616/5.07936. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.00336/5.05495. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.00163/5.06496. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.99385/5.08760. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.00073/5.06842. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.99672/5.07446. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.99607/5.05651. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.99524/5.05964. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.99700/5.07094. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.99857/5.07627. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.99419/5.10152. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.99165/5.07160. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.99326/5.07035. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.99809/5.07243. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.98862/5.10236. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.98928/5.06146. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.99399/5.06870. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.99634/5.07468. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.99151/5.08180. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.99496/5.06695. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.98840/5.13392. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.99324/5.06582. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.99199/5.08893. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.99353/5.11407. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.99536/5.08210. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.99157/5.10248. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.98713/5.08853. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.99719/5.06062. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.00387/5.03837. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 5.00040/5.05210. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.99873/5.06605. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.99372/5.07273. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.99295/5.06218. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.99254/5.06384. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.99501/5.07162. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.99249/5.07718. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.99373/5.04700. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.99924/5.04512. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.99831/5.06716. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.99425/5.07277. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.99104/5.09583. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.98918/5.08498. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.99228/5.10490. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.99131/5.10115. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.98691/5.07515. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.99163/5.10238. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.98206/5.10235. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.98171/5.13350. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: 0.010188710438961876\n",
      "Epoch 0, Loss(train/val) 4.84754/4.77049. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.78442/4.77501. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.78265/4.77084. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.78073/4.77044. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.77997/4.76966. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.77904/4.77182. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.78036/4.77306. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.77951/4.77190. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.77886/4.77052. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.77841/4.76958. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.78013/4.77295. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.77952/4.77326. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.77651/4.77234. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.77729/4.77316. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.77507/4.77230. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.77582/4.77448. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.77517/4.77295. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.77553/4.77670. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.77539/4.77628. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.77537/4.77366. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.77274/4.77609. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.77585/4.77554. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.77612/4.77519. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.77410/4.77700. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.77508/4.77625. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.76858/4.78060. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.77245/4.78307. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.77581/4.77302. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77158/4.77481. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.77047/4.77928. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.77010/4.77588. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.76923/4.79717. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.77618/4.77330. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.76861/4.79491. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.77420/4.77694. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.76993/4.78042. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 4.76808/4.78030. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.76686/4.78453. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.76800/4.78231. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.76912/4.78784. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.76947/4.78220. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.77032/4.77941. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.76850/4.78511. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.76483/4.78371. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.76686/4.78805. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.76641/4.77963. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 4.76416/4.79191. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.76433/4.79291. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.76720/4.78576. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 4.76682/4.78715. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.76421/4.77935. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.76182/4.78004. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.76080/4.79109. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.76074/4.78078. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.76193/4.77953. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.76199/4.77950. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.76385/4.79582. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.76197/4.78809. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.75614/4.78223. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.75781/4.79089. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.75655/4.78253. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.75864/4.77961. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76131/4.76762. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.76572/4.79079. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.76357/4.76604. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.75605/4.77085. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.75752/4.77684. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.76472/4.76846. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.75663/4.79584. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 4.76587/4.79853. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.76620/4.77852. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.75917/4.79780. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76181/4.80123. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.75770/4.79714. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.76693/4.78018. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.76210/4.77397. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.76065/4.77902. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.75475/4.78774. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.76531/4.77482. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.75642/4.79529. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.75587/4.79370. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.75400/4.76358. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.75157/4.77150. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.75789/4.77654. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75159/4.79548. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.75653/4.76917. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.75515/4.79742. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.75134/4.77641. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.75589/4.79364. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.75351/4.76660. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.75577/4.77863. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.74908/4.77486. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.74721/4.77642. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.74847/4.78087. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.74661/4.76369. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.75166/4.79398. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.75557/4.76726. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.76791/4.78004. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.76627/4.79284. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.76015/4.78378. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.055227791305300936\n",
      "Epoch 0, Loss(train/val) 4.85848/4.82267. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.82819/4.82399. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.82601/4.82294. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.82655/4.81802. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.82614/4.81818. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.82746/4.81685. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.82827/4.81652. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.82592/4.81991. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.81999/4.81911. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.81764/4.81261. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.81962/4.80579. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.82038/4.81157. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.81838/4.80195. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.81809/4.80854. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.81977/4.82374. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.81911/4.81685. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.81568/4.81629. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.81612/4.81377. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.81666/4.81773. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.81353/4.81743. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.81196/4.81859. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.81027/4.81325. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.81514/4.81308. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.81292/4.81877. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.81185/4.81929. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.80923/4.82796. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.80564/4.82402. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.80681/4.81881. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.81302/4.82680. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.80756/4.83215. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.80260/4.82683. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.80356/4.81986. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.80494/4.83044. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.80007/4.82507. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.80118/4.82451. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.80120/4.82118. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.79532/4.82321. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.80569/4.82398. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.80548/4.83373. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.80073/4.83437. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.79932/4.82706. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.79915/4.82947. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.79541/4.83059. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.79987/4.82910. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.79635/4.83377. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.79752/4.83681. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.79598/4.83789. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.80033/4.83615. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.79763/4.83150. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.79411/4.82694. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.79249/4.82292. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.79516/4.82727. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.79692/4.82211. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.79868/4.81995. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.79283/4.82484. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.79552/4.82758. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.79454/4.82666. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.79087/4.83653. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.79635/4.83134. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.79825/4.83082. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.79219/4.83346. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.79576/4.81608. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.79835/4.82583. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.79614/4.82796. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.79362/4.83273. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.78836/4.83106. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.79187/4.82850. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.79169/4.83458. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.79209/4.82009. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.79255/4.82891. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.79091/4.82649. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78616/4.82649. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.79300/4.82784. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.79039/4.83520. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.78775/4.82815. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.79186/4.82146. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.79018/4.82316. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.79144/4.81977. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.78772/4.82917. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.79700/4.82309. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.79203/4.82201. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.79423/4.83124. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.79075/4.83510. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.78791/4.82908. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.78984/4.82792. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.78898/4.83151. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.78646/4.82453. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.79249/4.82481. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.78564/4.82800. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.78485/4.82249. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.78688/4.81887. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.79403/4.82707. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.78989/4.81986. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.78473/4.83015. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.78912/4.82743. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.79395/4.83268. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.78783/4.83602. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.79336/4.81842. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.79226/4.82306. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.78835/4.82126. Took 0.09 sec\n",
      "ACC: 0.484375, MCC: -0.0020131159011798102\n",
      "Epoch 0, Loss(train/val) 4.90019/4.86483. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.83296/4.83491. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.82457/4.82194. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.82590/4.82443. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.82413/4.82949. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.82359/4.82521. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.82324/4.82757. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 4.82515/4.82030. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.82479/4.82247. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.82346/4.81986. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.82219/4.81879. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.81796/4.82286. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.82046/4.82545. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.82177/4.81196. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.81604/4.82588. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.82042/4.81019. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.82057/4.82320. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.81952/4.81469. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.81363/4.82449. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.82089/4.80603. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.81322/4.82464. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.81545/4.81715. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.81827/4.81950. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.81448/4.81634. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.80948/4.82854. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.81017/4.82729. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.81354/4.81051. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.81326/4.82490. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.81147/4.82719. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.80652/4.80995. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.80710/4.82376. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.80925/4.81735. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.80725/4.82073. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.80652/4.82149. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.80397/4.83636. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.80229/4.81781. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.80534/4.82608. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.80388/4.82873. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.81034/4.82947. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.80677/4.82593. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.80635/4.80772. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.80400/4.82801. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.80641/4.81726. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.80682/4.80964. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.79826/4.81261. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.80331/4.81562. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.80025/4.81784. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.80141/4.82225. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.79752/4.82369. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.79894/4.83273. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.79501/4.82337. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.79964/4.82048. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.79973/4.81707. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.79274/4.82078. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.79745/4.83288. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.79288/4.82608. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.80054/4.83766. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.80119/4.81833. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.80151/4.83043. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.79943/4.82974. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.80361/4.81647. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.79345/4.82252. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.79336/4.83486. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.79178/4.84083. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.79190/4.85046. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.78994/4.82659. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.79540/4.82576. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.79373/4.82878. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.78869/4.82143. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.78546/4.83764. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.79815/4.84056. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.79043/4.82050. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.78559/4.83200. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.78769/4.83005. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.78879/4.84611. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.79136/4.83580. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78946/4.82997. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78355/4.83483. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.78531/4.83381. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.79314/4.83581. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.79153/4.81560. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.78163/4.82618. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.78796/4.82524. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.79072/4.82528. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.78408/4.82149. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.78081/4.82479. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.79141/4.82316. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.79390/4.82256. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.78485/4.81862. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.77770/4.83146. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.78474/4.80375. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.78635/4.80237. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.79745/4.82373. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.79339/4.82502. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.78549/4.82580. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.78266/4.82707. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.78397/4.83005. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.78331/4.82274. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.78414/4.83614. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.77954/4.84007. Took 0.09 sec\n",
      "ACC: 0.5, MCC: -0.00700902994282404\n",
      "Epoch 0, Loss(train/val) 4.71067/4.76554. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.72378/4.75108. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.70313/4.74529. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.69539/4.69598. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.68702/4.71102. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.68684/4.72560. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.68691/4.72797. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.68826/4.73340. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.68763/4.73007. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.68596/4.72935. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.68434/4.73769. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.68781/4.72998. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.68788/4.72450. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.68560/4.72971. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.68546/4.72536. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.68326/4.71763. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.68511/4.72431. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.68408/4.72799. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.68351/4.73494. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.68386/4.74511. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.68165/4.73846. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.67964/4.74037. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.68341/4.72556. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.68054/4.73868. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.68126/4.73717. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.68133/4.72798. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.67966/4.72982. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.67758/4.74214. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.68005/4.72979. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.67948/4.72197. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.67226/4.73798. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.67381/4.72620. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.67184/4.73112. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.68004/4.72624. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.67726/4.72915. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.67321/4.73691. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.67515/4.72853. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.67212/4.73448. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.66739/4.73845. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.67083/4.72319. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.67296/4.72645. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.66895/4.73210. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.66825/4.73138. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.66851/4.71693. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.66498/4.75591. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.67200/4.72398. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.66902/4.72641. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.67095/4.71714. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.66564/4.72360. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.66417/4.73637. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.66597/4.73056. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.66254/4.73547. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.66491/4.73214. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.66615/4.72821. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.66136/4.75020. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.66339/4.73437. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.66713/4.73954. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.66515/4.71989. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.66314/4.73587. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.66384/4.72452. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.66159/4.74555. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.66235/4.73074. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.66002/4.74122. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.65759/4.74455. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.66387/4.73282. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.66610/4.72506. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.65711/4.75614. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.66096/4.73972. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.65756/4.75144. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.66178/4.73555. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.65230/4.73617. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.65368/4.74057. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.66131/4.74956. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.65711/4.73412. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.65795/4.74337. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.65577/4.72802. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.65459/4.72694. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.65061/4.75356. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.65380/4.74333. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.65090/4.74184. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.65501/4.73906. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.65280/4.74763. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.65707/4.72710. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.65141/4.73621. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.64749/4.75090. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.65833/4.74096. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.65422/4.73181. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.65242/4.74242. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.65527/4.73726. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.65083/4.72895. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.65329/4.73641. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.65358/4.73911. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.65268/4.75024. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.65007/4.74630. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.64919/4.74770. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.65344/4.73338. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.64886/4.74656. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.65295/4.74308. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.65330/4.73186. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.65096/4.75573. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.04866091991018888\n",
      "Epoch 0, Loss(train/val) 4.97677/4.89322. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.85687/4.91657. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.85995/4.87088. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.84835/4.85765. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.84281/4.86495. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.84606/4.87449. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.84850/4.87378. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.84546/4.87238. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.84374/4.87110. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.84299/4.87364. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.84114/4.87475. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.84272/4.87130. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.84136/4.87010. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.84381/4.87122. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.83787/4.86750. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.83976/4.87060. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.84052/4.86807. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.84166/4.86395. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.83855/4.86576. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.83892/4.86264. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.83530/4.86879. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.83948/4.86214. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.83859/4.86715. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.83802/4.86049. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.83770/4.86422. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.83367/4.86578. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.83516/4.86986. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.83819/4.87109. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.83782/4.86133. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.83348/4.87232. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.83578/4.87120. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.83184/4.85285. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.83680/4.85298. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.83961/4.87777. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.83975/4.86545. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.83459/4.86410. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.83383/4.86702. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.83416/4.86345. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.83270/4.85614. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.83358/4.87299. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.83127/4.85772. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.83555/4.86923. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.83163/4.85416. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.83180/4.86197. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.82828/4.87422. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.82854/4.87038. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.82869/4.86752. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.82706/4.87080. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.82521/4.87020. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.82571/4.87827. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.83075/4.86306. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.82637/4.87583. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.82553/4.85043. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.82340/4.87167. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.82358/4.86219. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.82442/4.86435. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.81986/4.85536. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.82398/4.88503. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.81981/4.86641. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.82513/4.88278. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.82587/4.87504. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.83107/4.87096. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.82466/4.86996. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.82447/4.88191. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.82676/4.86602. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.81837/4.87126. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.82288/4.87449. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.82172/4.87421. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.81960/4.86632. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.82113/4.86693. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.82535/4.86088. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.81971/4.86061. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.81834/4.85688. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.81962/4.85277. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.81778/4.86037. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.81857/4.86443. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.81353/4.86759. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.82319/4.86799. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.82079/4.86513. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.82518/4.87151. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.82447/4.87383. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.82176/4.86851. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.82217/4.86177. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.82296/4.86751. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.81696/4.85332. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.81159/4.86777. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.82346/4.86056. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.81835/4.85531. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.81671/4.87983. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.82074/4.86319. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.81419/4.86287. Took 0.14 sec\n",
      "Epoch 91, Loss(train/val) 4.81574/4.86386. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.81508/4.86605. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.81838/4.86106. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.81729/4.86245. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.81549/4.86423. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.81736/4.86759. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.81158/4.87762. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.81382/4.86596. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.81202/4.86628. Took 0.09 sec\n",
      "ACC: 0.546875, MCC: 0.09416472308615167\n",
      "Epoch 0, Loss(train/val) 4.94385/4.92454. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.93972/4.92321. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.92079/4.91446. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.92370/4.91475. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.91911/4.91330. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.92094/4.91420. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.92125/4.91643. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.91975/4.92283. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.91859/4.93159. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.91894/4.93546. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.91900/4.92978. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.91617/4.92693. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.91393/4.92387. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.91035/4.92439. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.90835/4.92446. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.90748/4.93387. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.90938/4.93449. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.90782/4.93220. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.90808/4.92983. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.91304/4.92529. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.90464/4.93377. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.90667/4.93147. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.90604/4.93186. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.90649/4.93722. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.90654/4.93999. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.90662/4.93191. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.90728/4.93327. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.90835/4.92795. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.90476/4.93677. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.90163/4.93235. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.90196/4.95286. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 4.90631/4.93991. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.90386/4.94445. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.90372/4.94334. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.89967/4.94116. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.90650/4.93985. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.90017/4.95741. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.90164/4.94699. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.89843/4.95725. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.90190/4.95623. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.89972/4.95029. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.90158/4.95220. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.89843/4.95502. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.89855/4.97988. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.90073/4.95155. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.89541/4.97963. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.89707/4.97590. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.89760/4.94295. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.89753/4.94807. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.89585/4.96595. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.89592/4.96519. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.89265/4.98765. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.89748/4.94841. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.89779/4.96034. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.88940/4.97006. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.88513/4.99948. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.89508/4.95957. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.89298/4.97424. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.89479/4.96585. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.89223/4.95478. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.88923/4.98023. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.88641/4.98538. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.88803/4.99926. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.89218/4.97822. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.88890/4.97784. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.88756/4.95980. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.88789/4.99853. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.88963/4.97887. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.88212/4.99502. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.88817/4.97140. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.88297/4.98002. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.88761/4.96920. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88944/4.99980. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88307/4.98677. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.88237/4.98352. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.88348/4.98652. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.88562/4.99557. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.88257/4.99370. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.88300/5.01498. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88052/4.97945. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.88624/5.08503. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.91241/4.92269. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.91152/4.93470. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.91065/4.93918. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.90978/4.94291. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.91001/4.94747. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.90880/4.95650. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.90394/4.95993. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.90327/4.96323. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.89848/4.98952. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.89984/4.98667. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.90646/4.98909. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.90595/4.97378. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.89978/4.98052. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.89534/4.99665. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.89569/5.00445. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.90046/5.01185. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.89914/4.99590. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.89825/4.99853. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.89289/5.01944. Took 0.08 sec\n",
      "ACC: 0.390625, MCC: -0.2057101528827944\n",
      "Epoch 0, Loss(train/val) 5.02629/5.01318. Took 0.17 sec\n",
      "Epoch 1, Loss(train/val) 4.94607/4.95380. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 4.94328/4.92629. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.93744/4.92197. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.93517/4.92153. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.93361/4.92570. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.92847/4.92912. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.92952/4.92965. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.92351/4.93104. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.92228/4.94530. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.92019/4.93907. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.92120/4.96062. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.91837/4.94786. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.91569/4.96431. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.91177/4.94035. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.91756/4.94715. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.91564/4.93420. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.91057/4.95392. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.91004/4.95757. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.91241/4.95510. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.91010/4.96561. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.91069/4.96422. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.90345/4.97407. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 4.90184/4.95685. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.90818/4.96826. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.90417/4.96505. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.90310/4.97078. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.90502/4.96029. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.90079/4.96933. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.90494/4.97091. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.90317/4.96051. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.90488/4.97971. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.90217/4.98131. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.89742/4.96717. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.90355/4.98055. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.90351/4.96579. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.89981/4.97217. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.89686/4.98435. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.90058/4.96904. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.90035/4.99001. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.89957/4.98278. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.89951/4.98258. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.90111/4.97923. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.89790/4.98302. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.90048/4.99683. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.90546/4.97771. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.89766/4.98332. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.90024/4.98687. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.89970/4.98253. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.90198/4.98988. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.90399/4.98564. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.89963/4.99242. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.89300/5.00898. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.90205/4.99555. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.89865/4.98698. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.89809/4.99225. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.89649/4.99211. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.89452/4.98883. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.89794/4.99838. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.89350/5.00009. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.90134/4.98439. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89347/5.00725. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.90020/4.99011. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.89568/4.99650. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.89286/5.00711. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.89334/5.00569. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.89663/4.98226. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.89320/4.99666. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.89213/5.00639. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.89255/5.00738. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.89979/4.99348. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.89170/4.99421. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88932/4.99929. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.89443/4.99471. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.88987/4.99677. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.89398/4.99121. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.89333/5.00295. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.89139/4.99853. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.89133/4.99393. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.89206/5.00466. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.89174/5.00489. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.89044/5.01098. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.89279/4.99558. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.89158/5.02607. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.89078/5.01182. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.89035/5.01628. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.89192/5.02020. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.88360/5.01416. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.88791/5.01084. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.89248/5.01120. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.88982/5.00477. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.88551/5.00676. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.88887/5.00097. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.88725/5.01576. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.88748/5.01527. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.88875/5.00458. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.88352/5.01913. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.88805/5.01331. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.88195/5.02217. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.89237/5.01089. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.10677275981195139\n",
      "Epoch 0, Loss(train/val) 4.81396/4.71263. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.74541/4.75700. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.73852/4.77576. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.73419/4.75759. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.73101/4.75884. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.73325/4.76087. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.73225/4.76145. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.73152/4.76021. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.73305/4.76178. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.72987/4.76093. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.72914/4.75762. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.72988/4.75993. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.72862/4.76451. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.72805/4.76253. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.72640/4.76133. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.72730/4.76368. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.72731/4.76859. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.72638/4.76797. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.72460/4.76531. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.72520/4.76443. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.72453/4.76652. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.72503/4.77006. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.72740/4.76531. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.72546/4.76313. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.72299/4.76923. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.72347/4.76282. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.72470/4.76893. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.72436/4.76873. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.72311/4.77136. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.72316/4.76126. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.72280/4.76466. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.72331/4.77771. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.72221/4.77527. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.72122/4.77381. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.72314/4.76938. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.72200/4.77099. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.72314/4.77011. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.72161/4.77339. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.72263/4.77086. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.72496/4.76381. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.72353/4.77341. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.72319/4.77175. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.72025/4.77236. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.72165/4.77123. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.72112/4.77621. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.71995/4.77239. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.72198/4.77138. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.72052/4.78769. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.72115/4.77330. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.72184/4.78406. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.72063/4.77372. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.72308/4.78782. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.71484/4.78180. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.72011/4.79278. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.71952/4.78625. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.71710/4.78821. Took 0.13 sec\n",
      "Epoch 56, Loss(train/val) 4.72330/4.77217. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.72378/4.76214. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.71840/4.77639. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.72309/4.77392. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.72030/4.78415. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.72127/4.78210. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.72072/4.78010. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.71866/4.80080. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.72309/4.78006. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.72279/4.76194. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.72725/4.77667. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.72016/4.77498. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.71859/4.78386. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.71775/4.78560. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.71919/4.79309. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.71626/4.78529. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.71372/4.79514. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.71140/4.80596. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.71473/4.79165. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.71828/4.80128. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.71554/4.78839. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.71122/4.80238. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.71003/4.79979. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.71366/4.80324. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.71367/4.78373. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.70816/4.80198. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.70906/4.80786. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.71353/4.77937. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.71418/4.80883. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.70886/4.79937. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.70792/4.80586. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.71086/4.82298. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.70986/4.81631. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.70325/4.81045. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.70774/4.80074. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.70708/4.81390. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.71415/4.80705. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.70735/4.80786. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.70628/4.82156. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.69886/4.79731. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.72074/4.76191. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.70703/4.77419. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.70219/4.80236. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.70913/4.78806. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.12609970674486803\n",
      "Epoch 0, Loss(train/val) 5.04763/5.03994. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.95422/5.00997. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.95101/5.00208. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.94502/5.00182. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.94613/5.01177. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.94516/5.00996. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.94303/5.01473. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.94402/5.01057. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.94738/5.00945. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.94499/5.00692. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.94677/5.01728. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.94460/5.01068. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.94314/5.01229. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.94322/5.01491. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.94099/5.01961. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.94193/5.01501. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.94346/5.02335. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.94229/5.01715. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.94379/5.01101. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.93966/5.01804. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.94120/5.01582. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.93972/5.01466. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.93666/5.02511. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.93926/5.02477. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.93670/5.02037. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.93752/5.01913. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.93594/5.02277. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.94854/4.99292. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.94374/4.99267. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.94115/5.00886. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.93730/5.02228. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.93490/5.02844. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.94012/5.01549. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.93952/5.01369. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.93160/5.03397. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.93424/5.02510. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.93561/5.03095. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.93551/5.02031. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.93412/5.03399. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.92991/5.03600. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.92914/5.03019. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.92903/5.03552. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.92758/5.04980. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.93564/5.03452. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.93154/5.04410. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.92851/5.04624. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.93233/5.04611. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.92961/5.03935. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.92589/5.06444. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.93185/5.02990. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.93384/5.03580. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.93062/5.02868. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.93256/5.03844. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.93605/5.02262. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.93172/5.02203. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.92847/5.03127. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.93037/5.05209. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.93000/5.04632. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.92693/5.05302. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.92681/5.05539. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.92799/5.04992. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.92629/5.06301. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.93232/5.06822. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.93572/5.05666. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.93157/5.05083. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.93078/5.04826. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.93117/5.08167. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.92805/5.06518. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.92533/5.10880. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.92732/5.05643. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.92930/5.06004. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.92627/5.07576. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.92134/5.07865. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.92176/5.09745. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.92368/5.08580. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.92398/5.06142. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.92046/5.09157. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.92452/5.05888. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.92401/5.06725. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.92553/5.07505. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.91911/5.08710. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.92231/5.08695. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.92475/5.06135. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.92195/5.10571. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.91877/5.08943. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.92217/5.06019. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.92466/5.04937. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.92518/5.05648. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.92652/5.06479. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.92634/5.05826. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.92244/5.06453. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.92587/5.06768. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.92884/5.06962. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.92444/5.06057. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 4.92129/5.07309. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.92655/5.04768. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.91826/5.06780. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.92110/5.06685. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.92516/5.06379. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.91759/5.07716. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.09607689228305229\n",
      "Epoch 0, Loss(train/val) 4.74514/4.73280. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.70828/4.71305. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.70482/4.72226. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.70878/4.72047. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.70612/4.72289. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.70469/4.72713. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.70452/4.73349. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.70104/4.73345. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.70271/4.72630. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.70375/4.72532. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.69917/4.72122. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.70106/4.72833. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.70108/4.73420. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.69832/4.72790. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.69742/4.72907. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.69562/4.72730. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.69791/4.72959. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.69544/4.72916. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.69233/4.73461. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.69226/4.73271. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.69122/4.73093. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.69045/4.72954. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.69762/4.73543. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.69323/4.73707. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.69546/4.73031. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.69095/4.73217. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.69006/4.73684. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.68914/4.74379. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.68771/4.73060. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.69232/4.73808. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.68551/4.74000. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.68460/4.74972. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.68904/4.73037. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.68740/4.74393. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.68794/4.73324. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.68940/4.74313. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.68852/4.74311. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.68674/4.74783. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.68488/4.74282. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.68525/4.74870. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.68827/4.73333. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.67948/4.75480. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.68374/4.73966. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.68773/4.73773. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.68359/4.74109. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.67870/4.73765. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.68418/4.75025. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.68029/4.76513. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.68506/4.74012. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.68016/4.75710. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.67662/4.74502. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.67996/4.75611. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.68257/4.74545. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.67747/4.75796. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.67840/4.75599. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.67508/4.75030. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.67671/4.75741. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.67795/4.78134. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.68080/4.73511. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.67657/4.76113. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.68064/4.72929. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.67762/4.76441. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.67686/4.75215. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.67695/4.77138. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.67932/4.75491. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.67859/4.75312. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.67372/4.76282. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.67999/4.75740. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.67566/4.76680. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.67586/4.76705. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.67103/4.76350. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.67844/4.73048. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.67552/4.76938. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.67822/4.75324. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.67808/4.75655. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.66962/4.79354. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.67871/4.74714. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.67358/4.77283. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.67317/4.76385. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.67439/4.76177. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.67156/4.78112. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.67119/4.76466. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.68162/4.75787. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.66954/4.76843. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.67670/4.76034. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.67161/4.76908. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.67554/4.75380. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.67698/4.76191. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.67129/4.76741. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.67294/4.74124. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.67510/4.75972. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.66771/4.76852. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.67048/4.75546. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.66762/4.75927. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.67306/4.75680. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.67304/4.76585. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.66811/4.76321. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.67179/4.77631. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.67489/4.75230. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.66842/4.76180. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.10121457489878542\n",
      "Epoch 0, Loss(train/val) 4.88839/4.87648. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.86713/4.84557. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.85688/4.85893. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.85356/4.86682. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.85722/4.87189. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.85552/4.86895. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.85784/4.87035. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.85721/4.86979. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.85410/4.86414. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.85628/4.86631. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.85688/4.86515. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.85426/4.86567. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85610/4.86384. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.85400/4.86904. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.85652/4.86553. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.84972/4.86097. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.85226/4.86079. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.85356/4.86050. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.85619/4.85677. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85119/4.84365. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.85485/4.84446. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.84998/4.83460. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.85191/4.84195. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.84975/4.84194. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.85248/4.83585. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.85214/4.83381. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.84916/4.82678. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.84979/4.82358. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.85038/4.83869. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84809/4.83806. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.85012/4.82980. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.85347/4.83832. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 4.84896/4.84446. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.85106/4.83569. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.85033/4.84080. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84834/4.83831. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84853/4.83910. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84548/4.83724. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.84689/4.84216. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84793/4.83832. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84335/4.83634. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.84444/4.83705. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84443/4.83204. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.84645/4.83831. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.84328/4.83177. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.84501/4.83761. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.84305/4.85228. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.84281/4.84259. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84371/4.84357. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.84377/4.84397. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.84497/4.84485. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.84143/4.84241. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.84316/4.84813. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.84096/4.83910. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.84197/4.84229. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.83832/4.82936. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.84467/4.84064. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.84063/4.83639. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.83730/4.83194. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.84116/4.84305. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.84036/4.84045. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83552/4.83802. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.83904/4.85117. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.83834/4.84961. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.83734/4.85519. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.83822/4.83736. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.84026/4.84140. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.84018/4.85591. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.83436/4.84981. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83654/4.84565. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 4.83650/4.85232. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.83539/4.84803. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83818/4.84377. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.83740/4.85044. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83536/4.84830. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83786/4.85204. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.83383/4.84599. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.83462/4.84689. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.83290/4.84526. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.83441/4.85510. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83001/4.86136. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.83477/4.85283. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.83880/4.85907. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.83337/4.85417. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83165/4.85722. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83178/4.85943. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83442/4.85127. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.82820/4.86101. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83276/4.85562. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.83273/4.85132. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.83480/4.84875. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.82300/4.84641. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83796/4.84974. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83723/4.85301. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.82912/4.85969. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.83112/4.85801. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.82740/4.85675. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.82824/4.85061. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.83440/4.85244. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.82825/4.85909. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.1111111111111111\n",
      "Epoch 0, Loss(train/val) 4.73872/4.78958. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.73113/4.72274. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.71856/4.72383. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.70558/4.72377. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.70212/4.72894. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.70358/4.73114. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.70397/4.72780. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.70093/4.72723. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.70705/4.72681. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.70526/4.72187. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.70187/4.72686. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.70340/4.73118. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.70219/4.72865. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.70536/4.72271. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.69970/4.72580. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.69706/4.73196. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.70113/4.72827. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.69391/4.73179. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.70412/4.72993. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.69662/4.73377. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.69727/4.73108. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.69349/4.74237. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.69741/4.72577. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.69566/4.72576. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.69305/4.72912. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.69234/4.74027. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.69467/4.73412. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.69399/4.73268. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.69005/4.73920. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.69075/4.73495. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 4.69287/4.72787. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 4.68721/4.74708. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.68728/4.75154. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.68387/4.76001. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.69291/4.73676. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.69409/4.74125. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.69984/4.73116. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.69834/4.72953. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.69820/4.73273. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.69759/4.73575. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.69250/4.74330. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.69899/4.73214. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.69366/4.74039. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.69642/4.73672. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.69624/4.72748. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.69452/4.73196. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.68856/4.75093. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.69139/4.72978. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.68903/4.73900. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.68830/4.74492. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.69065/4.72871. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.69261/4.75032. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.68422/4.74653. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.68922/4.73545. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.69498/4.73016. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.69859/4.72507. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.69098/4.73767. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.69242/4.73742. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.69185/4.73841. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.68876/4.72705. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.69154/4.72927. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.68826/4.72970. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.69108/4.73187. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.69052/4.71845. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.68663/4.71824. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.68828/4.71022. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.68286/4.72944. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.69022/4.72867. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.68555/4.73386. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.68637/4.72392. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.68371/4.71487. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.69182/4.72393. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.67998/4.71894. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.68483/4.72798. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.68686/4.71595. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.67868/4.73009. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.69299/4.73826. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.68155/4.70701. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.68798/4.72333. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.68147/4.72672. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.67900/4.73393. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.67917/4.74382. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.68811/4.73098. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.68918/4.71787. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.68487/4.71748. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.68596/4.72135. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.68263/4.72054. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.68926/4.72402. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.68772/4.71319. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.68207/4.71770. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.68557/4.71454. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.68557/4.70062. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.67693/4.71540. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.68087/4.73010. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.68993/4.71435. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.68237/4.73077. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.68124/4.72545. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.68197/4.73357. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.68306/4.72663. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.68470/4.72833. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.040082172520323485\n",
      "Epoch 0, Loss(train/val) 4.81765/4.79842. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.80972/4.78413. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.79753/4.78224. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79354/4.78320. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79118/4.78259. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79159/4.78064. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.78993/4.78166. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.78710/4.78501. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.78658/4.78939. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.78455/4.78801. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.78315/4.78889. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.78054/4.79252. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.78060/4.78947. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.78282/4.78885. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.78345/4.79171. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 4.78242/4.79125. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78697/4.77504. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.78498/4.77677. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.78286/4.77683. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.78061/4.78191. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.77929/4.78948. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.77663/4.78949. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.78120/4.78761. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.77593/4.79436. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.77936/4.79256. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.78077/4.79428. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.77550/4.80447. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.77688/4.79856. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77693/4.80779. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.77570/4.80394. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.77683/4.80257. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.77506/4.79187. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.77364/4.79127. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.76992/4.79890. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.76688/4.80743. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.77021/4.79838. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.77200/4.81790. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.76783/4.80074. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.77354/4.80761. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.77489/4.80133. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.76859/4.81165. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.76661/4.80245. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.76770/4.80773. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.76827/4.80769. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.76969/4.80386. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.77221/4.78414. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.76892/4.79613. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.77229/4.79367. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77260/4.79279. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.76821/4.79818. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.76859/4.79723. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.76336/4.80485. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.77032/4.81341. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.76055/4.79828. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.76796/4.80168. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.76123/4.81107. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.76693/4.79614. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.76225/4.80075. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.76452/4.80707. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.75946/4.81884. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.76267/4.81833. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.76442/4.81345. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.76130/4.81945. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.75889/4.82406. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.76899/4.79214. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.75698/4.81424. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.77141/4.80261. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.77002/4.79200. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.76143/4.81344. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.75495/4.80863. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.76183/4.80178. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.75432/4.81147. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.75773/4.83019. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.75186/4.81478. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.78757/4.80319. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.75747/4.82297. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78669/4.79352. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.77339/4.78588. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.76710/4.80498. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.76660/4.78550. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.76693/4.78611. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.76418/4.78313. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.76194/4.78743. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76823/4.79020. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.76520/4.78858. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.75268/4.80741. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.75947/4.80638. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.75868/4.81234. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.76547/4.78935. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.76229/4.79288. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.76495/4.78678. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.76148/4.80236. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.75356/4.77908. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.75921/4.78735. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.75220/4.78438. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.76180/4.77504. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.76673/4.79628. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.76495/4.80323. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.75527/4.78963. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.75596/4.78657. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.09607689228305229\n",
      "Epoch 0, Loss(train/val) 4.90875/4.88927. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.86834/4.87855. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.86852/4.87711. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.86912/4.87964. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.86875/4.87458. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.86954/4.87825. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.86831/4.88734. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.86617/4.89087. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.85840/4.89953. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.85935/4.90059. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.85558/4.90750. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.85922/4.90231. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85372/4.90590. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.85573/4.91103. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.85106/4.91196. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 4.84708/4.91824. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.84998/4.92909. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.85108/4.92391. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.85083/4.90935. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.84370/4.90789. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.84804/4.90396. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.84487/4.91682. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.84808/4.90868. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.84362/4.91941. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.84755/4.90317. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.84372/4.91219. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.84690/4.87950. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.85691/4.86555. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.85797/4.88830. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84781/4.90805. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.85189/4.90096. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.84268/4.89829. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.84177/4.89640. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.83980/4.89751. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.84565/4.88453. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.83930/4.89766. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.83783/4.89802. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84247/4.90685. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.84220/4.91629. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84015/4.90114. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.83842/4.89718. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.83207/4.91130. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84305/4.88811. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.83699/4.90773. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.83178/4.89731. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.83190/4.89885. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.83386/4.89740. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.83349/4.90014. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.83509/4.89766. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.83709/4.90359. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.83568/4.89815. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.83179/4.89699. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.83297/4.89735. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.83333/4.90311. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.83241/4.89676. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.83222/4.89010. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.82885/4.88473. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.83265/4.89745. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.82901/4.88850. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83249/4.89655. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.82937/4.89770. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83306/4.88515. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.83062/4.88360. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.83080/4.88618. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.82952/4.87853. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.82558/4.88570. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.83092/4.88923. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.83420/4.89485. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.82895/4.89507. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83341/4.89423. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.82834/4.90341. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.82915/4.89405. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83301/4.88806. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.82674/4.88970. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.82632/4.89372. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83154/4.87215. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.82620/4.88756. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.82761/4.88069. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.82841/4.88523. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.82919/4.89493. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.82944/4.88966. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.82817/4.88872. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.82684/4.87320. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.82528/4.88393. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.82560/4.88515. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.83105/4.88794. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83028/4.88580. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.82455/4.88275. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.82206/4.88470. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.82048/4.87770. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.83361/4.88376. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.82633/4.91308. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.82436/4.90921. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.82607/4.88734. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.82610/4.86974. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.82947/4.88649. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.82387/4.87461. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.81906/4.90669. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.82389/4.90532. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.82249/4.88943. Took 0.08 sec\n",
      "ACC: 0.609375, MCC: 0.21971768720102058\n",
      "Epoch 0, Loss(train/val) 4.85486/4.79222. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.79358/4.80511. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.79078/4.81599. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.79206/4.82200. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.79192/4.81578. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 4.79355/4.81063. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.79599/4.81097. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.78887/4.81073. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.78637/4.80283. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.78892/4.80581. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.78505/4.81382. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.78909/4.80214. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.78685/4.80572. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.78451/4.80608. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.78490/4.80716. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.78538/4.80788. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.78334/4.81087. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.78296/4.81651. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.78296/4.81748. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.78510/4.81380. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.78223/4.81778. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78139/4.81665. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.77588/4.82507. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.77704/4.82228. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.78017/4.82001. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.77963/4.81965. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.77822/4.82471. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.77818/4.82622. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77636/4.82505. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.77804/4.81632. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.77793/4.82544. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.77205/4.82974. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.77949/4.81122. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.77342/4.82173. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.77100/4.83215. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.77790/4.81293. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.77689/4.82169. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.77284/4.81787. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.77326/4.81804. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.77479/4.81570. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.77109/4.82486. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.77322/4.81878. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.76893/4.82648. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.77620/4.82234. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.76858/4.83352. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.77540/4.81715. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.76602/4.82961. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77052/4.81902. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.76693/4.82034. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.77309/4.81089. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.76890/4.82038. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.76782/4.82938. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.77098/4.82127. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.76966/4.81816. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.77196/4.82975. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.76639/4.83153. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.76822/4.82541. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.76561/4.83414. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76749/4.83059. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.76938/4.81005. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.76729/4.82323. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.76266/4.82188. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76907/4.82140. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.76817/4.81653. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.76755/4.82688. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.77074/4.82783. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.76410/4.83612. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.76993/4.80504. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.76399/4.82596. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76498/4.82429. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.76216/4.82011. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.76580/4.80434. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76235/4.81631. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.76772/4.82046. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.76683/4.82471. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.75400/4.84641. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.76410/4.81127. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.76555/4.81780. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.75996/4.83203. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.76731/4.82823. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.76431/4.81122. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.76419/4.81730. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.76075/4.83421. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76581/4.80655. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.75177/4.83996. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.76438/4.82118. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.75867/4.83453. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.75819/4.82986. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.75723/4.84382. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.75919/4.83341. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.76254/4.81469. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.76063/4.82997. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.75776/4.82870. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.75566/4.82915. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.76071/4.82051. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.75960/4.83667. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.75972/4.82288. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.75169/4.83157. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.76463/4.82927. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.75370/4.84655. Took 0.09 sec\n",
      "ACC: 0.46875, MCC: -0.059863071616150634\n",
      "Epoch 0, Loss(train/val) 5.14655/5.04068. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.04795/5.03892. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 5.04898/5.03680. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.04663/5.03669. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.04543/5.03567. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.04952/5.03582. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.04678/5.03807. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.04440/5.04184. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.04251/5.04418. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.04031/5.04172. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.04335/5.04464. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.03828/5.04185. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.03811/5.04381. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.03816/5.04767. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 5.03893/5.04690. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.03731/5.04890. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.03708/5.05293. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.03322/5.05442. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.03530/5.05262. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.03235/5.05310. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.03196/5.05159. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.03184/5.05926. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.03130/5.05566. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.02971/5.05533. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.02932/5.05581. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.02813/5.06349. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.02948/5.05258. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 5.02578/5.06494. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.02768/5.07337. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 5.02360/5.07193. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.02605/5.06349. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.02337/5.08329. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.02241/5.07117. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.01803/5.08558. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.02457/5.04220. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.01775/5.07840. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.01638/5.08382. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 5.01828/5.07144. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.01985/5.09136. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.01533/5.10174. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.01832/5.07999. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.01574/5.09354. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.01628/5.08021. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.01073/5.16749. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 5.00857/5.05834. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.03879/5.03195. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.03779/5.04922. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.03053/5.06178. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.02626/5.05562. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.02388/5.08257. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.01945/5.08502. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.01911/5.06720. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.01367/5.08800. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.01091/5.08800. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.01291/5.07941. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.01001/5.11565. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.01357/5.07638. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.01682/5.08300. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.00608/5.11506. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 5.01285/5.07695. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.00937/5.12231. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.00503/5.10869. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 5.01157/5.09467. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.00641/5.13606. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.00425/5.10192. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.00612/5.11840. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.00674/5.11758. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.00942/5.10053. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.00641/5.13436. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.00287/5.11476. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.00829/5.10675. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.00646/5.11549. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.99935/5.14844. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.00231/5.07751. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.00226/5.14115. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.00269/5.10574. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.00073/5.11815. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.99896/5.10908. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.99813/5.12777. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.00587/5.08840. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.00032/5.10600. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 5.00624/5.12605. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 5.00885/5.06379. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.99413/5.12689. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.00790/5.07370. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.00324/5.10956. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.99521/5.10430. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.99954/5.14238. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.99713/5.09721. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 5.00408/5.12324. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.99368/5.12103. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.99828/5.12718. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.99746/5.09858. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.00103/5.12687. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.99628/5.12474. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.99652/5.15728. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.99537/5.10236. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.99369/5.13803. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.99794/5.13634. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.99297/5.13664. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.0778902520624173\n",
      "Epoch 0, Loss(train/val) 5.27241/5.19259. Took 0.19 sec\n",
      "Epoch 1, Loss(train/val) 5.21375/5.19443. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 5.20867/5.19513. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 5.20482/5.19584. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.20461/5.19957. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 5.19811/5.20366. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 5.20099/5.20713. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.19796/5.21112. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.19820/5.20837. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.19807/5.21395. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.19474/5.22021. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.19472/5.21435. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.19380/5.22422. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 5.19277/5.22963. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 5.19281/5.21645. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 5.19338/5.22538. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.19124/5.22496. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.19022/5.22001. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.18993/5.21738. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.19062/5.22499. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 5.19134/5.22935. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.18595/5.24241. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 5.18808/5.22441. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 5.18589/5.22500. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 5.18273/5.22986. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 5.18607/5.21909. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.18620/5.22890. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 5.18305/5.22830. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 5.18287/5.23723. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.18309/5.22654. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.18397/5.23519. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.18080/5.23602. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 5.17956/5.24085. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.18250/5.24477. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 5.18066/5.24028. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 5.18185/5.22962. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 5.18096/5.22999. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.17239/5.24511. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.18016/5.24616. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.17746/5.24344. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.17474/5.25215. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 5.18347/5.24792. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.18305/5.26297. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.18183/5.23440. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.18389/5.24419. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 5.18113/5.24878. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.17634/5.25714. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.17767/5.25027. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 5.17951/5.25734. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.17729/5.25135. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.17126/5.26435. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 5.17635/5.25041. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.17390/5.24770. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 5.17424/5.26133. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.17450/5.24431. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 5.17343/5.27030. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.16787/5.27656. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.17695/5.28548. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.17001/5.25317. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.17046/5.26382. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.17824/5.26592. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.17868/5.23618. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.17505/5.26202. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.17619/5.25339. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.17510/5.24801. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.17372/5.24735. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 5.17939/5.23453. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.17896/5.24836. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.17538/5.27273. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.17853/5.23814. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 5.18000/5.23928. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.17092/5.25728. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.17521/5.24322. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.17396/5.25105. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.17673/5.23554. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.16900/5.24913. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.17632/5.24327. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.17035/5.25052. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.17014/5.26198. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.17573/5.24465. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 5.17161/5.24022. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.17794/5.23652. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.18612/5.20576. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.18099/5.23610. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.18360/5.21966. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.17566/5.23867. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.17548/5.22245. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.18340/5.22543. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.17761/5.22968. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.17294/5.23357. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.17109/5.23158. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.17652/5.23421. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.17207/5.22844. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.17558/5.24397. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.17361/5.25011. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.17092/5.25374. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.17529/5.23795. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 5.17268/5.24958. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.17409/5.24918. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.16910/5.26810. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: 0.004102738897530597\n",
      "Epoch 0, Loss(train/val) 5.08178/5.02811. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.99560/4.99187. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.98379/4.99374. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.98074/5.00604. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.98233/5.01457. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.98087/5.02744. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.97852/5.03089. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.98263/5.00443. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.97855/5.00303. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.97666/5.01434. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.97873/5.01074. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.97884/5.01002. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.97981/5.01198. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.97727/5.01395. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.97491/5.00940. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.97652/5.01549. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.97533/5.01720. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.97387/5.01876. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.97312/5.02474. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.97399/5.01575. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.97255/5.01808. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.97157/5.03324. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.97206/5.02904. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.97120/5.03052. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.96908/5.04215. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.96938/5.02048. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.96820/5.03475. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.96738/5.02700. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.96592/5.04043. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.96487/5.03433. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.96613/5.04432. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.96697/5.02770. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.96441/5.09040. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.96866/5.01891. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.96868/5.03134. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.96685/5.01926. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.96940/5.02220. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.96394/5.04588. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.96311/5.04973. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.96928/5.03384. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.96533/5.01757. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.96238/5.04134. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.96374/5.03995. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.96640/5.02683. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.96035/5.04253. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.96139/5.04172. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.96255/5.04049. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.95646/5.04743. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.95964/5.05533. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.96334/5.02531. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.96197/5.03794. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.96196/5.03861. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.95591/5.04762. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.95952/5.06325. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.95905/5.03978. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.96574/5.04038. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.96914/5.02975. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.96532/5.03908. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.96445/5.06065. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.96252/5.04384. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.96284/5.04446. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.95544/5.06155. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.96235/5.05067. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.96164/5.05092. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 4.95566/5.06076. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.95790/5.06431. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.95841/5.06562. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.95629/5.06165. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.95370/5.04888. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.95743/5.06127. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.95548/5.06873. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.95043/5.06945. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.95239/5.09026. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.95960/5.03105. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.95659/5.06226. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.95484/5.06554. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.94525/5.09460. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.96018/5.04622. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.95386/5.05193. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.94837/5.08634. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.94808/5.11594. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.95425/5.04709. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.95770/5.05594. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.94562/5.07652. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.95843/5.06647. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.95887/5.03949. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.95238/5.06673. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.95545/5.08541. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.95373/5.06271. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.95090/5.08546. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.95049/5.07089. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.94946/5.08262. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.95235/5.05405. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.95324/5.07456. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.95297/5.07014. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.94559/5.09180. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.94971/5.06475. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.95548/5.06814. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.94857/5.07977. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.94472/5.08634. Took 0.08 sec\n",
      "ACC: 0.59375, MCC: 0.19136555680572745\n",
      "Epoch 0, Loss(train/val) 4.83262/4.83928. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.78769/4.79241. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.78747/4.81128. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.78516/4.80996. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.78388/4.81362. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.78205/4.81273. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.78189/4.81651. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.77997/4.82406. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.78030/4.81915. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.77823/4.83136. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.77704/4.81725. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.77599/4.82656. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.77704/4.81713. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.77742/4.81614. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.77679/4.81499. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.77347/4.82124. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.77423/4.80740. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.77487/4.81481. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.77266/4.81895. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.77283/4.81745. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.77207/4.81793. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.77445/4.81119. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.77344/4.82530. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.77676/4.81213. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.77477/4.81678. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.77202/4.81869. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.77079/4.81296. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 4.77078/4.81651. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77326/4.81090. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.77279/4.81103. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.77855/4.78303. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78005/4.78390. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.77725/4.79160. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78057/4.79341. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.77723/4.79028. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.77566/4.79958. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.77419/4.80171. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.77426/4.80498. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.77400/4.80478. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.77178/4.80673. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.77165/4.80681. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.76847/4.81167. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.77241/4.81063. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.77353/4.80314. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.77241/4.78299. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.76978/4.77346. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77344/4.77008. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77289/4.81201. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.76763/4.80314. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.77049/4.78649. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77200/4.76395. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77229/4.78674. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.76954/4.78754. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.76574/4.78702. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.76837/4.79885. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.76848/4.82884. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.77045/4.82737. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.77199/4.80999. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76777/4.83360. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.76370/4.81684. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.76677/4.81015. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.76395/4.82325. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76916/4.82059. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.76293/4.81662. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.75718/4.81858. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.76435/4.80604. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.76378/4.81634. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.75912/4.82780. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.76620/4.81026. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.76367/4.80721. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.76157/4.81893. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.76254/4.80819. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76367/4.82030. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.75692/4.82206. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.76420/4.81230. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.76044/4.83062. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.76309/4.82383. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.76398/4.81120. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.76333/4.81045. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.76925/4.81391. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.76332/4.80536. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.76477/4.81742. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.76245/4.82219. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76258/4.81580. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.76150/4.83028. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.75940/4.82023. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.75974/4.82128. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.75885/4.82084. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.76055/4.80614. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.76481/4.82556. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.75856/4.82319. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.75839/4.83370. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.76139/4.82239. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.75714/4.81092. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.75350/4.85837. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.75699/4.82001. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.75750/4.82493. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.75219/4.83568. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.75429/4.86201. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.75182/4.83347. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.0625\n",
      "Epoch 0, Loss(train/val) 4.79984/4.66963. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.72919/4.68911. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.72016/4.68969. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.72646/4.69410. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.72635/4.69018. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.72602/4.68924. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.72636/4.69419. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.72548/4.69639. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.72418/4.69747. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.72083/4.70136. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.71705/4.70395. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.72136/4.70442. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.72026/4.70430. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.71988/4.70580. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.72063/4.70675. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.71846/4.71144. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.71696/4.71052. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.71396/4.71495. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.71532/4.71595. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.71859/4.71284. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.71385/4.72312. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.71431/4.72234. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.71180/4.73105. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.71563/4.72148. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.71695/4.73322. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.71396/4.73074. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.71343/4.74859. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.71414/4.72819. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.71050/4.74471. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.71083/4.74941. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.71354/4.72917. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.71083/4.74417. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.70855/4.75492. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.71040/4.73752. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.70901/4.75479. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.71149/4.73630. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.70568/4.77326. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.71049/4.74547. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.70578/4.75363. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.71292/4.71599. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.71296/4.71636. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.71146/4.73356. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.71177/4.72809. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.71203/4.73592. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.71126/4.73249. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.70918/4.74242. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.71159/4.73562. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.70939/4.74600. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.70534/4.74419. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.70891/4.73405. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.70831/4.74766. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.70737/4.73877. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.70865/4.74472. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.70481/4.73544. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.70779/4.73757. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.70593/4.74085. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.70406/4.73843. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.70865/4.74274. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.70506/4.74020. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.70235/4.73648. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.70454/4.74404. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.70617/4.74195. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.71165/4.72359. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.70808/4.73783. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.70435/4.74606. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.70733/4.72626. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.71222/4.72823. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.70186/4.74206. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.70166/4.75058. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.70199/4.74856. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.70242/4.75780. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.70070/4.74743. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.70115/4.74970. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.70305/4.74354. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.70117/4.74229. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.70464/4.74404. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.69883/4.74992. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.70217/4.74923. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.70161/4.74081. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.70079/4.75145. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.70537/4.76967. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.69679/4.76121. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.69616/4.74684. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.69992/4.75028. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.70630/4.73775. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.69807/4.74492. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.69362/4.74660. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.69652/4.74996. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.69704/4.76192. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.69512/4.76583. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.70152/4.74812. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.69756/4.74478. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.69316/4.75721. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.69493/4.76346. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.69595/4.76850. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.69587/4.75958. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.69787/4.75968. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.70056/4.75526. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.69794/4.75245. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.69590/4.75487. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.049929648757959806\n",
      "Epoch 0, Loss(train/val) 4.87084/4.81761. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.81477/4.78618. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.80897/4.78276. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.80462/4.78104. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.80419/4.78040. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.80195/4.77860. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.80339/4.77954. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.80187/4.77872. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.80001/4.77945. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79944/4.77548. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79902/4.77264. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.79878/4.76540. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.79632/4.77208. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.79565/4.77143. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.79437/4.76425. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.79462/4.75955. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.79173/4.75852. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.79203/4.76071. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.79388/4.76248. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.79076/4.76391. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.79195/4.76296. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78945/4.76408. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.79033/4.76254. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.79237/4.76834. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.79114/4.76444. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.79102/4.76188. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.79286/4.76551. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.78960/4.76167. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.78741/4.76296. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.78876/4.76463. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.78915/4.76417. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78821/4.75947. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.78769/4.76344. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78899/4.76128. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78642/4.76442. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.78764/4.75980. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.78560/4.75866. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.78607/4.75849. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.78258/4.75914. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.78462/4.76136. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.78442/4.76547. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.78124/4.75730. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.78305/4.76606. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.78098/4.77853. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.77932/4.76775. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.78549/4.77154. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.78225/4.74942. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.78303/4.77648. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77882/4.77530. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.78390/4.77582. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77600/4.76785. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77935/4.76691. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.77547/4.76574. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.77419/4.75280. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.78902/4.78505. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.80125/4.79128. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.79472/4.78059. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.79037/4.76016. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.78948/4.76142. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.78906/4.76032. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.78956/4.75354. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.78356/4.75409. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.78477/4.75958. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.78573/4.75176. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.78416/4.74563. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.78628/4.75822. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.78343/4.76516. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.78362/4.75821. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.77999/4.76007. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.78328/4.76982. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.78275/4.76214. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.77897/4.76373. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.78099/4.76028. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.78609/4.75827. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.78167/4.76000. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.78072/4.76927. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.77845/4.76064. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78080/4.76547. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.78186/4.76683. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.77779/4.76776. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.78111/4.76814. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.77974/4.76702. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.77925/4.78146. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.78306/4.77301. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.77521/4.77098. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.77856/4.78242. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.77717/4.77994. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.77462/4.78276. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.77143/4.78517. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.77968/4.78442. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.77707/4.79359. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.77435/4.77922. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.77719/4.77754. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.77630/4.77130. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.77546/4.77831. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.77895/4.76310. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.77925/4.79081. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.77427/4.77732. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77208/4.77359. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.77377/4.80828. Took 0.08 sec\n",
      "ACC: 0.625, MCC: 0.2324766006178762\n",
      "Epoch 0, Loss(train/val) 4.74473/4.78489. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.75016/4.73444. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.73084/4.74907. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.73277/4.75649. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.73633/4.75273. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.73699/4.74129. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.74102/4.72691. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.73737/4.72234. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.73352/4.72279. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.72831/4.72245. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 4.73064/4.72131. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.73131/4.72119. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.73128/4.72152. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.72986/4.72283. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.72883/4.72474. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.72713/4.72579. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.72811/4.72342. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.72655/4.72519. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.72753/4.72409. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.72558/4.72022. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.72387/4.72285. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.72391/4.72391. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.72505/4.72084. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.72320/4.71872. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.72157/4.71687. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.72321/4.71394. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.72244/4.71310. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.71946/4.71025. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.71761/4.71118. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.72143/4.70653. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.72292/4.70784. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.71740/4.70835. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.71844/4.70936. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.71660/4.71205. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.71990/4.71079. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.71488/4.70486. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.71804/4.70804. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.71487/4.71203. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.71816/4.71230. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.71398/4.71522. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.71233/4.71670. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.71695/4.71026. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.71146/4.71552. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.71786/4.71184. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.71328/4.71449. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.71218/4.71532. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.70791/4.72573. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.71083/4.72034. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.70789/4.71270. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.70385/4.72201. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.71489/4.72062. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.71317/4.72045. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.70701/4.73053. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.71131/4.73095. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.71169/4.72740. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.70535/4.73637. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.71492/4.72412. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.70426/4.73198. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.71257/4.71924. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.70538/4.72192. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.70679/4.72022. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.69641/4.71798. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.70731/4.72939. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.70652/4.71663. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.69902/4.73634. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.70566/4.72697. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.70201/4.72950. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.70982/4.74240. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.70536/4.73049. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.70759/4.72778. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.70665/4.72783. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.70544/4.72428. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.70351/4.72134. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.70348/4.71830. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.70311/4.72834. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.70289/4.73915. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.70536/4.73328. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.69731/4.72433. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.70225/4.73738. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.69968/4.73479. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.70475/4.71860. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.68995/4.72464. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.70130/4.72805. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.70357/4.72881. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.69868/4.72190. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.71104/4.69000. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.71609/4.69403. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.71733/4.70042. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.70689/4.69763. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.71308/4.70695. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.70625/4.70532. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.70745/4.71480. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.70442/4.72379. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.70469/4.71413. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.70459/4.70508. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.70172/4.71477. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.69865/4.71371. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.70582/4.71143. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.70748/4.72275. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.70278/4.70228. Took 0.09 sec\n",
      "ACC: 0.4375, MCC: -0.1659919028340081\n",
      "Epoch 0, Loss(train/val) 4.94602/4.91847. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.91465/4.96055. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.90955/4.94411. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.91452/4.94134. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.91067/4.94478. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.91058/4.95067. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.90861/4.95561. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.90801/4.94625. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.91035/4.95338. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.90567/4.96499. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.90819/4.97476. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.90890/4.96198. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.90671/4.96242. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.90981/4.95222. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.90864/4.93641. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.90310/4.92171. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.90385/4.92520. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.90268/4.92359. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.90157/4.92737. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.90367/4.93080. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.89883/4.92498. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.90443/4.92492. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.89982/4.91799. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.90126/4.91934. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.89902/4.91793. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.89723/4.92216. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.89815/4.92357. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.89823/4.93732. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.90115/4.92190. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.89948/4.92910. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.89899/4.91828. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.89620/4.92894. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.89382/4.91117. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.90074/4.92259. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.89932/4.91433. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.89715/4.92714. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.89645/4.91707. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.89688/4.92073. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.89322/4.91965. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.89062/4.92918. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.89139/4.92823. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.89454/4.93608. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.89028/4.92896. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.89048/4.92869. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.88714/4.93028. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.89204/4.93721. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.88788/4.93954. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.89152/4.91593. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.88582/4.92613. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.89152/4.92312. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.88385/4.92703. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.88461/4.94430. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.88400/4.92099. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.88449/4.92636. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.89046/4.92918. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.88957/4.93372. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.88300/4.92916. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.88319/4.93368. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.88333/4.94632. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.88759/4.93093. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.88079/4.92707. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.88456/4.94330. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.88055/4.92266. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.88183/4.95732. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.88355/4.94094. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.87723/4.92656. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.88642/4.94532. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.88429/4.92995. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.87604/4.93813. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.88535/4.92915. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.86906/4.94301. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.87802/4.94311. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88171/4.92987. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.87331/4.93671. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.87171/4.92040. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.87979/4.94503. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.87150/4.95249. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.87572/4.93940. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.88113/4.93993. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.87847/4.92950. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.87558/4.92500. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.87921/4.92789. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.87355/4.92634. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.87345/4.92913. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.87458/4.93497. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.86981/4.93632. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.87296/4.92925. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.88020/4.90612. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.87319/4.93358. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88207/4.93032. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.87293/4.92802. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.87402/4.94145. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.87890/4.93372. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.87325/4.93670. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.87327/4.93532. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.87637/4.93083. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.87175/4.93820. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.86882/4.93465. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.87126/4.94769. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.87084/4.95593. Took 0.08 sec\n",
      "ACC: 0.671875, MCC: 0.3522819383711917\n",
      "Epoch 0, Loss(train/val) 4.96842/4.90721. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.91522/4.91298. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.91562/4.91264. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.91180/4.90624. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.91373/4.90608. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.91078/4.90496. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.91408/4.89693. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.90487/4.90751. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.90760/4.90706. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.91097/4.90024. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.90677/4.91654. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.90646/4.92419. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.91338/4.92534. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.90726/4.91853. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.90255/4.91391. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.90750/4.90432. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.89939/4.90491. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.89764/4.91243. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.89787/4.91069. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.89973/4.91508. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.89512/4.91017. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.89984/4.91557. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.89207/4.90792. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.89201/4.91138. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.89374/4.90289. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.89319/4.89702. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.88820/4.91198. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.88771/4.93268. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.89961/4.92305. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.89946/4.95323. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.90681/4.92063. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.89911/4.92662. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.89642/4.92921. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.89481/4.91899. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.90007/4.92353. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.89632/4.92590. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.89262/4.92734. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.89388/4.92413. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.89324/4.92486. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.89343/4.93111. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.89020/4.93061. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.89189/4.93462. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.89295/4.94066. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.89218/4.93327. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.88960/4.95080. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.89098/4.94045. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.88826/4.94203. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.89206/4.93126. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.88360/4.95290. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.88993/4.92932. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.89231/4.93142. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.88912/4.93994. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.88282/4.95791. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.88586/4.94412. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.88043/4.97363. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.88438/4.92343. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.88784/4.93718. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.88723/4.93359. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.88812/4.93251. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.87906/4.94799. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.88215/4.93716. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.88381/4.94688. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.88553/4.91978. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.88360/4.93811. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.88468/4.93140. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.88622/4.93044. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.87171/4.94832. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.87948/4.93148. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.88049/4.93004. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.91433/4.89828. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.90436/4.90217. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.90226/4.90241. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.89996/4.90749. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.89217/4.91632. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.89548/4.91469. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.88996/4.93315. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88546/4.93002. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.88307/4.93845. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.89410/4.92771. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88065/4.94164. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.88535/4.93088. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.88423/4.92106. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.87925/4.93745. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.87800/4.94484. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.88681/4.92529. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.88680/4.91841. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.87835/4.96334. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.88145/4.94730. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.88219/4.92760. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.87819/4.94534. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.87797/4.93055. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.87655/4.93594. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.87535/4.94683. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.87690/4.94361. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.87992/4.90762. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.87984/4.95480. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.87315/4.93442. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.87089/4.94529. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.87612/4.95838. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.88187/4.92493. Took 0.08 sec\n",
      "ACC: 0.3125, MCC: -0.3694581280788177\n",
      "Epoch 0, Loss(train/val) 5.02387/4.94932. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.94537/4.94777. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.94778/4.95021. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.94914/4.95060. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.94546/4.95020. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.94815/4.95094. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.94750/4.95274. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.94513/4.95660. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 4.94334/4.95850. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.94671/4.96273. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.94180/4.96489. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.94534/4.96596. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.94342/4.96719. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.93803/4.96387. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.94224/4.96506. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.94380/4.96334. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.94030/4.97219. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.94014/4.98631. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.94162/4.98758. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.93792/4.98825. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.93687/4.98634. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.93210/5.00302. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.94803/4.96783. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.93660/4.98534. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.94093/4.97906. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.93411/4.99014. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.93024/4.99632. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.93398/5.00243. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.93073/4.99339. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.93305/4.99809. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.93473/4.99225. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.92799/5.00297. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.93288/5.00030. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.92980/4.99866. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.92948/5.00030. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.92167/5.00086. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.92413/5.00765. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.93228/4.99787. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.91924/5.01208. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.92447/4.99973. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.92680/4.99456. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.92965/4.99074. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.92643/4.99513. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.92856/4.98694. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.92543/4.98928. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.91942/4.99676. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.92577/4.99700. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.92389/5.00280. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.92122/5.01746. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.92475/4.99061. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.92785/4.99505. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.92948/4.96669. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.92969/4.98469. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.92810/4.97793. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.92018/5.01736. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.93134/4.97338. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.92313/4.98044. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.91893/5.00610. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.91896/5.02282. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.93527/4.97152. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.92626/4.98568. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.92898/4.97691. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.92213/4.98495. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.92717/4.98322. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.92053/4.99246. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.92485/4.99135. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.91985/4.99662. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.92155/4.99891. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.91824/4.99533. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.91866/4.99862. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.91998/5.00465. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.92088/4.99117. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.91686/5.00623. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.91735/5.00556. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.91590/5.01840. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.91487/5.00701. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.91723/5.02790. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.90963/5.02000. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.92002/5.02636. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.92223/5.00565. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.91999/5.00560. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.91821/5.00967. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.91325/5.01072. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.91136/5.02553. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.91073/5.02394. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.91275/5.02687. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.91657/5.02135. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.91803/5.03362. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.91540/5.03559. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.91582/5.02367. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.90879/5.04903. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.91698/5.01636. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.91629/5.02724. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.92919/4.98858. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.92580/4.98549. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.92403/4.98602. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.92733/4.98393. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.91933/4.99641. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.91588/4.99996. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.92351/4.99801. Took 0.08 sec\n",
      "ACC: 0.59375, MCC: 0.18029556650246306\n",
      "Epoch 0, Loss(train/val) 5.00990/4.93247. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.94294/4.98215. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.94952/5.03837. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.95101/5.06885. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.95544/5.05300. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.95403/4.99960. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.94471/4.97865. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.93926/4.99707. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.94150/4.99838. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.94260/4.99236. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.94281/4.98632. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.93733/4.98423. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.94111/4.99432. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.93981/4.99351. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.93990/4.99500. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.93807/4.98484. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.93781/4.98857. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.93734/4.99574. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.93628/4.99829. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.94016/4.98907. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.93353/4.99133. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.93512/4.99956. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.93464/5.00450. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.93725/4.99569. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.93270/4.99356. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.93318/4.99338. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.93595/5.00834. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.93615/5.00423. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 4.93239/5.00712. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.93325/4.99885. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.93216/5.01130. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.92788/5.01633. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.93100/5.01850. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.93327/5.01592. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.93929/4.99719. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.93559/4.99190. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.93138/5.00144. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.93074/5.00784. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.92824/5.00888. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.92919/5.01337. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.92682/5.01146. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.92971/5.01427. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.92552/5.02610. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.92410/5.04129. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.93066/5.01747. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.93100/5.00499. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.92576/5.00786. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.92122/5.04324. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.92727/5.02022. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.92466/5.02146. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.91892/5.03679. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.92337/5.01604. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.92956/5.01210. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.91791/5.03092. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.92222/5.01883. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.92356/5.01975. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.91358/5.04933. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.92534/5.01775. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.92046/5.01972. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.92311/5.01847. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.91478/5.03285. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.91820/5.03348. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.91848/5.01809. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.91538/5.04223. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.91389/5.03201. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.91370/5.02490. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.91574/5.06282. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.92007/5.01642. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.91630/5.05057. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.91491/5.02762. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.91143/5.04624. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.91044/5.05674. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.91124/5.01714. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.91798/5.02041. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.91099/5.01594. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.90916/5.04218. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.91231/5.03347. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.91191/5.05069. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.91611/5.01731. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.91414/5.03617. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.91764/5.02685. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.90624/5.05369. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.91432/5.01545. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.91501/5.02620. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.91204/5.01368. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.90944/5.03542. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.90623/5.01301. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.90570/5.06145. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.90764/5.04734. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.90955/5.01796. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.90696/5.05121. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.90878/5.02223. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.90266/5.02706. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.90774/5.02512. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.90812/5.05819. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.91001/5.00716. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.90765/5.00673. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.90646/5.01052. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.90422/5.01143. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.90621/5.05342. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.10421364855284106\n",
      "Epoch 0, Loss(train/val) 4.86189/4.76808. Took 0.18 sec\n",
      "Epoch 1, Loss(train/val) 4.77255/4.80651. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 4.77398/4.82871. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 4.77813/4.83709. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.77935/4.85088. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.77831/4.86336. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.78403/4.85764. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.78334/4.83807. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.77497/4.81884. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.77470/4.81050. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.77218/4.80522. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.76997/4.81766. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.76948/4.82467. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.77167/4.81467. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.76830/4.81574. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.76715/4.81936. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.76785/4.82112. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.76570/4.82799. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.76957/4.81492. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.76198/4.83422. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.76906/4.82805. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.76741/4.81410. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.76163/4.83572. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.76653/4.82355. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.76496/4.82261. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.76042/4.83258. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 4.76653/4.81980. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.76334/4.81757. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.76053/4.83163. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.76370/4.82938. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.76235/4.82099. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.76134/4.82807. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.76042/4.81458. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.76066/4.84038. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.76382/4.82403. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.76340/4.82230. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.75775/4.83176. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.76186/4.81244. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 4.75798/4.82605. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.75689/4.83354. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.76048/4.80944. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.75424/4.82743. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.76160/4.83158. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.75311/4.83440. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.75673/4.81098. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.76047/4.82665. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.75017/4.83207. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.75828/4.83463. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.75608/4.81286. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.75671/4.83880. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.75789/4.81623. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.75707/4.82410. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.75538/4.80153. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.75091/4.83088. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.75149/4.82625. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.75033/4.83257. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.75498/4.82831. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.74561/4.84223. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.75195/4.82396. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.75547/4.82216. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.75106/4.82512. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.75377/4.81680. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.74713/4.82724. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.75330/4.83356. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.74989/4.82803. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.75482/4.82927. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.74704/4.83532. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.75143/4.82281. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.74531/4.82666. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.75212/4.84202. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.74729/4.81368. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.75329/4.82899. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.74053/4.82759. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.75549/4.83461. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.74669/4.82447. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.75032/4.84724. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.74391/4.83002. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.74720/4.84120. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.74857/4.82020. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.74614/4.83386. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.74470/4.84053. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.74870/4.84502. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.74728/4.81295. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.74558/4.82869. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.74137/4.83675. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.74250/4.82041. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.75057/4.83159. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.73902/4.83741. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.74851/4.83870. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.75812/4.79634. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.74449/4.81755. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.74376/4.80308. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.74569/4.84759. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.74387/4.85200. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.74096/4.80241. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.74219/4.83837. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.74781/4.82010. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.74468/4.83884. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.74293/4.82344. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.74485/4.83257. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.007889684472185849\n",
      "Epoch 0, Loss(train/val) 5.05265/5.02887. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.01215/5.04371. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.01825/5.01458. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.00660/5.00968. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.99670/5.00776. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.99679/5.01186. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.00098/5.01460. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.99791/5.01463. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.99624/5.01494. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.99454/5.01547. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.99578/5.01734. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.99558/5.01840. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.99561/5.02294. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.99257/5.02186. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.99182/5.02623. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.99374/5.01996. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.98901/5.03747. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.99284/5.02136. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.99104/5.03753. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.98927/5.02161. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.98876/5.03530. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.98373/5.03316. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.98685/5.04168. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.98497/5.04086. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.98552/5.03180. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.98534/5.03988. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.98234/5.03180. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.98113/5.05280. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.98154/5.03625. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.97822/5.03846. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.97975/5.03980. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.97682/5.05626. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.97705/5.03318. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.98307/5.03592. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.97702/5.04947. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.97568/5.05041. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.98003/5.04385. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.98163/5.03544. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.97284/5.04165. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.97802/5.04903. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.97492/5.05965. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.97480/5.05615. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.97041/5.06373. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.97706/5.04430. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.97635/5.04849. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.97421/5.06942. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.97510/5.05781. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.97407/5.05798. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.97078/5.08226. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.96495/5.07556. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.96670/5.07125. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.97432/5.05042. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.97520/5.05289. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.96907/5.07016. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.97523/5.06764. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.96592/5.08256. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.97433/5.05492. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.96734/5.07943. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.97069/5.05979. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.96754/5.07151. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.96748/5.10072. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.96971/5.06014. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.96869/5.08201. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.97387/5.07830. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.96777/5.06919. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.96522/5.12441. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.96929/5.05473. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.96912/5.07372. Took 0.11 sec\n",
      "Epoch 68, Loss(train/val) 4.96021/5.11532. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.96490/5.06296. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.96955/5.07444. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.96178/5.12591. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.96996/5.05834. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.96598/5.10457. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.95652/5.08670. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.96091/5.06822. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.96198/5.09436. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.96969/5.07411. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.95771/5.08635. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.96257/5.08550. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.96407/5.04899. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.96459/5.11016. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.96249/5.04939. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.96168/5.09840. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.96498/5.06709. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.96486/5.06288. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.96113/5.10454. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.95752/5.06853. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.95937/5.09334. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.95559/5.11091. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.96712/5.06546. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.96023/5.14116. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.95899/5.05043. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.95882/5.08615. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.95810/5.07566. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.95859/5.09716. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.96211/5.08352. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.96456/5.09620. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.95313/5.10616. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.95581/5.06503. Took 0.09 sec\n",
      "ACC: 0.4375, MCC: -0.12609970674486803\n",
      "Epoch 0, Loss(train/val) 5.04731/4.95636. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.99247/4.99656. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.99156/5.01305. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.99456/4.98754. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.99042/4.99634. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.99734/4.98961. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.99330/5.01243. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.99204/5.03107. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.99275/5.04808. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.99335/5.05496. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.99480/5.05142. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.99215/5.04335. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.98944/5.03342. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.98757/5.03095. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.98877/5.03304. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.98542/5.04517. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.98935/5.05114. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.99028/5.06685. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.98793/5.05901. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.99093/5.06305. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.98554/5.05864. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.98538/5.05802. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.98552/5.07134. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.98550/5.06721. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.98068/5.06880. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.97942/5.09232. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.98237/5.08704. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.98217/5.07075. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.98439/5.08139. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.98214/5.08052. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.97832/5.08331. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.98188/5.08493. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.97712/5.10301. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.98081/5.11101. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.97769/5.08509. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.97842/5.09163. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.98065/5.09560. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.97585/5.09975. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.97421/5.10501. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.97792/5.10605. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.97524/5.10612. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.97144/5.10311. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.97660/5.08027. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.97503/5.08126. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.97127/5.11284. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.97139/5.08871. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.97209/5.09719. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.97455/5.07195. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.97609/5.09486. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.97050/5.10495. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.96503/5.14146. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.96979/5.11362. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.96908/5.08612. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.96699/5.12808. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.97027/5.12410. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.96561/5.14041. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.96501/5.16608. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.96739/5.11580. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.96717/5.11249. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.96328/5.11185. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.96892/5.11104. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.96578/5.10860. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.96530/5.07871. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.96837/5.11283. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.96872/5.12036. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.96437/5.15327. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.96620/5.13396. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.97473/5.07025. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.97602/5.04445. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.96576/5.08795. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.95784/5.11986. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.96912/5.11260. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.97027/5.13657. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.96668/5.11411. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.97117/5.09360. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.96847/5.09033. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.97241/5.09115. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.96638/5.10985. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.96624/5.11161. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.97343/5.08646. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.96750/5.09249. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.96490/5.10328. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.97109/5.09437. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.97018/5.08351. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.96950/5.08083. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.96674/5.06658. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.96367/5.08578. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.96487/5.08032. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.96086/5.09432. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.96474/5.07784. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.96550/5.08390. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.96857/5.08638. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.96406/5.06618. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.96670/5.08334. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.96259/5.10794. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.96809/5.08456. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 4.96258/5.06519. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.95810/5.08334. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.97057/5.08225. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.96830/5.06105. Took 0.08 sec\n",
      "ACC: 0.640625, MCC: 0.2882306768491569\n",
      "Epoch 0, Loss(train/val) 4.80856/4.75787. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.77937/4.74358. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.78031/4.76046. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.76241/4.75049. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.75305/4.74403. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.75235/4.74905. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.75401/4.75549. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.75278/4.76219. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.74937/4.76131. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.74893/4.76305. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.74991/4.74728. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.75139/4.75294. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.74937/4.76185. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.74944/4.76789. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.74852/4.76142. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.75013/4.75634. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.75578/4.75142. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.75466/4.75071. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.75365/4.75042. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.75167/4.74846. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.75377/4.74816. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.75202/4.74818. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.74948/4.74601. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.75037/4.74934. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.75266/4.74637. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.75200/4.74679. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.74883/4.74827. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.75144/4.74810. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.74976/4.74753. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.74892/4.75097. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.75066/4.75468. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.74662/4.75479. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.74704/4.75467. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.74374/4.77221. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.75196/4.75638. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.74422/4.76290. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.74646/4.75680. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.74267/4.76862. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.74349/4.76699. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.74207/4.77085. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.74392/4.76590. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.74249/4.77057. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.73782/4.76815. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.74117/4.77128. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.74291/4.76651. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.74075/4.77084. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.73837/4.77379. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.73628/4.77100. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.73957/4.76974. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.73859/4.77163. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.73788/4.76952. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.73757/4.76527. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.73471/4.77488. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.73800/4.75805. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.73656/4.76593. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.73372/4.78197. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.73689/4.76500. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.73207/4.76899. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.73564/4.75250. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.73253/4.76739. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.73342/4.76877. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.73415/4.76905. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.73169/4.77281. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.73893/4.75549. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.73482/4.78094. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.73026/4.77022. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.73183/4.76754. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.73243/4.77021. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.73699/4.76857. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.73320/4.78013. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.72888/4.77682. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.72880/4.76358. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.73397/4.76322. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.72933/4.77913. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.73161/4.77201. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.72930/4.76359. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.72587/4.78729. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.72821/4.77991. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.73436/4.76546. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.72799/4.77289. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.72885/4.77010. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.72605/4.77391. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.73075/4.77121. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.72755/4.76905. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.72207/4.78185. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.72644/4.75826. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.72860/4.77036. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 4.73148/4.77807. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.73256/4.76718. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.72464/4.77677. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.71606/4.79405. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.72706/4.75908. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.72979/4.76998. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.72930/4.77839. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.72951/4.76485. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.72365/4.78723. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.72307/4.77451. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.72962/4.75854. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.72273/4.77676. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.72530/4.76851. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.1777495495783369\n",
      "Epoch 0, Loss(train/val) 4.96679/5.00777. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.94533/4.94147. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.93597/4.93936. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.93564/4.93564. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.93512/4.93575. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.93457/4.93489. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.93602/4.93151. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.93279/4.92975. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.93610/4.92884. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.93541/4.93013. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.93034/4.92986. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.93328/4.92880. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.93068/4.92884. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.92968/4.92671. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.93449/4.92627. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.93027/4.92822. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.92972/4.92612. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.92998/4.92534. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.93325/4.92628. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.92748/4.92455. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.92650/4.92367. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.92667/4.92334. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.92775/4.92183. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.93038/4.92211. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 4.92709/4.92074. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.93410/4.93759. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.93021/4.93330. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.92745/4.92978. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.92444/4.92690. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.92924/4.91997. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.92687/4.91305. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.92967/4.92995. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.92903/4.93212. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.92686/4.92566. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.92128/4.92918. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.92937/4.92091. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.92306/4.92103. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.92379/4.91030. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.92459/4.92787. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.91888/4.92394. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.92589/4.92324. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.92367/4.92614. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.92298/4.92118. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.92267/4.92331. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.91938/4.91571. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 4.92843/4.92439. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.92478/4.92574. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.92618/4.92369. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.92034/4.92394. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.92111/4.92117. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.92279/4.92169. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.92117/4.91460. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 4.92347/4.91505. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.92327/4.91394. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.91870/4.90884. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.91715/4.91313. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.91878/4.91193. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.91890/4.91203. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.91449/4.92132. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.91282/4.91355. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.91955/4.92240. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.92124/4.91427. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.91693/4.91669. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.92149/4.91361. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.91756/4.90533. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.91764/4.91642. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.91830/4.91369. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.91791/4.91533. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.91958/4.92424. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.91321/4.91862. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.91141/4.90737. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.91345/4.91153. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.91783/4.91755. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.91430/4.90318. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.91846/4.89891. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.91604/4.90878. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.91075/4.91275. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.91389/4.90142. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.91587/4.90656. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.91641/4.90925. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.91107/4.91243. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.91171/4.91049. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.90636/4.90748. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 4.91580/4.91559. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.91873/4.90624. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.91486/4.90937. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.91193/4.90677. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.90922/4.90419. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.91331/4.91375. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.91262/4.90303. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.91453/4.90492. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.90788/4.91807. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.91952/4.90969. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.91188/4.90779. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.91268/4.91803. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.91319/4.91005. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.91219/4.91635. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.90741/4.92203. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.90718/4.91088. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.90759/4.90807. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.07936507936507936\n",
      "Epoch 0, Loss(train/val) 4.82386/4.76366. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.75619/4.81035. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.76270/4.83261. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.76698/4.81956. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.76961/4.79026. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.76777/4.76307. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.76219/4.75626. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.75636/4.75998. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.75754/4.75897. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.75840/4.76035. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.75487/4.75809. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.75874/4.75400. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.75514/4.75146. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.75408/4.75395. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.75621/4.75217. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.75071/4.75048. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.75247/4.74370. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.75063/4.74664. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.75084/4.74859. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.75482/4.74426. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.75275/4.73778. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.75094/4.73939. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.75051/4.74325. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.74973/4.73582. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.74865/4.74165. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.74957/4.73450. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.74379/4.73890. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.74758/4.73698. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.74661/4.72970. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.74617/4.73382. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.74685/4.72803. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.74529/4.72627. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.74583/4.74352. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.74910/4.73783. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.74687/4.73495. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.74344/4.73096. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.73998/4.72757. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.74333/4.73522. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.74141/4.73103. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.73735/4.73346. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.73799/4.73509. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.74057/4.73896. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.74439/4.74569. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.74954/4.78387. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.75162/4.76582. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.74823/4.77639. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.74679/4.76793. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.74493/4.76873. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.74648/4.77733. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.74378/4.75827. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.74165/4.75768. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.74265/4.76641. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.74019/4.76766. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.74465/4.75027. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.73998/4.75140. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.73924/4.74904. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.73298/4.78297. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.74173/4.75608. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.73904/4.74340. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.73570/4.75760. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.73818/4.75960. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.73645/4.75684. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.75086/4.72953. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.74812/4.73713. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.74045/4.74592. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.74096/4.75219. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.73986/4.74223. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.74056/4.73810. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.74263/4.74403. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.73999/4.73949. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.73566/4.76129. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.73582/4.74634. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.73332/4.74970. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.74305/4.74510. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.73610/4.75037. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.73840/4.76875. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.73459/4.74706. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.73218/4.75711. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.73477/4.76389. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.73515/4.75965. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.73521/4.75103. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.73081/4.75712. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.73408/4.79253. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.74696/4.73885. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.74703/4.74154. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.74125/4.75302. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.73891/4.74640. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.74524/4.74032. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.73927/4.75989. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.73465/4.75205. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.73356/4.75238. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.73580/4.75148. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.73276/4.75379. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.73459/4.75488. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.73280/4.75498. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.73229/4.78567. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.73349/4.78036. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.73016/4.76944. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.72653/4.77701. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.74959/4.73603. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.08818077954471167\n",
      "Epoch 0, Loss(train/val) 5.00527/4.90598. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.91628/4.89918. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.90779/4.89800. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.90263/4.90054. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.90215/4.89989. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.90318/4.90068. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.90452/4.90251. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.90571/4.89996. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.90242/4.89668. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.90094/4.89570. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.89902/4.89690. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.90110/4.89801. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.90065/4.89744. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.89658/4.89345. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.89867/4.90078. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.90342/4.90488. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.89474/4.90083. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.89688/4.89851. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.89376/4.89537. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.89871/4.88946. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.89956/4.88166. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.89792/4.88385. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.89700/4.88180. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.89737/4.88216. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.89801/4.88013. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.89432/4.87403. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.89933/4.87948. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 4.89393/4.87776. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.89278/4.87676. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.89284/4.88149. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.89634/4.87817. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.89224/4.87449. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.89315/4.88278. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.88819/4.87532. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.89109/4.87564. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.89086/4.86975. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.89260/4.87120. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.89396/4.87456. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.89214/4.87370. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.88865/4.88252. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 4.88810/4.87834. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.89197/4.87961. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.88719/4.88375. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.88903/4.88121. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.89002/4.87726. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.88980/4.88592. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.88964/4.88591. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.89138/4.88633. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.88676/4.87460. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.89807/4.88372. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.88914/4.88720. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.88766/4.88918. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.89102/4.88623. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.89041/4.88114. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.88908/4.89339. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.88903/4.87855. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.88809/4.88849. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.88857/4.88305. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.89054/4.88021. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.88275/4.88719. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.88600/4.88467. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.88769/4.88586. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.88600/4.88941. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 4.88514/4.88491. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.88409/4.89039. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.88621/4.88671. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.88174/4.88305. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.88026/4.88346. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.88314/4.89055. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.87996/4.89861. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.87961/4.87215. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.89258/4.88315. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88404/4.88574. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88368/4.88252. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.88010/4.87086. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.88395/4.89532. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88034/4.90491. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.88058/4.91019. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.88282/4.89372. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88311/4.89429. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.87836/4.89349. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.87938/4.90665. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.87797/4.90520. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.87401/4.90122. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.87834/4.90772. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.88210/4.90556. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.87417/4.91462. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.88196/4.90238. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.87699/4.91073. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.87433/4.89624. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.87636/4.91866. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.87487/4.90883. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.86958/4.91865. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.88100/4.91136. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.87137/4.92053. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.87755/4.90045. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.87187/4.91068. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.87322/4.91178. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.87478/4.90413. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.87023/4.90828. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.0710915894200724\n",
      "Epoch 0, Loss(train/val) 4.98537/4.98568. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.91577/4.93939. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.93721/4.91031. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.93249/4.90980. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.92242/4.90817. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.91578/4.90736. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.91542/4.90530. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.91920/4.90150. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.91696/4.90212. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.91319/4.89866. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.91190/4.90313. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.91766/4.89921. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.91248/4.90102. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.90982/4.89887. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.91089/4.90312. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.90974/4.89835. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.91077/4.90086. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.91329/4.89887. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.90826/4.90268. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.90892/4.90267. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.90783/4.90277. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.90561/4.89920. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.90588/4.90654. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.90464/4.90191. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.90421/4.90006. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.90515/4.90494. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.90318/4.90660. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.90224/4.90334. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.90288/4.91151. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.90242/4.90936. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.90052/4.90252. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.89699/4.91529. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.89981/4.91096. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.90048/4.91400. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.89816/4.91644. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.89624/4.91132. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.89882/4.91079. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.89736/4.90767. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.89548/4.90376. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.89957/4.90306. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.89646/4.91258. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.89418/4.90639. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.89415/4.90166. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.89907/4.90565. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.89833/4.90858. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.89703/4.91333. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.89331/4.91082. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.89857/4.90780. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.89493/4.90466. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.89502/4.90899. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.88890/4.91590. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.89592/4.90964. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.89489/4.91838. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.89312/4.92331. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.88905/4.92369. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.89564/4.92302. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.89297/4.92924. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.88824/4.92190. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.89080/4.91938. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.89254/4.92326. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.89301/4.93783. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89504/4.92087. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.88963/4.92207. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.88733/4.92913. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.88722/4.92402. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.89104/4.91463. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.88985/4.92775. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.88882/4.91994. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.89360/4.91695. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.88760/4.94424. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.88864/4.92793. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.88826/4.93096. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88603/4.93367. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88468/4.95110. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.88693/4.92568. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.88551/4.96149. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88346/4.95621. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.88305/4.95283. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.88744/4.93297. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88503/4.93625. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.88354/4.93336. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.88195/4.91772. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.88648/4.98529. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.90692/4.94538. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.90200/4.94352. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.89372/4.94378. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.89305/4.94033. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.88863/4.95799. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.88371/4.97073. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.89005/4.93659. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.88971/4.91953. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.88259/4.93879. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.88304/4.93114. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.88063/4.93658. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.87686/4.93568. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.88268/4.95545. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.87780/4.93538. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.87667/4.93300. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.88060/4.92970. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.87845/4.93673. Took 0.09 sec\n",
      "ACC: 0.4375, MCC: -0.12267022872162016\n",
      "Epoch 0, Loss(train/val) 4.94449/4.97092. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.91187/4.89705. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.90344/4.90747. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.90521/4.91421. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.90247/4.90186. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.89577/4.90187. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.89313/4.90283. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.89185/4.91245. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.89259/4.91787. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.89120/4.91709. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.88951/4.91576. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.89007/4.91657. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.89106/4.92301. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.88654/4.91968. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.88676/4.91938. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.89158/4.92140. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.88593/4.90924. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.88852/4.92223. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.88691/4.91783. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.88519/4.91623. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.88511/4.93299. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.88673/4.90504. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.88476/4.92130. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.88342/4.90838. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.89032/4.89700. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.88397/4.93043. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.88343/4.92140. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.88375/4.92642. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.88209/4.92901. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.88291/4.91952. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.88153/4.92447. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.88116/4.91826. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.88522/4.89806. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.88953/4.90978. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.88933/4.90372. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.88587/4.91306. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.88980/4.91001. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.88798/4.91714. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.88865/4.90550. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.88625/4.91774. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.88252/4.91240. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.88966/4.90259. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.88325/4.91660. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.88234/4.92163. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.88407/4.90876. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.88438/4.91435. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.88189/4.91909. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.88183/4.92535. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.88107/4.92188. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.88089/4.92668. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.88642/4.92540. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.88457/4.92438. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.88619/4.92708. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.87912/4.93347. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.87739/4.95275. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.88271/4.92797. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.88045/4.95539. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.87824/4.92392. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.87905/4.94836. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.87787/4.91964. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.87503/4.95781. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.87413/4.93803. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.87530/4.93655. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.87443/4.90999. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.88273/4.90801. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.87980/4.91430. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.88166/4.90508. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.87571/4.91126. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.88486/4.90356. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.88114/4.89946. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.87764/4.91977. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.87889/4.91935. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.87858/4.91850. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.87649/4.92895. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.87386/4.93252. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.87545/4.93303. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88080/4.91630. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.87753/4.91453. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.88257/4.90592. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88043/4.91817. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.87394/4.92256. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.87487/4.92125. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.87337/4.93112. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.87844/4.92124. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.87412/4.92393. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.87729/4.93212. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.87752/4.91940. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.87690/4.93531. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.87306/4.94902. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88543/4.89983. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.87874/4.90946. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.87829/4.96212. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.88096/4.92849. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.87842/4.95328. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.87953/4.93675. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.87989/4.93218. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.87398/4.96459. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.88171/4.92740. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.87486/4.96818. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.87518/4.93862. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.0657512425947884\n",
      "Epoch 0, Loss(train/val) 4.99153/4.89784. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.89365/4.90851. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.89013/4.91750. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.88689/4.91556. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.88527/4.91179. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.88538/4.91783. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.88568/4.92063. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.88172/4.92583. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.88332/4.91344. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.87773/4.92739. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.87856/4.91011. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.87789/4.92389. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.87324/4.91430. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.88115/4.92110. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.87795/4.91355. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.86933/4.93384. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.87437/4.91209. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.87499/4.92324. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.87676/4.91958. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.87745/4.91899. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.87017/4.93172. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.87229/4.92402. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.86985/4.93303. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.87221/4.92711. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.87262/4.93541. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.86899/4.93569. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.86936/4.93218. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.86859/4.93977. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.87215/4.92948. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.87116/4.95298. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.86450/4.92780. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.86477/4.94890. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.86604/4.95959. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.86825/4.93740. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.86733/4.94402. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.86328/4.93868. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.86777/4.93979. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.86562/4.94106. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86593/4.94239. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.86550/4.94292. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.86865/4.93475. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.86763/4.94382. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.86085/4.94510. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.86273/4.95224. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.85930/4.94315. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.86383/4.95112. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.86782/4.94171. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.86448/4.95162. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.86594/4.94949. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.85665/4.95371. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.86078/4.96460. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.86013/4.94992. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.85931/4.95823. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.86655/4.93878. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.86307/4.95240. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.85662/4.95585. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.86402/4.94726. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.85743/4.95938. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.86042/4.94834. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.85317/4.95704. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.85837/4.95706. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.85826/4.96615. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.85780/4.95926. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.86069/4.94240. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.85083/4.96397. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.85697/4.94245. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.85306/4.96456. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.85356/4.96279. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.85174/4.95664. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.85035/4.95086. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.85311/4.97296. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.85765/4.96540. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.85812/4.96042. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.85755/4.96427. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.85640/4.95545. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.85378/4.97535. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.86719/4.94965. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.86489/4.93548. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.85560/4.94127. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.85350/4.94411. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.86013/4.95011. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.86370/4.95639. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.85550/4.94137. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.86186/4.93766. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.85690/4.94889. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.85957/4.94173. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.85841/4.94981. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.85896/4.95533. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.85900/4.95399. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.85202/4.95276. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.85815/4.94775. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.85166/4.94203. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.84920/4.95616. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.84995/4.95879. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.85242/4.95145. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.84841/4.96683. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.84933/4.95196. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.85005/4.94428. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.85131/4.97807. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.84692/4.95082. Took 0.08 sec\n",
      "ACC: 0.625, MCC: 0.2135744251723958\n",
      "Epoch 0, Loss(train/val) 5.04296/4.97118. Took 0.18 sec\n",
      "Epoch 1, Loss(train/val) 4.94312/4.94290. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 4.94530/4.94153. Took 0.13 sec\n",
      "Epoch 3, Loss(train/val) 4.94380/4.94901. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.94124/4.95190. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.93646/4.95678. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.94039/4.95705. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.93678/4.95907. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.93788/4.95982. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.93988/4.94122. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.93869/4.94667. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.93534/4.95558. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.93495/4.95902. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.93317/4.95105. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.93767/4.95242. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.93688/4.95349. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.93500/4.95453. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.93374/4.95769. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.93302/4.96249. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.93079/4.96696. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.93417/4.96875. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.92954/4.97111. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.93288/4.97096. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.92722/4.96899. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.92841/4.96935. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.92975/4.96077. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.92500/4.96699. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.92893/4.96726. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.92508/4.94626. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.93607/4.94449. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.92596/4.95247. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.92269/4.96916. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.92438/4.96402. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.93031/4.93558. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.92865/4.96039. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.92189/4.97202. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.92278/4.96869. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.91790/4.97788. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.92064/4.98232. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.92275/4.97196. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.92600/4.96667. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.92402/4.96159. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.92172/4.97275. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.91972/4.97543. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.91619/4.98027. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.92046/4.97411. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.91487/4.98989. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.92308/4.97303. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.91368/4.97389. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.91261/4.98284. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.91383/4.97206. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.91249/4.97106. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.91712/4.97520. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.90643/4.97846. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.90644/4.98783. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.90828/4.99320. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.91620/4.98106. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.91105/4.99139. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.90866/4.98425. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.90902/4.97653. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.90674/4.99935. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.91622/4.99720. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.90704/4.99454. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.90824/4.99149. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.90872/4.98221. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.90699/4.98299. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.90178/5.00069. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.90621/4.98813. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.90434/4.99413. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.90456/4.99012. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.90432/4.99656. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.89855/5.00965. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.89655/5.03314. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.89429/5.02657. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.90161/5.00353. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.90399/4.99443. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.89899/5.01388. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.90000/5.01303. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.89175/5.04464. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.90713/5.00414. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.89790/4.99881. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.89407/5.03618. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.90456/5.02564. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.89459/5.00132. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.89472/5.00415. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 4.89019/5.05555. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.89979/5.03005. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.89715/5.00758. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.89286/5.03604. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.89721/5.03909. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.89688/5.01190. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.89872/5.01104. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.89007/5.03013. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.89156/5.02702. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.89191/5.02426. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.89959/5.00388. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.89733/5.02201. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.89708/5.00159. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.88577/5.02253. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.88797/5.02396. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.0518191557963571\n",
      "Epoch 0, Loss(train/val) 5.11166/5.05557. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.05393/5.05259. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.05662/5.05614. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.05961/5.05015. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.05744/5.04948. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.05005/5.04923. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.04547/5.04764. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.04763/5.04792. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.04752/5.04891. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.04591/5.05049. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.04259/5.05148. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.04431/5.05151. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 5.04235/5.05022. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.03971/5.05419. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.04065/5.05830. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.03900/5.06047. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.03766/5.06699. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.03744/5.07282. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.03560/5.06843. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.03760/5.06734. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.03872/5.06978. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.03389/5.07420. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.03147/5.08771. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.03715/5.07775. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.03513/5.07945. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.03428/5.07988. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.02770/5.08989. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.03428/5.07765. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.03370/5.08711. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.03215/5.08888. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 5.02930/5.09207. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.03035/5.09431. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.03155/5.09441. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.03153/5.07741. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.02468/5.11811. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.02995/5.07687. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.02574/5.09927. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.03451/5.08385. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.02482/5.10879. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 5.02633/5.09335. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.02734/5.09620. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.02586/5.09928. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.02616/5.09958. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.02878/5.08930. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.02023/5.11088. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.02096/5.10099. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.02513/5.08875. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.02561/5.09309. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 5.02324/5.09116. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.02861/5.07864. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.01777/5.09501. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.02442/5.07899. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.02305/5.06679. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.02737/5.07297. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.02539/5.07910. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.01724/5.09794. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 5.02369/5.07130. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.01299/5.09615. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.02769/5.06609. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.01389/5.08411. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.02359/5.07454. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.01481/5.08370. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.02603/5.08282. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 5.01311/5.10200. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.01653/5.07725. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.01638/5.08493. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.01151/5.10250. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.01591/5.08448. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.01483/5.07547. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.00687/5.06461. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.03046/5.07825. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 5.01164/5.09255. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.01550/5.09061. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.01347/5.07588. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.01095/5.10504. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.01049/5.10067. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.01159/5.07985. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.01408/5.07541. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.00785/5.10082. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.01184/5.10706. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.00153/5.09454. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.00824/5.08458. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.00799/5.08993. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.00755/5.11062. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.00863/5.09817. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.00491/5.08580. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.00318/5.12302. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.00493/5.07434. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 5.01013/5.09408. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 5.01428/5.10631. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.00837/5.11187. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.00715/5.09055. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.00794/5.07207. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.00739/5.09669. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.00855/5.09268. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.01183/5.09670. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.00639/5.11109. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.99761/5.11739. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.00921/5.08111. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.01466/5.09522. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.1241446725317693\n",
      "Epoch 0, Loss(train/val) 4.84638/4.80778. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.81523/4.80105. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.80872/4.80423. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.81044/4.80493. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.80766/4.80506. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.80628/4.81004. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.80713/4.81452. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.80539/4.81737. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.80496/4.81418. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.80405/4.81640. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.80011/4.81164. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.80113/4.81538. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.80278/4.80522. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.80187/4.81278. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.80113/4.81565. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.79997/4.81333. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.79996/4.81698. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.79845/4.81707. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.79972/4.82092. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.79525/4.83089. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.79458/4.84048. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.79564/4.83788. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.79517/4.84269. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.79218/4.84955. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.79187/4.85015. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.79072/4.85882. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.79171/4.84566. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.78811/4.84663. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.78452/4.86334. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.79023/4.85848. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.79303/4.84024. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.79257/4.82694. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.78489/4.83043. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78791/4.83225. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78947/4.83452. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.78024/4.83888. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.78297/4.84805. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.78047/4.84632. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.78481/4.83719. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.78725/4.85647. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.78456/4.83156. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.78255/4.85393. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.78137/4.84639. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.78238/4.86225. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.78754/4.83322. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.77968/4.86519. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77646/4.85865. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.78001/4.85341. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.78180/4.85354. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.77960/4.86980. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.78396/4.86168. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.79710/4.82297. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.78753/4.83976. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.79107/4.83599. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.79624/4.82587. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.78584/4.84288. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.78913/4.84656. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.79353/4.84944. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.78954/4.84647. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.78830/4.85548. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.78438/4.85927. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.78115/4.85744. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.78525/4.86574. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.78357/4.86752. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.78653/4.85701. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.78233/4.86485. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.78464/4.86877. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.78424/4.85894. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.77868/4.87482. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.78070/4.87205. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.77893/4.87101. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.77471/4.88238. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.77711/4.87931. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.77963/4.85676. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.77497/4.87959. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.77784/4.87418. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.77555/4.87818. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.77495/4.87593. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.77909/4.87645. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.77153/4.88306. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.77378/4.87400. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.77405/4.90056. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.78138/4.85217. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.78734/4.83529. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.79474/4.82113. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.78808/4.84003. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.78424/4.85925. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.77967/4.87630. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.78198/4.85738. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.77657/4.86135. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.77960/4.87402. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.77715/4.86644. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.77517/4.89135. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.78045/4.86186. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.77281/4.87937. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.77595/4.88600. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.77391/4.86754. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.77716/4.85641. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77865/4.85773. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.77703/4.86684. Took 0.08 sec\n",
      "ACC: 0.578125, MCC: 0.1563263498701806\n",
      "Epoch 0, Loss(train/val) 4.84820/4.76908. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.76163/4.76993. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.75535/4.77601. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.75522/4.78692. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.75237/4.78986. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.75320/4.79670. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.75301/4.80025. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.75286/4.79299. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.74836/4.80076. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.75043/4.79924. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.74615/4.80734. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.74993/4.80431. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.74716/4.81922. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.74561/4.80630. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.74521/4.81187. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.74529/4.81129. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.74624/4.80734. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.74209/4.80468. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.74514/4.80404. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.74261/4.80821. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.74333/4.80998. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.74315/4.80359. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.73886/4.81326. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.74109/4.81638. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.74170/4.80693. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 4.73967/4.81370. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.74130/4.82107. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.73603/4.84111. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.74288/4.81665. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.74019/4.82558. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.73864/4.83241. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.74377/4.80941. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.74028/4.81037. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.73233/4.83382. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.73582/4.81057. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.73666/4.82199. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.73537/4.82678. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.73708/4.82669. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.73262/4.82746. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.73811/4.81326. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.74191/4.77601. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.74076/4.79249. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.73846/4.80597. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.74400/4.79639. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.74381/4.78797. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.74361/4.78475. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.73839/4.80374. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.73698/4.81644. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.74245/4.80189. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.74003/4.79408. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.73838/4.82122. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.74141/4.80497. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.73679/4.80853. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.74184/4.81124. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.73855/4.81268. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.73801/4.83208. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.73781/4.81513. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.73833/4.81949. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.73339/4.83686. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.73812/4.82517. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.73561/4.83564. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.73551/4.80579. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.73576/4.81957. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.73640/4.80616. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.73110/4.84516. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.75052/4.79033. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.75274/4.76806. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.74125/4.79213. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.74062/4.78589. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.74008/4.79453. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.73996/4.78539. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.73916/4.77207. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.73250/4.90756. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.73994/4.80459. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.74333/4.78188. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 4.73799/4.79055. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.73503/4.80863. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.73462/4.79840. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.73330/4.79813. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.73438/4.78862. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.73483/4.79068. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.72969/4.81087. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.73490/4.80194. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.73234/4.79688. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.73250/4.81155. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.73207/4.80406. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.74375/4.78951. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.73652/4.81705. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.73730/4.80430. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.73234/4.82355. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.73427/4.82209. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.73217/4.82768. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.73057/4.82104. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.73138/4.80852. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.73511/4.79337. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.73445/4.79488. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.73861/4.78585. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.73408/4.77839. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.73389/4.78901. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.73414/4.81345. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.013980301652228746\n",
      "Epoch 0, Loss(train/val) 4.54229/4.51135. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.49064/4.47433. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.48699/4.48270. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.48042/4.48325. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.47743/4.47736. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.47520/4.47716. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.47570/4.48027. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.47578/4.48129. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.47433/4.48102. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.47242/4.48169. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 4.47550/4.47975. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.47094/4.47896. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.46983/4.47836. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.46990/4.48033. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.47131/4.47888. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.46894/4.48450. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.46966/4.48476. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.46727/4.48754. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.46730/4.48423. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.46971/4.49198. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.46600/4.49192. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.46447/4.48377. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.46575/4.48810. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.46403/4.47439. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.46829/4.46965. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.47153/4.46625. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.46600/4.46994. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.46848/4.47030. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.46576/4.47623. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.46601/4.48094. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.46380/4.47607. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.46708/4.48221. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.46740/4.47474. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.46255/4.47942. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.46110/4.47513. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.46639/4.47784. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.46412/4.47294. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.46372/4.46772. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.46175/4.46094. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.46224/4.46725. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.46287/4.48037. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.46452/4.47608. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.46002/4.46692. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.45808/4.45802. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.45798/4.47408. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.45922/4.46705. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.45549/4.47140. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.45940/4.46703. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.45616/4.47629. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.45932/4.48558. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.45699/4.49794. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.46183/4.48772. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.45609/4.46901. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.45572/4.47417. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.45848/4.46368. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.45463/4.49116. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.45531/4.46866. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.45320/4.50272. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.45430/4.47906. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.45168/4.47467. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.45970/4.47321. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.45422/4.47988. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.45505/4.49599. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.44685/4.48128. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.44779/4.48899. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.44955/4.47436. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.44717/4.46757. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.46277/4.46742. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.46685/4.46689. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.46184/4.46235. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.46272/4.46454. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.45780/4.44845. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.45714/4.44298. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 4.45299/4.45943. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.45212/4.44544. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.45639/4.45846. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.45626/4.45335. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.45066/4.44989. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.45155/4.44467. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.45592/4.46110. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.45568/4.45776. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.45245/4.46558. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.45092/4.44734. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.44776/4.44445. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.45124/4.45982. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.45591/4.46140. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.45627/4.45273. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.45139/4.46230. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.44726/4.46441. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.44699/4.44927. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.44675/4.46580. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.44814/4.47796. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.44581/4.45333. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.44872/4.45602. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.44775/4.47455. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.44832/4.46415. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.44364/4.45484. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.44674/4.47177. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.44512/4.46062. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.44247/4.45631. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.04350001325672093\n",
      "Epoch 0, Loss(train/val) 4.83903/4.81108. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.81647/4.80428. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.81762/4.80350. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.80938/4.80273. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.81355/4.80217. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.81063/4.80889. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.81295/4.81368. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.81433/4.81301. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.81331/4.80151. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.81091/4.79859. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.80557/4.79986. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.80497/4.80535. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.80435/4.80015. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.80285/4.80524. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.80291/4.80740. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.80524/4.80348. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.79986/4.80284. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.79993/4.80748. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.80152/4.81106. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.80174/4.80707. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.79548/4.81159. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.79729/4.82341. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.79656/4.81993. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.79890/4.81903. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.80292/4.80943. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.81523/4.81549. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.80358/4.81182. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.81273/4.81898. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.80736/4.81750. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.80331/4.82502. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.80134/4.82740. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.79956/4.82409. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.79726/4.83219. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.80337/4.82766. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.80093/4.82039. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.80192/4.82914. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.80958/4.82742. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.80545/4.81866. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.80358/4.81294. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.79936/4.82231. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.79586/4.82618. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.79569/4.85258. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.80036/4.84837. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.79971/4.82637. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.79664/4.83607. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.79650/4.84720. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.79695/4.83473. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.79259/4.84415. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.79286/4.83107. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.79296/4.83099. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.79203/4.83391. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.79211/4.83688. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.78932/4.82963. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.79143/4.81926. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.78998/4.82871. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.78675/4.84012. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.79306/4.83183. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 4.78856/4.83911. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.78857/4.82869. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.78522/4.82802. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.78708/4.83902. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.78452/4.84221. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.78886/4.83116. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.78521/4.83405. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.78946/4.83292. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.79166/4.82932. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.78988/4.83933. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.78405/4.83860. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.78561/4.82868. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.78510/4.82862. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.78924/4.85608. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78613/4.83080. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.78151/4.83693. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.78557/4.82868. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.78983/4.81405. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.78665/4.81996. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78630/4.83028. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78440/4.83342. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.78630/4.83258. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.78406/4.83702. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.78416/4.83330. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.77774/4.82770. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.78536/4.82120. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.77771/4.82111. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.79002/4.81939. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.78454/4.83407. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.77719/4.82701. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.77514/4.83344. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.77758/4.84009. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.78358/4.82026. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.77801/4.82946. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.78867/4.82419. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.78049/4.82863. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.78289/4.82666. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.78598/4.82814. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.78296/4.82474. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.78109/4.82608. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.78304/4.83547. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.78200/4.83519. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.78067/4.82278. Took 0.09 sec\n",
      "ACC: 0.53125, MCC: 0.051799406700962544\n",
      "Epoch 0, Loss(train/val) 4.99644/4.91374. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.92921/4.91928. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.93965/4.91642. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.94189/4.92858. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.94171/4.95745. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.93479/4.94520. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.92774/4.93539. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.92810/4.94031. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 4.92695/4.94720. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.92740/4.94769. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.92660/4.95430. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.92739/4.95241. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.92516/4.95329. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.92355/4.95403. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.92019/4.96059. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.92190/4.96384. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.92023/4.96286. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.91555/4.96500. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.91887/4.95479. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.91210/4.95790. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.91463/4.95635. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.91145/4.96197. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.91317/4.95105. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.91101/4.95509. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.90643/4.95333. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.91189/4.94755. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.91021/4.95504. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.90957/4.95330. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.90853/4.95123. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.90910/4.94593. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.90219/4.95008. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.90824/4.94692. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.90488/4.94487. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.90421/4.95065. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.90591/4.94583. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.90394/4.94068. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.90147/4.95109. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.90266/4.95157. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.90230/4.95009. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.90635/4.94966. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.90654/4.94544. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.90377/4.94128. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.90024/4.94531. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.90032/4.94660. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.89902/4.95112. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.89906/4.94997. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.90435/4.94478. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.90078/4.93305. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.89714/4.93522. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.89805/4.94826. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.89749/4.94560. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.90041/4.94383. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.89178/4.94616. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.89841/4.94159. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.89221/4.94080. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.89630/4.94065. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.89355/4.93934. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.89124/4.96093. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.89987/4.94312. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.89852/4.94318. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.89466/4.93802. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89362/4.93861. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.89086/4.94624. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.89379/4.94611. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.89221/4.94313. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.89152/4.94513. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.88831/4.94821. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.88930/4.94897. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.89324/4.94785. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.88795/4.94582. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.88950/4.95380. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.88503/4.95698. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88304/4.97036. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.89353/4.96700. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.88177/4.95214. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.88654/4.94460. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88767/4.95324. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.88387/4.97491. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.88969/4.95228. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.88098/4.95514. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.88325/4.95858. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.87770/4.95857. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.89458/4.94123. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.88541/4.94935. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.88206/4.96625. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.88450/4.95920. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.88954/4.98177. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.90973/4.92720. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 4.91974/4.93673. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.90446/4.96186. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.89774/4.96272. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.88668/4.95092. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.88123/4.97979. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.88695/4.95630. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.87874/4.96201. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.88022/4.96244. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.88349/4.95103. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.88060/4.95664. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.87909/4.96677. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.88134/4.95860. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.18153846153846154\n",
      "Epoch 0, Loss(train/val) 5.08979/4.97393. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.95512/4.93821. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.95517/4.94529. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.94547/4.94385. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.94292/4.94170. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.94451/4.94064. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.94781/4.94001. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.94691/4.93969. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.94674/4.93905. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.94095/4.93856. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.93994/4.93787. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.94217/4.94256. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.94058/4.94460. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.93934/4.94414. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.93718/4.94562. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.93647/4.94856. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.93566/4.94981. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.93948/4.94614. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.93371/4.94700. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.93265/4.94646. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.93336/4.94318. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.93187/4.94223. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.93208/4.94379. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.93179/4.94004. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.93279/4.94024. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.92947/4.94033. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.93182/4.94417. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.92920/4.94192. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.93047/4.94403. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.92625/4.94609. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 4.93323/4.94639. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.93029/4.95048. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.93021/4.94726. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.92644/4.94963. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.92468/4.94711. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.92768/4.95105. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.92147/4.95392. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.93184/4.94898. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.92653/4.94641. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.92572/4.94553. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.92798/4.94169. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.92518/4.94495. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.92305/4.94704. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.92849/4.94370. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.93220/4.95669. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.92978/4.94354. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.92913/4.94488. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.93220/4.94497. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.92857/4.94328. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.92356/4.95101. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.92453/4.95558. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.92341/4.94761. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.92920/4.95095. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.92488/4.94972. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.92300/4.94330. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.92379/4.94157. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.91996/4.94344. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.92642/4.94156. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.91999/4.94416. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.92084/4.94153. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.92185/4.94604. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.92272/4.94576. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.92351/4.94366. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.92488/4.94358. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.91997/4.94810. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.92250/4.95059. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.92211/4.94644. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.92117/4.93972. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.91753/4.94335. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.92482/4.94244. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.92005/4.94788. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.92157/4.95016. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.91430/4.94795. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.92102/4.94469. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.91686/4.94783. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.91747/4.94416. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.92270/4.94382. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.92075/4.94167. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.92417/4.94348. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.91366/4.94613. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.91526/4.94712. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.91296/4.94824. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.91866/4.94764. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.91675/4.94810. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.91560/4.94750. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.91410/4.94533. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.91187/4.94271. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.91460/4.94234. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.91814/4.94397. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.91293/4.93857. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.91450/4.94181. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.91097/4.94964. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.91397/4.93718. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.90978/4.95311. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.90655/4.94594. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.91265/4.95344. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.91593/4.95122. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.91227/4.95415. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.91294/4.94247. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.91353/4.94450. Took 0.08 sec\n",
      "ACC: 0.40625, MCC: -0.1980295566502463\n",
      "Epoch 0, Loss(train/val) 5.05059/5.01717. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.98308/4.98979. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.97763/4.98242. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.97945/4.97852. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.97934/4.97229. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.97677/4.97194. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.97901/4.97366. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.97282/4.97423. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.97555/4.97513. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.97514/4.97652. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.97270/4.97642. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.97048/4.97858. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.97207/4.97807. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.97301/4.97023. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.96941/4.97115. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.96708/4.97814. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.96807/4.98229. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.96762/4.98172. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.96600/4.98792. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.96178/4.99650. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.96668/4.99179. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.96420/4.98764. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.96056/4.99700. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.96791/4.97695. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.96283/4.96933. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.96084/4.98337. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.96282/4.97573. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.96400/5.00099. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.96363/5.01731. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.96305/5.01080. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.96350/4.96261. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.96573/5.00222. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.95828/5.02095. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.96372/4.99523. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.96211/4.99980. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.95529/5.01553. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.96122/5.00855. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.95910/5.00877. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.95677/5.02103. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.95699/5.02218. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.96184/5.02746. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.95644/5.01802. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.96006/4.99366. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.96106/4.99438. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.96147/5.03464. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.96228/4.99089. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.95957/5.00514. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.95878/5.01953. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.95614/5.00677. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.95624/5.01928. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.95685/4.99835. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.95129/5.01911. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.95129/5.01105. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.95157/5.01555. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.95591/5.01172. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.94832/4.99794. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.95377/5.01084. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.95179/5.01674. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.94889/5.01482. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.94954/5.02016. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.94631/5.01857. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.94728/5.01944. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.94595/5.00900. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.95411/5.02402. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.95272/5.00182. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.94627/5.03174. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.94870/5.01292. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.94896/5.01914. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.94346/5.02542. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.94446/5.02136. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.94359/5.04552. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.94711/5.01630. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.94404/5.02274. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.95285/5.00673. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.95210/5.01624. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.94395/5.02234. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.94135/5.02881. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.94365/5.02005. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.94046/5.03407. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.95137/5.01212. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.94480/5.02802. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.94042/5.02516. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.94551/5.01814. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.94748/5.02014. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.94362/5.02437. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.94758/5.02862. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.93912/5.01618. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.93953/5.06686. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.94685/5.03834. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.94283/5.01266. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.93951/5.02955. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.94371/5.03882. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.94441/5.02780. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.94327/5.06184. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.93409/5.04379. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.94038/5.03689. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.94111/5.04206. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.94018/5.00059. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.94176/5.01562. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.94198/5.03218. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.054187192118226604\n",
      "Epoch 0, Loss(train/val) 4.63711/4.63938. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.61483/4.62558. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.60822/4.61477. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.61085/4.61392. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.61144/4.61158. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.61013/4.61180. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.61194/4.61068. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.60663/4.61405. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.60383/4.61546. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.60392/4.61523. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.60713/4.61706. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.60418/4.62078. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.59915/4.62433. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.60306/4.62606. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.60114/4.62681. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.59847/4.63867. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.60275/4.64276. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.59861/4.64205. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.59657/4.64358. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.59970/4.64738. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.59855/4.63975. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.59986/4.63232. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.60174/4.63469. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.59870/4.63855. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.59539/4.64174. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.59618/4.64271. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.59667/4.65415. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.59630/4.64773. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.59410/4.64853. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.59748/4.64643. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.59425/4.65511. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.59225/4.66825. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.58942/4.66084. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.59155/4.66239. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.60087/4.62585. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.59528/4.63044. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.59274/4.64647. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.59084/4.65671. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.59113/4.63890. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.59169/4.63691. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.59140/4.64944. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.58996/4.65795. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.59076/4.65168. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.58881/4.64258. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.58371/4.66198. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.58415/4.66985. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.58695/4.66281. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.58363/4.67078. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.58382/4.67174. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.58239/4.66806. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.58383/4.67241. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.58713/4.66836. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.58333/4.67865. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.58251/4.65984. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.57759/4.68513. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.58194/4.65956. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.58293/4.66753. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.58514/4.66619. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.58075/4.66744. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.58250/4.67122. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.58077/4.68385. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.58248/4.66753. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.58300/4.67236. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.60137/4.61750. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.59841/4.61759. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.59562/4.62417. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.59016/4.63290. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.58862/4.63798. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.59223/4.63952. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.58936/4.64627. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.58653/4.65823. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.58502/4.65932. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.58326/4.65503. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.58705/4.64379. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.58442/4.65707. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.57932/4.68371. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.57900/4.66764. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.58140/4.66636. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.57802/4.66834. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.58347/4.65656. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.58800/4.64733. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.57890/4.66180. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.57857/4.67258. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 4.58572/4.65674. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.57793/4.67767. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.57978/4.66465. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.57767/4.67386. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.58341/4.65030. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.57983/4.67210. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.57809/4.68290. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.57602/4.66855. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.57799/4.67072. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.57613/4.68569. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.57665/4.67868. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.57702/4.68884. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.57364/4.68106. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.57371/4.68806. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.57434/4.68496. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.57597/4.68271. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.57697/4.68583. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.051799406700962544\n",
      "Epoch 0, Loss(train/val) 4.89494/4.87526. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.86883/4.86365. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 4.86238/4.85783. Took 0.13 sec\n",
      "Epoch 3, Loss(train/val) 4.86046/4.85943. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.85945/4.86077. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.85687/4.85958. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.85941/4.86268. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.85244/4.86446. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.85635/4.86406. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.85127/4.86942. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.85081/4.87457. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.85271/4.87936. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.84798/4.87760. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.84561/4.88305. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.84860/4.89023. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.85797/4.87397. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.84935/4.86329. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.84935/4.86532. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.84763/4.87255. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.84767/4.87548. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.84062/4.89292. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.84232/4.89248. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.83973/4.88436. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.84807/4.88662. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.84622/4.89780. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.84197/4.89802. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.83814/4.89465. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.84104/4.90025. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.83321/4.91979. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84109/4.90157. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.83931/4.89216. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.86296/4.85109. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.85487/4.85700. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.85359/4.86239. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.85126/4.86937. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84855/4.87058. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84595/4.86936. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84598/4.87480. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84832/4.87691. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.85139/4.89549. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.85877/4.86058. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.85705/4.85872. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.85172/4.86136. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.85151/4.86952. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.85034/4.88284. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.84784/4.87984. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.85005/4.87671. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.84515/4.88103. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84826/4.87595. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.84612/4.87202. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.84201/4.88308. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.84371/4.88094. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.84167/4.87010. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.84215/4.85866. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.84572/4.86280. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.84582/4.86381. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.84338/4.87530. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.84365/4.86099. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.84436/4.87470. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.84071/4.89004. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.84006/4.88609. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83937/4.88210. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.83900/4.89546. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.84145/4.87355. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.84259/4.86288. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.83924/4.88209. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.83356/4.88036. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.83914/4.88587. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.83915/4.87558. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.83584/4.87409. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.83632/4.88845. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.83264/4.88917. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83751/4.87419. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.83713/4.87694. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83134/4.88997. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.83846/4.86957. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.83739/4.88316. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.83545/4.89453. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.83548/4.89031. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.83586/4.87538. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.83489/4.86517. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.83511/4.88676. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.83793/4.87819. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.83333/4.88705. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.83098/4.86979. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83104/4.89073. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83044/4.87876. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.83134/4.87111. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83045/4.87671. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.82675/4.88482. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.82428/4.88262. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.82768/4.88147. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.82921/4.87393. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.82920/4.85914. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.83141/4.86107. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.82055/4.88733. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.82951/4.86837. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.82805/4.86884. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.82904/4.86736. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.82306/4.87290. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.0\n",
      "Epoch 0, Loss(train/val) 4.72365/4.73281. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.67844/4.68512. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.67541/4.68994. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.67605/4.70102. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.67463/4.69322. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.67385/4.69246. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.67662/4.68912. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.67502/4.68852. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.67233/4.68414. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.67406/4.68020. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.67676/4.67403. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.67316/4.67278. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.67366/4.67505. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.66861/4.67972. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.67155/4.68187. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.66864/4.69024. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.66223/4.68734. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.66484/4.68171. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.66425/4.69246. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.66176/4.69933. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.66744/4.69221. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.65905/4.69746. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.66482/4.70042. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.66023/4.71444. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 4.65969/4.71989. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.65586/4.72692. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.66228/4.68845. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.65496/4.73795. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.66208/4.70058. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.65602/4.72578. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.65087/4.72737. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 4.64880/4.76265. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.65034/4.74574. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.65433/4.72275. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.65106/4.72747. Took 0.17 sec\n",
      "Epoch 35, Loss(train/val) 4.64620/4.74189. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.64787/4.73631. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.64467/4.77336. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.65285/4.72384. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.64687/4.73507. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.64468/4.74404. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.65059/4.72517. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.65216/4.77265. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.64696/4.76111. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.65422/4.73529. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.65345/4.72941. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.64885/4.76901. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.64472/4.75007. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.65152/4.77780. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.64828/4.73916. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.64803/4.76987. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.64432/4.74364. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.64005/4.77978. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.64947/4.74995. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.64621/4.75934. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.64687/4.73421. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.64620/4.76846. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.64053/4.74899. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.65206/4.75224. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.64520/4.75118. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.64773/4.76888. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.64013/4.79141. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.63848/4.78647. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.64829/4.77492. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.64359/4.75820. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.64326/4.75612. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.63796/4.78200. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.64371/4.78115. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.64429/4.77163. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.63729/4.75227. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.64381/4.81458. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.65613/4.68188. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.66631/4.69305. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.65008/4.75400. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.64876/4.77026. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.64971/4.70733. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 4.63856/4.78618. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.65101/4.74703. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.64634/4.74879. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.64030/4.78451. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.64272/4.76009. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.64414/4.77232. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.63676/4.79030. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 4.64735/4.73782. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.63739/4.77420. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.64628/4.77624. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.63608/4.80567. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.63596/4.78013. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.63614/4.79077. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.63560/4.77898. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.63908/4.79097. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.64340/4.78285. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.63954/4.76735. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.63708/4.75935. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.64252/4.77915. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.64124/4.76266. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.64292/4.79795. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.63661/4.75473. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.63764/4.80799. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.64098/4.79669. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.03126526997403612\n",
      "Epoch 0, Loss(train/val) 4.70830/4.65021. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.63684/4.63827. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.63333/4.63830. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.63026/4.63953. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.63365/4.64171. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.63080/4.64616. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.63072/4.65351. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.62678/4.65755. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.62076/4.65975. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.62807/4.65110. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.62536/4.66152. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.62352/4.64874. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.62504/4.64580. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.62172/4.65400. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.62731/4.65983. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.62659/4.65683. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.62829/4.66238. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.62528/4.67441. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.62304/4.67343. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.62212/4.67423. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.61895/4.68231. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.62360/4.67497. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.62294/4.67402. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.61803/4.68387. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.61876/4.68353. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.62243/4.67639. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.61919/4.67990. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.61857/4.67505. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.61701/4.68151. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.61840/4.68241. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.61290/4.69534. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.61488/4.68535. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.61603/4.68259. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.61343/4.68905. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.61665/4.68424. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.61746/4.69301. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.61094/4.69919. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.60907/4.70464. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.60928/4.71304. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.60788/4.71800. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.60585/4.70272. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.61243/4.69199. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.61390/4.68854. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.61143/4.69650. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.60679/4.69858. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.61125/4.70156. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.61778/4.69638. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.61604/4.70658. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.61074/4.70980. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.61290/4.71780. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.60830/4.72286. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.61365/4.70733. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.61311/4.71715. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.61232/4.70290. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.61013/4.71687. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.60711/4.73352. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.61209/4.71342. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.61088/4.72558. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.60994/4.72039. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.60961/4.72737. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.60736/4.74203. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.60706/4.72268. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.60420/4.74064. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.59965/4.76239. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.60789/4.73173. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.60533/4.74387. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.60270/4.74336. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.60580/4.74358. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.60618/4.72434. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.60730/4.72887. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.60401/4.72979. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.60276/4.75414. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.60580/4.73706. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.61363/4.67845. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.62079/4.66585. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.61036/4.70101. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.60979/4.69644. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.61220/4.69793. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.61001/4.71405. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.61355/4.69669. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.60846/4.71044. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.60670/4.71125. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.60857/4.70881. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.61279/4.70421. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.60157/4.73049. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.60703/4.71199. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.60612/4.70942. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.60531/4.71955. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.60220/4.71698. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.60224/4.71771. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.60849/4.69638. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.60852/4.71802. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.60983/4.73136. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.60595/4.71867. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.60717/4.70353. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.60489/4.71576. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.60193/4.72318. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.60649/4.71721. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.60448/4.73661. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.60513/4.72739. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.0\n",
      "Epoch 0, Loss(train/val) 5.04994/5.00300. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.02258/5.00891. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.01809/5.00704. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.02018/5.00940. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 5.01552/5.00906. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.01993/5.00951. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.01629/5.00931. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.01444/5.01037. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.01315/5.00947. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.00948/5.00961. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.01406/5.00596. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.01172/5.00688. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.01202/5.00609. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.00895/5.00406. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 5.00771/5.00778. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.01333/5.00554. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.00766/5.00696. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.00689/5.00762. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.00895/5.00971. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.00768/5.00735. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.00862/5.00649. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.00531/5.00901. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 5.00567/5.00934. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.00675/5.01024. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.00668/5.01206. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.00433/5.01203. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.00347/5.01652. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.00404/5.02140. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.00516/5.02069. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.00610/5.01915. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.00368/5.02149. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.00183/5.02006. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.00163/5.02283. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.99951/5.02153. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.00113/5.01802. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 5.00095/5.03827. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.99568/5.03604. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.99750/5.03179. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.99738/5.02223. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 5.00007/5.02946. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.00128/5.02325. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.99743/5.03657. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.99564/5.03991. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.99626/5.03169. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.99920/5.03452. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.99404/5.05542. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.99510/5.03508. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.99588/5.04589. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.99779/5.04492. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.99436/5.04302. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.99322/5.03686. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.99687/5.04527. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.98845/5.05488. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.99427/5.05098. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.99542/5.02955. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.99816/5.03256. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.99766/5.04186. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.98949/5.04471. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.99089/5.03567. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.99155/5.04413. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.98945/5.06985. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.99633/5.04230. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.98944/5.04928. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.98825/5.04278. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.98882/5.04721. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.98959/5.06576. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.98448/5.06553. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.99132/5.07645. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.98480/5.07557. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.98738/5.04773. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.99275/5.04909. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.99083/5.07497. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.99001/5.05140. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.99006/5.05517. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.98429/5.07184. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.98931/5.05155. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.98664/5.07761. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.99128/5.06093. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.98426/5.07962. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.98528/5.06006. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.98777/5.07693. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.98823/5.05780. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.99232/5.06245. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.98588/5.08406. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.98512/5.04215. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 4.98663/5.05724. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.98311/5.06762. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.98521/5.07269. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.98372/5.05812. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.98702/5.07385. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.99084/5.06058. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.97981/5.07947. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.98953/5.07235. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.99147/5.04634. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.98408/5.06629. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.98332/5.08070. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.98329/5.09023. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.98937/5.07457. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.99034/5.06304. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.98349/5.07463. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.10421364855284106\n",
      "Epoch 0, Loss(train/val) 4.83989/4.77704. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.80135/4.77764. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.79394/4.78531. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79030/4.78752. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79341/4.78408. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.78778/4.78155. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.78788/4.77726. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.78669/4.77697. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.78459/4.77673. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.78926/4.77593. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.78395/4.77639. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.78177/4.77483. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.78473/4.77878. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.78203/4.78325. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.78067/4.77729. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.78187/4.77392. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.77958/4.78153. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.78055/4.78152. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.77799/4.78130. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.77647/4.78401. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.77469/4.78195. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.77279/4.78679. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.77615/4.78843. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.77650/4.78596. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.77594/4.78577. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.77347/4.78267. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.77076/4.78968. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.77372/4.79058. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77037/4.78813. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.77027/4.78773. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.77680/4.79497. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.77474/4.78208. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.77284/4.78182. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.77348/4.79299. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.76770/4.79352. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.76689/4.79071. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.76804/4.79392. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.76666/4.78791. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.76587/4.79453. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.76801/4.79336. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.76318/4.80578. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.77369/4.79318. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.77004/4.79994. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.77606/4.79683. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.77786/4.78261. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.77287/4.79638. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77467/4.79458. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.76982/4.79930. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.76894/4.79092. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.77059/4.78579. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.76501/4.80395. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77223/4.79092. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.76620/4.79419. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.76414/4.79725. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.76377/4.80138. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.76724/4.78779. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.76353/4.79363. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.76564/4.79389. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76364/4.79613. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.76308/4.79521. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.76290/4.79564. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.76646/4.79075. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76729/4.79796. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.76156/4.80019. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.76147/4.79489. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.75866/4.79612. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.75855/4.79809. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.76418/4.79191. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.76195/4.83234. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.77632/4.78682. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.76855/4.79627. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.76939/4.77901. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76596/4.79006. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.76873/4.80391. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.76593/4.79758. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.76630/4.79411. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.76218/4.80129. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.76469/4.78744. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.76273/4.79929. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.76465/4.79676. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.76609/4.79290. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.75966/4.79497. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.76265/4.79351. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76069/4.79544. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75429/4.79663. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.76480/4.79471. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.75992/4.79807. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.75949/4.79732. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.76238/4.79157. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.76090/4.79396. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.76279/4.79209. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.77698/4.80178. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.77826/4.77730. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.77645/4.77385. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.77708/4.76987. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.76822/4.77223. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.76403/4.78196. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.76266/4.79293. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.75604/4.79382. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.76186/4.78268. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.1259881576697424\n",
      "Epoch 0, Loss(train/val) 5.10893/5.07959. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.04872/5.08882. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.04433/5.08074. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 5.04498/5.06813. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.04431/5.07053. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.04403/5.06544. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.04150/5.06651. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.04020/5.07325. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.04106/5.07385. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.03887/5.08235. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 5.04023/5.08057. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.03750/5.07847. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.03731/5.07980. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 5.04195/5.06951. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.04092/5.05563. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.03531/5.04998. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.03494/5.03796. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.03340/5.06172. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 5.03442/5.06499. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.03154/5.06650. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 5.03378/5.05946. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.03144/5.05709. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.03221/5.05509. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.02842/5.06404. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.03075/5.06527. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.03023/5.05070. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.02556/5.05481. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.02270/5.06593. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.02303/5.06024. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.02383/5.05972. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.02299/5.06354. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 5.02566/5.05295. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.02520/5.04997. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.01895/5.04340. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 5.01964/5.06877. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.02278/5.09039. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.02774/5.07767. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.02919/5.07811. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.02662/5.05989. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.02281/5.06639. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 5.02053/5.07349. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.02115/5.07397. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.01917/5.06512. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 5.01581/5.07554. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.02343/5.05133. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.01525/5.06416. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.01559/5.05414. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.02144/5.05069. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.02094/5.03873. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 5.01986/5.04015. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.01293/5.04738. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 5.01358/5.07033. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.01983/5.05433. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.01586/5.03181. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.02100/5.08284. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 5.02965/5.06712. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.02605/5.05393. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.02889/5.04325. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.02124/5.06621. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.02186/5.04883. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.01438/5.05141. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.01629/5.06631. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.01165/5.06308. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.01467/5.06084. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.02169/5.06472. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 5.02433/5.07069. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.02579/5.07170. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.02124/5.06427. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.01918/5.06284. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.01889/5.06289. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.01978/5.04725. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.01151/5.03229. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.01879/5.05923. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.01897/5.05089. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.01772/5.03749. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 5.01383/5.04804. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.01309/5.04602. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.01576/5.03623. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.01001/5.04228. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.01708/5.04557. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 5.00820/5.04610. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.01447/5.03777. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.02060/5.04741. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.01904/5.03556. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.01257/5.05433. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.01636/5.04362. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.01177/5.03801. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.01519/5.04238. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.00992/5.04646. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.01206/5.04868. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.01241/5.03183. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.01272/5.04623. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 5.01596/5.03273. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.00913/5.04328. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.01560/5.04035. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.01769/5.04550. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 5.00735/5.03481. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.01238/5.03896. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.00732/5.04329. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.00832/5.04346. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: 0.022792310997917512\n",
      "Epoch 0, Loss(train/val) 4.90117/4.84668. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.87868/4.85290. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.87117/4.84815. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.86850/4.84623. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.87120/4.84541. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.87182/4.85064. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.87000/4.86468. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.86465/4.87077. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.86269/4.86265. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.86644/4.86018. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.86387/4.86363. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.86460/4.86462. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.86007/4.86233. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 4.86007/4.85993. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 4.85965/4.86055. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 4.85673/4.87461. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.85632/4.86730. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.85220/4.87706. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.85135/4.87244. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85399/4.86124. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.84974/4.86646. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.85012/4.86283. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.84786/4.86237. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.84624/4.86340. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.85168/4.86673. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.84847/4.87919. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.85147/4.85468. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.85163/4.86301. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.84842/4.86398. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84843/4.86870. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.84901/4.86239. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.84994/4.86621. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84651/4.86000. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.84623/4.86432. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.84865/4.84783. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.84068/4.86254. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84579/4.86162. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.84408/4.85941. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84591/4.84427. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84350/4.85903. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84551/4.84538. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.84466/4.84965. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 4.84354/4.86258. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.84627/4.85626. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.84103/4.85603. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.86033/4.85438. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.84899/4.87467. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.85019/4.86162. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84152/4.85798. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.84541/4.86431. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.84990/4.86443. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.84791/4.85514. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.84357/4.85801. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.84411/4.85376. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.84443/4.85101. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.84267/4.85126. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.84065/4.84656. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.84359/4.83273. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.84259/4.84099. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.84335/4.86706. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.83818/4.84388. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.84324/4.84974. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.83922/4.84702. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.84218/4.83951. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.83732/4.85254. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.83751/4.85247. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.84174/4.84754. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.84326/4.85255. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.84558/4.86009. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83976/4.84934. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.84158/4.84113. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 4.83518/4.85175. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 4.83131/4.84010. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.83461/4.86218. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83441/4.83719. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.83683/4.89366. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.83051/4.90849. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.84858/4.87756. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.84195/4.84001. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.84327/4.88013. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.84971/4.88697. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.84394/4.87738. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.83919/4.83861. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.84639/4.84885. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.84721/4.86813. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.84429/4.85643. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83919/4.84816. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.83782/4.85152. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.82809/4.83991. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.83591/4.85343. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.83897/4.83955. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.83548/4.83816. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83474/4.85429. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83831/4.83572. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.83449/4.84558. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.82851/4.84686. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.83445/4.83868. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.82962/4.84261. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.82814/4.84818. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.82925/4.85848. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.021109792565893827\n",
      "Epoch 0, Loss(train/val) 4.96558/4.96629. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 4.94268/4.95426. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.93425/4.97267. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.93646/4.98321. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.93453/4.98941. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.93509/4.96831. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.93371/4.94689. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.92817/4.94403. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.92638/4.94955. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.92493/4.95719. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.92511/4.95635. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.92848/4.94448. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.92516/4.94521. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.92158/4.95491. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.92193/4.95267. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.91982/4.95407. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.92252/4.96238. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.92159/4.95673. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.92276/4.95337. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.91616/4.95058. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.91995/4.94800. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.91689/4.95610. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.91881/4.96288. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.91699/4.95371. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.91724/4.95359. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.91414/4.95746. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.91737/4.95767. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.91413/4.95950. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.91655/4.96252. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.91416/4.96304. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.91517/4.96652. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.91584/4.95920. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.91420/4.96072. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.91128/4.95659. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.91180/4.95797. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.91093/4.96102. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.91360/4.95788. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.91205/4.96700. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.92124/4.93552. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.92129/4.92497. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.91924/4.92800. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.91831/4.93442. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.91834/4.93581. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.91448/4.94103. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.91592/4.93450. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.91446/4.94557. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.90877/4.94991. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.91165/4.95085. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.91060/4.97423. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.92875/4.97247. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.92011/4.95196. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.91601/4.95006. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.90982/4.95803. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.91317/4.95590. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.91476/4.95127. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.90991/4.95412. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.91006/4.94817. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.90538/4.96337. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.90973/4.95513. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.91102/4.95941. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.90979/4.95805. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.90902/4.95452. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.90345/4.96115. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.91054/4.96101. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.90536/4.95810. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.90676/4.96805. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.90743/4.95907. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.90258/4.96358. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.90764/4.95263. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.90869/4.95598. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.90192/4.96868. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.90664/4.96049. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.90594/4.96830. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.90632/4.96089. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.90442/4.95295. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.89841/4.96637. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.90723/4.94113. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.90719/4.95540. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.90405/4.96738. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.90365/4.95375. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.90837/4.94712. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.90545/4.95633. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.89849/4.96349. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.90326/4.95259. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 4.90058/4.95846. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 4.90452/4.96397. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.90192/4.97460. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.90257/4.97058. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.90577/4.96086. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.89812/4.99350. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.91216/4.96934. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.91005/4.97154. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.90765/4.95522. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.90153/4.96029. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.89791/4.96340. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.90421/4.96243. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.89726/4.96811. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.90402/4.96310. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.90181/4.96057. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.90305/4.95405. Took 0.08 sec\n",
      "ACC: 0.40625, MCC: -0.18547738582708967\n",
      "Epoch 0, Loss(train/val) 4.85643/4.77113. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.77560/4.81478. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.78193/4.84737. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.78673/4.86592. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79127/4.84879. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.78853/4.81483. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.78168/4.79917. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.77924/4.80496. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.77913/4.81271. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.77799/4.81205. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.77900/4.80956. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.77590/4.81379. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.77642/4.82076. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.77562/4.82143. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.77852/4.81532. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.77398/4.81323. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.77366/4.81627. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.76879/4.83275. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.77290/4.82423. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.77297/4.82784. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 4.77547/4.81172. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.77143/4.81140. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.76649/4.82093. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.76804/4.82840. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.76990/4.80658. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.76711/4.81758. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.76501/4.81998. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.76382/4.79952. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.76207/4.82161. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.76032/4.82642. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.76358/4.81706. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.76024/4.83096. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.75548/4.83727. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.76193/4.82814. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.75882/4.83326. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.75597/4.83810. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.76089/4.83622. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.75810/4.82499. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.75555/4.83348. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.75508/4.82683. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.75571/4.81194. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.75313/4.81545. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.75210/4.81890. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.75187/4.83211. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.75264/4.84352. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.74916/4.83978. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.75285/4.83999. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.75499/4.82687. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.75682/4.81413. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.74798/4.83735. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.75180/4.86678. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.75002/4.84984. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.75189/4.84277. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.74596/4.84068. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.74827/4.86298. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.75244/4.84984. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.74778/4.83960. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.75208/4.82618. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.74792/4.83506. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.74640/4.84754. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.74833/4.83876. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.75208/4.82490. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.74951/4.83734. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.74514/4.83528. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.74985/4.83099. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.76421/4.81205. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.75165/4.82288. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.75051/4.82941. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.75138/4.81752. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.75562/4.82061. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.74572/4.83471. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.75061/4.84144. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.74697/4.85157. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.74723/4.84026. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.74998/4.83875. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.74432/4.85399. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.74734/4.84157. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.74642/4.84726. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.74774/4.84307. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.74020/4.84291. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.74404/4.87034. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.75446/4.83845. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.74594/4.84790. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.75027/4.83259. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.74631/4.84171. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.74418/4.83524. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.74461/4.83926. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.74197/4.85787. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.73995/4.85210. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.73984/4.84957. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.75260/4.83294. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.74867/4.83199. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.74218/4.85440. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.74684/4.83162. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.73939/4.87372. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.74965/4.84300. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.74152/4.85625. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.74502/4.84147. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.74683/4.84485. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.74365/4.84281. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.03734655007457467\n",
      "Epoch 0, Loss(train/val) 4.66154/4.61316. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.60809/4.60874. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.59723/4.61788. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.60254/4.61649. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.59971/4.62332. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.60096/4.62375. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.60300/4.62291. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.59825/4.62903. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.59840/4.62610. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.59566/4.62817. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.59896/4.62314. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.59756/4.62487. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.59276/4.63327. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.59672/4.63489. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.59401/4.63216. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.59703/4.63582. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.59452/4.63868. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.59374/4.63462. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.59331/4.64342. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.59388/4.63585. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.59444/4.63286. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.59046/4.63568. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.59404/4.64021. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.59034/4.63044. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.59323/4.63106. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.59299/4.63356. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.58895/4.64698. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.59283/4.64842. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.59420/4.64159. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.59200/4.64039. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.58836/4.65340. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.58719/4.65478. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.59275/4.64558. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.58724/4.66754. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.59369/4.67400. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.58581/4.65269. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.58714/4.64912. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.58884/4.66093. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.58587/4.67511. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.59063/4.64910. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.58868/4.66243. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.58811/4.67391. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.58618/4.67637. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.58549/4.66150. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.58578/4.68161. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.59122/4.69586. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.58896/4.67401. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.58683/4.66630. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.58250/4.66905. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.58397/4.69222. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.58557/4.67845. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.58767/4.65189. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.58532/4.66169. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.58518/4.66899. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.58452/4.68075. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.58300/4.68794. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.58355/4.69162. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 4.58545/4.65200. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.58519/4.66552. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.58294/4.67738. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.58243/4.67192. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.58190/4.68677. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.58836/4.69536. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.58090/4.67532. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.58425/4.66774. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.58399/4.69924. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.58265/4.65689. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.57916/4.69464. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.58377/4.67921. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.58361/4.65606. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.58482/4.69263. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 4.57711/4.67892. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.57863/4.70581. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.58270/4.66377. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.58872/4.67303. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.58281/4.67214. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.57997/4.68006. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.57834/4.68119. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.57874/4.66875. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.58201/4.66327. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.57701/4.68966. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.57821/4.70701. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.58297/4.66778. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.58021/4.66372. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.58037/4.66108. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.58035/4.68678. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.57296/4.67746. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.57780/4.65994. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.58235/4.70464. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.57687/4.68610. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.57452/4.71078. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.57467/4.69960. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.57501/4.69605. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.57549/4.67419. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.58081/4.70950. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.57611/4.72149. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.58052/4.65350. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.57645/4.70431. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.57343/4.68348. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.57183/4.69094. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.06362847629757777\n",
      "Epoch 0, Loss(train/val) 5.07142/5.05199. Took 0.18 sec\n",
      "Epoch 1, Loss(train/val) 5.05877/5.03782. Took 0.17 sec\n",
      "Epoch 2, Loss(train/val) 5.05728/5.03646. Took 0.15 sec\n",
      "Epoch 3, Loss(train/val) 5.05715/5.03589. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.06333/5.06274. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.05808/5.07401. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.05007/5.05931. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.04905/5.05959. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.05248/5.06718. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 5.05141/5.07053. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.05187/5.07331. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 5.05021/5.07571. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.04941/5.07957. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 5.04952/5.06930. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.04957/5.07731. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.04789/5.08175. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 5.04864/5.07789. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.04799/5.08156. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.05010/5.07183. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.04965/5.07622. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 5.04912/5.08556. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 5.04878/5.08614. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 5.04716/5.08603. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.04472/5.08941. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 5.04803/5.07909. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 5.04558/5.09162. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 5.04518/5.08304. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.04311/5.09460. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.04100/5.09179. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 5.04452/5.10612. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.04346/5.10095. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.03997/5.10519. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.03922/5.11143. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.04185/5.10174. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.04334/5.10581. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 5.04169/5.10077. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.03941/5.10993. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.03644/5.11278. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 5.03824/5.12125. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.03595/5.11633. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.03617/5.11114. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 5.03688/5.11551. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.03341/5.12860. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.03037/5.13144. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.03800/5.11139. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.02987/5.12848. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.03338/5.11909. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.03196/5.12304. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.03245/5.11061. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.03687/5.11033. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 5.03581/5.11701. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.03190/5.12096. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.03042/5.10646. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.02701/5.13658. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 5.02946/5.13455. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 5.03165/5.12342. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 5.02766/5.13576. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.02658/5.13519. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 5.03115/5.13251. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 5.02502/5.13564. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 5.02372/5.13370. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.02441/5.15074. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 5.02974/5.12641. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.02764/5.13660. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 5.02659/5.09029. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.02971/5.09831. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.02512/5.10886. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.02688/5.11941. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 5.02760/5.13935. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.01560/5.15564. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.02435/5.13671. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.03117/5.12199. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.02466/5.15161. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.02447/5.14597. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 5.02434/5.15220. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.03516/5.12869. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 5.01468/5.15774. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.02069/5.14305. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 5.02076/5.15542. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.02034/5.13111. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 5.01617/5.12593. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.02022/5.14836. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 5.01396/5.14423. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 5.02159/5.13876. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.01682/5.17761. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.01986/5.16291. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.01652/5.19401. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 5.02328/5.18115. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.01271/5.19158. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 5.01551/5.18834. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.02475/5.15278. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.01559/5.17498. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 5.01773/5.17422. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.01258/5.20321. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.02149/5.19158. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.01385/5.19836. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.01658/5.19467. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.01683/5.23711. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.01453/5.19531. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.01346/5.20000. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.011958266722236254\n",
      "Epoch 0, Loss(train/val) 4.95011/4.88909. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.90111/4.88817. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.89627/4.88708. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.89264/4.88788. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.89311/4.89102. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.89429/4.89530. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.89172/4.90242. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.89029/4.91303. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.88899/4.91957. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.88506/4.93223. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.88592/4.94072. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.88564/4.93101. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.88195/4.93922. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.88080/4.93953. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.88136/4.93423. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.88061/4.93781. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.87983/4.93702. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.87930/4.93980. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.87600/4.94577. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.87800/4.94062. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.87428/4.94211. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.87524/4.93960. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.88133/4.94112. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.87024/4.95601. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.87634/4.91078. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.87361/4.96528. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.87036/4.93503. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.87649/4.95856. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.87234/4.97645. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.87141/4.96559. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.87096/4.97904. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.86940/4.97379. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.86911/4.96596. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.87209/4.95882. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.87079/4.96695. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.86624/4.97424. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.87360/4.97538. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.86505/4.98667. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.87086/4.96773. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.86927/4.97124. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.86197/4.99110. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.86971/4.96667. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.86698/4.96920. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.86336/4.98457. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.86764/4.97612. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.86546/4.98048. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.86638/4.97897. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.86840/4.97128. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.86012/4.98328. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.86806/4.98311. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.86306/4.97763. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.86510/4.99723. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.86030/4.99929. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.86594/4.97685. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.86506/4.98735. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.85896/4.99360. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.86425/4.97419. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.85770/5.01120. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.86269/4.98687. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.86207/5.00085. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.86108/4.98582. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.86528/4.99097. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.86233/4.99085. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.86206/4.99881. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.85863/4.98582. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.85731/4.98564. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.85981/4.99300. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.85573/5.01953. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.85885/4.98282. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.85816/4.99933. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.85505/5.02054. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.85390/5.01286. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.86304/4.97235. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.85548/5.01203. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.85858/4.98801. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.85781/5.01055. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.85507/5.02782. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.85421/4.98727. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.85309/5.03656. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.85860/5.01025. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.86294/4.98698. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.85388/5.02620. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.85945/5.00181. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.85626/5.01430. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.86260/5.02723. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.86052/4.98962. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.85392/4.99241. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.85868/5.00121. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.85342/4.99360. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.85660/5.00263. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.85666/4.99759. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.84895/5.03255. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.85317/5.00577. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.85655/5.01695. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.85166/5.02427. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.85637/5.01439. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.85427/5.01408. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.85485/4.99662. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.84807/5.02365. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.85617/5.00672. Took 0.09 sec\n",
      "ACC: 0.46875, MCC: -0.059863071616150634\n",
      "Epoch 0, Loss(train/val) 4.85548/4.82444. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.83559/4.83554. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.84567/4.84032. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.84335/4.85813. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.83486/4.83825. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.82185/4.84062. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.82708/4.84851. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.82457/4.85302. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.82183/4.85433. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.82429/4.85744. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.82444/4.86369. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.82048/4.85196. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.82200/4.88200. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.81963/4.85648. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.82251/4.86795. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.82319/4.87348. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.81951/4.86139. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.81826/4.86649. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.81944/4.86617. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.81794/4.87233. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.81850/4.87034. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.82113/4.86970. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.81978/4.86057. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.81952/4.87087. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.81951/4.87483. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.81417/4.87464. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.81862/4.87827. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82035/4.86784. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.81617/4.87804. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.81186/4.88724. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.81330/4.88716. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.81842/4.87664. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.81612/4.88536. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.81654/4.88412. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.81524/4.87576. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.81147/4.87383. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.80621/4.91155. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.81551/4.87671. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.81271/4.88695. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.81507/4.86381. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.81088/4.88223. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.80968/4.89556. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.81279/4.89367. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.81045/4.89068. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.81401/4.88268. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.81145/4.88198. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.82759/4.83603. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.82149/4.83890. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.82162/4.83864. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.82272/4.83821. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.82072/4.83889. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.82036/4.83725. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.82012/4.83851. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.81966/4.83960. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.82031/4.83866. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.82008/4.83937. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.82097/4.83973. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.82144/4.83869. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.81866/4.83823. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.82090/4.83736. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.81737/4.84082. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.81612/4.84226. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.82143/4.84125. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.81702/4.84359. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.81664/4.84395. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.81844/4.84768. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.81630/4.84733. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.81695/4.84461. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.81590/4.85425. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.81307/4.85586. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.81377/4.86157. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.81238/4.85433. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.81415/4.85361. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.81212/4.85996. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.81595/4.86806. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.81224/4.85910. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.81236/4.87471. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.80951/4.87511. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.81044/4.85919. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.81675/4.85262. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.81238/4.86002. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.80878/4.87243. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.80259/4.88665. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.80972/4.86722. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.81007/4.87109. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.80624/4.87723. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.80614/4.88744. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.80797/4.87329. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.80675/4.87601. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.81274/4.87360. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.80452/4.87132. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.80853/4.87051. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.81025/4.86982. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.80673/4.87784. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.80502/4.89135. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.80131/4.89012. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.80367/4.88469. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.80286/4.89337. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.80008/4.87669. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.80381/4.89786. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.09607689228305229\n",
      "Epoch 0, Loss(train/val) 4.85077/4.78390. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.80862/4.78188. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.80582/4.78236. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.80699/4.78572. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.80078/4.78540. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.80328/4.78538. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.80518/4.78665. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.80023/4.78901. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.80028/4.78867. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.80059/4.79215. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79936/4.79427. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 4.79778/4.80035. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.79586/4.79960. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.79648/4.78689. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.80102/4.79240. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.80163/4.79332. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.80168/4.79498. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.80156/4.80160. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.79533/4.79856. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.79672/4.79877. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.79617/4.80237. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.79445/4.80293. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.79616/4.80737. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.79319/4.79236. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.79603/4.81450. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.79636/4.81174. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.79195/4.80060. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.79807/4.79603. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.79291/4.80785. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.79185/4.80473. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.79044/4.80554. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.79404/4.80842. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.79220/4.81937. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.79165/4.81716. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.79131/4.83187. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.79063/4.82564. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.78906/4.82526. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.78858/4.83182. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.79018/4.83676. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.78861/4.80867. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.79169/4.80168. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.79585/4.80311. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.79081/4.80907. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.79193/4.81260. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.78901/4.81456. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.79225/4.81505. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.78713/4.81601. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.78805/4.81989. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.78826/4.82392. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.78712/4.83474. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.78378/4.84140. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.78413/4.84599. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.78293/4.82964. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.78297/4.83369. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.78658/4.82681. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.78427/4.84247. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.78345/4.83905. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 4.78509/4.81197. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.79020/4.83609. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.78694/4.82603. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.78888/4.82756. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.78701/4.82639. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.79381/4.81019. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.79085/4.82161. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.78906/4.83251. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.78897/4.83186. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.78750/4.84173. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.78682/4.84424. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.78613/4.84478. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.78556/4.84062. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.78396/4.83004. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.78299/4.85778. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.79035/4.83906. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.78792/4.81928. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 4.78835/4.83336. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.78671/4.83206. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78210/4.84027. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78565/4.83578. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.78706/4.83405. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.78395/4.84359. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.78261/4.86396. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.77779/4.86329. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.77835/4.84803. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 4.78623/4.84340. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.78191/4.85654. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.78130/4.84983. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.78136/4.85805. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.78359/4.85748. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.78099/4.84290. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.78465/4.84873. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.78486/4.84802. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.78505/4.85206. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.78098/4.84502. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.78146/4.85615. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.78063/4.85137. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.77760/4.85189. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.78460/4.84403. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.77878/4.85099. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 4.78707/4.85409. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.78453/4.84416. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.11555252264060875\n",
      "Epoch 0, Loss(train/val) 5.04939/5.03694. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.03196/5.02882. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.02489/5.03245. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.02585/5.02961. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.02318/5.03130. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.02653/5.02937. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.02410/5.02759. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.02290/5.02680. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 5.02205/5.02411. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.02376/5.02332. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.02596/5.02344. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.02379/5.02115. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.02432/5.01930. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.01945/5.01768. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.02175/5.01565. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.01946/5.01620. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.01790/5.01605. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 5.01735/5.01563. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.01215/5.01374. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.01771/5.01319. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.01388/5.01570. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.01362/5.01216. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 5.01229/5.01338. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.01365/5.01051. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.01098/5.01392. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.01546/5.01878. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.00989/5.01596. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 5.01194/5.01764. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.01161/5.01464. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.00965/5.01920. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 5.00873/5.02107. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.00648/5.02358. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.00881/5.02493. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.00924/5.02545. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 5.01074/5.02295. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.00716/5.02480. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.00864/5.02991. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.00348/5.02874. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.00841/5.02753. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.00714/5.02343. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.00747/5.02558. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.01154/5.02570. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.00459/5.02973. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 5.00693/5.03143. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.00707/5.02110. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.01060/5.01996. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 5.00346/5.03840. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.00348/5.03790. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.00395/5.03503. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 5.00565/5.03194. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.00600/5.03109. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.00318/5.03336. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 5.00443/5.03246. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.00367/5.02801. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.00346/5.02583. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.00288/5.02178. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.00152/5.03053. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.00445/5.02358. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.00446/5.02450. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 5.00511/5.02609. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.00467/5.02547. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 5.00325/5.02609. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.00254/5.03103. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.00181/5.03093. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.99933/5.02821. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.99738/5.02756. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.00576/5.01192. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.99369/5.01986. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 5.00394/5.01917. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.00095/5.03088. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 5.00196/5.02764. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.99994/5.03031. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.99766/5.03841. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.99898/5.03932. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.99913/5.03880. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.00099/5.01739. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.99904/5.02131. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.00041/5.02247. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.99494/5.03536. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.99046/5.03717. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.00117/5.02965. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.99682/5.02673. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.00014/5.02765. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.99428/5.04509. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.99235/5.04049. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.99997/5.04360. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.99617/5.03373. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.99578/5.04426. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.99569/5.04304. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.99617/5.03869. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.99214/5.04928. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.99767/5.04994. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.99085/5.04930. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.99360/5.04198. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.98974/5.04344. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.99680/5.03638. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.99918/5.03331. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.99611/5.04772. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.99406/5.05111. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.98584/5.05755. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.10677275981195139\n",
      "Epoch 0, Loss(train/val) 4.65915/4.62170. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.62733/4.62347. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.63045/4.63843. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 4.63898/4.64209. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.64840/4.61984. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.63518/4.62457. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.62439/4.62151. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.62658/4.62132. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.62659/4.62218. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.62696/4.62252. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.62754/4.62016. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.62620/4.62085. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.62427/4.62226. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.62327/4.62364. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.62599/4.62526. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.62429/4.62398. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.61872/4.62617. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.62079/4.62880. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.62366/4.61614. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.61768/4.62482. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.61573/4.62575. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.61708/4.62680. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.61448/4.63124. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.61801/4.63090. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.61342/4.63441. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.61213/4.63614. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.62210/4.62910. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.60981/4.63186. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.61154/4.62476. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.60917/4.63558. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.60839/4.62830. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.61171/4.63858. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.60883/4.62264. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.61383/4.63177. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.60424/4.63987. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.60767/4.62738. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.60998/4.63080. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.60087/4.62487. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.60771/4.64412. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.60730/4.62886. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.61164/4.63361. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.59868/4.65113. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.59714/4.65423. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.60322/4.64243. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.61025/4.61213. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.60716/4.61844. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.60294/4.64393. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.60195/4.63239. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.60715/4.64586. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.60662/4.65482. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.60439/4.65346. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.59782/4.65269. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.60171/4.65960. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.60028/4.65014. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.59884/4.64823. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.59620/4.65624. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.59902/4.65464. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.59580/4.63947. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 4.60197/4.64745. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.59830/4.65840. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.60044/4.64527. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.60505/4.65274. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.59149/4.66296. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.59979/4.66131. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.59517/4.64758. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.59792/4.64517. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.60078/4.64962. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.60903/4.63606. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.60128/4.62985. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.60515/4.63406. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.60121/4.65286. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 4.60801/4.64457. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.60930/4.62475. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.60389/4.63757. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.60367/4.64352. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.60295/4.63230. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.59669/4.63504. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.60242/4.66544. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.59430/4.63448. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.59987/4.63128. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.60229/4.64578. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.59801/4.63784. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.59140/4.63241. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.59128/4.64886. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.59116/4.66028. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.59295/4.65414. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.59291/4.63336. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.59152/4.67712. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.59316/4.64323. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.58692/4.66434. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.59868/4.64296. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.59075/4.64874. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.58966/4.64797. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.59370/4.66647. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.58793/4.67402. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.59244/4.63310. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.59211/4.64321. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.59500/4.64880. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.58888/4.65025. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.58770/4.65316. Took 0.09 sec\n",
      "ACC: 0.46875, MCC: -0.06362847629757777\n",
      "Epoch 0, Loss(train/val) 4.77513/4.74091. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.72968/4.73132. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.72069/4.72736. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.72281/4.72552. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.72006/4.72276. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.72240/4.72242. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.72046/4.71975. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.72007/4.71910. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.71810/4.71384. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.71722/4.71480. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.71702/4.71742. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.71559/4.71661. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.71578/4.70932. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.71465/4.69895. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.71511/4.70210. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.71028/4.70814. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.70921/4.70292. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.70640/4.70009. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.70992/4.69742. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.70861/4.72225. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.72471/4.72046. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.71890/4.72940. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.71369/4.72196. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.70816/4.71982. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.70523/4.71350. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.71050/4.71832. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.70690/4.70733. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.70711/4.71846. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.70135/4.71688. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.70115/4.72489. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 4.70386/4.70425. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.69647/4.71548. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.69430/4.70486. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.69865/4.69970. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.70079/4.71869. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.69379/4.70895. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.69437/4.69998. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.69624/4.69954. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.69598/4.69863. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.69155/4.69664. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.68458/4.70949. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.69037/4.70514. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.69792/4.71314. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.69260/4.70545. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.68908/4.70296. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.69009/4.69477. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.68817/4.68979. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.68729/4.69670. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.69145/4.69319. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.68732/4.70604. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.69176/4.70280. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.68654/4.68964. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.68181/4.69695. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.68626/4.72542. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.68632/4.70087. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.68386/4.69059. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.68636/4.69509. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.68737/4.69810. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.68316/4.71896. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.68767/4.70237. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.68198/4.69509. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.68560/4.70469. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.68066/4.72017. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.68711/4.69092. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.68219/4.70616. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.68169/4.70132. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.68021/4.70416. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.68602/4.70407. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.68334/4.69983. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.67892/4.70715. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.68293/4.70040. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.68524/4.71568. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.67692/4.70337. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.68119/4.70396. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.68207/4.70971. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.68582/4.68903. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.68248/4.72101. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.68478/4.70916. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.67960/4.69958. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.68092/4.69710. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.68301/4.69635. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.68196/4.71788. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.67895/4.70743. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.67046/4.69796. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.68317/4.70546. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.68357/4.71136. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.67923/4.69806. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.68026/4.69335. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.67973/4.69352. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.68101/4.69692. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.67684/4.71106. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.68069/4.71160. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.68283/4.71036. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.67604/4.71302. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.68077/4.70687. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.67674/4.69045. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.67812/4.70545. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.67143/4.72180. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.67837/4.69524. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.67863/4.70786. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.09423188886809433\n",
      "Epoch 0, Loss(train/val) 4.88497/4.82441. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.81915/4.81169. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.81164/4.81192. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.80762/4.82702. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.80550/4.82558. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.80481/4.83923. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.80551/4.84368. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.80372/4.84304. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.80479/4.83632. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.80110/4.84030. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.80222/4.83878. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.80204/4.83834. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.79891/4.84193. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.80105/4.83358. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.79821/4.84038. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.79723/4.83994. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.79589/4.83735. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.79653/4.83474. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.79941/4.83173. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.78945/4.83232. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.79812/4.82814. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.79451/4.83808. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.79564/4.82619. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.78896/4.84149. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.79564/4.81959. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.78784/4.83365. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.79689/4.81771. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.78969/4.84093. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.79444/4.82363. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.79015/4.82850. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.78866/4.83389. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78921/4.83330. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.79055/4.83584. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78903/4.83686. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.79000/4.83540. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.78882/4.84783. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.78920/4.82829. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.78912/4.83160. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.78623/4.82376. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.78743/4.82700. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.78746/4.83289. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.78865/4.83182. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.78671/4.83777. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.78812/4.83974. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.78815/4.83193. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.78158/4.82561. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.79090/4.83156. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.78375/4.83756. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.78509/4.82282. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.78230/4.83289. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.78505/4.82578. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.78992/4.82131. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.78422/4.84004. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.78439/4.82272. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.78563/4.82369. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.78574/4.82281. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.77946/4.83791. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.78195/4.82060. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.78469/4.83165. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.78077/4.84082. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.78260/4.81575. Took 0.12 sec\n",
      "Epoch 61, Loss(train/val) 4.78151/4.82996. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.77714/4.83241. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.79646/4.81063. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.79162/4.81096. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.78944/4.84651. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.78615/4.81897. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.78229/4.82296. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.78456/4.83604. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.78007/4.82648. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.77821/4.82213. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.77886/4.82701. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.78603/4.82203. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.78482/4.81952. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.78152/4.82051. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.78095/4.82961. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78353/4.81977. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78204/4.82803. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.77760/4.82995. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.77650/4.83763. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.77579/4.83683. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.78112/4.83449. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.79368/4.81257. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.78624/4.81792. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.78086/4.82720. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.77862/4.81969. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.77599/4.84071. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.78448/4.82614. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.78231/4.82218. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.77582/4.82867. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.77475/4.83029. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.78075/4.82566. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.77631/4.83131. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.77844/4.84285. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.77709/4.82021. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.77487/4.83067. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.77643/4.82938. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.77412/4.83085. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77990/4.84169. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.77661/4.84189. Took 0.08 sec\n",
      "ACC: 0.40625, MCC: -0.18029556650246306\n",
      "Epoch 0, Loss(train/val) 4.84395/4.78728. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.79913/4.79317. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.79519/4.80106. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79322/4.81051. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.79205/4.81021. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79404/4.80387. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.78950/4.79814. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.78814/4.79532. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.78718/4.79136. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 4.78904/4.79289. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.78766/4.78622. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.78464/4.78953. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.79004/4.78296. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.78666/4.78276. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.78454/4.78796. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.78020/4.79805. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78317/4.80138. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.78199/4.81570. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.78590/4.81035. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.77946/4.82278. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.77723/4.82797. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.77677/4.81974. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.77657/4.81734. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.78485/4.78579. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 4.78760/4.77931. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.78626/4.78616. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.77683/4.79862. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.77946/4.79496. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77864/4.79429. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.78154/4.80341. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.77642/4.79583. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.76975/4.82876. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.77019/4.80565. Took 0.11 sec\n",
      "Epoch 33, Loss(train/val) 4.77275/4.81187. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.77665/4.80979. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.77328/4.79910. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.77020/4.83424. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.76887/4.81972. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.77142/4.82847. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.76864/4.83056. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 4.76939/4.80738. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.76711/4.83117. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.76637/4.81153. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.76348/4.83089. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.76524/4.81825. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.76394/4.82385. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.76449/4.81173. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.76452/4.84105. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.76559/4.82357. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.77553/4.82194. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.76990/4.85694. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.76645/4.83824. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.76429/4.83317. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.76451/4.83574. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.76188/4.83591. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.76729/4.83712. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.76554/4.83134. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.76195/4.83975. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76139/4.83494. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.75901/4.83865. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.76420/4.83546. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.75858/4.86898. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.75754/4.84970. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.76154/4.83217. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.76304/4.84390. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.76160/4.84357. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.75701/4.85956. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.75754/4.84844. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.75754/4.83365. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.75740/4.86042. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.75312/4.83756. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.75563/4.84241. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76006/4.82915. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.76349/4.84857. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.75227/4.87701. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.75924/4.84094. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.75275/4.88232. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.75506/4.83253. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.75660/4.83983. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.75074/4.83564. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.75800/4.85131. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.75610/4.85804. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.75206/4.85742. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.75129/4.86567. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75164/4.84973. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.75865/4.88314. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.74999/4.82389. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.75030/4.85590. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.74916/4.86009. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.74977/4.83274. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.75316/4.83833. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.75290/4.83842. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.74786/4.82698. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.74432/4.85795. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.75809/4.86438. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.75120/4.86251. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.74736/4.84556. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.75183/4.84277. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.74435/4.85878. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.74290/4.87808. Took 0.09 sec\n",
      "ACC: 0.546875, MCC: 0.09416472308615167\n",
      "Epoch 0, Loss(train/val) 4.77602/4.78815. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.74757/4.76971. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.74169/4.75082. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.74325/4.74543. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.73956/4.74531. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.73571/4.74479. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.73529/4.75762. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.73829/4.75236. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.73056/4.76089. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.73341/4.75596. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.72755/4.76721. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.72995/4.76094. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.72799/4.76792. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.73135/4.75886. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.72549/4.77067. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.72551/4.76320. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.72650/4.76751. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.72400/4.77351. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.72301/4.78126. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.72262/4.77801. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.72350/4.76405. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.72741/4.78328. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.72316/4.80990. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.71940/4.80046. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.71916/4.81686. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.71910/4.84002. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.73174/4.77016. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.72396/4.76551. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.72066/4.77431. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.71541/4.79974. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.71944/4.77765. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.72520/4.75852. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.72989/4.77839. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.72826/4.77869. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.72391/4.77772. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.72305/4.77836. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.72242/4.79266. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.72356/4.77287. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.72622/4.76479. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.72238/4.77039. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.72096/4.78395. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.72180/4.78234. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.72377/4.76661. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.72254/4.76395. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.72100/4.77010. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.72080/4.76351. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.71828/4.76799. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.71866/4.77097. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.71456/4.77803. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.71786/4.77729. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.71784/4.79089. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.71080/4.78183. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.71164/4.78020. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.71484/4.78348. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.71549/4.76645. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.71465/4.76840. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.70337/4.77535. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.71058/4.78285. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.70824/4.80252. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.71197/4.79674. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.70487/4.79941. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.70839/4.79377. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.71067/4.77186. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.70793/4.82403. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.70886/4.79814. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.70564/4.77978. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.70267/4.80353. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.70503/4.78103. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.69624/4.82972. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.70933/4.81049. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.69798/4.81504. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.71661/4.75804. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.72632/4.76661. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.72779/4.75570. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.71925/4.75342. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.72178/4.75253. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.71825/4.74699. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.71556/4.76978. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.72130/4.76370. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.71236/4.77877. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.71087/4.78513. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.71063/4.79008. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.71121/4.79645. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.71304/4.78226. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.71044/4.79232. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.71658/4.78845. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.70394/4.80494. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.70990/4.77981. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.71549/4.77281. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.70978/4.83881. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.70756/4.78089. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.70702/4.80163. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.71426/4.80339. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.70709/4.77331. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.70195/4.81386. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.70383/4.78223. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.70774/4.83203. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.70824/4.80764. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.70156/4.81332. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.70842/4.80401. Took 0.08 sec\n",
      "ACC: 0.5, MCC: -0.011953709238683663\n",
      "Epoch 0, Loss(train/val) 5.11299/5.11256. Took 0.19 sec\n",
      "Epoch 1, Loss(train/val) 5.09821/5.06950. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 5.08548/5.07292. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.08364/5.07091. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 5.08223/5.06809. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 5.08367/5.07007. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.08395/5.06852. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 5.08447/5.06835. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 5.08310/5.06936. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 5.08175/5.06884. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.08130/5.06987. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.07949/5.06652. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 5.07964/5.06684. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.08071/5.06835. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.07771/5.06541. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 5.07842/5.06920. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.07721/5.06366. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.07544/5.06704. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 5.07462/5.06878. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.07400/5.06495. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.07264/5.06507. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 5.07236/5.06650. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.07373/5.07355. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 5.07423/5.06955. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.06563/5.07774. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.07349/5.06803. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.07571/5.07129. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.07387/5.07560. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.06858/5.08165. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.07281/5.07657. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.07142/5.07523. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 5.06955/5.07878. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.07343/5.07695. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.07246/5.07079. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.06956/5.07498. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.06752/5.07959. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.06669/5.07980. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 5.06626/5.07748. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.06771/5.08226. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.06302/5.08919. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.06624/5.08947. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 5.06355/5.09058. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.06946/5.08144. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 5.06950/5.09379. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 5.06005/5.09888. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.06558/5.08647. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.06387/5.09025. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 5.06723/5.08366. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.06318/5.08662. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 5.06070/5.09246. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.05917/5.10407. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.06568/5.10019. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.05972/5.09320. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.05959/5.10801. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.06307/5.09588. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 5.05685/5.10268. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.06304/5.09457. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.05723/5.10181. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.05961/5.10292. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.06621/5.09864. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.06092/5.12111. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 5.06055/5.10065. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.06038/5.10168. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.06032/5.10874. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.05890/5.09496. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.05950/5.10590. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.06446/5.11289. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.05893/5.10467. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.05872/5.10021. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.05769/5.10413. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 5.05677/5.10857. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 5.05125/5.09871. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 5.05727/5.10640. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 5.06085/5.10369. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.05849/5.09765. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.05144/5.10331. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.05654/5.09158. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.05951/5.09656. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.05939/5.09592. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.05576/5.09727. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 5.05509/5.10259. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.05641/5.10093. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.05449/5.10595. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 5.05622/5.10624. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 5.05673/5.11364. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 5.06338/5.09114. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.05782/5.09406. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.05673/5.10891. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.06522/5.09602. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.05548/5.08726. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 5.05349/5.10995. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.05971/5.11446. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.05142/5.10815. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.05445/5.11571. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.05323/5.12478. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.04993/5.12519. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 5.05054/5.13135. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 5.05249/5.12747. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.05215/5.09936. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 5.05876/5.09193. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.1463182264089704\n",
      "Epoch 0, Loss(train/val) 4.88461/4.86194. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.86652/4.91563. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.86637/4.96869. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.86784/4.89198. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.85758/4.87060. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.85416/4.88965. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.85730/4.88473. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.85547/4.88640. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.85388/4.89827. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.85469/4.89652. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.84921/4.88777. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.85498/4.89727. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85211/4.89293. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.85129/4.89859. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.84848/4.91318. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.85297/4.91192. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.84723/4.91660. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.84946/4.91083. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.84687/4.91999. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.84760/4.91557. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.84557/4.93052. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.84455/4.92103. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.84664/4.94170. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.84528/4.92536. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.84343/4.93224. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.84596/4.93642. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.84218/4.92731. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.84094/4.93697. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.84418/4.94568. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84490/4.93500. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.84270/4.94837. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.83999/4.93803. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.83755/4.97268. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.84236/4.92280. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.83864/4.94167. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.83559/4.95317. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.83910/4.95170. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.83659/4.95400. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.83327/4.97073. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.83772/4.93685. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.83256/4.99613. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.83922/4.90830. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.83139/4.99778. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.83527/4.95504. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.83737/4.96615. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.83271/4.95627. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.83661/4.93236. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.83317/4.95267. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.83697/4.95704. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.82297/5.02268. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.82997/4.95612. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.82568/4.99135. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.82896/4.97636. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.84661/4.91856. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.85073/4.88940. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.84509/4.89116. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.83886/4.93091. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.84099/4.95722. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.83964/4.93203. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.83055/4.97757. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.83268/5.01136. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83471/4.94259. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.82914/4.97583. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.83809/4.98081. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.83069/4.97765. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.83003/4.97464. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.83202/4.97743. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.82782/4.97456. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.83352/4.98167. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.83166/4.99497. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.82622/4.96596. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.82729/5.01502. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.82960/4.98004. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.82696/4.97505. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83202/5.00502. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83071/4.99781. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.82794/5.00375. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.82383/4.99221. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.82510/5.04113. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.82080/4.97301. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.82890/5.00395. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.82792/4.99814. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.82867/4.99778. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.82245/5.03968. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.82701/4.98626. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.82552/4.98084. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.82439/4.98799. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.82644/5.00945. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.82038/5.01381. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.82512/4.99024. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.81861/5.02749. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.81921/5.01786. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.81853/4.98716. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83395/4.93671. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.83325/4.94778. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.82817/4.97254. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 4.82368/4.94342. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.83144/5.03879. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.82690/4.94697. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.83645/4.94181. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.06825406626599889\n",
      "Epoch 0, Loss(train/val) 5.00008/4.95017. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.94927/4.96099. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.94311/4.96109. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.94412/4.95824. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.94400/4.96251. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.94027/4.96085. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.94502/4.95931. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.94260/4.96003. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.94380/4.95996. Took 0.12 sec\n",
      "Epoch 9, Loss(train/val) 4.94298/4.96028. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.94311/4.95965. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.94241/4.95748. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.94084/4.95663. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.94088/4.95892. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.94354/4.95908. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 4.93946/4.95925. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.93943/4.96172. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.93748/4.96347. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.93848/4.95995. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.93989/4.96158. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.93811/4.96143. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.93520/4.96549. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.93760/4.96748. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.93516/4.97256. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.93503/4.97317. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.93406/4.97718. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.93443/4.97681. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.93677/4.97325. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.93256/4.99281. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.92979/4.99272. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.92809/4.99689. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.93149/4.99857. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.92708/5.00116. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.92894/4.99468. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.93387/4.99711. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.93980/4.97238. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.93511/4.98128. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.93450/4.98836. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.93140/4.99252. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.92873/4.99629. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.93153/4.99777. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.92776/5.00732. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.93066/5.00820. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.92911/5.01550. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.92774/5.02310. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.92736/5.00707. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.93021/5.01226. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.93083/5.00770. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.92819/5.02293. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.92685/5.01147. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.92597/5.01185. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.92483/5.02760. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.92713/5.03010. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.92043/5.04226. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.92158/5.04984. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.92312/5.03481. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.92144/5.03621. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.92253/5.04971. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.92277/5.03670. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.92566/5.04690. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.92569/5.03102. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.92639/5.03086. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.91892/5.03916. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.92188/5.03992. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.91764/5.04659. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.92195/5.04744. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.92103/5.03114. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.91796/5.05398. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.92358/5.02471. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.92191/5.02624. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.91745/5.05660. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.91584/5.05785. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.92013/5.02256. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.91756/5.06269. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.92259/5.03385. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.91491/5.06624. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.91460/5.05320. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.91701/5.05516. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.91454/5.03213. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.91311/5.07764. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.91969/5.03372. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.91404/5.06796. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.91692/5.04685. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.91288/5.07402. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.91477/5.02859. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.91694/5.05204. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.92137/5.02302. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.91480/5.03852. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.91168/5.06316. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.91763/5.02022. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.91404/5.06482. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.91397/5.05549. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 4.91303/5.04522. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.90983/5.05119. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.90931/5.07441. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.91498/5.04237. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.91651/5.07264. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.91649/5.03518. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.91773/5.03644. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.91624/5.03409. Took 0.09 sec\n",
      "ACC: 0.40625, MCC: -0.1889822365046136\n",
      "Epoch 0, Loss(train/val) 4.96127/4.85420. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.86289/4.85675. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.85550/4.84836. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.85304/4.84453. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.85462/4.84056. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.85666/4.83823. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.85597/4.83660. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.85617/4.83712. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.85602/4.83637. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.85432/4.83888. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 4.85500/4.83884. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.85489/4.83916. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85391/4.84447. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.85282/4.84619. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.85022/4.83735. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.85002/4.85853. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.84956/4.85280. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.84835/4.85102. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.84989/4.85214. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.84834/4.85422. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.84476/4.85337. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.84728/4.85450. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.84396/4.85586. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.84554/4.85755. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.84833/4.85597. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.84412/4.86033. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.84042/4.85591. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.84353/4.85453. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.84183/4.85957. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84437/4.85852. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.84062/4.85805. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.84460/4.86015. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84364/4.86234. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.83950/4.85740. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.83956/4.85765. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84162/4.85603. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84286/4.86534. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.84115/4.86449. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84043/4.86300. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84123/4.86098. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.84065/4.86635. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.83967/4.86521. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84026/4.86630. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.83888/4.86496. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.83924/4.86232. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.83899/4.85566. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.84133/4.86083. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.83525/4.85434. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.83729/4.86101. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.83578/4.86292. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.83383/4.86503. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.83596/4.86521. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.83656/4.87069. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.83463/4.86959. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.83274/4.86461. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.83745/4.86867. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.83673/4.85734. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.83191/4.86642. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.83391/4.85422. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.82848/4.86472. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.83457/4.84803. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.83617/4.86187. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.83411/4.85640. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.83944/4.85475. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.83100/4.85544. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.83250/4.85320. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.83283/4.85352. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.83230/4.86321. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.83480/4.85761. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.83191/4.85209. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.82847/4.87037. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.83357/4.84663. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83076/4.85244. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.82185/4.85565. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83035/4.84694. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.82826/4.86188. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.82783/4.86135. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.83548/4.85818. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.82850/4.86734. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.83282/4.86001. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83044/4.84580. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.82344/4.87919. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.82733/4.85887. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.82343/4.85113. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.82842/4.83814. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.82782/4.85103. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.82525/4.85670. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.82558/4.86257. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.82499/4.86761. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.82402/4.86337. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.82767/4.86107. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.82045/4.86191. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.82556/4.86684. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.82556/4.85498. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.82197/4.86278. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.82946/4.86009. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.82463/4.85710. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.82514/4.86600. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.82496/4.86278. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.81859/4.87000. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.051799406700962544\n",
      "Epoch 0, Loss(train/val) 5.06019/4.96019. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.96312/4.96416. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.96194/4.96471. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.96720/4.96189. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.96417/4.97055. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.96530/4.98217. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.96064/4.98809. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.95925/4.99462. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.95927/4.99655. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.95985/4.99064. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.95412/4.98296. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.95213/4.98916. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.95498/4.99003. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.95316/4.98708. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.95227/4.98892. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.95235/4.98535. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.95371/4.98118. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.94967/4.98909. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.95054/4.98620. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.95125/4.98232. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.94542/4.98409. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.94825/4.98115. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.94592/4.98539. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.94595/4.97821. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.94634/4.96589. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.94022/4.97385. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.94588/4.97380. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.94496/4.97533. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.94476/4.97452. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.94278/4.97692. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.94106/4.97729. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.93895/4.99275. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.95311/4.97652. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.94891/4.98155. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.94564/4.98153. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.94245/4.98125. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.94627/4.96279. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.94982/4.98579. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.95059/4.97839. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.95016/4.97282. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.94905/4.96950. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.94671/4.97397. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.94453/4.97236. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.94047/4.95514. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.94382/4.97166. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.94373/4.97146. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.94006/4.97001. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.94322/4.96765. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.93949/4.96698. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.93943/4.95976. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.94028/4.96250. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.93285/4.96368. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.93469/4.97004. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.93474/4.96977. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.93694/4.97507. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.93837/4.96372. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.93442/4.96335. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.93208/4.96930. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.93168/4.97047. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.93333/4.96865. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.92998/4.97757. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.93277/4.97616. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.93418/4.95981. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.93148/4.98432. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.93628/4.96032. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.92816/4.97432. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.93147/4.96872. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.92432/4.97067. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.93033/4.96275. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.92805/4.97890. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.92606/4.97199. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.92778/4.96320. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.92505/4.97572. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.92888/4.96905. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.92381/4.96684. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.92647/4.97407. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.92654/4.97032. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.92134/4.96660. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.92847/4.98933. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.91731/5.00436. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.92233/4.99537. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.92871/4.99583. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.92384/4.99693. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.92003/4.98522. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.92222/4.99313. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.92192/4.97886. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.91706/4.98158. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.91238/4.98300. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.91700/4.98886. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.91783/4.99516. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.91477/5.00165. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.92522/4.98055. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.92184/4.99307. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.91803/5.00603. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.91480/5.01120. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.91646/5.00060. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.91459/4.97430. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.91458/4.98419. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.91813/4.98641. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.91559/5.00471. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.07100716024967263\n",
      "Epoch 0, Loss(train/val) 5.05848/5.02488. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.99507/4.99179. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.99899/4.98545. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.00121/4.98297. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 5.00451/4.98695. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.00318/4.99697. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.99881/4.98847. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.99324/4.98496. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.99402/4.98542. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.99703/4.98889. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.99519/4.98492. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.99290/4.98336. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.99153/4.98227. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.99042/4.98053. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.98678/4.98430. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.98708/4.98455. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.98911/4.98723. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.98644/4.99280. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.98439/4.97514. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.98547/4.98254. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.98413/4.98034. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.97958/4.97581. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.98424/4.98254. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.98172/4.98223. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.98023/4.96227. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.97950/4.97251. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.98077/4.98670. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.97890/4.97406. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.97750/4.97408. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.98030/4.99341. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.97615/4.97203. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.97670/4.98009. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.97543/4.97916. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.97839/4.97114. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.97782/4.97368. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.97468/4.96839. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.98214/4.97518. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.98040/4.98573. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.97196/4.96946. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.97771/4.98669. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.97660/4.96776. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.97319/4.96835. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.97723/4.95906. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.97657/4.97246. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.97588/4.96887. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.97175/4.98090. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.97721/4.97325. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.97413/4.95888. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.97301/4.97813. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.97151/4.97605. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.97514/4.98146. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.97678/4.99207. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.97987/5.00727. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.98194/4.99255. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.97771/4.99493. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.97986/4.98767. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.98056/4.98795. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.97465/4.98559. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.97748/4.98315. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.97679/4.98928. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.97662/4.98543. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.97724/4.98671. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.96297/4.97615. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.97586/4.98963. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.96702/4.98247. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.97893/4.99003. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.97424/4.97444. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.97132/4.98411. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.97088/4.98690. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.97288/4.98424. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.96848/4.98209. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.96310/4.98553. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.97105/4.99458. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.96676/4.99451. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.96456/4.98246. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.97330/4.97683. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.96691/4.99067. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.96528/4.98635. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.96611/4.99704. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.95953/4.99153. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.96538/4.98922. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.96762/4.99423. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.96574/4.97565. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.97343/4.97448. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.97334/4.99274. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.96447/4.97465. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.96746/4.99361. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.96626/4.98161. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.96841/4.98431. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.96910/4.97687. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.96336/4.96971. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.96156/4.97417. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.96892/4.97873. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.95971/4.97171. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.96277/4.99550. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.96459/4.96903. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.96005/4.98987. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.95550/4.97298. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.96445/4.99595. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.96227/4.98042. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.0\n",
      "Epoch 0, Loss(train/val) 4.84338/4.77884. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.79814/4.78613. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.79715/4.77590. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79060/4.77003. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.78019/4.76927. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.78067/4.77320. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.78219/4.77523. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.78092/4.77407. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.77983/4.77730. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.77657/4.78178. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.77479/4.77898. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.77599/4.77938. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.77505/4.77816. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.77387/4.77571. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.76806/4.79244. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.77341/4.78738. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.77079/4.77497. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.76568/4.78903. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.76287/4.79456. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.76822/4.77843. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.76345/4.79534. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.76197/4.78985. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.76244/4.77727. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.76499/4.78860. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.76347/4.78508. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.76387/4.79337. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.76090/4.78494. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.76299/4.78361. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.75641/4.78519. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.75780/4.78726. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.76276/4.77636. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.76154/4.78525. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.75640/4.79930. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.75898/4.78669. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.76039/4.77971. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.75495/4.80343. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.75965/4.77914. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.75452/4.80382. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.75554/4.76876. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.75296/4.79875. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.75919/4.78560. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.75724/4.78391. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.75116/4.78898. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.75494/4.80328. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.76077/4.76346. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.75777/4.78359. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.75143/4.78004. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.75146/4.77637. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.75660/4.77963. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.75285/4.78345. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.75218/4.77263. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.75339/4.78955. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.74962/4.78419. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.74971/4.80662. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.75139/4.76980. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.74666/4.78637. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.75207/4.78365. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.75303/4.78217. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.74968/4.78487. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.75218/4.77571. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.74750/4.77400. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.75451/4.78150. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.75103/4.79898. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.75173/4.77821. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.74794/4.78456. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.74889/4.78759. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.74523/4.82350. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.74893/4.75937. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.74423/4.77934. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.74627/4.78025. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.74275/4.78598. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.74743/4.77383. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.74626/4.77269. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.74719/4.77778. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.74890/4.77078. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.74343/4.80904. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.74918/4.77378. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.74456/4.79231. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.74371/4.77205. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.74443/4.76439. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.73998/4.78674. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.74558/4.77555. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.74742/4.77991. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.74420/4.78854. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.74589/4.77716. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.74146/4.77521. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.74624/4.78648. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.73815/4.78545. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.74693/4.76893. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.73547/4.79379. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.74030/4.78679. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.75130/4.77191. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.75056/4.78171. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.74685/4.78899. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.74583/4.79265. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.74426/4.79746. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.75077/4.79340. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.73994/4.79455. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.74358/4.78180. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.74429/4.77541. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.08222643447147887\n",
      "Epoch 0, Loss(train/val) 4.86315/4.80725. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.81916/4.80694. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.81122/4.81405. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.81046/4.81734. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.80985/4.82117. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.80995/4.82588. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.81133/4.82543. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.81070/4.82596. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.81115/4.82437. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.80975/4.82128. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.80905/4.81776. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.80712/4.81580. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.80630/4.81818. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.80576/4.81229. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.80948/4.80721. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.80464/4.80647. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.80581/4.79599. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.79977/4.79286. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.80006/4.81075. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.80130/4.79289. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.79364/4.79795. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.79323/4.78889. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.80036/4.79430. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.79512/4.78982. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.79734/4.79819. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.79549/4.78777. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.79282/4.78948. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.79003/4.78356. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.79384/4.80573. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.79467/4.78721. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.79055/4.79268. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.79089/4.80417. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.79220/4.79334. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.79434/4.79119. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78797/4.80928. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.78825/4.81028. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.78440/4.80076. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.78791/4.80716. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.78569/4.82722. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.78610/4.81089. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.79133/4.80722. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.78909/4.81049. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.79029/4.80045. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.78286/4.80424. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.78835/4.80582. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.77957/4.80693. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.79389/4.81188. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.78088/4.80986. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.78481/4.81471. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.78360/4.79669. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.77901/4.80582. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.79877/4.82679. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.79305/4.81094. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.78964/4.79804. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.79034/4.80250. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.78787/4.80658. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.78760/4.81391. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.78747/4.81644. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.79655/4.79599. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.78471/4.78903. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.78650/4.80797. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.79142/4.80591. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.78712/4.80178. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.78584/4.79885. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.78680/4.81317. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.78478/4.79881. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.78480/4.80010. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.78743/4.79885. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.78398/4.80505. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.78147/4.81352. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.79047/4.81083. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78761/4.79316. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.78707/4.78480. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.78452/4.79712. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.78752/4.80615. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.78314/4.81009. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78671/4.80669. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.78212/4.79869. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.78293/4.80567. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.78229/4.79464. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.78756/4.79922. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.78833/4.79097. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.78241/4.80063. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.78067/4.79986. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.78355/4.79474. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.78345/4.79976. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.78332/4.80141. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.78388/4.80035. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.77951/4.79821. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.77639/4.79701. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.78238/4.79566. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.78275/4.81002. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.78118/4.80006. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.78000/4.80472. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.77931/4.80759. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.78220/4.80457. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.77842/4.80413. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.78024/4.81390. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.78128/4.81635. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.77735/4.81204. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.019772733298509516\n",
      "Epoch 0, Loss(train/val) 4.67450/4.69728. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.62488/4.62825. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.61698/4.61055. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.60569/4.62010. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.60683/4.62537. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.60920/4.62333. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.60877/4.62113. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.60731/4.62214. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.61134/4.62131. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.60756/4.62271. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.60557/4.62349. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.60486/4.62242. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.60591/4.62100. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.60711/4.61985. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.60439/4.62116. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.60224/4.61962. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.60744/4.61904. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.60635/4.61583. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.60055/4.62000. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.60407/4.61752. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.60451/4.62369. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.59839/4.61209. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.59847/4.62675. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.59926/4.60205. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.59820/4.63195. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.59642/4.61373. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.59721/4.61158. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.59309/4.61353. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.59102/4.61849. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.59534/4.62731. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.59168/4.60445. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.58957/4.60796. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.58806/4.62999. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 4.59496/4.59994. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.58744/4.62532. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.58515/4.60234. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.58544/4.62234. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.59077/4.60724. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.58643/4.61400. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.58438/4.61506. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.58500/4.61892. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.58061/4.60959. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.58608/4.63246. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.59710/4.61894. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.60229/4.59925. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.59980/4.60627. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.59616/4.60256. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.59512/4.60542. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.59221/4.60873. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.58871/4.60844. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.59163/4.60159. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.59013/4.60420. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.58870/4.60248. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.58486/4.59884. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.59576/4.63729. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.58893/4.61543. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.58782/4.62739. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.58146/4.62414. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.59266/4.63661. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.57825/4.62217. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.58588/4.61558. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.57578/4.63668. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 4.59025/4.62924. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.58696/4.61617. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.58637/4.61075. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.58550/4.58560. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.58827/4.61734. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.58275/4.61099. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.58147/4.63383. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.57543/4.59394. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.59063/4.60125. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.58788/4.59540. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.57989/4.60448. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.58315/4.60387. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.57808/4.66564. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.58523/4.61768. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.57463/4.62401. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.57432/4.66578. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.58323/4.64473. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.57293/4.65072. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.57067/4.63319. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.57802/4.60782. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.58149/4.61913. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.57809/4.64341. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.57913/4.62870. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.57588/4.64936. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.57799/4.65251. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 4.56921/4.64755. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.57126/4.64738. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.57143/4.63905. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.56388/4.67602. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.56919/4.67865. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.56472/4.64695. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 4.57119/4.65710. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.56834/4.69392. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.59217/4.62163. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.58617/4.61137. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.58336/4.61554. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.57925/4.61757. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.58283/4.62196. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.08304547985373996\n",
      "Epoch 0, Loss(train/val) 4.95673/4.91059. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.95408/4.98514. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.95555/5.06164. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.95229/4.94243. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.92901/4.95812. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.93121/4.96505. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.93044/4.96762. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.92985/4.97501. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.92930/4.97774. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.92757/4.97922. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.92498/4.98807. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.93033/4.97821. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.92534/4.97495. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.92171/4.98157. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.92561/4.97290. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.92155/4.98741. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.92454/4.97810. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.92127/4.97526. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.91936/4.98501. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.91915/4.97186. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.91857/4.98208. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.91963/4.97890. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.91582/4.98249. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.91682/4.98797. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.91970/4.99477. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.91358/4.97792. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.91225/5.00431. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.91159/4.98343. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.91043/4.98711. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.91882/4.96768. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.91480/4.97816. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.91610/4.97204. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.91055/4.98463. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.91459/4.97200. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.90638/4.99119. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.91119/4.98683. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.90970/4.98593. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.90572/5.01809. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.90479/5.01192. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.90517/5.00609. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.90256/5.02959. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.90719/5.01441. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.90615/5.01604. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.89853/5.01869. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.90353/5.02647. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.89735/5.02698. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.90190/5.01704. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.90311/4.96953. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.90362/5.00981. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.89771/5.02333. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.89759/5.03509. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.90265/4.99353. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.89901/5.01410. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.89992/4.98667. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.91858/4.95544. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.91159/4.97145. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.90512/5.00568. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.90709/4.97562. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.89968/4.96904. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.90045/5.02530. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.89804/5.02346. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.89656/5.03369. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.89600/5.03515. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.89497/5.05309. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.89473/5.04552. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.89949/4.98960. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.89476/5.03144. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.89427/4.98920. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.90015/5.02788. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.89773/5.00256. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.89125/5.01510. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.89689/4.98207. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88824/5.07063. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.90099/4.99637. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.89040/5.05165. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.89341/5.01430. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88654/5.03241. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.89150/5.00679. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.88570/5.02160. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.88808/5.02212. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.89085/4.99093. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.88189/4.99719. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.89020/5.01306. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.88555/5.01293. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.88791/5.02087. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.88290/5.00801. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.88711/5.06250. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.88578/4.99441. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.88350/5.02476. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88633/5.02717. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.88232/5.01624. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.88418/5.05839. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.89077/5.01638. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.88280/5.02959. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.88380/5.04287. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.88759/5.00130. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.88327/4.99618. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.88157/4.99770. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.91030/4.98777. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.92381/4.95441. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.12022678317975014\n",
      "Epoch 0, Loss(train/val) 5.05514/5.00064. Took 0.19 sec\n",
      "Epoch 1, Loss(train/val) 5.01652/4.99994. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 5.02460/5.00005. Took 0.13 sec\n",
      "Epoch 3, Loss(train/val) 5.02927/5.00831. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.02357/5.02432. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.01220/5.00971. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 5.00933/5.00668. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.01101/5.01074. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.01279/5.01132. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.01187/5.01005. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.01081/5.01012. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.01238/5.01070. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.01139/5.00924. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.01207/5.01212. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.01179/5.01019. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.01029/5.01012. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 5.00990/5.01054. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.01052/5.00935. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.00883/5.00861. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 5.01054/5.00639. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.00987/5.00325. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.00907/5.00546. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.00683/5.00459. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 5.00506/5.00597. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.00627/5.00084. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.00843/5.00782. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.00740/5.01281. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.00742/5.00526. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.00703/5.00898. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.00371/5.00972. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.00816/5.00883. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.00413/5.01297. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 5.00379/5.01693. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.00343/5.01932. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.00195/5.01942. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.00410/5.01554. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.00181/5.01582. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.00396/5.02157. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.99925/5.02698. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.00116/5.02030. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.00006/5.02823. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.00381/5.01781. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.99713/5.01990. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.00230/5.03508. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 5.00241/5.03037. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.01008/5.01424. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.00407/5.02060. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.99970/5.02172. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.99676/5.02630. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.99789/5.02478. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.00131/5.02861. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.99794/5.02719. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.99446/5.02177. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.99772/5.02803. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.99164/5.03205. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.99226/5.04344. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.99647/5.01861. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.99341/5.02737. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.99227/5.03615. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.99253/5.03424. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.98524/5.04989. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.99106/5.03830. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.98984/5.02687. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.99103/5.02192. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.98942/5.03906. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.99017/5.04077. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.98393/5.04364. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.98296/5.04837. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.98742/5.03634. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.98735/5.03714. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.98132/5.05511. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.98793/5.05179. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.98394/5.05065. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.97691/5.09147. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.98806/5.06182. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.97250/5.06035. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.98042/5.07950. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.98135/5.08420. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.97829/5.07923. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.97792/5.09074. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.98114/5.05163. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.97349/5.05118. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.98226/5.07381. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.98381/5.07178. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.97125/5.10409. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.97481/5.06745. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.97370/5.06952. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.97884/5.06987. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.97176/5.10620. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.96788/5.08413. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.98423/5.07405. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.97514/5.09600. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.96574/5.13252. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.97027/5.11253. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.97634/5.12108. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.98713/5.10386. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.98828/5.10173. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.98679/5.08517. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.97314/5.11980. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.98836/5.08395. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.0905982365507463\n",
      "Epoch 0, Loss(train/val) 4.86998/4.89892. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.84591/4.86449. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.83871/4.87900. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.84103/4.88391. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.84155/4.87950. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.83986/4.87408. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.83696/4.87201. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.84087/4.88605. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.83648/4.88200. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.83455/4.88553. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.83505/4.87891. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.83515/4.87468. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.83510/4.88043. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.83434/4.87954. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.83503/4.87169. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.83711/4.87626. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.83244/4.86910. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.83318/4.87676. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.83105/4.87689. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.83096/4.86924. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.83068/4.87619. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.83016/4.87412. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.82537/4.88509. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.82912/4.86113. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.82789/4.87936. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.82325/4.88992. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.82607/4.87960. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82389/4.88761. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.82332/4.88703. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.81913/4.89535. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.82375/4.88133. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.82431/4.90668. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.82606/4.88334. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.83910/4.87669. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.82919/4.89238. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.82630/4.89210. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.81832/4.90986. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.81748/4.91133. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.81941/4.90747. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.81767/4.91795. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.81999/4.89552. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.81741/4.90722. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.81624/4.90764. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.81585/4.90185. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.81468/4.90615. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.81673/4.89435. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.81832/4.88357. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.81759/4.90024. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.81172/4.91806. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.81694/4.89681. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.81932/4.92456. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.80982/4.91021. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.81768/4.90433. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.82047/4.91232. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.81176/4.92591. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.81276/4.91838. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.81344/4.92115. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.81616/4.91716. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.82158/4.92623. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.81481/4.93510. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.82146/4.92628. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.81905/4.94146. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.81625/4.93889. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.81503/4.94847. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.81703/4.91890. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.82814/4.89062. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.82071/4.92846. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.81630/4.90308. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.82193/4.90529. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.81724/4.93940. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.81945/4.92309. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.81863/4.93596. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.81214/4.94348. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.81512/4.93603. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.81391/4.92261. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.81103/4.95273. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.81068/4.93892. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.81180/4.95514. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.81068/4.95573. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.80462/4.96485. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.80910/4.96579. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.81282/4.94709. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.80952/4.95586. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.80729/4.97659. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.80213/4.96610. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.80812/4.96050. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.80052/4.98654. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.80604/4.95757. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.80366/5.00062. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.80406/5.01979. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.80764/5.01471. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.80436/4.99380. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.80112/5.01758. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.80012/5.00843. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.79858/4.99974. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.79961/5.03834. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.80774/4.98054. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.80174/4.98933. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.80125/5.00934. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.79919/4.97952. Took 0.08 sec\n",
      "ACC: 0.59375, MCC: 0.18497841147451874\n",
      "Epoch 0, Loss(train/val) 4.81541/4.73396. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.75722/4.79483. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.76430/4.81139. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.76572/4.75954. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.76404/4.73219. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.75870/4.73278. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.75478/4.74432. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.74934/4.75148. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.75421/4.74781. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.75456/4.74652. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.75208/4.74703. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.75239/4.74945. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.75400/4.74904. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.75172/4.75105. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.74926/4.75144. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.75025/4.75115. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.74929/4.75406. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.75030/4.74425. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.74734/4.75006. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.74738/4.74991. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.74662/4.76136. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.75096/4.76311. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.75326/4.74148. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.74535/4.75950. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.74783/4.78608. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.74380/4.79096. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.74061/4.81273. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.74603/4.78499. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.74175/4.78090. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.74238/4.76384. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.74400/4.75376. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.73984/4.77316. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.73602/4.78238. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.73712/4.81307. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.73536/4.79038. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.73503/4.81225. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.73395/4.81988. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.73286/4.79687. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.74557/4.75191. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.74421/4.77163. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.73726/4.78895. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.73707/4.79695. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.73604/4.81182. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.73979/4.79455. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.73341/4.81351. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.73040/4.80747. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.73766/4.74058. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.73826/4.77913. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.73961/4.79493. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.73633/4.80795. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.73247/4.82139. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.73590/4.78080. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.72937/4.81251. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.72801/4.82334. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.73237/4.81679. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.72952/4.83887. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.73374/4.79563. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.72693/4.83968. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.72756/4.84357. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.73021/4.83678. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.72973/4.84038. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.73364/4.81686. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.72581/4.84024. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.72395/4.82631. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.73082/4.83653. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.73117/4.83255. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.73076/4.84131. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.73075/4.84296. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.72999/4.81979. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.73167/4.83058. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.72990/4.81660. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.73436/4.81716. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.73178/4.82608. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.73342/4.85127. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.72878/4.82534. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.72509/4.85124. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.72654/4.83742. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.72689/4.83453. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.72787/4.82374. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.72692/4.83527. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.72321/4.85756. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.72355/4.84586. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.72091/4.84598. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 4.72787/4.81521. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.72324/4.82835. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.73381/4.80684. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.72955/4.83678. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.72432/4.83737. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.72791/4.83081. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.72487/4.83704. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.72581/4.84450. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.72632/4.82934. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.72237/4.85382. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.72336/4.84472. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.72066/4.87725. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.72308/4.84527. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.72988/4.85610. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.73055/4.83432. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.72286/4.90597. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.72392/4.83141. Took 0.08 sec\n",
      "ACC: 0.578125, MCC: 0.15694120514358612\n",
      "Epoch 0, Loss(train/val) 5.14841/5.12125. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 5.14593/5.15424. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.12598/5.13024. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.12768/5.13414. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 5.13193/5.13785. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.13011/5.13330. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.12723/5.12999. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.13072/5.12832. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.12997/5.12653. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.13111/5.12702. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.13447/5.12951. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.12852/5.13325. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 5.12616/5.13705. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.12597/5.13646. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.12657/5.14128. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.12842/5.13891. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.12774/5.14391. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.12408/5.14546. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.12205/5.15463. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.12288/5.14549. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.12583/5.14762. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.12145/5.14277. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.12093/5.15432. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 5.12102/5.13870. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.12030/5.14074. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.12339/5.14372. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.11773/5.14024. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.11968/5.13952. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.11808/5.14626. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 5.11927/5.15066. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.12065/5.14852. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.11619/5.16899. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.11625/5.13143. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.11827/5.15469. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.11704/5.13899. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.11715/5.14830. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.11446/5.14311. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.11923/5.14114. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.11515/5.16345. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 5.10891/5.16451. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 5.11437/5.14272. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.11740/5.15393. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.11392/5.16840. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 5.11173/5.15291. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 5.10997/5.17024. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.10878/5.17373. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.10863/5.15826. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.10906/5.16152. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.10901/5.17029. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.11110/5.16658. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.10541/5.16978. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 5.10288/5.17420. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.11894/5.15847. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.11566/5.16695. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 5.10869/5.17424. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.10482/5.16006. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.11285/5.17041. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 5.11025/5.17850. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.10723/5.16205. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.10726/5.17892. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 5.10254/5.19507. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.10254/5.16362. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.10455/5.17637. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.10736/5.16596. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.10996/5.19427. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.10547/5.18125. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.10089/5.19923. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.10651/5.19102. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.10402/5.16699. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 5.10867/5.16842. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.10451/5.19978. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.10560/5.17232. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 5.10660/5.17454. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.10273/5.18003. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.11038/5.16696. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.10140/5.20296. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.10166/5.18635. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.10242/5.17493. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.09893/5.19809. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.09987/5.18654. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.10041/5.19867. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.10792/5.19356. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.10609/5.16691. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.12144/5.12417. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 5.12951/5.12813. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.12673/5.13794. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.12483/5.13773. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.12275/5.14124. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.12257/5.14145. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 5.12163/5.14238. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 5.12056/5.14814. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.12041/5.14371. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.11823/5.15033. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.11972/5.14660. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.11912/5.15023. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.12030/5.14837. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 5.11824/5.14842. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.11707/5.14580. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.11872/5.14328. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 5.11445/5.15516. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.10421364855284106\n",
      "Epoch 0, Loss(train/val) 4.98910/4.92997. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.92379/4.90065. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.92972/4.90079. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.92889/4.90339. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.92985/4.91257. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.92670/4.92766. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.92444/4.93042. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.91974/4.92477. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.92019/4.92858. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.91630/4.92641. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.91729/4.92236. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.91430/4.92014. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.91523/4.92645. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.91471/4.92943. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.91691/4.90815. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.91893/4.92102. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.91821/4.93032. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.91266/4.92129. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.91495/4.93117. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.91317/4.93104. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.91409/4.91483. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.91263/4.92139. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.91141/4.92551. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.91317/4.91284. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.90891/4.90985. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.90826/4.92030. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.90957/4.92161. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.90878/4.91412. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.90485/4.91854. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.90751/4.89823. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.91166/4.90893. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.90954/4.92568. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.90683/4.92567. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.90534/4.91779. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.90466/4.91778. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.90091/4.92363. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.90361/4.91895. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.90210/4.92230. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.90573/4.92600. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.90711/4.90874. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.90475/4.94091. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.89980/4.92534. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.90336/4.93062. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.90301/4.92390. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.90084/4.91995. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.90305/4.92102. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.89873/4.93011. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.89461/4.91587. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.89888/4.92773. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.89482/4.92361. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.89670/4.92666. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.89481/4.93793. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.89964/4.92535. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.89470/4.92429. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.90247/4.92131. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.89896/4.94177. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 4.89622/4.92805. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.89857/4.93008. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.89346/4.93407. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.90644/4.90662. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.90519/4.93634. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89811/4.92945. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.89777/4.93073. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.90063/4.91984. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.89763/4.92128. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.89552/4.92748. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.89262/4.94545. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.89286/4.93175. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.89069/4.92637. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.89674/4.93830. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.89801/4.91775. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.89462/4.94626. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.88872/4.92659. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.89508/4.92376. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.89357/4.92664. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.89492/4.93159. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 4.89213/4.93481. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.89648/4.93972. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.89379/4.91774. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.89625/4.94016. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.89010/4.92935. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.89324/4.93293. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.88789/4.94348. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.89221/4.92918. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.88861/4.93543. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.89110/4.94727. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.88983/4.92445. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.88873/4.93750. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.88587/4.93126. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88830/4.91734. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.88780/4.95120. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.88423/4.94135. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.89259/4.93904. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.89232/4.92755. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.88480/4.94640. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.88793/4.93235. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.88375/4.92969. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.88970/4.93370. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.88596/4.94994. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.88369/4.93887. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.004034850217792016\n",
      "Epoch 0, Loss(train/val) 4.92052/4.83286. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.85846/4.85990. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.85044/4.88688. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.85502/4.89573. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.85364/4.89760. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.85061/4.88903. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.84647/4.88590. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.84921/4.88240. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.84441/4.89343. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.84483/4.89046. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.84804/4.89019. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.84294/4.89019. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.84273/4.89260. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.84206/4.88383. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 4.84136/4.88565. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.84112/4.88771. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.83723/4.90112. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.84154/4.89940. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.83807/4.89072. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.83884/4.88559. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.83764/4.87719. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.83811/4.87599. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.83731/4.86676. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.83952/4.86421. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.83851/4.86750. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.83223/4.87266. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.83476/4.87821. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.83697/4.87209. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.83778/4.87913. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.83447/4.86481. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.83281/4.85980. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.83435/4.86786. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.83283/4.87804. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.83720/4.87926. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.83504/4.88201. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.83283/4.87731. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.83330/4.87307. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.83109/4.88951. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.82856/4.88389. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.83429/4.88329. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.83134/4.88134. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.82713/4.89226. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.82726/4.88051. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.82666/4.88361. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.83037/4.89513. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.82894/4.90441. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.82116/4.90452. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.82167/4.89755. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.82642/4.90119. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.82527/4.91325. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.82411/4.89378. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.82472/4.89681. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.81843/4.90575. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.82525/4.91756. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.82969/4.91759. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.82417/4.91670. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.82153/4.90389. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.82335/4.89897. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.82235/4.90732. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.81980/4.91064. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.81556/4.90549. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.82144/4.89961. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.82082/4.90199. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.81934/4.91621. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.81999/4.91681. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.82134/4.91398. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.81853/4.90044. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.82055/4.90932. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.81850/4.91519. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.82475/4.91595. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.82315/4.90455. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.82071/4.90060. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.82016/4.91717. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.81840/4.91095. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.82106/4.90868. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.81849/4.90288. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.81887/4.90137. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.81865/4.89253. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.82202/4.89908. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.81170/4.90702. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.81636/4.90497. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.81364/4.91860. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.81628/4.91661. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.81908/4.92801. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.81569/4.90518. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.82316/4.89722. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.81902/4.92407. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.81214/4.91399. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.81642/4.90754. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.81935/4.89803. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.81443/4.90089. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.81227/4.93217. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.81387/4.89853. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.81246/4.90100. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.81562/4.89986. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.81130/4.91304. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.81719/4.89727. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.81335/4.90597. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.80743/4.91238. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.81179/4.90838. Took 0.09 sec\n",
      "ACC: 0.46875, MCC: -0.08818077954471167\n",
      "Epoch 0, Loss(train/val) 4.87661/4.82179. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.80246/4.80991. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.81545/4.83509. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.81641/4.85758. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.80169/4.82659. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.79058/4.81398. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.78930/4.82056. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.78830/4.82448. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.78905/4.82910. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.78999/4.81388. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.78841/4.81727. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.78548/4.81314. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.78058/4.82609. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.78659/4.82400. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.78512/4.82696. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.78649/4.80871. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78331/4.81825. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.77991/4.81548. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.78407/4.81549. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.78405/4.81174. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.78213/4.80412. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78369/4.81948. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.79113/4.81971. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.79412/4.80740. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.78779/4.82687. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.78586/4.81873. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.79018/4.81997. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.78871/4.81503. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.79203/4.81127. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.79134/4.81289. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.79040/4.81220. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.79145/4.81459. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.78882/4.80844. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.79019/4.81596. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.78872/4.80770. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.77921/4.81405. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.77346/4.80515. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.78027/4.81310. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.78050/4.80645. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.77200/4.80180. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.77155/4.80572. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.77225/4.80621. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.77714/4.80476. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.76803/4.80842. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.76863/4.80667. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.77470/4.80975. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77412/4.78622. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.76632/4.80066. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77254/4.80318. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.77235/4.80492. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.76484/4.79648. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.76301/4.81475. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.76491/4.81062. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.76748/4.79831. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.76389/4.80804. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.76898/4.80402. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.76431/4.80126. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.76759/4.80704. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 4.76118/4.81165. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.76594/4.80591. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.75764/4.80751. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.76677/4.79270. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76630/4.79340. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.76083/4.80842. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.76295/4.80173. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.76351/4.81438. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.76714/4.80120. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.75995/4.80887. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.75813/4.82311. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76277/4.81992. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.76020/4.81023. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.75604/4.81753. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76575/4.81050. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.75908/4.82883. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.76146/4.80839. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.76504/4.81242. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.76277/4.80056. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.75418/4.81522. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.76234/4.80171. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.76459/4.79942. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.76017/4.81825. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.75807/4.81213. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.75925/4.81583. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.75984/4.81086. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75696/4.79655. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.75350/4.81577. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.75873/4.79946. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.75381/4.81297. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.75334/4.81065. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.75739/4.80253. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.76230/4.80244. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.75730/4.81581. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.75758/4.81754. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.75186/4.81511. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.75719/4.80522. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.75308/4.80937. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.76355/4.79797. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.76381/4.80789. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.75524/4.80982. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.75501/4.82754. Took 0.09 sec\n",
      "ACC: 0.53125, MCC: 0.05707352953442433\n",
      "Epoch 0, Loss(train/val) 4.78327/4.77877. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.75745/4.76834. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.76286/4.80708. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.76546/4.77855. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.75017/4.74751. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.74009/4.76164. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.74497/4.76786. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.74489/4.76438. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.74436/4.76178. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.74425/4.76397. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.74181/4.75973. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.74518/4.76921. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.74086/4.75641. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.74084/4.74720. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.74028/4.75179. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.74174/4.75402. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.74200/4.75981. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.73844/4.78009. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.74276/4.75054. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.74172/4.75394. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.74087/4.75225. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.74073/4.75313. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.74084/4.75602. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.74141/4.75808. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.74104/4.77159. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.74067/4.76761. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.74421/4.76983. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.74400/4.75663. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.73949/4.77263. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.74066/4.74973. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.73970/4.76254. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.74365/4.74577. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.74159/4.74881. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.73907/4.74811. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.73507/4.77928. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.73729/4.75656. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.74075/4.73789. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.73677/4.74972. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.73718/4.74786. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.73674/4.74449. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.73755/4.74722. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.73597/4.75492. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.73457/4.75185. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.73548/4.75453. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.73193/4.75144. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.73207/4.76325. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.73593/4.74947. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.73009/4.76398. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.73220/4.76607. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.73633/4.79617. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.73409/4.75891. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.73221/4.76994. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.72694/4.77038. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.73034/4.77500. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.72459/4.79253. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.73230/4.76280. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.72366/4.80096. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.72720/4.78961. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.73726/4.77370. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.72588/4.78147. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.72604/4.77293. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.72557/4.77809. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.72511/4.77838. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.72541/4.77478. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.72252/4.77764. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.72236/4.78137. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.72446/4.77229. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.72994/4.76820. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.72729/4.78383. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.72074/4.77148. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.72121/4.79280. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.72154/4.78027. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.72550/4.77720. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.71849/4.79303. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.72425/4.77501. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.71722/4.78375. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.71973/4.80182. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.72605/4.77105. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.71778/4.78659. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.73360/4.76411. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.72151/4.77410. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.72247/4.77779. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.72078/4.76560. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.72453/4.75527. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.71716/4.77752. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.71984/4.76126. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.71923/4.77317. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.72042/4.77107. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.71547/4.77215. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 4.72288/4.75940. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.71475/4.76640. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.72133/4.76775. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.72918/4.76455. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.72131/4.76001. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.72088/4.75105. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.71331/4.76666. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.71914/4.75264. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.71603/4.75777. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.71278/4.75871. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.71426/4.77653. Took 0.08 sec\n",
      "ACC: 0.5, MCC: -0.021109792565893827\n",
      "Epoch 0, Loss(train/val) 4.96749/5.04499. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.92356/5.03155. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.92111/5.01181. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.92263/4.96480. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.91761/4.93397. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.91506/4.93706. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.90954/4.95003. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.90906/4.97050. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.91330/4.95724. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.91334/4.95151. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.90867/4.94702. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.91137/4.93586. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.90854/4.94461. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.90689/4.94882. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.90773/4.95138. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.90684/4.94034. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.90623/4.93902. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.90654/4.93658. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.90257/4.95424. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.90370/4.95165. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.90402/4.94886. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 4.90307/4.94862. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.90348/4.95264. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.90183/4.96137. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.90109/4.95466. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.90062/4.95243. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.89752/4.94623. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.89881/4.95860. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.90016/4.94605. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.89799/4.95425. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.90454/4.94549. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.89913/4.95144. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.90071/4.94670. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 4.89881/4.95166. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.89864/4.94757. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.89771/4.96119. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.89677/4.94472. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.89778/4.94413. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.89556/4.94493. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.89706/4.94839. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.90883/4.92872. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.89642/4.94807. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.89638/4.95480. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.89435/4.94952. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.89473/4.94880. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.89630/4.96143. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.89697/4.93818. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.89855/4.94932. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.89381/4.95109. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.89086/4.96114. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.90017/4.94700. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.89721/4.94914. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.88815/4.95234. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.89099/4.95740. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.89364/4.96150. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.90060/4.93737. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.89989/4.93846. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.90134/4.94051. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.89126/4.94675. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.89683/4.95045. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.89098/4.95621. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89521/4.94639. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.89135/4.95105. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.89532/4.93985. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.89772/4.94008. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.89398/4.95464. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.89246/4.95030. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.89214/4.94300. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.89414/4.95922. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.88847/4.96300. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.89335/4.94622. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.88903/4.95794. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.89066/4.95237. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.89381/4.94714. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.89116/4.93817. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.89157/4.96622. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.88148/4.96220. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.89367/4.94888. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.89021/4.95594. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.88897/4.94027. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.89177/4.94926. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.88346/4.95958. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.88863/4.93309. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.88880/4.93828. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.88147/4.96153. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.88402/4.95731. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.88939/4.96887. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.89051/4.93234. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.88233/4.94777. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88739/4.93332. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.88578/4.95966. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.88339/4.94249. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.88855/4.95112. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.88554/4.95557. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.88715/4.92608. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.88126/4.96222. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.88995/4.93573. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.88475/4.94548. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.88451/4.95672. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.88525/4.94013. Took 0.08 sec\n",
      "ACC: 0.625, MCC: 0.24334975369458128\n",
      "Epoch 0, Loss(train/val) 5.01138/4.98781. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.96542/5.00997. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.96657/4.98769. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.96954/4.96868. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.96749/4.95445. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.96643/4.95144. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.96174/4.95693. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.96091/4.95698. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.96145/4.95516. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.95886/4.95505. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.96037/4.95825. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.95752/4.95756. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.95881/4.95421. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.96047/4.95834. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.95784/4.96044. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.95743/4.96460. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.95565/4.96737. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.95700/4.97084. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.95648/4.96576. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.95164/4.97355. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.95472/4.96962. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.95333/4.97037. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.95305/4.96877. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.95277/4.96668. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.94879/4.97397. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.94759/4.98344. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.95275/4.97422. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.94978/4.97162. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.94719/4.97614. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.95034/4.98587. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.95594/4.96672. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.95004/4.96679. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.94614/4.96741. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.94604/4.97424. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.94645/4.97238. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.94780/4.97119. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.94640/4.97771. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.94222/4.98206. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.94670/4.98037. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.94229/4.97800. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.94415/4.97645. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.94590/4.97436. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.94183/4.98372. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.94424/4.98354. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.94429/4.98056. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.93689/4.99339. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.93669/4.99628. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.93824/5.00430. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.94755/4.97852. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.93926/4.99388. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.93429/4.99842. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.93907/4.99828. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.94342/4.99207. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.93478/4.99883. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.93512/5.00617. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.93570/5.01186. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.93903/4.99482. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.93616/5.00903. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.93951/4.99766. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.92496/5.02582. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.93817/5.00114. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.93735/4.99504. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.93527/5.01665. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.93188/5.01434. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.93196/5.00886. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.93336/5.02023. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.93129/5.02240. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.93242/5.01828. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.93678/4.99641. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.93982/5.00560. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.93157/5.00477. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.93713/4.98278. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.93877/5.00144. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.93786/5.00383. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.93738/5.01025. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.92928/5.03607. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.93252/5.03865. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.92504/5.03105. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.92973/5.03610. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.93972/5.02273. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.92908/5.02937. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.93361/5.02933. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.92743/5.01945. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.92867/5.02953. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.93363/5.02268. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.92837/5.04142. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.92678/5.01402. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.93100/5.04952. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.92495/5.03889. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.92541/5.05119. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.92438/5.02838. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.92804/5.04332. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.93281/5.02120. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.91895/5.05968. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.92879/5.02047. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.91983/5.03799. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.92429/5.05329. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.91641/5.08435. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.91795/5.01822. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.92594/5.04149. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.16511700362686718\n",
      "Epoch 0, Loss(train/val) 4.77174/4.77575. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 4.72781/4.70047. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 4.72651/4.70188. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.72205/4.72175. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.72208/4.73533. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.71702/4.75029. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.71793/4.75223. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.71267/4.73950. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.71154/4.75189. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.71187/4.72799. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.71231/4.72394. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.71312/4.73528. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.70917/4.74606. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.71272/4.74135. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.70797/4.73511. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.70504/4.74047. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.70872/4.72788. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.71385/4.71119. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.70966/4.70702. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.71289/4.71509. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.70930/4.70978. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.71013/4.72239. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.70972/4.71634. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.70721/4.72919. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.70676/4.73261. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.70501/4.73655. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.70829/4.73768. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.70451/4.73196. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.70676/4.73349. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.70393/4.72981. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.70457/4.72998. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.70267/4.73896. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.70452/4.73445. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.69723/4.74369. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.70261/4.74042. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.70246/4.74737. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.70585/4.74486. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.69851/4.73934. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.69862/4.73061. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.70103/4.73228. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.70090/4.73680. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.69785/4.74903. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.69303/4.75361. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.69995/4.75316. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.70160/4.74366. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.69973/4.74193. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.69513/4.74979. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.69842/4.74275. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.69761/4.75582. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.69838/4.74162. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.69382/4.74568. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.69779/4.74745. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.69531/4.75721. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.69687/4.74716. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.69080/4.75192. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.69729/4.73972. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.69233/4.75449. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.68981/4.75991. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.69625/4.75618. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.69460/4.73146. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.69652/4.74984. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.69258/4.75960. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.68957/4.75720. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.69107/4.76122. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.69763/4.74961. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.69002/4.74710. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.69125/4.76520. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.69461/4.74436. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.69299/4.75472. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.68805/4.76111. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.69185/4.75272. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.68984/4.76215. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.69167/4.75165. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.68795/4.74688. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.69156/4.76953. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.68585/4.74524. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.69809/4.74966. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.68487/4.75371. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.68880/4.74783. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.69053/4.75117. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.68376/4.75899. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.68508/4.75793. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.68463/4.75323. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.68335/4.75417. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.67774/4.76636. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.69479/4.75237. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.68336/4.74999. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.68957/4.77080. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.68546/4.74934. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.68656/4.75177. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.68876/4.75468. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.68373/4.76194. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.69004/4.76089. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.68280/4.74921. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.68603/4.77182. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.68602/4.75008. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.67861/4.76186. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.67950/4.78082. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.68213/4.76401. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.67913/4.76524. Took 0.08 sec\n",
      "ACC: 0.625, MCC: 0.2277100170213244\n",
      "Epoch 0, Loss(train/val) 4.81901/4.78193. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.78577/4.78372. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.78307/4.77719. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.78888/4.78458. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79131/4.79239. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.78799/4.78021. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.78505/4.77932. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.77912/4.78108. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.78342/4.78265. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.77965/4.78535. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.78238/4.78198. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.78085/4.78379. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.77809/4.78558. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.77521/4.78530. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.77609/4.79158. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.77685/4.79922. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.77908/4.79186. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.77668/4.77940. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.77624/4.78230. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.77577/4.79487. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.77339/4.79484. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.77337/4.79524. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.77250/4.79769. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.77315/4.79549. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.77011/4.80203. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.77012/4.80112. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.77021/4.79392. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.76809/4.80698. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.76601/4.79684. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.77250/4.81158. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.76768/4.80240. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.77053/4.81682. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.76314/4.81993. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.76402/4.80777. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.77106/4.80092. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.76743/4.81462. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.76278/4.80803. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.76029/4.81400. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.76032/4.81392. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.76783/4.81099. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.76054/4.82730. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.75943/4.82056. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.75952/4.81678. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.76424/4.81073. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.76188/4.81089. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.75839/4.81374. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.75809/4.81531. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.75774/4.82489. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.75669/4.82177. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.76021/4.83355. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.75593/4.81461. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.76384/4.83299. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.75391/4.81297. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.75434/4.81473. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.75820/4.81495. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.75738/4.83114. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.75373/4.82560. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.75727/4.81069. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.75868/4.84620. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.75768/4.82524. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.75313/4.84197. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.75494/4.83684. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76254/4.83308. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.75066/4.82985. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.75080/4.84322. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.75332/4.84061. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.75159/4.84363. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.75477/4.83019. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.75278/4.82971. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.75014/4.81538. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.75014/4.84263. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.75132/4.85976. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.75229/4.83849. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.74943/4.83309. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.74344/4.85182. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.74953/4.84041. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.74854/4.82181. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.74854/4.85467. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.74710/4.85287. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.74614/4.84138. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.74943/4.84396. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.74823/4.83690. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.74332/4.84483. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.74876/4.85586. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.74112/4.82852. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.75221/4.82871. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.75020/4.82382. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.75224/4.83519. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.74216/4.84315. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.74269/4.84312. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.75124/4.82667. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.73946/4.85155. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.74301/4.86548. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.74638/4.85186. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.74609/4.85100. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.74728/4.82320. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.74026/4.88422. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.73497/4.87114. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.74712/4.81961. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.74433/4.86141. Took 0.09 sec\n",
      "ACC: 0.46875, MCC: -0.0635386119257087\n",
      "Epoch 0, Loss(train/val) 4.77484/4.78343. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.76573/4.77121. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.75814/4.77906. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.76093/4.78126. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.76195/4.77812. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.75876/4.77819. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.75755/4.77966. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.75537/4.78407. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.75302/4.79189. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.75324/4.79076. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.75481/4.78922. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.75372/4.79010. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.75315/4.79646. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.75269/4.80313. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.75392/4.79648. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 4.75303/4.79540. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.75300/4.80088. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.75061/4.80006. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.75234/4.79998. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.74944/4.80488. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.74974/4.80913. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.75087/4.80199. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.75158/4.79842. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.75086/4.80248. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.75103/4.81546. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.75200/4.81154. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.75042/4.80305. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.74696/4.81136. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.75169/4.81343. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.74802/4.80707. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.74607/4.81743. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.74834/4.81465. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.74930/4.81744. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.74725/4.82307. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.74638/4.82903. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.74669/4.82086. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.74664/4.81917. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.74626/4.82168. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.74175/4.83568. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.74320/4.82433. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.74415/4.82915. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.74067/4.83758. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.74685/4.81508. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.74614/4.81897. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.74473/4.82852. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.73942/4.84854. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.74347/4.84421. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.74009/4.85471. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.74104/4.85344. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.74186/4.84636. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.74345/4.82687. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.73828/4.86332. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.74210/4.85152. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.74515/4.82669. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.74559/4.82610. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.74164/4.84179. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.74026/4.85076. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.73804/4.86210. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.73838/4.84255. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.74023/4.85575. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 4.73780/4.85687. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.73861/4.85545. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.74180/4.84751. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.74294/4.84613. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.73695/4.83301. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.74458/4.82673. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.74616/4.78594. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.74180/4.85258. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.74116/4.83853. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.74179/4.80130. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.74147/4.82117. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.74319/4.83040. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.73986/4.82037. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.74055/4.85166. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.73801/4.84028. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.73757/4.84071. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.74469/4.81026. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.74641/4.79909. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.74821/4.78879. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.74792/4.80251. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.74003/4.81901. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.73919/4.85191. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.74179/4.83875. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.73704/4.84006. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.74038/4.83637. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.73981/4.84784. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.73860/4.83771. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.73658/4.84630. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.73549/4.86546. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.73744/4.83449. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.73695/4.86702. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.73647/4.84295. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.73871/4.84772. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.73661/4.85172. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.73456/4.85862. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.73152/4.85577. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.73683/4.86944. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.73591/4.84001. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.73941/4.84146. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.73136/4.86114. Took 0.08 sec\n",
      "ACC: 0.625, MCC: 0.2504897164340598\n",
      "Epoch 0, Loss(train/val) 4.91887/4.90323. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.86712/4.93276. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.86713/4.92352. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.86075/4.91360. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.86182/4.90370. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.86097/4.90341. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.85720/4.91110. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.85670/4.90308. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.85513/4.90115. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.85969/4.92326. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.85805/4.91180. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.85507/4.90634. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85701/4.91663. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.85642/4.91702. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.85669/4.91382. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.85942/4.90456. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.85799/4.90904. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 4.86092/4.90273. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.86170/4.88871. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.86050/4.88737. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.85481/4.90646. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.86461/4.89004. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.85945/4.88624. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.86065/4.88749. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.86091/4.88500. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.86034/4.88878. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.85768/4.89596. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.85754/4.89625. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.85682/4.89881. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.85579/4.90234. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.85712/4.90607. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 4.85651/4.89454. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.85619/4.89370. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.85575/4.89857. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.85360/4.90712. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.85487/4.89609. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.85487/4.90073. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.85594/4.89275. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.85288/4.90053. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.85185/4.90547. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.85359/4.89839. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.85400/4.90306. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 4.85420/4.89421. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.85128/4.90668. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.85121/4.90712. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.85195/4.90058. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.84526/4.92960. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.85056/4.90322. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.85363/4.90363. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.85017/4.93319. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.85244/4.88845. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.85055/4.89369. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.84528/4.93386. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.84686/4.91972. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.84415/4.92106. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.84398/4.93350. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.85148/4.92103. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.85394/4.91571. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.85506/4.91675. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.84892/4.91165. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.84734/4.92519. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.84414/4.93683. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.84750/4.92962. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.84728/4.92004. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.84640/4.91897. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.84108/4.93418. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.84908/4.93880. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.84898/4.91778. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.84576/4.90360. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.84294/4.94361. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 4.84758/4.94204. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.84239/4.91754. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.84176/4.92100. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.84931/4.92961. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.84827/4.90326. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.84758/4.91329. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.85151/4.90211. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.85083/4.91343. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.84661/4.90903. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.84493/4.90919. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.84475/4.92567. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.84349/4.91234. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.84494/4.89134. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.84373/4.90874. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.84516/4.91468. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.84184/4.92027. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.84319/4.92549. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.84906/4.92451. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.84188/4.91875. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.84410/4.91503. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.84477/4.91702. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.84254/4.92667. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83937/4.92981. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.84243/4.94799. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.84799/4.91564. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.84246/4.92294. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.84048/4.93378. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.83762/4.93421. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83583/4.93955. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.83931/4.94667. Took 0.08 sec\n",
      "ACC: 0.390625, MCC: -0.2556657194498359\n",
      "Epoch 0, Loss(train/val) 5.11971/5.04271. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.04787/5.03537. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.04317/5.03926. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.04090/5.04277. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 5.04368/5.04279. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.03963/5.04327. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.03900/5.04780. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.03644/5.04590. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.03419/5.04145. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.03314/5.04877. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 5.03200/5.04863. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.03449/5.03929. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 5.03357/5.04263. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.03166/5.04839. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.03307/5.04672. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 5.03351/5.04516. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.03323/5.04228. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 5.03123/5.05374. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 5.03221/5.04759. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.02879/5.05386. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.03091/5.04635. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 5.03158/5.05086. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.03312/5.04353. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.02802/5.04842. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.02961/5.04156. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.02831/5.05449. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 5.02852/5.05683. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.03065/5.04852. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.02976/5.04286. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.03152/5.04790. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.03046/5.04735. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.03119/5.04629. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.02927/5.04961. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.02780/5.04823. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.02770/5.04921. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 5.02439/5.05123. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.02563/5.05052. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 5.02920/5.05363. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.02765/5.04245. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.02311/5.05039. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.02224/5.05102. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 5.02435/5.05820. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.02410/5.04192. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.02430/5.05774. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 5.02163/5.06294. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.02086/5.05147. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.01801/5.04205. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 5.02063/5.04361. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 5.02408/5.03634. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.02245/5.04025. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.02471/5.06668. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.02035/5.06059. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.02225/5.04674. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.01601/5.05803. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.01943/5.05288. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.01822/5.06491. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.02078/5.06242. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 5.02207/5.05104. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.01717/5.05953. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.01682/5.07394. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.01636/5.06299. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.01555/5.06265. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.01832/5.06851. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.01319/5.06574. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.01885/5.06190. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.01915/5.05976. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.01401/5.06646. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.01156/5.06748. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.01577/5.06987. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.01025/5.06087. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.01409/5.06833. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.01064/5.07353. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.00883/5.06767. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.01463/5.05673. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.01215/5.05815. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.01282/5.06577. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 5.00655/5.06435. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.00624/5.05633. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.01036/5.05029. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 5.01035/5.07529. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.01014/5.06029. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 5.01206/5.05988. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.01499/5.05807. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 5.00787/5.05396. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 5.00503/5.06105. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.00136/5.05748. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.01324/5.05563. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.00551/5.05435. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 5.01323/5.04980. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.01615/5.03393. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.01454/5.05337. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.01179/5.05691. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.01104/5.07493. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.00624/5.06801. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 5.00762/5.05022. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.01071/5.05363. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.01084/5.06473. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 5.00609/5.04089. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.00996/5.04967. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.00869/5.05602. Took 0.09 sec\n",
      "ACC: 0.578125, MCC: 0.16150120428611295\n",
      "Epoch 0, Loss(train/val) 4.86736/4.78195. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.80513/4.79323. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.79841/4.79393. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79947/4.79548. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79891/4.79912. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79708/4.80003. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.79630/4.80341. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.79521/4.80225. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.79894/4.80760. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79364/4.80712. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79447/4.80015. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.79446/4.80360. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.79593/4.81270. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.79433/4.80535. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.79383/4.80412. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.79488/4.80053. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.79043/4.80412. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.79405/4.80211. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.79646/4.80319. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.79427/4.81021. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.79436/4.80991. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.79541/4.81226. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.79065/4.81485. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.79145/4.81776. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.79143/4.81703. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.79213/4.81354. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.79194/4.81626. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.78699/4.80603. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.78749/4.80326. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.79106/4.80359. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.78896/4.80921. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.78387/4.81025. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.79049/4.80828. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78644/4.81072. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78639/4.81251. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.78133/4.82096. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.78661/4.81933. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.78593/4.80781. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.78528/4.81056. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.78140/4.81280. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.78468/4.81249. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.78224/4.82006. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.78547/4.81268. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.78639/4.80155. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.77871/4.80906. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.78366/4.80753. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77819/4.80537. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.78376/4.79540. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77996/4.80526. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.78380/4.80341. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.79837/4.80544. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.78930/4.79775. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.77770/4.80431. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.78232/4.80161. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.77987/4.80325. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.79038/4.82461. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.79285/4.80715. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.79138/4.80493. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.78920/4.80722. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.78918/4.80190. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.78697/4.80965. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.78759/4.80069. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.79026/4.81038. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.78790/4.80836. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.78777/4.81323. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.78423/4.81717. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.79004/4.80485. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.78508/4.82701. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.78675/4.81686. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.78605/4.81547. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.78440/4.82078. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.78524/4.82276. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.78189/4.82028. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.78339/4.81622. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.77997/4.80945. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.78645/4.81014. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78184/4.81084. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78499/4.81060. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.78657/4.80507. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.77999/4.81293. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.78084/4.80950. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.78266/4.81141. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.78522/4.81567. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.78282/4.81094. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.77963/4.80857. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.77864/4.81051. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.78137/4.79703. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.78209/4.82203. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.78098/4.81298. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.78525/4.81747. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.77767/4.82125. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.77594/4.82534. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.78503/4.80948. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.77823/4.81436. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.77908/4.81651. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.77517/4.81268. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.78029/4.80626. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.77984/4.80678. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77977/4.80870. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.77819/4.81805. Took 0.09 sec\n",
      "ACC: 0.390625, MCC: -0.22877657129023765\n",
      "Epoch 0, Loss(train/val) 5.03768/4.99363. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.00336/4.97262. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.00609/4.96531. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.01342/4.96555. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 5.01900/4.98162. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.00765/4.99932. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.99897/4.98869. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.99997/4.98548. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.99786/4.98014. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.99476/4.97704. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.99598/4.97644. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.99386/4.98181. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.99486/4.97116. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.99227/4.98580. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.99139/5.00156. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.99293/4.96827. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.98955/5.01170. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.98971/4.96731. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.99070/4.98610. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.99063/4.98525. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.98944/4.98470. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.98523/5.00632. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.99036/4.97083. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.99178/4.98153. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.99107/4.98792. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.98944/4.97475. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.98649/4.97272. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 4.98710/4.98071. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.98262/5.00283. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.98426/4.96336. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.98492/4.99968. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.98876/4.97228. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.98959/4.97672. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.98508/4.98510. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.98338/4.97962. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.98441/4.97817. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.98338/4.98646. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.98346/4.98980. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.98512/4.99035. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.98157/4.99335. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.98113/4.99418. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.98218/4.98972. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.98136/4.99930. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.98039/5.00039. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.98135/5.00081. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.97825/4.98806. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.98326/4.99089. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.98020/4.99714. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.98045/5.00859. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.98356/5.00000. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.97406/4.99792. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.97593/5.00641. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.97251/5.00112. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.97587/5.00779. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.97726/5.00657. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.97564/5.01129. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.97057/5.02050. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.97490/5.00822. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.97680/4.99832. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.97260/5.00194. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.97652/5.00248. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.97762/4.99451. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.97616/5.00517. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.97054/5.01136. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.96954/5.00126. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.97372/4.99912. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.96727/5.01400. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.97431/5.00152. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.96551/5.00184. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.97187/5.00673. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.97127/5.00342. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.96999/4.99696. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.96339/5.02986. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.97224/5.01028. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.96265/5.01965. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.97273/4.99910. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.96669/5.01931. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.96118/5.00505. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.96680/5.02296. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.96446/5.01340. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.96667/5.00623. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.96720/5.00152. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.96492/5.00797. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.96572/5.01626. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.96319/5.02014. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.96184/4.99626. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.96385/5.00174. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.96069/5.00643. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.96723/5.01852. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.95952/5.01487. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.96187/5.01586. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.96806/5.00654. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.96390/5.01306. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.95820/5.01574. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.96325/5.02368. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.95666/4.99757. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.95622/5.01627. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.96120/5.01011. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.95356/5.01266. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.96414/4.99637. Took 0.08 sec\n",
      "ACC: 0.609375, MCC: 0.21057989315866538\n",
      "Epoch 0, Loss(train/val) 5.01860/4.98774. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.98051/4.99708. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.98231/5.04019. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.98460/5.04226. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.98356/5.00015. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.97327/4.99212. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.96636/5.00810. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.96731/5.02184. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.96878/5.00694. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.96559/5.00286. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.96377/5.01101. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.96429/5.00856. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.96109/5.00979. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.96370/5.00383. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.96091/5.01256. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.96193/5.00468. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.95890/4.99926. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.96030/5.00929. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.95655/5.00799. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.95677/5.00253. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.95818/5.00600. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.95817/5.00269. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.96146/4.98656. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.95815/4.99430. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.95913/4.99846. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.95676/5.00695. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.95531/5.01124. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.95876/4.99773. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.95327/5.01008. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.95595/4.99648. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.95380/5.00293. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.95546/4.99659. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.95181/5.00033. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.95028/5.00638. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.95279/4.99371. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.95114/4.99627. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.95167/4.99195. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.95068/4.99698. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.95279/5.00350. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.95021/4.99477. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.94830/4.99282. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.95040/4.99597. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.94988/4.99326. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.94578/5.00834. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.94830/5.00047. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.95139/4.99709. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.94866/4.99655. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.94649/4.99765. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.94574/4.98713. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.94894/4.99391. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.94832/5.00812. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.94458/4.99203. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.94677/4.99780. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.94720/4.99932. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 4.94609/4.99592. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.94429/5.00257. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.94173/5.01016. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.94467/5.00976. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.94589/5.00283. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.93998/5.00481. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.95076/4.99413. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.95700/4.98136. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.95637/4.98410. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.94709/4.99266. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.95007/4.97621. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.95572/4.97194. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.95289/4.97897. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.95214/4.97252. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.94838/4.97747. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.94895/4.97875. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.95088/4.98258. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.95131/4.99738. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.94791/4.99747. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.94968/4.99377. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 4.94415/4.99490. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.94490/4.99615. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.94472/4.99005. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.94319/5.00373. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.94608/4.99738. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.94237/4.99845. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.93986/5.01030. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.94228/5.00725. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.94479/4.99892. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.94104/4.99728. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.93880/5.01160. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.94186/5.01184. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.94172/5.00342. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.93917/5.00803. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.94240/5.00940. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.94393/5.00542. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.93386/5.02192. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.94701/5.00154. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.94278/5.01501. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.94078/5.01533. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.93807/5.01993. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.94416/5.00620. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.94450/5.00228. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.94453/5.01094. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.93747/5.00266. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.94012/5.01493. Took 0.09 sec\n",
      "ACC: 0.46875, MCC: -0.054187192118226604\n",
      "Epoch 0, Loss(train/val) 4.93218/4.96649. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.89350/4.91953. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.89349/4.90414. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.89012/4.89655. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.89224/4.88895. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.88951/4.88579. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.88634/4.88955. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.88400/4.89713. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.88345/4.90582. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.88436/4.90480. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.88979/4.89519. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.88394/4.90446. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.88243/4.91315. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.87884/4.92087. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.87981/4.91802. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.88240/4.91872. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.88122/4.92778. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.87778/4.92273. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.87761/4.92074. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.88082/4.93258. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.87441/4.92400. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.87554/4.92730. Took 0.13 sec\n",
      "Epoch 22, Loss(train/val) 4.87833/4.94676. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.87444/4.91352. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.86967/4.92786. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.87082/4.92904. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.87389/4.93047. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 4.87508/4.93971. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.87134/4.94560. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.87746/4.92128. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.87487/4.92570. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.86986/4.93662. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.86915/4.93824. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.87158/4.94805. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.87951/4.92360. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.87668/4.91918. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.87498/4.92507. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.87024/4.93427. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.87714/4.90328. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.87648/4.91140. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.87667/4.91983. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.87154/4.92596. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.87289/4.93222. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.87050/4.93508. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.86930/4.92949. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.87106/4.94024. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.86530/4.93840. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.86961/4.93174. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 4.86517/4.94445. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.86575/4.93424. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 4.86501/4.93057. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.86033/4.94220. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.86492/4.94142. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.85976/4.96692. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.86533/4.95577. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.86339/4.94219. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.86333/4.94774. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.86187/4.94979. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.85939/4.98760. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.86019/4.93125. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.86290/4.94251. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.85411/4.96132. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.86547/4.95044. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.85642/4.95100. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.86031/4.92283. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.86108/4.97625. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.86128/4.93308. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.86105/4.96193. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.85639/4.97290. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.87083/4.95969. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.86702/4.94079. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.86793/4.95506. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.86363/4.93909. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.85856/4.96061. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.86332/4.94104. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.85660/4.96321. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.85310/4.94356. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.86095/4.98334. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.85333/4.94367. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.86044/4.95505. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.85564/4.94507. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.86037/4.95376. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.86599/4.92812. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.86770/4.92350. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.86198/4.94177. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.86856/4.94196. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.86999/4.91867. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.85937/4.93407. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.86092/4.93102. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.85855/4.93188. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.86186/4.92619. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.85514/4.92902. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.85794/4.93531. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.84939/4.92364. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.85517/4.93798. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.85676/4.93144. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.85743/4.92969. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.85615/4.90516. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.85106/4.94360. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.85694/4.91259. Took 0.09 sec\n",
      "ACC: 0.53125, MCC: 0.07192118226600985\n",
      "Epoch 0, Loss(train/val) 4.89958/4.91017. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.87586/4.90997. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.87216/4.90931. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.87249/4.90951. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.87295/4.91443. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.87008/4.92595. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.87502/4.90965. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.86762/4.92900. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.87001/4.91230. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.86571/4.92196. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.86856/4.92212. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.86606/4.93814. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.86820/4.93393. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.87064/4.91823. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.86811/4.90459. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.86596/4.89430. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.86852/4.88691. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.86454/4.90409. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.86267/4.89822. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.86240/4.90752. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.85958/4.91246. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.86091/4.91192. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.85913/4.91464. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.86038/4.90948. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.85877/4.91696. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.85748/4.91813. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.85525/4.92177. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.85697/4.91105. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.85788/4.91688. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.85763/4.92177. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.85305/4.91646. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.85525/4.92632. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.85473/4.91096. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.85334/4.92436. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.85307/4.91752. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.85262/4.92180. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.85178/4.91797. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.85450/4.91447. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84953/4.92853. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.85037/4.91572. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.85046/4.91732. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.85398/4.91875. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84917/4.90840. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.84765/4.94163. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.84798/4.92764. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.84815/4.92517. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.84629/4.94689. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.84827/4.92460. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84850/4.93161. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.84882/4.93720. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 4.84944/4.91260. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.84830/4.93306. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.84679/4.93249. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.84406/4.93688. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.84964/4.91561. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.86514/4.91479. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.85757/4.90993. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.86006/4.90215. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.85762/4.91123. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.85443/4.91422. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.85390/4.89898. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.85005/4.91649. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.85443/4.89455. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.85312/4.91215. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.84890/4.92336. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.84594/4.92016. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.84599/4.93369. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.84658/4.90244. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.85583/4.92192. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.84712/4.92361. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.84545/4.94778. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.84879/4.92718. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.84405/4.94099. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.84517/4.92986. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.84682/4.95606. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.83930/4.94261. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.84071/4.95832. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.84428/4.92147. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.84918/4.91865. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.84411/4.96351. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83801/4.96100. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.83881/4.94515. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.84153/4.95584. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.84297/4.91945. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.84656/4.92913. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.84153/4.92147. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83916/4.95393. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.84047/4.92330. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.84334/4.94062. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.83891/4.94384. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.84308/4.90105. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.84213/4.94616. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83835/4.93071. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.84197/4.90183. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.84275/4.95043. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.83958/4.94642. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.83766/4.94949. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.83677/4.96515. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.84400/4.94758. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.84157/4.94633. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.051799406700962544\n",
      "Epoch 0, Loss(train/val) 4.85506/4.75170. Took 0.18 sec\n",
      "Epoch 1, Loss(train/val) 4.76257/4.75897. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 4.75518/4.75829. Took 0.16 sec\n",
      "Epoch 3, Loss(train/val) 4.75179/4.75569. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.74793/4.76135. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.75016/4.76164. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.75084/4.76575. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.74951/4.77017. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.75031/4.76713. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.74641/4.77077. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.74761/4.76675. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.74344/4.77015. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.74605/4.77102. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.74360/4.77508. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.74391/4.77262. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.74273/4.77637. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.74204/4.76501. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.74423/4.78047. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.74239/4.76772. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.74187/4.78423. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.74197/4.77218. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.74155/4.78359. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.74184/4.77928. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.74007/4.78261. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.73880/4.77811. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.73913/4.78205. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.73501/4.78769. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.73462/4.78768. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 4.73694/4.78931. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.73426/4.78798. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.73795/4.78680. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.73739/4.78804. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.73242/4.78392. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.73038/4.78469. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.73382/4.78143. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.73718/4.78804. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.73094/4.79153. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.73081/4.79108. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.72835/4.79206. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.72851/4.78929. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.73307/4.79194. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.72887/4.79511. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.72839/4.79200. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.72896/4.80121. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.72566/4.78586. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.72975/4.79695. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.72448/4.79024. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.72891/4.78869. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.72403/4.78520. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.73509/4.78395. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.72911/4.78892. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.72396/4.81003. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.73447/4.79488. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.73052/4.77899. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.72594/4.77758. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.72656/4.79305. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.72697/4.80792. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.72551/4.80177. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.72732/4.79403. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.72374/4.81569. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.72199/4.81663. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.73041/4.80583. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.72446/4.80559. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.72755/4.79643. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.71980/4.82296. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.73052/4.79900. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.72662/4.80395. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.72718/4.79691. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.72235/4.83889. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.72849/4.80374. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.72288/4.81513. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.72139/4.81571. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.72578/4.81093. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.72386/4.78861. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.72511/4.80902. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.71795/4.82445. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.72435/4.81778. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.72521/4.79713. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.72160/4.79753. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 4.71760/4.82779. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.72151/4.81925. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.72256/4.78971. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.71857/4.78757. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.71292/4.82372. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.71620/4.83287. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.72000/4.80112. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.72010/4.81351. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.71731/4.80569. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.71108/4.82164. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.71440/4.82212. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.71906/4.81967. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.71952/4.79744. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.72546/4.80157. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.71289/4.78406. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.71996/4.78266. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.71974/4.78788. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.71224/4.80745. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.70878/4.81952. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.71328/4.79644. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.71522/4.79252. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.0629940788348712\n",
      "Epoch 0, Loss(train/val) 4.83487/4.83194. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.79919/4.80723. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.79122/4.81266. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79431/4.81991. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79623/4.81130. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79151/4.81636. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.79173/4.81250. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.79327/4.81595. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.79425/4.81380. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79220/4.82822. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.78560/4.83885. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.79332/4.82238. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.78574/4.84864. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.78506/4.84880. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.78732/4.85134. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.78280/4.85567. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.78607/4.84341. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.78470/4.83499. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.78328/4.86818. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.78171/4.82537. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.78111/4.83251. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78179/4.84220. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.78091/4.83668. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.77605/4.83697. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.77952/4.82766. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.78121/4.85077. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.78186/4.83001. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.77647/4.84820. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77388/4.83423. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.77377/4.84938. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.77741/4.84322. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.77162/4.85766. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.77331/4.84103. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.77169/4.85609. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.77489/4.83400. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.77268/4.87351. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.77588/4.84895. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.78001/4.84886. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.77180/4.83993. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.77295/4.85105. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.77100/4.86194. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.76966/4.83450. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.77017/4.89862. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.77393/4.83971. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.76623/4.86204. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.77570/4.84114. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.76493/4.86480. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77160/4.86987. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77168/4.84714. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.77267/4.83247. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.76050/4.87914. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77053/4.83553. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.77208/4.87277. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.77407/4.83877. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.76823/4.86717. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.76573/4.88631. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.76566/4.84819. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.76630/4.87562. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76854/4.87783. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.76645/4.87972. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.76852/4.85328. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.76752/4.86791. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76262/4.89249. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.77010/4.85196. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.76703/4.88748. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.76373/4.87267. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.76051/4.86558. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.76259/4.87202. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.76185/4.87727. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76255/4.87911. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.76704/4.89601. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.75676/4.87978. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.75698/4.93779. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.76151/4.87547. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.76324/4.89145. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.76583/4.88302. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.75913/4.88536. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.76084/4.89836. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.76811/4.85815. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.75915/4.93137. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.75954/4.89655. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.76112/4.87962. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.75411/4.88693. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76069/4.92309. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75791/4.87209. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.76478/4.91719. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.75876/4.85587. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.76334/4.90400. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.75744/4.89034. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.76070/4.88583. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.75771/4.92948. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.75458/4.86351. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.76551/4.90880. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.75670/4.89808. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.76003/4.87128. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.75616/4.93672. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.75530/4.92209. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.76480/4.87288. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.76845/4.84413. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.75833/4.91784. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.08952126702661474\n",
      "Epoch 0, Loss(train/val) 4.75807/4.69152. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.69244/4.68050. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.69246/4.67808. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.69344/4.67692. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.69347/4.67782. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.69338/4.67878. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.68999/4.67846. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.69215/4.67712. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.69032/4.67846. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.69146/4.67906. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.69291/4.68139. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.68912/4.68440. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.69008/4.68743. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.68628/4.68982. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.68629/4.69201. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.68529/4.69648. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.68451/4.69964. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.68409/4.69983. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.68591/4.70453. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.68492/4.70860. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.68318/4.71428. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.68084/4.70931. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.68357/4.70750. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.68206/4.70918. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.67837/4.71157. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.68177/4.71119. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.68170/4.71261. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.68398/4.69797. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.68286/4.70523. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.67967/4.71736. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.67884/4.71845. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.67625/4.71740. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.67837/4.71348. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.67650/4.73221. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.67300/4.73543. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.67143/4.73146. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.67688/4.72862. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.67302/4.73685. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.67332/4.73382. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.67782/4.73475. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.67243/4.74077. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.67366/4.74147. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.67171/4.75160. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.67235/4.74364. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.66920/4.74977. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.67463/4.75725. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.67454/4.75996. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.66708/4.76462. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.67058/4.75819. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.67017/4.75546. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.67024/4.74145. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.67338/4.73247. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.66455/4.78030. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.67025/4.74870. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.67169/4.73923. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.66720/4.76111. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.66858/4.73082. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.66438/4.75288. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.66987/4.75320. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.66478/4.73767. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.66593/4.75806. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.66822/4.72019. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 4.66260/4.76542. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.65956/4.76241. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.66546/4.76027. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 4.65904/4.75567. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.67106/4.76255. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.66534/4.77462. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.66750/4.79707. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.65800/4.79037. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.66347/4.75528. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.66350/4.78781. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.66232/4.77478. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.66042/4.75862. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.65670/4.80038. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.66609/4.80812. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.65946/4.79582. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.65272/4.83010. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.65507/4.80998. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.66316/4.78584. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.65533/4.79837. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.64928/4.84462. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.66196/4.73953. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.66774/4.77114. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.65272/4.82095. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.65847/4.79256. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.65682/4.79116. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.65269/4.79123. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.65814/4.77711. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.65827/4.78313. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.65186/4.76037. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 4.65727/4.77619. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.66116/4.75017. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.65274/4.76823. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.65833/4.75659. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.65562/4.76505. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 4.65079/4.75320. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.64901/4.76767. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.64759/4.75325. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.64892/4.76520. Took 0.09 sec\n",
      "ACC: 0.515625, MCC: 0.03126526997403612\n",
      "Epoch 0, Loss(train/val) 4.87115/4.83781. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.83942/4.83848. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.83956/4.83662. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.84058/4.83631. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.83978/4.83594. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.84003/4.83511. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.83511/4.83450. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.83776/4.83552. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.84179/4.83625. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.83779/4.83663. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.83485/4.83681. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.83647/4.83749. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.83526/4.83772. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.83478/4.83903. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.83452/4.83931. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.83572/4.84019. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.83558/4.84096. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.83235/4.84295. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.83569/4.84352. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.83605/4.84341. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.83478/4.84385. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.83415/4.84595. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.83448/4.84889. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.83203/4.84967. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.83118/4.85174. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.82908/4.85843. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.83324/4.85395. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.83142/4.85277. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.83217/4.85772. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.83300/4.85917. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.82964/4.85983. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.83024/4.86812. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.83286/4.85983. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.82952/4.86590. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.82969/4.86139. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.82707/4.86730. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.82689/4.86784. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.82568/4.87637. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 4.82712/4.86220. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.82457/4.86717. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.82856/4.85564. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.82593/4.86260. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.82322/4.85702. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.82493/4.86469. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.82645/4.85885. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.82341/4.86662. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.82755/4.86447. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.82441/4.87228. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.82079/4.86672. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.82381/4.87885. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.82206/4.87984. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.81786/4.87170. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.82298/4.87820. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.81990/4.87381. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.81984/4.88265. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.81894/4.87689. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.81164/4.88312. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.81586/4.88110. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.80840/4.88606. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.81183/4.87262. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.81201/4.87633. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.81564/4.87644. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.81330/4.87165. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.81283/4.88583. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.81675/4.87049. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.80772/4.90248. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.81388/4.86552. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.81706/4.87010. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.81311/4.87089. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.81133/4.86683. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.80911/4.88493. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.80841/4.88041. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.80693/4.88475. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.81353/4.87437. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.80783/4.89578. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.81033/4.89156. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.80765/4.88700. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.81337/4.90403. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.80901/4.87764. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.80957/4.88007. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.81217/4.88037. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.80378/4.89260. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.80534/4.88078. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.80370/4.87122. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.80966/4.88207. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.80673/4.89396. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.80405/4.88221. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.80133/4.89256. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.80156/4.88411. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.80254/4.88897. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.80317/4.87152. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.80650/4.87257. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.80076/4.90692. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.79184/4.87519. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.80102/4.86714. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.80085/4.86751. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.79208/4.93369. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.80211/4.86618. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.80027/4.89884. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.80160/4.88228. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.15318083468998522\n",
      "Epoch 0, Loss(train/val) 4.98338/4.89571. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.92977/4.89590. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.94586/4.90274. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.94046/4.93800. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.92204/4.92221. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.92316/4.91728. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.92434/4.92208. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.92298/4.92005. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.92259/4.91954. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.92295/4.92260. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.92090/4.92235. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.91902/4.92352. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.91961/4.91746. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.91955/4.91396. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.92004/4.91636. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.91879/4.92023. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.91581/4.91545. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.91879/4.91488. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.91792/4.91788. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.91707/4.91765. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.91573/4.91228. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.91725/4.91624. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.91531/4.91524. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.91568/4.91672. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.91535/4.91369. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.91513/4.91416. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.91498/4.91614. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.91405/4.91357. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.91419/4.91233. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.91318/4.91169. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.91473/4.91396. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.91506/4.91419. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.91565/4.91657. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.91353/4.91544. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.91099/4.91417. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.91303/4.91173. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.91338/4.91442. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.91544/4.91942. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.91048/4.91102. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.91163/4.90650. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.91352/4.91764. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 4.91179/4.90850. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.91223/4.92201. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.91475/4.91525. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.91138/4.91666. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.90910/4.91397. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.90970/4.91269. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.90880/4.91436. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.91216/4.91192. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.90892/4.91338. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.91055/4.90740. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.90877/4.91028. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.90651/4.91155. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.90863/4.91353. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.90946/4.92208. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.91409/4.93020. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.91111/4.92618. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.91221/4.92870. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.90956/4.91698. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.91154/4.92180. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.91106/4.92185. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.91063/4.92273. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.91128/4.91528. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.91248/4.93101. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.90894/4.92106. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.90926/4.91866. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.90926/4.92375. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.90896/4.91554. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.90938/4.92654. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.90755/4.92296. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.91017/4.92853. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.90513/4.92030. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.90922/4.92501. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.90754/4.92467. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.90655/4.92657. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.90451/4.92392. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.90206/4.93158. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.90280/4.93219. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.90682/4.93028. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.90885/4.91628. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.90918/4.92223. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.90527/4.92476. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.90214/4.93240. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.90444/4.92253. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.90366/4.92310. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.89968/4.93078. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.90533/4.93543. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.90248/4.92458. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.90244/4.93455. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.90131/4.91019. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.89960/4.92051. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.90241/4.93200. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.89992/4.93291. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.89788/4.92198. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.90449/4.91300. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.90301/4.90289. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.90085/4.92507. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.90396/4.93235. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.89607/4.93089. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.89850/4.93342. Took 0.09 sec\n",
      "ACC: 0.4375, MCC: -0.10421364855284106\n",
      "Epoch 0, Loss(train/val) 4.94254/4.89941. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.87378/4.89493. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.87167/4.89785. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.87149/4.90531. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.87312/4.90566. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.87048/4.90715. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.86833/4.90871. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.87266/4.90481. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.86967/4.90738. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.86924/4.91011. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.86795/4.91198. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.87042/4.90458. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.86851/4.90365. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.86898/4.90132. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.86890/4.90667. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.86560/4.90327. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.86831/4.90242. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.86795/4.89701. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.86827/4.89653. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.86481/4.90094. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.86226/4.90474. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 4.85900/4.90813. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.86140/4.90908. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.86616/4.89948. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.85996/4.90509. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.85848/4.90214. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.85643/4.90040. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.86036/4.89401. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.85771/4.89816. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.85973/4.90118. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.86062/4.89231. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.85625/4.89323. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.85902/4.89458. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.85157/4.91039. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.85523/4.89793. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.85249/4.90628. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.85261/4.89577. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.85182/4.89858. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84854/4.89510. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84842/4.92684. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.85557/4.88436. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.85233/4.89986. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84819/4.90827. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.84646/4.90509. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.84315/4.91021. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.85058/4.90022. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.84891/4.90093. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.84612/4.90949. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.84424/4.91387. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.84502/4.89534. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.84139/4.90603. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.84578/4.93152. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.84237/4.92348. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.83894/4.91720. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.84929/4.88897. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.86113/4.88101. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.85138/4.90399. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.84952/4.90524. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.84647/4.90221. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.85162/4.90192. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.84880/4.91034. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.84860/4.92210. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.84500/4.92928. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.84375/4.90586. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.85937/4.92237. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.84648/4.92896. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.83956/4.93209. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.84101/4.93240. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.83903/4.94904. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83292/4.92991. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.84869/4.92700. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.84502/4.92554. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83775/4.93643. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.84274/4.91762. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 4.83186/4.95419. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.84145/4.91576. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.83529/4.95138. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.84387/4.94957. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.83169/4.93862. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.83447/4.91741. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83635/4.93866. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.82740/4.93874. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.84157/4.89550. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.82908/4.97558. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.84091/4.91004. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83473/4.93829. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83132/4.90433. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.82671/4.93971. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.82604/4.90762. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.85660/4.87977. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.86731/4.87696. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.85402/4.89631. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.84174/4.92130. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83630/4.92278. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.86771/4.85415. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.85644/4.88851. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.85444/4.87631. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.83647/4.92512. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83686/4.92969. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.83500/4.90907. Took 0.09 sec\n",
      "ACC: 0.453125, MCC: -0.11383959916814836\n",
      "Epoch 0, Loss(train/val) 5.03824/5.03635. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.96473/5.01152. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.97234/5.02066. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.98458/4.98731. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.98969/4.97117. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.98127/4.97185. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.97318/4.97431. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.96472/4.97902. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.96766/4.97711. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.96767/4.97580. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.96559/4.97920. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.96266/4.97776. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.96191/4.97718. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.96201/4.97654. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.96183/4.97739. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.95613/4.98387. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.95642/4.97704. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.95738/4.97827. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.95742/4.98052. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.95509/4.97771. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.95377/4.98054. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.95774/4.97140. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.95225/4.98578. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.95337/4.97044. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.95520/4.96844. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.95572/4.96833. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.94959/4.98589. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.95077/4.98697. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.95046/4.98713. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.94962/4.99693. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.94754/4.98569. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.94668/4.98555. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.94847/4.97921. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.94696/4.97957. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.94581/4.98189. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.94324/4.97890. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.94264/4.97242. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.94998/4.97673. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.94658/4.96951. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.94210/4.99467. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.94992/4.97111. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.94486/4.98522. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.93636/4.99763. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.94149/4.98338. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.94510/4.97753. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.94202/4.99495. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.93902/4.98565. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.94497/4.98323. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.93866/4.98409. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.94096/5.01061. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.93756/4.99096. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.93594/5.00196. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.94056/4.98296. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.94106/4.98952. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.93736/5.00851. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.93782/5.00128. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.94004/4.98353. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.93283/4.99818. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.93712/4.98833. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.93519/5.00102. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.93586/4.99719. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.93977/5.00551. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.92920/5.02043. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.93945/4.99001. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.93481/5.01459. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.93087/5.01375. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.93878/5.00095. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.92942/5.00636. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.93319/4.99107. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.93586/4.99894. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.93122/5.01040. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.93051/5.01200. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.92703/5.01779. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.93212/5.01031. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.93479/4.99616. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.93342/4.99942. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.93166/5.00209. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.93026/5.00690. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.93073/4.99299. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.93239/4.99872. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.92321/5.03260. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.92782/4.99613. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.93358/4.98867. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.92178/5.01824. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.93482/5.01403. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.92223/5.02705. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.93011/4.99142. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.93148/4.99962. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.93056/4.98913. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.92600/5.00498. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.92682/5.01643. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.92857/5.00618. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.92612/4.99437. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.93114/4.99088. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.92710/5.01950. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.92426/4.97722. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.92001/5.00582. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.92917/4.98276. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.92775/4.99777. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.92292/5.00444. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.046127024215707656\n",
      "Epoch 0, Loss(train/val) 4.92864/4.85210. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.85819/4.87448. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.86419/4.88760. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.86215/4.88720. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.86496/4.86614. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.86416/4.85406. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.85714/4.85117. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.85346/4.85144. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.85373/4.85236. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.85488/4.85185. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.85378/4.84980. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.85269/4.84737. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85393/4.84774. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.85193/4.85086. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.85388/4.84827. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.85251/4.84800. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.85098/4.84647. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.84722/4.85036. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.85383/4.84780. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.84810/4.84843. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.85059/4.84708. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.84792/4.85110. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.84799/4.85388. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.84892/4.85234. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.84400/4.85788. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.84707/4.85619. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.84470/4.86127. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.84274/4.86523. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.84125/4.87131. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84120/4.85299. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.84053/4.86545. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.84183/4.85986. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.84127/4.86634. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.83564/4.89089. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.83734/4.86321. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.83841/4.86313. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.83946/4.86419. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.83553/4.86167. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.83105/4.87669. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.83604/4.85700. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.83556/4.85750. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.83411/4.87178. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.83119/4.88013. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.82894/4.88327. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.83563/4.85825. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.83088/4.88035. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.82842/4.87052. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.83090/4.86921. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.82674/4.88520. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.82805/4.87540. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.82782/4.87329. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.83145/4.88399. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.83096/4.86499. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.83514/4.85389. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.82013/4.87219. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.82991/4.88246. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.82606/4.87629. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.82877/4.88928. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.82514/4.87274. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.82578/4.87610. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.82433/4.89982. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.82838/4.86832. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.82125/4.87335. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.82962/4.88142. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.82347/4.88985. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.81696/4.87804. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.82370/4.89878. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.81889/4.87417. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.82170/4.88393. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.82288/4.89259. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.81565/4.91506. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 4.81674/4.89901. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.82298/4.88565. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.82187/4.88098. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.82326/4.89843. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.81900/4.88230. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.81813/4.90165. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.81981/4.87970. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.80944/4.88112. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.81067/4.91108. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.81459/4.88661. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.81749/4.90462. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.81536/4.90836. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.80959/4.89901. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.81945/4.89164. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.81706/4.88890. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.81105/4.90033. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.82396/4.89531. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.81463/4.90303. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.80818/4.90510. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 4.81155/4.90142. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.81241/4.89982. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.81299/4.90054. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.81673/4.88562. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.80583/4.89996. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.81483/4.89661. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.81576/4.90040. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.81474/4.88733. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.80407/4.91224. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.80879/4.88335. Took 0.09 sec\n",
      "ACC: 0.546875, MCC: 0.08845235543978211\n",
      "Epoch 0, Loss(train/val) 4.87320/4.80367. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.81439/4.77491. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.80384/4.77718. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79659/4.78692. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79521/4.78808. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79979/4.78791. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.79445/4.79358. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.79730/4.79444. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.79669/4.78979. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79318/4.79147. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79508/4.79213. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.79429/4.79206. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.79291/4.79407. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.79080/4.79579. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.78999/4.79798. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.78488/4.81597. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.79029/4.79681. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.78509/4.79180. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.78494/4.79607. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.78240/4.79281. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.77807/4.80458. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.77922/4.79979. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.78165/4.79350. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.77245/4.79624. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.77716/4.79256. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.77273/4.79184. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.76887/4.79897. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.78213/4.78925. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77260/4.79066. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.77450/4.79716. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.77768/4.78235. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 4.77154/4.79364. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.77191/4.77712. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78661/4.77357. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.77915/4.78595. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.77471/4.78861. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.77644/4.79153. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.77583/4.79868. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.77071/4.78548. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.77070/4.78419. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.77176/4.77858. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.77123/4.79720. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.77755/4.78556. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.76666/4.79685. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.77005/4.79074. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.76602/4.81296. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77389/4.81608. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.76734/4.79854. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.76037/4.78572. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.76797/4.79200. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.76354/4.78514. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.76342/4.77913. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.76221/4.79170. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.76635/4.77218. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.76045/4.77525. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.75895/4.79695. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.76182/4.79077. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.76156/4.77424. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76492/4.79784. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.75953/4.81638. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.76160/4.79654. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.75398/4.79509. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.75816/4.77911. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.75458/4.80393. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.76412/4.80018. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.75946/4.82008. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.76998/4.78845. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.77205/4.76942. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.76532/4.79761. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76205/4.78101. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.75678/4.78949. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.76031/4.78060. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.75521/4.80227. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.75996/4.77845. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.75852/4.78824. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.75697/4.79172. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.75888/4.79574. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.75520/4.78014. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.75564/4.78327. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.75144/4.79795. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.76065/4.78328. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 4.74951/4.78308. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.75297/4.79232. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.75466/4.79892. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75146/4.80516. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.75137/4.80817. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.75266/4.76840. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.74999/4.79546. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 4.75574/4.81982. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.76355/4.78484. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.75339/4.79913. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.74532/4.78814. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.75309/4.79273. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.75580/4.78771. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.74976/4.78962. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.75132/4.79542. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.75321/4.77951. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.75454/4.77703. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.75619/4.77448. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.75290/4.78837. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.10677275981195139\n",
      "Epoch 0, Loss(train/val) 4.92470/4.91120. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.89540/4.90251. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.89102/4.90135. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.89170/4.89838. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.88947/4.90106. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.88845/4.90053. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.89161/4.89565. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.88880/4.89459. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.88890/4.89531. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.88741/4.89523. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.88813/4.89027. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.88381/4.89121. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.88491/4.89125. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.88359/4.89164. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.88488/4.89188. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.88478/4.89131. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.88201/4.89553. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.88407/4.89585. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.87759/4.90425. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.88027/4.90405. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.87986/4.90318. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.87867/4.90645. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.87488/4.90874. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.87644/4.90754. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.87881/4.91152. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.87539/4.91526. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.87438/4.91749. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.87341/4.91888. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.86968/4.92965. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.87718/4.92889. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.87241/4.92897. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.87566/4.91223. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.87729/4.91313. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.87620/4.92105. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.87047/4.93280. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.87119/4.93548. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.86927/4.94398. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.87035/4.93420. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86722/4.93976. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.86774/4.94299. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.86678/4.93730. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.87155/4.93697. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.86767/4.95121. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.86995/4.93291. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.86771/4.93312. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.86531/4.93688. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.86185/4.94916. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.86474/4.93239. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.86006/4.93710. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.86526/4.94489. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.86605/4.94665. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.86203/4.94509. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.87048/4.93908. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.86575/4.96953. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.86169/4.97327. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.86520/4.94698. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.85890/4.97081. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.86243/4.94616. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.86189/4.94340. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.86064/4.93749. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.86075/4.96895. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.85762/4.96641. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.85487/4.96829. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.86140/4.93700. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.86660/4.94860. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.85335/4.96611. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.85580/4.96938. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.85724/4.94782. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.85305/4.98354. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.85513/4.94560. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.86834/4.95171. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.85529/4.98017. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.85837/4.96335. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.84696/4.99052. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.85904/4.94436. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.85660/4.95556. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.85790/4.96528. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.86237/4.93940. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.86191/4.96458. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.85316/4.96864. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.84662/4.98846. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.85131/4.99280. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.85183/4.95930. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.85252/4.95859. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.85264/4.98722. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.84501/4.97884. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.84847/4.99656. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.85251/4.97243. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.85593/4.95223. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.85092/4.97587. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.85086/4.98866. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.84844/4.97023. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.85583/4.95280. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.86015/4.98038. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.84480/5.02160. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.83936/5.01247. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.84962/4.98811. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.85134/5.00149. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.85016/4.99181. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.83890/5.02945. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.11383959916814836\n",
      "Epoch 0, Loss(train/val) 4.93622/4.92329. Took 0.18 sec\n",
      "Epoch 1, Loss(train/val) 4.91282/4.89721. Took 0.17 sec\n",
      "Epoch 2, Loss(train/val) 4.90607/4.89860. Took 0.15 sec\n",
      "Epoch 3, Loss(train/val) 4.90568/4.90069. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 4.90510/4.90312. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.90290/4.90219. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.90448/4.90360. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.90209/4.90647. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.90020/4.91093. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.90078/4.91058. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.89669/4.91877. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.89928/4.92185. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.89648/4.92178. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.89706/4.92199. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.89673/4.92530. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.89503/4.92417. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.89178/4.93297. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.89822/4.92973. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.90083/4.92408. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.89885/4.92556. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.89711/4.93692. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.89632/4.94596. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.89431/4.95092. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.89650/4.94020. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.89360/4.94295. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.89673/4.93815. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.89167/4.93433. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.89320/4.94190. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.89316/4.94382. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.89220/4.94382. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.89090/4.94309. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.89346/4.94491. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.89017/4.95472. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.88652/4.95929. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.89213/4.94421. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.88691/4.95682. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.89196/4.95095. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.88791/4.96361. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.88721/4.96637. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.89392/4.94970. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.88246/4.97156. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.89004/4.95944. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.88325/4.96861. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.87830/4.97142. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.88234/4.97525. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.87915/4.96660. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.89445/4.92493. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.89241/4.94469. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.88871/4.96233. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.88413/4.97456. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.88242/4.97409. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.88276/4.96923. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.88231/4.97042. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.88472/4.97771. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.88957/4.98390. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.88409/4.98062. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.88050/4.99187. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.88228/4.99539. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.88568/4.98443. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.88297/4.98167. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.87919/4.98785. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.88123/4.95043. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.88778/4.94437. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.88617/4.96923. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.88257/4.96967. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.88482/4.97347. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.88590/4.96915. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.88405/4.98976. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.88656/4.97396. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.87992/4.98616. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.88158/4.96663. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.88298/4.97085. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88441/4.97774. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.87997/4.97057. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.87837/4.99277. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.87692/4.97949. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.87751/4.98189. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.88289/4.96460. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.87869/4.98089. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.87902/4.97662. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.87865/4.97661. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.87795/4.98647. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.87502/5.01486. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.87472/4.99499. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.87461/4.99022. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.87922/4.97778. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.87608/4.99594. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.87049/4.98985. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.88140/4.96671. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.87472/5.02131. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.87223/4.99213. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.88054/4.98182. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.87428/5.02293. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.87786/4.98642. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.87710/5.00100. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.87743/5.00736. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.87841/4.98509. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.87522/4.99982. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.87270/4.99760. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.87405/4.99251. Took 0.08 sec\n",
      "ACC: 0.578125, MCC: 0.16150120428611295\n",
      "Epoch 0, Loss(train/val) 4.93605/4.90310. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.93897/4.90692. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.92200/4.91142. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.91830/4.92257. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.91834/4.92888. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.91065/4.92811. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.91529/4.92644. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.91383/4.92917. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.91372/4.93074. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.91584/4.93447. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.91249/4.94057. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.91447/4.94020. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.91305/4.93543. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.91320/4.94180. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.91110/4.94676. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.90868/4.94926. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.90992/4.95026. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.90788/4.93765. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.91081/4.93714. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.90946/4.95033. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.90540/4.95036. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.90798/4.94720. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.90664/4.96678. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.90777/4.96313. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.90677/4.97048. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.90475/4.97326. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.90471/4.96652. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.90278/4.99481. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.89940/4.99046. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.90695/4.96427. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.90205/4.96431. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.90153/4.98250. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.90192/4.96819. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.89752/4.98326. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.89731/4.97777. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.90072/4.97685. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.90072/4.97253. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.89901/4.98202. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.89188/4.98887. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.89693/4.99046. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.89213/4.99233. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.89662/4.99005. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.89511/4.98604. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.89473/4.96590. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.89505/4.99110. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.89132/4.98805. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.90096/4.95742. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.89735/4.98334. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.89396/4.99881. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.89577/4.99119. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.89247/5.01046. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.89630/4.99060. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.88841/5.00778. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.89579/4.98033. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.89429/4.99594. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.88749/5.01786. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.88824/5.02171. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.89275/4.98577. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.89091/4.99098. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.88776/5.01481. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.88829/5.01248. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.89645/4.97434. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.88605/5.01978. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.88780/5.01849. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.88713/4.99389. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.88536/5.01347. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.87986/5.03579. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.88611/5.04348. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.88391/5.02112. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.88444/5.02729. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.88858/5.01863. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.88117/5.04036. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88440/5.00486. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88177/5.02443. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.89806/4.99067. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.89233/5.01027. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88577/5.02339. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.88702/5.04966. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.88672/5.01128. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88351/5.05582. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.88186/5.03602. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.88178/5.02114. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.87986/5.02663. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.87601/5.04119. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.88008/5.06219. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.87696/5.03534. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.87764/5.06879. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.87675/5.03710. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.87685/5.04716. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88190/5.04439. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.88139/5.08271. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.87566/5.01846. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.87349/5.07957. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.87245/5.03649. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.87576/5.03235. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.87502/5.07764. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.88046/5.03311. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.88148/5.03322. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.87229/5.04999. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.87401/5.07361. Took 0.08 sec\n",
      "ACC: 0.59375, MCC: 0.18670576735092864\n",
      "Epoch 0, Loss(train/val) 5.16349/5.08364. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.07130/5.04261. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 5.05407/5.04310. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.04756/5.04546. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.04717/5.05141. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.04921/5.05117. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 5.05165/5.05142. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.04964/5.04933. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.04826/5.04794. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 5.05007/5.04738. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.04770/5.04847. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.04743/5.04950. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.04330/5.05120. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.04608/5.05372. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.04073/5.05504. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.04090/5.05592. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.03925/5.05778. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.03634/5.06456. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.04531/5.06121. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 5.03878/5.06554. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.04131/5.06688. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.03947/5.06090. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.03636/5.06842. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.03537/5.07533. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.03655/5.07212. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 5.03396/5.07571. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.03701/5.06718. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 5.03997/5.06413. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.03363/5.07298. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.03430/5.08910. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 5.03444/5.08884. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.03467/5.08949. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.03082/5.09441. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 5.03072/5.10457. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.02917/5.10986. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.03051/5.09889. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 5.03557/5.09224. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.02767/5.09890. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.02990/5.10171. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 5.02860/5.10034. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 5.02662/5.11096. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.03129/5.09493. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.02843/5.10188. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 5.02224/5.13048. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.02464/5.11281. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.01961/5.11027. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 5.02870/5.08604. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.02925/5.10596. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 5.02228/5.12585. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.02763/5.10753. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 5.02475/5.13846. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.02349/5.14770. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.02277/5.10952. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 5.02150/5.15076. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.02687/5.13638. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 5.02026/5.13076. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.01497/5.15043. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.01769/5.16860. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.02388/5.10015. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 5.01482/5.15856. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 5.02558/5.10903. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.01315/5.17527. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.02179/5.12512. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.01712/5.18402. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.01716/5.14275. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.01616/5.14312. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.01431/5.14263. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 5.01894/5.11009. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 5.01504/5.17460. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.01005/5.16119. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.01989/5.11917. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 5.00866/5.17260. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.01735/5.13870. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.01306/5.15227. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.01012/5.15910. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.00794/5.16681. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 5.00486/5.13517. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 5.01032/5.16152. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.00943/5.16848. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.02392/5.11703. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.01060/5.15957. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 5.00751/5.14207. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.00765/5.17532. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.00889/5.13917. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 5.01435/5.13579. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.00696/5.19717. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.01090/5.14128. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.00953/5.16063. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.01443/5.14281. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.01377/5.12668. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.99917/5.17061. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.00168/5.18290. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 5.01136/5.15518. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.00028/5.16979. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.00695/5.15222. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 5.00272/5.16550. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.00428/5.14921. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.00001/5.18012. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.00266/5.17317. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.99949/5.17520. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.06262242910851495\n",
      "Epoch 0, Loss(train/val) 4.84066/4.81933. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.79830/4.80287. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.79329/4.79705. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79042/4.79267. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79147/4.79264. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79087/4.78924. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.78963/4.78793. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.78634/4.78841. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.78925/4.78789. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.78207/4.78505. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.78885/4.78346. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.78226/4.78950. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.78494/4.79108. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.77785/4.79023. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.78203/4.80090. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.78027/4.79534. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78142/4.79650. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.78165/4.80228. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.77974/4.80624. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.77924/4.80582. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.78048/4.80896. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.77623/4.80950. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.77732/4.81383. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.77841/4.80625. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.77938/4.80639. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.77471/4.81429. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.77519/4.81389. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.77363/4.81220. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.77745/4.82063. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.77482/4.81981. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 4.77671/4.80995. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.77216/4.82794. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.77689/4.82230. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.77542/4.82694. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.77507/4.82118. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.77198/4.82714. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.77287/4.82417. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.76949/4.83039. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.77317/4.82242. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.77362/4.83622. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.76950/4.84225. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.77249/4.82697. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.77130/4.81101. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.78100/4.81206. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.77734/4.80973. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.77263/4.81706. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77325/4.82063. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77480/4.80798. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77712/4.80874. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.77645/4.82792. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77672/4.81557. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77440/4.82317. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.77388/4.82917. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.77274/4.83523. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.76652/4.83791. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.77167/4.82860. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.77198/4.83650. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.76962/4.84202. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76783/4.85175. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.76923/4.84970. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.76886/4.84990. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.77036/4.83283. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.76793/4.84148. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.76637/4.83898. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.76706/4.84353. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.76621/4.85204. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.76770/4.82980. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.77923/4.81383. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.77341/4.81240. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.76889/4.83274. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.76706/4.83816. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.77089/4.84322. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76755/4.84121. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.76924/4.84150. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.76671/4.85668. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.76827/4.83956. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.76576/4.86046. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.76097/4.84571. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 4.76991/4.84609. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.76773/4.85305. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.77023/4.83536. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.76533/4.84660. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.76154/4.85858. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76697/4.87028. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.76290/4.85310. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.76930/4.84401. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.76643/4.84067. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.76978/4.83373. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.76732/4.83905. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.76783/4.82742. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.77012/4.83514. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.76345/4.85275. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.76537/4.85696. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.76611/4.85350. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.75863/4.84387. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.77021/4.82802. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.76622/4.83474. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.76291/4.85709. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.76225/4.84743. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.75572/4.85992. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.02834008097165992\n",
      "Epoch 0, Loss(train/val) 4.97575/4.95776. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.95688/4.95314. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.95545/4.96874. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.96497/4.96418. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.97662/4.94904. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.96603/4.95868. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.95312/4.94474. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.95586/4.94545. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.95404/4.94524. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.95380/4.94277. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.95436/4.94245. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.95383/4.94139. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.95560/4.94158. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.95580/4.94368. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.95218/4.93871. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.95438/4.93877. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.95313/4.93853. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 4.95288/4.94153. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.95046/4.94034. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.95037/4.93839. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.94749/4.93825. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.95029/4.93943. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.95018/4.94279. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.94122/4.93227. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.94819/4.93820. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.94408/4.93875. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.94054/4.93509. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.94231/4.93100. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.94312/4.92058. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.94060/4.93167. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.94363/4.93085. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.94335/4.93151. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.93712/4.93190. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.93393/4.92749. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.93574/4.92781. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.93890/4.93733. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.93516/4.94011. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.93147/4.93663. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.93388/4.93382. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.93458/4.93270. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.93973/4.93282. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.93825/4.93038. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.93448/4.92490. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.93422/4.92563. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.92908/4.92668. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.93441/4.93305. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.93161/4.92663. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.92707/4.92784. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.93169/4.93972. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.93099/4.94335. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.92759/4.94535. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.92527/4.92985. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.92445/4.94301. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.93199/4.93644. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.92322/4.94268. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.92298/4.97579. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.93112/4.95508. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.93011/4.94979. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.93112/4.93734. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.93723/4.93703. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.92881/4.93138. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.92918/4.93277. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.93405/4.93423. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.92821/4.94707. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.93488/4.93641. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.92638/4.94090. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.92865/4.93650. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.92626/4.94091. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.92650/4.94105. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.92719/4.93787. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.92330/4.94095. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.92334/4.94762. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.93309/4.93171. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.94241/4.92482. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.93451/4.91261. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.93863/4.94414. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.93919/4.92099. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 4.93468/4.93429. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.92800/4.92931. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.92997/4.93263. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.93331/4.93145. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.92953/4.93202. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.93194/4.93563. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.92563/4.92976. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.92843/4.92672. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.93021/4.93854. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.92653/4.92006. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.93289/4.93245. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.92209/4.92998. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.92967/4.93743. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.92317/4.93572. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.92066/4.92415. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.92675/4.93889. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.92500/4.93839. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.92337/4.92437. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.93116/4.93257. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.92573/4.93704. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.91998/4.93998. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.92581/4.93438. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.92600/4.92786. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.07100716024967263\n",
      "Epoch 0, Loss(train/val) 4.93517/4.95450. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.85283/4.84827. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.84259/4.82809. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.84119/4.81429. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.84038/4.81313. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.83796/4.81622. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.83950/4.81651. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.83513/4.82119. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.83735/4.81423. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.84068/4.80790. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.83715/4.79922. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.83626/4.79423. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.83367/4.79127. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.82956/4.78785. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.83298/4.79105. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.83346/4.78931. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.82830/4.79547. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.82931/4.80048. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.82887/4.80255. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.82580/4.78865. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.82532/4.79786. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.82535/4.79187. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.83042/4.79878. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.82218/4.81077. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.82751/4.80685. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.82858/4.80041. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.82431/4.79511. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.82422/4.79356. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.81990/4.79831. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.82502/4.81161. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.83615/4.80658. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.83017/4.79403. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.82926/4.80051. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.81942/4.80594. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.82210/4.80627. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.82436/4.81781. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.81972/4.81306. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.82066/4.81967. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.82460/4.81536. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.81355/4.83518. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.81339/4.83581. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.81795/4.85120. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.81594/4.83302. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.81871/4.81715. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.81737/4.82471. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.82215/4.82248. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.81584/4.81681. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.81716/4.81053. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.81630/4.80795. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.81737/4.81869. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.80899/4.80839. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.81350/4.81973. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.81663/4.81103. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.81202/4.81756. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.81245/4.83385. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.80908/4.82925. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.81210/4.81372. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.82787/4.83109. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.82917/4.82057. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.82395/4.81900. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.82758/4.81510. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.82299/4.81198. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.81834/4.81640. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.81732/4.82571. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.81978/4.81439. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.81914/4.82292. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.81770/4.81458. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.81816/4.83399. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.81866/4.81599. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.81465/4.82645. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.81975/4.82159. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.81545/4.82541. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.81447/4.81224. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.81049/4.82224. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.81671/4.82686. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.81615/4.81452. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.80970/4.81396. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.81741/4.82162. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.81494/4.82193. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.80975/4.82529. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.81170/4.81951. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.81654/4.81472. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.80985/4.82116. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.81439/4.81396. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.81077/4.81965. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.80973/4.82818. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.81132/4.82826. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.81432/4.81726. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.80820/4.82170. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.80839/4.82473. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.81265/4.81616. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.80990/4.81622. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.81121/4.81038. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.81504/4.81145. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.80867/4.82318. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.81560/4.82471. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.80628/4.81195. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.81349/4.81801. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.80719/4.83678. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.80825/4.81369. Took 0.09 sec\n",
      "ACC: 0.53125, MCC: 0.051799406700962544\n",
      "Epoch 0, Loss(train/val) 4.64983/4.58725. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.59863/4.58838. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.58916/4.58248. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.58768/4.58078. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.59009/4.58455. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.58807/4.58029. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.58484/4.59131. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.58838/4.58800. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.58809/4.58948. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.58581/4.60939. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.59223/4.59782. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.58983/4.59002. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.58632/4.59010. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.58384/4.58876. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.58107/4.58551. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.57730/4.58374. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.57981/4.58480. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.57877/4.58232. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.57928/4.58121. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.57445/4.58019. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.59065/4.60188. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 4.58768/4.59547. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.58527/4.58885. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.58485/4.58804. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.58182/4.60723. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.59290/4.58914. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.58575/4.58586. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.58359/4.58844. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.57981/4.58882. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.57983/4.58458. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.57860/4.58081. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.57629/4.57707. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.58054/4.57646. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.57857/4.57096. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.57684/4.57387. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.57371/4.57621. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.57070/4.57473. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.57128/4.57538. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.57433/4.57718. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.57203/4.57603. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.57066/4.56764. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.57274/4.56182. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.57085/4.56774. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.57099/4.56761. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.57200/4.57275. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.57133/4.57301. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.56368/4.56895. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.56790/4.57871. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.56883/4.56762. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.57049/4.57865. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.56751/4.57729. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.57079/4.58056. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.56890/4.58171. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.56598/4.57657. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.57205/4.56489. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.56943/4.55648. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.56696/4.57890. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.56784/4.56654. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.56483/4.57514. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.55800/4.57644. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.56526/4.56994. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.56530/4.57752. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.56153/4.57093. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.56665/4.57381. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.56494/4.57523. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.56482/4.58263. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.56050/4.57551. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.56433/4.58398. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.56787/4.58022. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.57061/4.58332. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.56484/4.58501. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.55581/4.58705. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.56924/4.58389. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.56073/4.57993. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.56059/4.58955. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.56340/4.58065. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.56018/4.57908. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.56713/4.57767. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.55575/4.57008. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.56334/4.58035. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.56024/4.57594. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.55145/4.56739. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.56852/4.57781. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.55885/4.58198. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.55947/4.58215. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.56287/4.57610. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.55833/4.56584. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.55968/4.56697. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.55526/4.56329. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.56165/4.58286. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.56332/4.58110. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.55638/4.56510. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.56370/4.57853. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.56223/4.56087. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.55957/4.58129. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.55890/4.57461. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.55530/4.57695. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.55762/4.58100. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.55757/4.57605. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.55766/4.57536. Took 0.09 sec\n",
      "ACC: 0.515625, MCC: 0.06052275326688024\n",
      "Epoch 0, Loss(train/val) 4.88826/4.83825. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.84117/4.83263. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.83766/4.83102. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.83639/4.83053. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.83717/4.83011. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.83225/4.82968. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.83390/4.83010. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.83593/4.82872. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.83470/4.82772. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.83193/4.82815. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.83496/4.82868. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.83268/4.83951. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.83158/4.84224. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.82956/4.83893. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.82733/4.83386. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.82760/4.83346. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.82806/4.83755. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.83611/4.84564. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.83086/4.84641. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.82885/4.84064. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.82820/4.84023. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.82776/4.83793. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.82759/4.84253. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.82722/4.83981. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.82535/4.84124. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.82527/4.83899. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.82238/4.83330. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82357/4.83578. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.82119/4.83490. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.82051/4.83445. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.81895/4.83665. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.82762/4.84614. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.82290/4.83701. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.81852/4.83531. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.82943/4.82570. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.82229/4.83035. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.82001/4.83076. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.81687/4.83110. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.81516/4.82713. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.81973/4.82894. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.81765/4.82690. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.81857/4.81971. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.81354/4.84064. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.81705/4.85085. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.81905/4.83839. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.81719/4.82792. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.81349/4.82947. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.81658/4.83808. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.81367/4.82935. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.81264/4.84432. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.81020/4.84108. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.81138/4.84243. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.81590/4.83758. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.82008/4.83299. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.81461/4.84173. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.81580/4.83793. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.81821/4.83783. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.82055/4.83874. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.82127/4.86568. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.82125/4.85648. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.81525/4.86173. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.81534/4.86486. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.80836/4.87079. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.81247/4.86918. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.81478/4.87393. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.81218/4.87746. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.81249/4.87627. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.81271/4.87145. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.81097/4.88496. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.81078/4.87496. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.81432/4.87904. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.81664/4.87857. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.81738/4.85262. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.81661/4.85154. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.81815/4.86484. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.81518/4.85181. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.80955/4.85852. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.81391/4.86966. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.80806/4.86928. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.80398/4.87702. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.80286/4.88002. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.81042/4.86434. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.80982/4.86771. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.80731/4.86652. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.80387/4.87894. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.80521/4.87726. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.80561/4.86872. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.80855/4.87187. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.80524/4.88039. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.81154/4.86763. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.81411/4.86395. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.81299/4.86137. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.80295/4.86704. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.80662/4.87491. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.79983/4.87076. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.80475/4.87471. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.81097/4.87584. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.80260/4.85154. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.80340/4.86191. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.80163/4.86541. Took 0.08 sec\n",
      "ACC: 0.578125, MCC: 0.1777495495783369\n",
      "Epoch 0, Loss(train/val) 4.88879/4.85362. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.83571/4.83086. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.83174/4.82550. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.83555/4.82298. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.83404/4.82278. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.82700/4.82049. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.83068/4.82485. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.82648/4.82702. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.82847/4.82522. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.82741/4.82655. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.82405/4.83103. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.82644/4.82912. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.82607/4.83078. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.82591/4.83065. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.82732/4.83064. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.82275/4.83037. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.82178/4.83285. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.81921/4.83850. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.82462/4.83984. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.82399/4.83605. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.81771/4.84421. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.81985/4.84517. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.82156/4.84470. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.81892/4.84926. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.82750/4.82635. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.81982/4.85413. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.81965/4.84512. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82450/4.84003. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.81767/4.84154. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.82242/4.82708. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.82434/4.82561. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.82382/4.82620. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.81952/4.83227. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.81902/4.83364. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.82070/4.83080. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.82170/4.84332. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.82250/4.83816. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.81812/4.83792. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.81821/4.83693. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.81950/4.83749. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.81427/4.83637. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.82010/4.83083. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.81554/4.83919. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.81535/4.83596. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.81299/4.83701. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.81159/4.85675. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.82077/4.84127. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.81533/4.84001. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.81318/4.85037. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.81225/4.83991. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.80984/4.84935. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.81086/4.84813. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.80692/4.86379. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.80223/4.87201. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.81301/4.84831. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.81200/4.85373. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.80671/4.85226. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.80424/4.86379. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.80222/4.85845. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.80784/4.84284. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.80513/4.88119. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.80948/4.86887. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.80118/4.88429. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.79659/4.91426. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.80620/4.86253. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.79372/4.89886. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.79771/4.86926. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.80670/4.86778. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.79902/4.87984. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.80132/4.86908. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.80236/4.85665. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.79566/4.88429. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.79573/4.88443. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.79685/4.87112. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.79466/4.86996. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.78986/4.88058. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.79566/4.86741. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.79465/4.88377. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.79462/4.85880. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.79081/4.92239. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.79529/4.87784. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.79764/4.86492. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.79482/4.90556. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.78896/4.89508. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.79536/4.85119. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.79181/4.87708. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.78706/4.91296. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.78847/4.88290. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.78782/4.87999. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.78695/4.92243. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.78762/4.85732. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.78932/4.89422. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.78493/4.86724. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.79455/4.89251. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.78658/4.87290. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.78613/4.88149. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 4.78574/4.87277. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.78642/4.87055. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.78257/4.90967. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.78221/4.87667. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.0657512425947884\n",
      "Epoch 0, Loss(train/val) 5.06830/5.02442. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.98643/5.02859. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.99122/5.05419. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.98971/5.05867. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.98635/5.05248. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.98656/5.05716. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.98350/5.04866. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.98458/5.05956. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.98035/5.04711. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.98407/5.04865. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.98479/5.05638. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.98342/5.06431. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.97828/5.05461. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.98353/5.04728. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.98509/5.03329. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.98453/5.02930. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.98294/5.03959. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.98157/5.04437. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.98057/5.02371. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.97679/5.02978. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.97745/5.04277. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.98279/5.03245. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.97761/5.03360. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.97721/5.03867. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.97651/5.03923. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.97444/5.04167. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.97459/5.04199. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.97524/5.03615. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.97469/5.02987. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.97537/5.02339. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.97368/5.02719. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.97240/5.03000. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.96817/5.03790. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.96883/5.03428. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.96664/5.03761. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.96535/5.03863. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.97271/5.03360. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.97306/5.04066. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.97145/5.04535. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.96842/5.04179. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.97005/5.04916. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.96726/5.04036. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.96462/5.07039. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.96820/5.04650. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.96839/5.03454. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.96598/5.10131. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.96339/5.05119. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.96507/5.03474. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.96514/5.07639. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.96155/5.07595. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.96958/5.03874. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.96556/5.07272. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.96393/5.06097. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.97337/5.01489. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.96965/5.06293. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.96372/5.08842. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.96521/5.05419. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.96564/5.05188. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.96442/5.05872. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.96449/5.06850. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.96125/5.08478. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.96593/5.06815. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.96177/5.06723. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.96265/5.09190. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.96002/5.09192. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.96130/5.08783. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.96296/5.06772. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.96284/5.04714. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.96994/4.99557. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.97411/5.06503. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 4.96170/5.06035. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.96443/5.08732. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.95974/5.06933. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.95899/5.06028. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.96091/5.08539. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.95821/5.08744. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.95995/5.08301. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.95470/5.09766. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.95500/5.08653. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.96075/5.05801. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.95828/5.12241. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.95983/5.09193. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.96013/5.04751. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.95601/5.09784. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.96279/5.08119. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.95815/5.08186. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.95157/5.05038. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.96615/5.03190. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.97339/5.05889. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.96342/5.06933. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.95914/5.09506. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.95770/5.06458. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.95447/5.09864. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.96330/5.05644. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 4.96605/5.09501. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.95495/5.06121. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.95970/5.06296. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.95724/5.10502. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.95603/5.06385. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.96121/5.08993. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.051799406700962544\n",
      "Epoch 0, Loss(train/val) 4.70426/4.61692. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.62730/4.63399. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.62604/4.63874. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.62455/4.64143. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.62362/4.64248. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.62121/4.64847. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.62110/4.65274. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.62100/4.65142. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.61911/4.64812. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.61714/4.65003. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.61656/4.66171. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.61635/4.65680. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.61561/4.65745. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.61488/4.66838. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.61383/4.67541. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.61457/4.68006. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.61865/4.67083. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.61486/4.68255. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.61243/4.67635. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.61366/4.66853. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.60987/4.67281. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.60840/4.66743. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.60975/4.67511. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.61399/4.69544. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.60928/4.67926. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.61147/4.67494. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 4.60652/4.67568. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.60728/4.68943. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.61057/4.67646. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.60650/4.68153. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.60890/4.67306. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.60237/4.69097. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.60607/4.67561. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.60419/4.68062. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.60664/4.67592. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.60172/4.68221. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.60555/4.67682. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 4.60731/4.66941. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.59681/4.70357. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.60626/4.69887. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.61101/4.66449. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.60579/4.67780. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.60454/4.67613. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.60704/4.66253. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.59835/4.68981. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.59807/4.70104. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.60468/4.67684. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.59757/4.70950. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.59935/4.67876. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.60129/4.68263. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.59805/4.70112. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.60300/4.68009. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.60138/4.67982. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.60543/4.66036. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.61192/4.64285. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.61027/4.65021. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.60355/4.67525. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.60459/4.67109. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.59759/4.68539. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.59887/4.67413. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.60175/4.65712. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.60062/4.68049. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.59235/4.67174. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.59513/4.69758. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.59707/4.65534. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.59443/4.68367. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.59869/4.66153. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.59985/4.67349. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.59334/4.68370. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.59229/4.67076. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.59141/4.67471. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.60083/4.65802. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.59815/4.66886. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.59665/4.67543. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.59609/4.66395. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.59152/4.67547. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.59159/4.67913. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.58924/4.69264. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.59183/4.66714. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 4.59411/4.67177. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.58848/4.69519. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.59463/4.63853. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.59165/4.69066. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.59049/4.66142. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.59637/4.68225. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.59277/4.67501. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.58717/4.67371. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.58123/4.67371. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.58708/4.68364. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.59080/4.68201. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.59141/4.65949. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.59312/4.66140. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.59075/4.67339. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.58426/4.69283. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.58805/4.66068. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.58684/4.68099. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.58709/4.66418. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.58624/4.71233. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.58288/4.66695. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.58203/4.71679. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.02404930351219409\n",
      "Epoch 0, Loss(train/val) 4.91639/4.84072. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.83492/4.84552. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.83546/4.84744. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.83381/4.84732. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.83457/4.84682. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.83413/4.85262. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.83398/4.85260. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.83262/4.85124. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.83134/4.85784. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.83372/4.86105. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.83181/4.86183. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.83127/4.86077. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.83286/4.85882. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.82983/4.85882. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.83126/4.85681. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.82832/4.86232. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.82615/4.86331. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.82638/4.86222. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.82479/4.87228. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.82486/4.86687. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.82266/4.87223. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.82420/4.86843. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.82438/4.86925. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.82232/4.87291. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 4.82474/4.87210. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.82245/4.87926. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.82481/4.87554. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82011/4.87974. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.82383/4.87445. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.81943/4.88318. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.81958/4.88039. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.82257/4.87097. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.82295/4.87778. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.81987/4.88503. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.82317/4.87799. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.82044/4.88965. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.81970/4.88198. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.82106/4.87859. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.82231/4.85831. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.82541/4.87092. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.82304/4.87498. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.82301/4.87499. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.82256/4.88269. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 4.82066/4.87665. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.81876/4.87833. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.82061/4.87220. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.82221/4.87320. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.81644/4.87839. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.81758/4.88629. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.81918/4.87885. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.81948/4.87294. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.81892/4.88015. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.81902/4.88219. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.81308/4.87905. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.81566/4.88176. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.81481/4.88822. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.81229/4.88970. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.81732/4.88614. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.81388/4.89139. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.81427/4.88006. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.81667/4.89814. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.81341/4.89876. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.81535/4.88678. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.81253/4.89132. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.81173/4.89639. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.80797/4.90780. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.81516/4.91413. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.81579/4.89984. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.81370/4.90451. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.81152/4.91305. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.80996/4.89412. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.80998/4.90647. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.80957/4.91223. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.81108/4.91172. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.80754/4.91687. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.80825/4.91182. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.80549/4.91487. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.80908/4.92455. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.80659/4.90839. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.80852/4.93485. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.80680/4.91449. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.80328/4.92942. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.80522/4.93083. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.80201/4.93108. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.80201/4.93821. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.81374/4.89625. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.80428/4.92096. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.80381/4.91788. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.80772/4.91049. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.80323/4.92423. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.80280/4.92312. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.80571/4.91826. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.80437/4.93664. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.80336/4.92665. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.80349/4.94329. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.79876/4.91523. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.80265/4.91288. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.80508/4.94047. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.79736/4.92587. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.79942/4.94227. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.03328442159263018\n",
      "Epoch 0, Loss(train/val) 4.87964/4.76380. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.78424/4.79426. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.77762/4.78457. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.77564/4.77976. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.77134/4.78237. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.77002/4.78807. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.77099/4.79101. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.76947/4.79750. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.76996/4.80129. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.77034/4.80007. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.77085/4.80144. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.76709/4.80509. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.76553/4.80708. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.76444/4.81096. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.76446/4.81600. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.76393/4.82778. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.76413/4.82054. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.76600/4.83251. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.76084/4.81822. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.76342/4.81542. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.76120/4.82306. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.76328/4.81642. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.75917/4.82041. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.76165/4.82448. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.75945/4.82779. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.76242/4.81554. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.75827/4.83281. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.75972/4.84118. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.76004/4.82810. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.75974/4.83612. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.76123/4.83818. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.76142/4.80812. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.76107/4.79283. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.75951/4.80199. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.76025/4.80681. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.75757/4.80693. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.76005/4.81021. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.75736/4.80272. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.75707/4.81033. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.75642/4.81194. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.75359/4.81623. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.75816/4.80888. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.75547/4.81086. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.75634/4.80798. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.76074/4.80333. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.75566/4.80926. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.75394/4.81269. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.75618/4.81660. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.75276/4.81002. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.75658/4.80917. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.74844/4.83039. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.74935/4.83833. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.75212/4.81157. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.75167/4.82340. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.75106/4.82649. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.74927/4.83188. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.74815/4.85465. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.74737/4.85081. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.74865/4.82859. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.74694/4.85641. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.75154/4.84211. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.74378/4.85666. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.75003/4.83519. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.74963/4.84042. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.74424/4.85627. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.74948/4.84077. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.74918/4.83709. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.74390/4.85107. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.74259/4.85376. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.74152/4.85664. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.74871/4.85047. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.74687/4.85757. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.74277/4.85505. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.74437/4.87296. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.74165/4.86825. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.74258/4.86344. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.74547/4.85537. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.73590/4.86119. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.74562/4.83494. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.74248/4.84426. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.74047/4.86833. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.74146/4.86963. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.74214/4.85986. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.73165/4.89168. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.74311/4.86878. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.74322/4.84912. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.74106/4.87783. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.74468/4.87311. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.74196/4.86640. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.74398/4.85585. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.73946/4.85481. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.73737/4.85970. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.73512/4.87486. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.75050/4.88123. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.74295/4.86687. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.76032/4.85432. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.75032/4.85611. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.75150/4.85517. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.74420/4.85700. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.74194/4.87539. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.03126526997403612\n",
      "Epoch 0, Loss(train/val) 5.16388/5.06484. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.05974/5.10757. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.06634/5.13636. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.07297/5.14283. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 5.07255/5.12129. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.07593/5.09234. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.07339/5.08441. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 5.06912/5.08393. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.06270/5.08472. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.06089/5.09506. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 5.06348/5.09222. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.06266/5.08791. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.06164/5.08620. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.06117/5.08583. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.05685/5.09035. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 5.05909/5.08696. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.06092/5.08807. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.05715/5.09142. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 5.05628/5.09417. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.05714/5.09169. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.05578/5.09960. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 5.05546/5.09451. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.05148/5.10155. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.05386/5.09418. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 5.05367/5.10572. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.05444/5.09521. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.05406/5.09733. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.05305/5.09747. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.05220/5.09285. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 5.05220/5.09732. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.05171/5.10313. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.04895/5.10424. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 5.04936/5.11246. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.05171/5.09566. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.05300/5.10464. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 5.04848/5.09934. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.05094/5.10219. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 5.04878/5.10832. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.04707/5.10238. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 5.04984/5.10123. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.04823/5.11345. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 5.04859/5.09861. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.04985/5.10960. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.04641/5.10782. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.04609/5.12649. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.05071/5.10878. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.04801/5.11335. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 5.04883/5.10163. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.04922/5.09762. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.04345/5.12773. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 5.04635/5.10375. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.04214/5.12185. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.04904/5.08768. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.04809/5.09275. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.04872/5.09577. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 5.04704/5.09475. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 5.04656/5.10264. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.04754/5.10227. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 5.04639/5.09920. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 5.04263/5.10829. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 5.04524/5.12024. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.04331/5.11837. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.04347/5.10241. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.04515/5.11020. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.04310/5.11275. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.06426/5.08586. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.05460/5.07266. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 5.04831/5.10650. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.04550/5.11140. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.05892/5.08178. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.05046/5.08514. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.04404/5.10882. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.05309/5.09210. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.05053/5.09865. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 5.04171/5.11731. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.05089/5.08491. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.04880/5.07963. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.04800/5.10128. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.04054/5.11134. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.04749/5.08535. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.04443/5.11420. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 5.04928/5.09457. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.04334/5.11380. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.03927/5.11907. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 5.04160/5.11807. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.04862/5.10120. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.04350/5.10766. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 5.03879/5.11064. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 5.04033/5.12799. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 5.04095/5.12351. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 5.04097/5.12017. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 5.04515/5.11269. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 5.03944/5.11840. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 5.03957/5.12612. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 5.03760/5.11466. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 5.04042/5.12797. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 5.03956/5.11808. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 5.03694/5.13020. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 5.03175/5.13136. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 5.04058/5.13292. Took 0.09 sec\n",
      "ACC: 0.53125, MCC: 0.040082172520323485\n",
      "Epoch 0, Loss(train/val) 4.91161/4.86178. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.84140/4.83916. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.83989/4.83837. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.83841/4.83857. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.83581/4.83998. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.83649/4.84180. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.83525/4.84555. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.83736/4.85019. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.83443/4.85117. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.83315/4.85360. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.83372/4.85267. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 4.83363/4.85277. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.83062/4.85761. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.82887/4.86119. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.83277/4.85552. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.82656/4.85852. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.82990/4.85118. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.82640/4.85302. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.82294/4.84337. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.82585/4.85658. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 4.82621/4.84816. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.82598/4.85307. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.82339/4.84514. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.82184/4.84193. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.82471/4.84559. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.82402/4.83864. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.82491/4.84432. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82273/4.84790. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.81628/4.83568. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.82109/4.82481. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.81755/4.83339. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.81651/4.83157. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.81534/4.82537. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.81558/4.82691. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.81522/4.82511. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.81400/4.82159. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.80686/4.83458. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.81071/4.82080. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.80487/4.82184. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.81675/4.81922. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.80843/4.82751. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.80104/4.83320. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.80900/4.82642. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.80296/4.82772. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.80818/4.82383. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.79904/4.83556. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.80103/4.81607. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.80316/4.83068. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.80389/4.83096. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.79592/4.83169. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.79738/4.82455. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.80354/4.82333. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.80077/4.84905. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.80214/4.82969. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.79828/4.83609. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.79977/4.83424. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.79699/4.83647. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.80110/4.83553. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.79857/4.83604. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.79786/4.83707. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.79644/4.84809. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.79636/4.84796. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.79326/4.84984. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.79004/4.85243. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.79705/4.83514. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.80124/4.82881. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.79457/4.83935. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.78992/4.83881. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.78176/4.85817. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.78746/4.85934. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.79306/4.84380. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.79553/4.84516. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 4.78530/4.84985. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.78624/4.85714. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.78688/4.84282. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.78982/4.84863. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78609/4.84279. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78434/4.85899. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.78810/4.83584. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.78539/4.86336. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.78781/4.85311. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.78296/4.84664. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.79097/4.85874. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.78990/4.83984. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.77927/4.87144. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.78909/4.84073. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.77610/4.87942. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.78049/4.86646. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.77513/4.86261. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.78057/4.86965. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.78639/4.84750. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.77037/4.85006. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.78723/4.85472. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.77979/4.86131. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.77823/4.87022. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.78266/4.84835. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.77614/4.86361. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.77216/4.86568. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.76931/4.90075. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.78106/4.84729. Took 0.08 sec\n",
      "ACC: 0.5, MCC: -0.007889684472185849\n",
      "Epoch 0, Loss(train/val) 4.94111/4.88142. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.88416/4.87829. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.88194/4.87821. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.87947/4.88532. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.88177/4.88704. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.88214/4.88685. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.87903/4.88932. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.87489/4.89250. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.87438/4.88690. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.87598/4.89398. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.87398/4.89524. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.87463/4.89409. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.87417/4.89488. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.87312/4.89578. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.87687/4.89485. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.87254/4.90907. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.86926/4.89850. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.86944/4.90041. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.87060/4.90288. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.86845/4.90903. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.87048/4.90733. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.86729/4.90935. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.86731/4.90296. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.87264/4.91042. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.86930/4.89935. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.86504/4.91500. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.86807/4.91133. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.86556/4.90420. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.86142/4.92163. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.86590/4.91030. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.86202/4.92180. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.86615/4.91224. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.85975/4.91954. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.86488/4.90354. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.85957/4.91914. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.85701/4.91274. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.85751/4.90751. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.86428/4.90492. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86040/4.89771. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.85679/4.91256. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.85488/4.90445. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.85626/4.91836. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.85709/4.90786. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.85537/4.91618. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.85727/4.92215. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.85542/4.90687. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.85277/4.92639. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.85213/4.91914. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.85404/4.91647. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.85282/4.90867. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.85644/4.90911. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.84867/4.92668. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.85203/4.90877. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.85557/4.92663. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.85417/4.90876. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.84984/4.91481. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.85631/4.93355. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.85177/4.92567. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 4.85490/4.92375. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.85182/4.92776. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.84709/4.92488. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 4.84828/4.92632. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.84292/4.93098. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.85079/4.91174. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.84666/4.90498. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.84996/4.92029. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.84672/4.90427. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.84716/4.92823. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.84479/4.92150. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.84903/4.93401. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.84851/4.93138. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.84168/4.92954. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.84623/4.90660. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.85079/4.92525. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.84830/4.90734. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.84822/4.90465. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.84105/4.91333. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.85151/4.90705. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.84237/4.91286. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.84174/4.93333. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.84820/4.93943. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.85287/4.92653. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.84668/4.93809. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.85558/4.91988. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.85076/4.92127. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.84435/4.93043. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.84200/4.92980. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.84981/4.91532. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.84443/4.92768. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.84575/4.93244. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.84157/4.93030. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.84647/4.92786. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.83961/4.93383. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.84279/4.93567. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.84700/4.91639. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.84727/4.91145. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.84624/4.92708. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.84307/4.92941. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.84548/4.92769. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.84346/4.93872. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.01642880193633814\n",
      "Epoch 0, Loss(train/val) 4.90990/4.82557. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.81253/4.84040. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.81529/4.82905. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.81904/4.82339. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.81267/4.82479. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.81036/4.82929. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.81187/4.83208. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.81420/4.83111. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.80795/4.83716. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.80920/4.83616. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.81004/4.83890. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.80849/4.83703. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.80752/4.83343. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.80743/4.83369. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.81156/4.82715. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.81745/4.83330. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.81538/4.85460. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.81374/4.87352. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.81182/4.87119. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.81185/4.87060. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.80630/4.86724. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.80898/4.87109. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.80742/4.87263. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.81072/4.87388. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.80862/4.87166. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.80594/4.86587. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.80467/4.87110. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.80289/4.86446. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.80554/4.86613. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.80652/4.86476. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.81088/4.86840. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.80917/4.86723. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.80344/4.87656. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.80327/4.88015. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.80732/4.87985. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.80175/4.87241. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.79987/4.88061. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.80228/4.90021. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.80909/4.86069. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.81069/4.84649. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.80594/4.85340. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.80336/4.86537. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.80152/4.86565. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.80168/4.88102. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.80387/4.86211. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.80860/4.85743. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.80687/4.85140. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.80233/4.84903. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.80178/4.85375. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.79881/4.86375. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.80525/4.85521. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.80092/4.84811. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.80033/4.85622. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.80057/4.84570. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.79839/4.84805. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.79798/4.85039. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.80026/4.84607. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.79696/4.84822. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.79516/4.85333. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.79070/4.85786. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.79872/4.83144. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.79476/4.86681. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.79727/4.85855. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.79737/4.85294. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.79469/4.84284. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.80142/4.86429. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.79639/4.84322. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.79419/4.84447. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.79381/4.84592. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.79186/4.84951. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.79528/4.85289. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78945/4.85066. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.79482/4.85724. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.79680/4.85205. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.79269/4.84360. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.79175/4.85364. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.79314/4.85115. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.79356/4.83690. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.78688/4.84959. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.78932/4.88956. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.80505/4.87607. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.79652/4.84609. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 4.79438/4.84936. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.79044/4.84416. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.78161/4.86019. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.79432/4.85588. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.78885/4.85263. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.78565/4.85012. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.79002/4.84886. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.78732/4.84895. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.79399/4.85501. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.78766/4.86086. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.78939/4.85110. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.78811/4.86013. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.78850/4.85075. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.78911/4.84846. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.78783/4.84496. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.78609/4.84807. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77544/4.84928. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.78333/4.84170. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.16012815380508713\n",
      "Epoch 0, Loss(train/val) 4.87606/4.80374. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.82739/4.82335. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.82621/4.82444. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.82512/4.82738. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.82627/4.82032. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.82386/4.81700. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.82333/4.81838. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.82077/4.82047. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.82118/4.81982. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.82210/4.81953. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.81940/4.81915. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.81794/4.82127. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.81594/4.81996. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.81808/4.81821. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.81509/4.81983. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.81585/4.81834. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.81245/4.81878. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.81299/4.81782. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.81403/4.81921. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.81259/4.82018. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.81439/4.81538. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.81135/4.81669. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.81191/4.81505. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.80730/4.81992. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.80806/4.81766. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.80762/4.81810. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.80670/4.81914. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.80640/4.82000. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.80608/4.82491. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.80717/4.82337. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.80700/4.82441. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.80687/4.82858. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.80527/4.82571. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.80529/4.82982. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.80330/4.82676. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.80205/4.82464. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.80238/4.82535. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.80501/4.82057. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.80022/4.83212. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.80413/4.82903. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.80205/4.83514. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.80219/4.82653. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.80026/4.82946. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.80094/4.83638. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.79833/4.83343. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.79812/4.82930. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.80052/4.81887. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.79995/4.82498. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.79690/4.84281. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.80188/4.83266. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.79787/4.83525. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.80178/4.81899. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.79519/4.83523. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.79780/4.82595. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.79928/4.83201. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.79618/4.83545. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.79261/4.83827. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.79957/4.82833. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.79577/4.83514. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.79543/4.83415. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.80147/4.81832. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.79120/4.83104. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.79629/4.82961. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.79375/4.83350. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.79417/4.83803. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.79998/4.82962. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.79386/4.85089. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.79604/4.82434. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.79222/4.86469. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.79600/4.82663. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.79298/4.84035. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.79254/4.84867. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.79305/4.82696. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.79146/4.82799. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.79248/4.82649. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.78973/4.85111. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.79775/4.82695. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.79286/4.83222. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.79131/4.82954. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.79185/4.85120. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.79427/4.84799. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.78936/4.83780. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.79082/4.83740. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.78542/4.87706. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.79067/4.83068. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.79359/4.85187. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.78925/4.84042. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.78500/4.84546. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.78790/4.84845. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.78468/4.86547. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.79297/4.83651. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.78904/4.85209. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.78588/4.84387. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.79474/4.84710. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.78875/4.85481. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.78452/4.84038. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.78383/4.84128. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.79005/4.86534. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.78577/4.82579. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.78517/4.82478. Took 0.08 sec\n",
      "ACC: 0.5, MCC: -0.002944085489318884\n",
      "Epoch 0, Loss(train/val) 4.81370/4.79588. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.76157/4.74201. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.74454/4.73047. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.73389/4.73295. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.73494/4.73925. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.73627/4.74062. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.73721/4.73580. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.73759/4.74083. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.73686/4.74188. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.73386/4.74279. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.73380/4.73366. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.73398/4.74324. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.73288/4.74831. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.73147/4.74299. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.72980/4.75654. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.72339/4.76030. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.72895/4.77261. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.72815/4.77536. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.72673/4.77862. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.72425/4.79264. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.72492/4.77325. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.72664/4.78656. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.72324/4.77338. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 4.72269/4.76721. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.72197/4.76212. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.71987/4.77165. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.72184/4.77339. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.72214/4.76677. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.71961/4.78570. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.72080/4.77872. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.72335/4.77630. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.71871/4.77238. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.71924/4.80564. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.71756/4.78446. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.71927/4.77242. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.71858/4.79906. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.71524/4.77462. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.72026/4.80471. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.71887/4.78229. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.71730/4.78504. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.71614/4.79124. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.71453/4.78842. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.72235/4.80884. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 4.72127/4.76491. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.71522/4.79139. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.71994/4.78606. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.71482/4.80151. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.71692/4.78780. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.71447/4.81507. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.71441/4.82327. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.71460/4.79787. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.71526/4.78789. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.71703/4.82408. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.71450/4.81665. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.70773/4.80206. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.72215/4.81443. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.71430/4.79636. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.71078/4.81831. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.71625/4.81621. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.70830/4.81831. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.71307/4.80430. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.71439/4.80204. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.71075/4.79972. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.71378/4.83049. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.71133/4.78310. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 4.71143/4.83075. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.70729/4.81177. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.70410/4.81789. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.70940/4.82428. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.70710/4.83929. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.70793/4.78321. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.70776/4.83883. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.70870/4.85834. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.70674/4.85194. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.71252/4.81113. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.71025/4.81486. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.70615/4.83233. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.70967/4.80922. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.71010/4.82419. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.71292/4.78708. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.70895/4.79803. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.70606/4.80915. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.70168/4.82844. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.70690/4.83809. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.70176/4.82567. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.70604/4.82734. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.70577/4.78985. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.70703/4.77333. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.70470/4.81537. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.70722/4.78915. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.70382/4.81056. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.70897/4.82697. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.70009/4.84529. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.70132/4.82274. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.70461/4.83581. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.70389/4.82866. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.70306/4.82603. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.70408/4.81691. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.70254/4.80876. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.69720/4.81352. Took 0.08 sec\n",
      "ACC: 0.40625, MCC: -0.17577515967558022\n",
      "Epoch 0, Loss(train/val) 4.83706/4.80748. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.78882/4.74509. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.78928/4.74240. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.78964/4.75282. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.78642/4.74907. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.78874/4.74624. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.78382/4.75119. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.78415/4.74923. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.78530/4.75059. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.78545/4.75252. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.78181/4.75595. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.78323/4.76891. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.78692/4.75969. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 4.78283/4.76267. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.78070/4.76471. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.78400/4.77720. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78681/4.78049. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.78650/4.81709. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.78696/4.82759. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.77860/4.82547. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.77444/4.82108. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.77983/4.80767. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.78317/4.82147. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.78046/4.81530. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.77982/4.82150. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.77611/4.82002. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.77631/4.81491. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.77643/4.83018. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77540/4.83672. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.77253/4.83308. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.77888/4.81866. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78099/4.80102. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.77652/4.79977. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.77615/4.80640. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.77546/4.80889. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.77381/4.82701. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.77899/4.81171. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.77453/4.83044. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.76917/4.82507. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.76936/4.82340. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.77060/4.82456. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.77028/4.83872. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.77198/4.83502. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.76883/4.85160. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.76824/4.83093. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.76901/4.83934. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.76691/4.83444. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77067/4.83513. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.76725/4.84076. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.76035/4.83709. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.76448/4.83284. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.76367/4.84085. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.76218/4.85700. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.76057/4.84891. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.77006/4.83270. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.76294/4.84985. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.76367/4.84754. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.76458/4.83289. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.75872/4.85755. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.76419/4.85002. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.75452/4.86339. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.76236/4.83038. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.77180/4.83212. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.77030/4.83020. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.77046/4.83558. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 4.75989/4.84293. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.76812/4.84064. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.75715/4.87289. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.76170/4.83730. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.75899/4.85766. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.75909/4.84525. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.75970/4.85349. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76403/4.84073. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.76292/4.85539. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.75951/4.85979. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.76174/4.85036. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.75663/4.84734. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.75987/4.86050. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.75889/4.85123. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.75631/4.84534. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.75695/4.86270. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 4.75757/4.84360. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.75921/4.85157. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76104/4.86049. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75944/4.84067. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.75458/4.85653. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.75777/4.85065. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.75793/4.86126. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.75046/4.85498. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.75949/4.84279. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.76069/4.88470. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.74955/4.90375. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.77070/4.87102. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.77188/4.82057. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.77041/4.84027. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.76761/4.82073. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.76683/4.84225. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.76732/4.83183. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.76619/4.84989. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.76641/4.83104. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.0778902520624173\n",
      "Epoch 0, Loss(train/val) 4.93451/4.86731. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.87040/4.90004. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 4.87542/4.91178. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.87270/4.91163. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.87603/4.90672. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.87610/4.89078. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.87875/4.87825. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.87099/4.88322. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.86783/4.88622. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.86516/4.88951. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.86296/4.90017. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.86359/4.90277. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.86063/4.90414. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.86337/4.90658. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.85924/4.91684. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.85939/4.91732. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.85994/4.91407. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.85733/4.92328. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.85311/4.93040. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85437/4.92817. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.85741/4.91684. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 4.85294/4.92782. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.85570/4.92805. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.85153/4.93688. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.85212/4.94052. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.85556/4.93272. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.85184/4.92730. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.84906/4.93874. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.84933/4.93346. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.84884/4.95011. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.84984/4.92940. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.84652/4.93393. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.84850/4.93845. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.84642/4.94052. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.84619/4.94122. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84837/4.94233. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.85833/4.92122. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.84482/4.93840. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84612/4.94614. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.85080/4.93799. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.84433/4.94902. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.84343/4.92881. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.84281/4.95130. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.84621/4.93982. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.85139/4.93019. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.85012/4.91932. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.84633/4.93732. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.84274/4.94382. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.84384/4.93607. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.84344/4.95643. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.84458/4.94229. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.84171/4.94473. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.84112/4.95127. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.84209/4.93647. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.83960/4.94143. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.84378/4.93203. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.84225/4.93550. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.84235/4.93327. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.84374/4.93202. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83904/4.93908. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.83696/4.95104. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.84713/4.92954. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.84066/4.94291. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.84169/4.94409. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.83907/4.94404. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.83550/4.95490. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.83914/4.95978. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.83835/4.95374. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.83585/4.94398. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83959/4.94698. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.83816/4.95357. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.84061/4.94756. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83647/4.95067. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.83887/4.94143. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.83324/4.95142. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.83564/4.95020. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.83885/4.95398. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.83414/4.95615. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.84218/4.95104. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.83841/4.93470. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83670/4.95967. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.83457/4.95764. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.83926/4.94265. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.83472/4.96204. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83497/4.96412. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83197/4.96281. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.83136/4.94254. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.83098/4.97168. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83135/4.96596. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.83266/4.97690. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.83481/4.96388. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.83117/4.95210. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83517/4.96316. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83378/4.97189. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.83771/4.95193. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.82818/4.95262. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.82944/4.97356. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.83220/4.97780. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.82766/4.98295. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.83257/4.97072. Took 0.09 sec\n",
      "ACC: 0.40625, MCC: -0.1814627428602745\n",
      "Epoch 0, Loss(train/val) 4.98056/5.01394. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.97258/4.98727. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.95574/4.97747. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.95511/4.97829. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.95373/4.97807. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.95577/4.98425. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.95406/4.98784. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.95262/4.99353. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.95050/4.99484. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.94816/5.00158. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.94478/5.00368. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.94258/5.00829. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.94521/5.00227. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.94429/5.01029. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.94536/5.01145. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.94238/5.00760. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.94253/5.00329. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.94306/5.01875. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.94652/5.00534. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.94112/4.99503. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.94151/5.00343. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.93991/5.02046. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.93984/5.01000. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.93980/5.01430. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.93604/5.01226. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.94018/5.01302. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.93486/5.02040. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.93882/5.01933. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.93582/5.03716. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.93653/5.03401. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.93656/5.02077. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.93376/5.03202. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.94058/5.01844. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.93451/5.01439. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.93658/5.02983. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.93685/5.01605. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.93030/5.03746. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.92871/5.06771. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.93679/5.02815. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.93395/5.01922. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.93498/5.02495. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.93448/5.02557. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.92806/5.04849. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.93442/5.03362. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.93063/5.05633. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.92826/5.04553. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.92720/5.05761. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.92457/5.07813. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.92721/5.05905. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.93337/5.05262. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.93423/5.01603. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.92426/5.07425. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.92757/5.05412. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.92679/5.06729. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.92926/5.06049. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.92671/5.05817. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.91522/5.10033. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.92250/5.04956. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.93116/5.02310. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.92113/5.06555. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.92137/5.06300. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.92492/5.03452. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.92446/5.07805. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.92839/5.02995. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.91282/5.09172. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.92571/5.04716. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.92836/5.03896. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.92119/5.05360. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.91880/5.07298. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.90928/5.08948. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.91595/5.06109. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.91795/5.05446. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.91626/5.07476. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.91058/5.08629. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.91435/5.09917. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.90909/5.10989. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.92366/5.07956. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.91615/5.08132. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.91533/5.08279. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.92043/5.08053. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.91933/5.09020. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.91596/5.07007. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.90703/5.08451. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.91072/5.06074. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.90589/5.09330. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.91059/5.06198. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.91601/5.06938. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.90775/5.09400. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.90439/5.07235. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.91583/5.08563. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.91136/5.09460. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.90495/5.11092. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.91460/5.07187. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.90242/5.13640. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.91087/5.05345. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.90044/5.16534. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.91335/5.05888. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.90389/5.13873. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.91835/5.05300. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.90790/5.11248. Took 0.08 sec\n",
      "ACC: 0.40625, MCC: -0.18866080156402737\n",
      "Epoch 0, Loss(train/val) 4.85234/4.82827. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.79439/4.78570. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.79584/4.78468. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.79945/4.78373. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.80080/4.78357. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79895/4.78644. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.79400/4.78625. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.79522/4.78781. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.79217/4.78732. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79156/4.78469. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79227/4.78478. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.79434/4.78542. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.79255/4.78791. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.79366/4.78813. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.78872/4.79003. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.78980/4.79769. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78505/4.79813. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.78921/4.79583. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.78779/4.79281. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.78786/4.79391. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.78661/4.80221. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78532/4.80194. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.78369/4.79865. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.78629/4.79520. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.78425/4.79677. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.78437/4.78618. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.78684/4.78862. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.78199/4.79938. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.78283/4.79547. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.78309/4.79747. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.78428/4.80121. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78155/4.79854. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.78053/4.79802. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78029/4.80202. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78030/4.79877. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.78871/4.79688. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.78246/4.79365. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.78230/4.81033. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.78150/4.81555. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.77942/4.81842. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.77900/4.82077. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.77311/4.82979. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.77867/4.82674. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.78153/4.80689. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.78310/4.81859. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.78002/4.82867. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.78263/4.82737. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77852/4.83369. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.77179/4.84960. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.77761/4.84669. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77373/4.83772. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77333/4.83083. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.77320/4.84123. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.77450/4.83986. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.77167/4.83832. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.77303/4.83659. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.76922/4.85948. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.77759/4.85194. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.77544/4.84005. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.77594/4.83524. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.76832/4.85345. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.77032/4.84974. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76902/4.84851. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.76826/4.86298. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.77100/4.84894. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.77274/4.85262. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.76804/4.85461. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.77478/4.84903. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.77444/4.84628. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.77454/4.84827. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.76906/4.85447. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.77224/4.86831. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76250/4.84674. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.77033/4.87052. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.77682/4.83885. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.76817/4.85480. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 4.77081/4.85211. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.76801/4.86573. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.76356/4.87878. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.76879/4.84412. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.77513/4.83777. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.77355/4.85893. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.76285/4.85487. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76427/4.86420. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.76447/4.85680. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.76533/4.86557. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.76797/4.86703. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.76722/4.85811. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.77514/4.85303. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.77032/4.86425. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.76945/4.86503. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.76587/4.86105. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.76198/4.85385. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.76852/4.86317. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.75887/4.88319. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.76365/4.87152. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.76530/4.85217. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.76550/4.84490. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.76874/4.85256. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.75703/4.88054. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.09379580992210836\n",
      "Epoch 0, Loss(train/val) 4.84911/4.78104. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.77967/4.77870. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.77226/4.78052. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.77231/4.78377. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.77209/4.78202. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.77036/4.78162. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.77123/4.78191. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.76785/4.78351. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.76779/4.78253. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.76511/4.78062. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.76788/4.78439. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.76626/4.78832. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.76841/4.78528. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.77071/4.78177. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.76884/4.78469. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.76683/4.78654. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.76742/4.78234. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.76470/4.78250. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.76194/4.78545. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.76566/4.78251. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.76710/4.77596. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.76441/4.77273. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.76220/4.77073. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.76116/4.77216. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.76216/4.77721. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.76225/4.77934. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.76323/4.78810. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.76225/4.78987. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.76243/4.79635. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.76338/4.79413. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.76395/4.78386. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.76348/4.77873. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.75853/4.77266. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.76373/4.78868. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.76465/4.78294. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.76326/4.77582. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.76266/4.77821. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.75688/4.78458. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.75551/4.78693. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.75379/4.78801. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.75424/4.79543. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.75725/4.78707. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.75060/4.79187. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.75788/4.78617. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.75391/4.79407. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.75305/4.79576. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.75735/4.79014. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.75497/4.79187. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.75725/4.78290. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.75268/4.78558. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.75354/4.78255. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.75585/4.79097. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.74938/4.80041. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.74915/4.79732. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.75281/4.78866. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.75037/4.79990. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.75127/4.80147. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.74849/4.80746. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.75050/4.79476. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.74985/4.80197. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.74457/4.81614. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.74583/4.80056. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.75199/4.80574. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.75158/4.80504. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.74943/4.80639. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.74567/4.80580. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.74889/4.81104. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.75055/4.81047. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 4.75175/4.80718. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.74564/4.80951. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.74627/4.80921. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.74518/4.80405. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.74127/4.81040. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.74406/4.82030. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 4.74398/4.82562. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.74934/4.82748. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.74820/4.80651. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.74525/4.81102. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.73998/4.82120. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.74773/4.80562. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.74751/4.81242. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.74301/4.82964. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.74050/4.81284. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.74583/4.81731. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.74281/4.80663. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.74295/4.81076. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.74180/4.82999. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.73820/4.81403. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.74277/4.82285. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.73876/4.82230. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.74062/4.81679. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.74480/4.82631. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.74180/4.81573. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.73775/4.82610. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.73717/4.82632. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.74150/4.82202. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.73955/4.82102. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.73668/4.82422. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.74496/4.81563. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.75753/4.79988. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.0\n",
      "Epoch 0, Loss(train/val) 4.93507/4.87146. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.82736/4.81664. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.81521/4.78838. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79586/4.79059. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79859/4.79600. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79750/4.79499. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.79739/4.79309. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.79934/4.79616. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.79751/4.79700. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79772/4.79701. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79942/4.79326. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.79376/4.79250. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.79029/4.79409. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.79075/4.79377. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.78979/4.78912. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.79052/4.78725. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78810/4.78191. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.79001/4.78564. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.79027/4.77079. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.78854/4.78802. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.78677/4.78798. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78646/4.78617. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 4.78673/4.78419. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.78544/4.78430. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.78454/4.78100. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 4.78055/4.79135. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.78481/4.78432. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.78178/4.77578. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.78275/4.78160. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.79274/4.76442. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.79194/4.77612. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78526/4.77566. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.78189/4.78133. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78266/4.78565. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.78273/4.77728. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.78283/4.77060. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.78961/4.77294. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.79288/4.77357. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.78705/4.77236. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.78735/4.77024. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.78629/4.77116. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.78647/4.77156. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.78312/4.77506. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.78475/4.77773. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.78575/4.78022. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.78472/4.78069. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.78281/4.78274. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.78117/4.78879. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.78275/4.79194. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.78257/4.78817. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77884/4.79380. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.77723/4.80029. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.78093/4.79763. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.78156/4.79661. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.77847/4.79869. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.77615/4.79973. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.78995/4.78257. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.78951/4.77596. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.78328/4.77872. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.78253/4.78723. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.77902/4.79889. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.78408/4.78767. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.77916/4.80894. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.77978/4.79523. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.77944/4.80344. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.77102/4.81283. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 4.77923/4.78973. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.77883/4.78325. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.77407/4.79245. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.78226/4.79487. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.77785/4.79691. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.77954/4.79012. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.77338/4.80218. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.77805/4.80780. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.78034/4.78226. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.77466/4.78356. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.77636/4.78515. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.77548/4.80185. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.77550/4.78378. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.77312/4.78835. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.77166/4.78456. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.77293/4.77742. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.77304/4.78256. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.77461/4.78934. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.77730/4.78383. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.77445/4.78474. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.77096/4.78413. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.77166/4.79408. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.77494/4.78588. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.77195/4.78416. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.77269/4.78459. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.77205/4.79940. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.78238/4.78218. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.77513/4.78619. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.76925/4.78324. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.78440/4.77755. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.77318/4.78893. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.77533/4.77924. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.76799/4.80171. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.77373/4.78780. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.17004550636718183\n",
      "Epoch 0, Loss(train/val) 4.99761/5.02053. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.99062/4.97256. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.98099/4.98709. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.97932/4.98630. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.98229/4.99175. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.98230/4.99885. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.98484/4.98691. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.98441/4.97429. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.97776/4.96923. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.97627/4.97004. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.97661/4.96402. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.97685/4.96239. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.97858/4.96471. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.97639/4.96328. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.97491/4.96134. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.97567/4.96143. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.97477/4.96305. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.97474/4.96108. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.97241/4.96168. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.97309/4.96214. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.97120/4.96189. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.97193/4.96174. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.97159/4.96762. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.97228/4.97025. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.96985/4.97102. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.96931/4.97144. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.97055/4.96855. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.96834/4.97326. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.96919/4.96952. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.96806/4.97742. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.97148/4.98023. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.96850/4.97314. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.96604/4.97340. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.96781/4.97001. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.96423/4.96595. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.96432/4.96171. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.96440/4.97113. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.96257/4.97999. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.96782/4.98349. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.96491/4.97290. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.96693/4.97224. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.96490/4.97551. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.95951/4.97888. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.95875/4.97198. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.96062/4.97611. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.96010/4.97562. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.95867/4.97800. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.95988/4.97997. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.95882/4.98167. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.95729/4.98087. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.95928/4.97601. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.95737/4.97690. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.95493/4.98668. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.95976/4.98047. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.95867/4.99914. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.95716/4.98796. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.95736/4.99540. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.95397/4.97584. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.95303/4.99126. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.95538/4.98934. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.94864/4.99671. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.94959/4.99704. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.95157/4.98662. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.95635/4.97537. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.95185/4.99081. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.95214/4.98110. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.95258/4.99447. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.95312/4.99519. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.95008/4.98812. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.94757/4.99669. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.94679/5.00018. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.94682/4.98818. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.94826/4.99584. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.95374/4.98874. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.94792/4.99959. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.94464/5.00043. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.94663/4.99636. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.94537/5.00145. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.94991/4.92966. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.97127/4.94933. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.95872/4.94922. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.95846/4.95489. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.95724/4.96049. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.95629/4.99815. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.94495/4.96059. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.95587/4.97175. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.95470/4.97196. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.95125/4.99372. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.94805/5.00140. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.95142/5.01027. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.94518/4.97113. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.95123/4.98429. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.94594/4.99641. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.94188/4.99321. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.93459/5.00854. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.94515/5.00108. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.94626/4.98014. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.94351/4.99883. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.94954/4.98110. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.94498/4.98739. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.051799406700962544\n",
      "Epoch 0, Loss(train/val) 4.90653/4.87177. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.86899/4.85417. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.87116/4.85829. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.86617/4.86266. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.86614/4.86252. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.86382/4.86316. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.86060/4.86724. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.86086/4.86919. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.86004/4.87225. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.86074/4.87460. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.85897/4.86912. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.85615/4.86952. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85974/4.86201. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.85782/4.86600. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.85803/4.86937. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.85501/4.87096. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.85243/4.87379. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.85593/4.86641. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.85145/4.87535. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85233/4.86838. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.84787/4.86991. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.85026/4.86094. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.85450/4.86704. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.85205/4.87315. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.85365/4.86754. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.84968/4.87446. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.84899/4.87312. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.84798/4.86785. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.84902/4.87482. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84766/4.87160. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.84474/4.86274. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.84858/4.86809. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84577/4.87525. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.84990/4.86923. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.84447/4.87469. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.84537/4.87807. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.84743/4.87203. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84463/4.87820. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.84272/4.87994. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.83635/4.87406. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84493/4.87481. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.84233/4.86290. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84137/4.86209. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.84203/4.87092. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.84246/4.86819. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.83524/4.87799. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.83965/4.86458. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.83698/4.87163. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.83755/4.86632. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.83592/4.86474. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.83651/4.85784. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.83788/4.86545. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.82988/4.85672. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.83769/4.85911. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.83468/4.85006. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.83603/4.87099. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.83710/4.85863. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.83202/4.86620. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.82822/4.86718. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83927/4.85594. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.83289/4.85272. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.83685/4.85033. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.83150/4.86115. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.83492/4.85810. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.83705/4.85652. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.82893/4.85428. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.83687/4.85018. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.83500/4.86006. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.85469/4.87818. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.85241/4.84741. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.84202/4.85520. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.83711/4.85782. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.83625/4.85208. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.83575/4.84640. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83480/4.85250. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83095/4.85316. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.82692/4.85884. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.83141/4.86172. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.83200/4.85055. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.82880/4.85551. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83832/4.86319. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.83634/4.85170. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.83085/4.86593. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.84047/4.84446. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.82965/4.85080. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.82998/4.86096. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.82782/4.85676. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.83085/4.84004. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.82932/4.84796. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.83518/4.84848. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.83105/4.85447. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.83446/4.84575. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83210/4.86778. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83536/4.84395. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.83574/4.86812. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.83467/4.84933. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.83307/4.85347. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.83125/4.85549. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.84763/4.86645. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.83877/4.85956. Took 0.09 sec\n",
      "ACC: 0.46875, MCC: -0.1003113655692617\n",
      "Epoch 0, Loss(train/val) 4.96401/4.92670. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.93164/4.91815. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.92746/4.91948. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.92817/4.92165. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.92637/4.92373. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.92585/4.92720. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.92299/4.92950. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.92344/4.93244. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.92326/4.93484. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.92100/4.93480. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.92110/4.93391. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.91847/4.93722. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.91874/4.94024. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.91697/4.94294. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.91864/4.94553. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.91587/4.95096. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.91238/4.95227. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.91357/4.95431. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.91506/4.95441. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.91215/4.96037. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.91166/4.95642. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.91236/4.95962. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.91074/4.96209. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.90741/4.97056. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.91050/4.96587. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.90417/4.98647. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.90198/4.99196. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.90881/4.96758. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.90579/4.98376. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.90514/4.99331. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.90230/5.00297. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.90134/4.99226. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.89899/4.98781. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.89659/5.04291. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.90231/5.01066. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.89829/5.00207. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.90327/4.97953. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.89162/5.01216. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.89407/5.01904. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.89853/4.97531. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.89476/4.98268. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.90072/4.95731. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.89283/5.00959. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.89084/5.01826. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.89484/4.97911. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.89057/4.98383. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.89460/4.98940. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.88388/5.00271. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.88481/5.01975. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.89250/5.00277. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.88127/5.07463. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.88876/5.01071. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.87819/5.01991. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.88670/5.03617. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.89462/4.98175. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.88700/5.02527. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.88582/5.02559. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.87535/5.05996. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.88745/5.00847. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.88264/5.00536. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.88110/4.98574. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.87894/5.00520. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.87688/5.03138. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.88872/4.96517. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.87965/5.00082. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.87635/5.03825. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.88188/4.98494. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.87504/4.98560. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.87945/4.98498. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.87776/4.99207. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.87605/5.03271. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.87356/5.00824. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.87807/5.04330. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.87623/4.97968. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.87361/5.00220. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 4.87752/4.98883. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.87703/5.01673. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.86894/5.01574. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.87330/5.07229. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.87136/5.07216. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.87156/5.01517. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.87456/4.97375. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.86670/5.00894. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.86977/5.00507. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.87083/5.00745. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.86920/4.97224. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.86990/4.98748. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.87503/5.04852. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.87169/5.01426. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.86292/5.03031. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.86612/4.98631. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.86835/5.06536. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.86760/4.98736. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.87295/5.07800. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.86075/5.03422. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.86926/5.04700. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.86316/5.02663. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 4.86432/5.08411. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.86592/5.03634. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.87140/5.01272. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.019772733298509516\n",
      "Epoch 0, Loss(train/val) 5.12422/5.06566. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.07866/5.04588. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.06928/5.04988. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.05541/5.04932. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.05057/5.04818. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 5.05401/5.04873. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.05662/5.04992. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 5.05439/5.04900. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.05354/5.04777. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.05336/5.04722. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.04992/5.04480. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.04978/5.04522. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.04851/5.04792. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 5.04973/5.04456. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.04910/5.04696. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.04860/5.04796. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.04757/5.04559. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.04708/5.04610. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.04509/5.04466. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.04295/5.04448. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.04323/5.03914. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.03929/5.03830. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.04097/5.03215. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 5.03989/5.02439. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.04519/5.02866. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.03821/5.02560. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 5.04158/5.02812. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.03986/5.02557. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.03748/5.02994. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 5.03872/5.03760. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.03397/5.03772. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 5.03923/5.03204. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.03362/5.02666. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.03649/5.03018. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 5.03456/5.02924. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 5.03392/5.03837. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.03465/5.03483. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.03233/5.03116. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.03391/5.03049. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.03230/5.02202. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.03137/5.02805. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.02762/5.02296. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 5.03090/5.03790. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.02648/5.03292. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 5.03116/5.03892. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.02591/5.03872. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.02925/5.03326. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.03100/5.03774. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 5.02274/5.02702. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 5.02795/5.03747. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.03056/5.03017. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 5.02731/5.03923. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.02667/5.03616. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.02382/5.03661. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.02320/5.04122. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.02382/5.03220. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.02538/5.03548. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 5.01927/5.02932. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 5.02234/5.03750. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.02449/5.04573. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.01888/5.04114. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 5.02046/5.05740. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.01809/5.04502. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.01391/5.06210. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.02759/5.04507. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 5.01809/5.04480. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.01879/5.05692. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.01775/5.06637. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.03246/5.04503. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.03533/5.04682. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 5.03166/5.05045. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.02568/5.04138. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 5.02105/5.04336. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.02519/5.05061. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.02667/5.06869. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.02876/5.04287. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 5.01640/5.04062. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.01595/5.03913. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.01407/5.05939. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 5.01399/5.04896. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.01781/5.03747. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 5.01449/5.04480. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.01616/5.06013. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.01972/5.04048. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.02209/5.04497. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.01765/5.05609. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.01204/5.05729. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 5.01973/5.04857. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.01238/5.05616. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 5.01192/5.04669. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.00801/5.06213. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 5.01278/5.07091. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.01040/5.07927. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.01093/5.07430. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.00736/5.07796. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.01865/5.06419. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 5.00700/5.05562. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.00706/5.07307. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 5.00517/5.08076. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.00981/5.06844. Took 0.08 sec\n",
      "ACC: 0.578125, MCC: 0.14180020247260586\n",
      "Epoch 0, Loss(train/val) 4.82297/4.78779. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.81285/4.78099. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.80183/4.79276. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.78848/4.78617. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.77806/4.78104. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.78123/4.78248. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.78370/4.78180. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.77943/4.77968. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.77913/4.77706. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.78108/4.77674. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.77920/4.77212. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.77898/4.77369. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.77567/4.77126. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.77694/4.77138. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.77713/4.77274. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.77738/4.77145. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.77604/4.77833. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.77344/4.77332. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.77570/4.77522. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.77148/4.77322. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.77342/4.78504. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.77015/4.78076. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.77217/4.78779. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.77263/4.78583. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.76827/4.79400. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.77055/4.79191. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.76911/4.79620. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.76896/4.78639. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.76652/4.78895. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.76631/4.79865. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.76612/4.80428. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.76430/4.80893. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.76361/4.80854. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.75912/4.80826. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.76427/4.80007. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.76497/4.80861. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.76135/4.81883. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.76475/4.82225. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.76089/4.81403. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.76406/4.82132. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.75737/4.82526. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.76064/4.83586. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.75986/4.82805. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.75946/4.83249. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.75934/4.83283. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.75793/4.82612. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.75642/4.84012. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77169/4.78807. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.76558/4.80814. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.76405/4.80935. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.76051/4.81520. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.76074/4.83983. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.75915/4.84468. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.76231/4.82468. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.75940/4.83841. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.75531/4.84026. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.76048/4.83992. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.76135/4.84540. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.76094/4.82626. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.75058/4.84911. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.76100/4.84562. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.75272/4.84025. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.75136/4.84596. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.76238/4.83759. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.75506/4.83651. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.75609/4.83489. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.75496/4.84614. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.75017/4.85412. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.74538/4.85257. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.75036/4.85239. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.75299/4.85752. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.75163/4.85721. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.75079/4.84591. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.75305/4.84193. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.75359/4.83573. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.74622/4.84477. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.75661/4.83282. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.74674/4.84132. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.75555/4.85160. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.75110/4.85686. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.74979/4.85518. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.74707/4.85084. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.74732/4.85771. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.74839/4.83923. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75180/4.85284. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.74320/4.83832. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 4.74758/4.84850. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.74747/4.85661. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.74974/4.85641. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.74842/4.84777. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.74035/4.87264. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.74397/4.87843. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.74471/4.87368. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.75477/4.81494. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.75006/4.84025. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.74271/4.86131. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.74118/4.86947. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.73758/4.92197. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.75784/4.80602. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.74894/4.86134. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.02834008097165992\n",
      "Epoch 0, Loss(train/val) 4.96617/4.90851. Took 0.18 sec\n",
      "Epoch 1, Loss(train/val) 4.91396/4.92641. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 4.91033/4.92994. Took 0.12 sec\n",
      "Epoch 3, Loss(train/val) 4.91434/4.92327. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.91306/4.92776. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.91407/4.93699. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.91512/4.93968. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.91677/4.93144. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.91760/4.91285. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.91349/4.90779. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.91014/4.90900. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.90761/4.90992. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.91102/4.90983. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.91017/4.91056. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.90825/4.91344. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.91224/4.91546. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.91072/4.91495. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.90841/4.91480. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.90733/4.91588. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.90447/4.91802. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.90552/4.92359. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.90590/4.92606. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.90725/4.92397. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.90717/4.92524. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.90238/4.93215. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.90447/4.92998. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.90417/4.93704. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.90476/4.93540. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.90364/4.93513. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.90221/4.93744. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.90013/4.93341. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.89827/4.93588. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.89941/4.94292. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.89919/4.93662. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.89957/4.93563. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.89765/4.94664. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.89662/4.94146. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.89886/4.94251. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.89585/4.94536. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.89887/4.93893. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.89522/4.93997. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.89582/4.94521. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.89496/4.94253. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.89521/4.94029. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.89667/4.94267. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.89433/4.94343. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.89207/4.94875. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.89003/4.94231. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.89264/4.94429. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.89336/4.94438. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.89331/4.95397. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 4.88715/4.96176. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.88828/4.95008. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.88984/4.96347. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.89326/4.92540. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.89021/4.94942. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.88858/4.94952. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.88755/4.95495. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.88751/4.95755. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.89199/4.94916. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.88664/4.94363. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89015/4.93710. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.89378/4.92213. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.89433/4.93532. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.88974/4.99370. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.88821/4.93308. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.88742/4.96658. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.89095/4.93504. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.88865/4.94410. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.88955/4.94564. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.87752/4.97383. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.88582/4.96536. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88321/4.96131. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88443/4.94335. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.88797/4.94650. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.88105/4.96254. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.87967/4.97167. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.88603/4.96961. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.88516/4.96297. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.88493/4.97631. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.88023/4.95493. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.87850/4.95594. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.87518/4.96687. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.87862/4.95814. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.87869/4.96507. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.87711/4.95841. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.87609/4.96399. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.87656/4.96439. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.87081/4.96558. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.87343/4.94029. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.87761/4.95921. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.87742/4.94355. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.87852/4.93571. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 4.88519/4.94791. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.87759/4.94121. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.88053/4.93417. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.87525/4.93545. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.87285/4.93636. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.87318/4.94745. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.87437/4.93890. Took 0.09 sec\n",
      "ACC: 0.515625, MCC: 0.02757337634153148\n",
      "Epoch 0, Loss(train/val) 4.76372/4.75966. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.75516/4.74792. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.74342/4.74931. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.74715/4.75571. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.74374/4.76191. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.74338/4.76444. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.74330/4.76800. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.74688/4.76134. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.74305/4.75946. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.73926/4.76245. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.73720/4.76981. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.73706/4.78047. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.73516/4.79050. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.73782/4.78565. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.73459/4.79959. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.73242/4.79677. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.73378/4.79303. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.73331/4.80108. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.73440/4.79161. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.73255/4.79703. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.73068/4.79607. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.72654/4.81249. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.73003/4.80282. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.73081/4.80512. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.72872/4.80342. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.72534/4.81284. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.72864/4.79489. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.72786/4.79736. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.72845/4.80019. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.72757/4.80682. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.72649/4.80641. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.73131/4.81102. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.73130/4.79792. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.72608/4.81373. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.72415/4.81257. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.72693/4.80762. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.72117/4.82574. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.72498/4.80550. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.72281/4.81932. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.71917/4.82630. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.72597/4.82182. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.72044/4.82111. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.72362/4.82543. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.72309/4.81478. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.71844/4.84048. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.71945/4.82504. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.71987/4.81148. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.71845/4.85431. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 4.72300/4.81546. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.72238/4.83951. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.71780/4.83503. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.71952/4.83242. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.71383/4.82149. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.71971/4.81190. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.71555/4.82559. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.72134/4.83307. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.71145/4.85792. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.71256/4.81580. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.71730/4.86211. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.71075/4.81963. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.71647/4.82310. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 4.72163/4.80172. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.71834/4.84069. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.72044/4.82052. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.70867/4.87327. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.71593/4.83485. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.70898/4.84739. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.70614/4.86268. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.72400/4.80138. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.73747/4.78823. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.73234/4.78124. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.72814/4.81142. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.72407/4.82873. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.72403/4.83926. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.72253/4.82469. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.72078/4.83950. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.71874/4.83360. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.72034/4.83253. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.71940/4.85765. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.71972/4.85247. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.71783/4.83629. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.71185/4.85066. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.71675/4.84852. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.71638/4.85759. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.71567/4.84940. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.70883/4.82741. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.71939/4.83660. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.71647/4.85894. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.71299/4.83342. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.71326/4.83577. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.70992/4.82493. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.70490/4.88189. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.71174/4.81709. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.71913/4.83834. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.70257/4.85640. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.71609/4.84350. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.70189/4.86421. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.70943/4.85455. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.71437/4.84555. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.71192/4.84445. Took 0.08 sec\n",
      "ACC: 0.59375, MCC: 0.18670576735092864\n",
      "Epoch 0, Loss(train/val) 4.91397/4.88344. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.89602/4.93226. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.90042/4.85425. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.90470/4.84812. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.89593/4.87664. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.87839/4.87275. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.88007/4.86533. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.88001/4.86846. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.88045/4.87055. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.88091/4.86853. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.88176/4.87077. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.87525/4.87155. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.87759/4.88120. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.87909/4.87521. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.87841/4.88097. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.87582/4.87413. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.87367/4.86893. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.87253/4.86688. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.87402/4.86958. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.87462/4.87726. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.87289/4.87607. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.87348/4.87671. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.87286/4.87565. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.86955/4.87758. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.87356/4.87667. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.87249/4.87988. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.87172/4.87230. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.87267/4.88023. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.87071/4.87704. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.87295/4.87647. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.86730/4.87779. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.86893/4.88008. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.87189/4.87547. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.86686/4.88718. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 4.86699/4.87249. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.87442/4.88570. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.86719/4.88214. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.87179/4.87820. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86817/4.88594. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.86835/4.88507. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.86648/4.88462. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.86715/4.87481. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.86871/4.88176. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.86615/4.89545. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.86434/4.89376. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.86733/4.88190. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.86496/4.88517. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.86678/4.89229. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.86348/4.88568. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.86406/4.87141. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.86617/4.89102. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.86749/4.90012. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.86230/4.88908. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.86018/4.89886. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.86166/4.88938. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.86197/4.91175. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.86203/4.87564. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.86349/4.88262. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.86620/4.87184. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.86568/4.87800. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.85963/4.88574. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.85909/4.89118. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.85633/4.88925. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.86297/4.89229. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.86091/4.88745. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.85671/4.89445. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.85847/4.88930. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.86254/4.89005. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.85622/4.90145. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.86154/4.91337. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.85781/4.90100. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.85897/4.90225. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.85573/4.93344. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.86187/4.91221. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.85720/4.89041. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.85775/4.91812. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.85711/4.91714. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.85556/4.89678. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.85600/4.89920. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.85530/4.90487. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.85189/4.91197. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.85806/4.91726. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.85251/4.89635. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.85012/4.89993. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.87646/4.87770. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.87439/4.89105. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.86597/4.89662. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.86231/4.90587. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.86098/4.91240. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.86039/4.91527. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.86243/4.90355. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.85701/4.90620. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.85598/4.90604. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.85393/4.92238. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.84959/4.93158. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.84947/4.91595. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.85307/4.89909. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.85305/4.90489. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.84876/4.91867. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.84549/4.91194. Took 0.09 sec\n",
      "ACC: 0.46875, MCC: -0.06262242910851495\n",
      "Epoch 0, Loss(train/val) 5.11492/5.06436. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.06690/5.07656. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 5.06287/5.07878. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.06555/5.07875. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.06221/5.07539. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.06539/5.06984. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.06548/5.06655. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 5.06437/5.06682. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.06264/5.06896. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.05973/5.07040. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.06254/5.06947. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 5.06016/5.07243. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.05929/5.07075. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.06117/5.07061. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.06020/5.07268. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.05890/5.07235. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.05773/5.07139. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 5.05663/5.07796. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.05626/5.08597. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.05464/5.09004. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.05439/5.08677. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.05157/5.08258. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.05546/5.08890. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 5.05263/5.09387. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.05360/5.08887. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.04972/5.08871. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 5.05017/5.09435. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 5.04991/5.09222. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.05230/5.09503. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.05165/5.08452. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 5.05005/5.09230. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.05095/5.08996. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.04875/5.09481. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 5.05143/5.09076. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.05030/5.09161. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.05094/5.09510. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.04831/5.09425. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.04852/5.09261. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.04954/5.09454. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 5.04894/5.08732. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.04804/5.09062. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.05043/5.09559. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 5.04826/5.09747. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.04838/5.09758. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.04806/5.09778. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.04932/5.09295. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.04784/5.09676. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.04936/5.10107. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.04683/5.09890. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 5.04586/5.10017. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.04598/5.09369. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 5.04717/5.10145. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.04655/5.10027. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 5.04886/5.10053. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.04937/5.09824. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.04357/5.10389. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.04339/5.09957. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.04505/5.10719. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.04474/5.10616. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.04434/5.10236. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 5.04572/5.10240. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.04654/5.09794. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 5.04211/5.10670. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.04291/5.09688. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.04360/5.10434. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.04428/5.10692. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.04679/5.08680. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 5.04516/5.10485. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.04400/5.10308. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.04451/5.10405. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.04584/5.10804. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.04407/5.11080. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.04137/5.12598. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 5.04617/5.10987. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.04520/5.09115. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.04926/5.08338. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 5.04812/5.07540. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.04532/5.08552. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.04297/5.10520. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.04381/5.09737. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.04257/5.10138. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.04150/5.11466. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.04066/5.10645. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.03854/5.11397. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 5.03922/5.10633. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.04217/5.10324. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 5.04083/5.10668. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.04245/5.11949. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.04160/5.10468. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 5.04033/5.10801. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 5.03637/5.12201. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 5.04310/5.11170. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 5.04283/5.10596. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 5.03764/5.09256. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.03828/5.11582. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.04013/5.10482. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.03656/5.12256. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.04324/5.12008. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 5.03466/5.11467. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 5.03878/5.11842. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.0\n",
      "Epoch 0, Loss(train/val) 4.86557/4.84350. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.84635/4.85496. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.84601/4.86736. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.84797/4.87861. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.85287/4.85350. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.85116/4.83556. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.84140/4.83721. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.84130/4.83704. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.84120/4.83514. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.83644/4.83376. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.83951/4.83590. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.84041/4.83991. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.84132/4.83976. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.83701/4.83807. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.83736/4.84249. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.83328/4.85103. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.83484/4.85052. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.83466/4.86830. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.83591/4.85494. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.83331/4.85032. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.83123/4.85093. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.83335/4.84747. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.82818/4.85646. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.82850/4.85745. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 4.83215/4.84160. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.83258/4.84138. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.82974/4.84686. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82978/4.84721. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.82917/4.84318. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.82847/4.84417. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.82862/4.84239. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.83026/4.84966. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.82902/4.84420. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.82831/4.84353. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.82771/4.83714. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.82868/4.83958. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.82814/4.85269. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.82701/4.84637. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.82877/4.84804. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.82883/4.84996. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.82821/4.84458. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.82817/4.84849. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.82718/4.84391. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.82774/4.83991. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.82842/4.84662. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.82691/4.84732. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.82952/4.84643. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.82606/4.84656. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.82403/4.84888. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.82886/4.85682. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.82290/4.84932. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.82523/4.85278. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.82365/4.85186. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.83078/4.86972. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.82608/4.84455. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.82246/4.85138. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.82799/4.84461. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.82161/4.84072. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.82378/4.84511. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.82561/4.84764. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.82366/4.84309. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.82551/4.84831. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.82156/4.83787. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.82595/4.83992. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.82069/4.83899. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.82058/4.84296. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.82214/4.84526. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.82420/4.84113. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.82290/4.84599. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.82231/4.84920. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.82361/4.83133. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.82271/4.83973. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.82703/4.83975. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.81659/4.83723. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.83018/4.89167. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.83241/4.85316. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.83137/4.85724. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.83397/4.85636. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.83173/4.85854. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.83104/4.85673. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.82875/4.85478. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.82796/4.85250. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.82417/4.86679. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.82380/4.87179. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.82577/4.85909. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.82337/4.87063. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.82792/4.85036. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.82333/4.84375. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.82460/4.84722. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.82428/4.84003. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.82209/4.83621. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.82874/4.83679. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.82191/4.83968. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.81959/4.83281. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.82521/4.82913. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.82075/4.82058. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.82071/4.82626. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.82201/4.82459. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83252/4.85101. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.82579/4.83275. Took 0.09 sec\n",
      "ACC: 0.484375, MCC: -0.0518191557963571\n",
      "Epoch 0, Loss(train/val) 4.87241/4.81127. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.82638/4.80000. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.81925/4.80487. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.81655/4.80359. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.81780/4.79823. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.81347/4.79718. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.81124/4.78989. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.81480/4.79054. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.81583/4.79155. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.81464/4.80947. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.81088/4.80437. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.81290/4.80191. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.81099/4.80134. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.81339/4.80324. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.80895/4.80260. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.81092/4.80972. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.81001/4.80404. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.81330/4.79712. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.81180/4.79555. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.81159/4.79305. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.80814/4.78638. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.80861/4.79033. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.80841/4.78775. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.80593/4.78906. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.80757/4.79226. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.80316/4.79017. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.81052/4.78861. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.81147/4.78635. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.80485/4.78813. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.81021/4.78793. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.80934/4.78981. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.80667/4.78812. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.80713/4.78364. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.80758/4.78213. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.80502/4.78926. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.80204/4.78852. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.80496/4.78898. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.80148/4.79097. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.80621/4.78596. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.80168/4.78818. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.80767/4.79205. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.80435/4.79139. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.80075/4.79812. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.80312/4.79575. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.79970/4.79624. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.80099/4.79103. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.80319/4.78989. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.80671/4.79574. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.80417/4.79454. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.79747/4.79658. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.80193/4.78864. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.80144/4.78872. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.79793/4.79546. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.80095/4.79580. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.79871/4.79854. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.80340/4.79984. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.79489/4.79867. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.79988/4.80722. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.79609/4.80738. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.79786/4.79688. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.79921/4.79877. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.79649/4.79701. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.80294/4.79585. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.79914/4.80543. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.79771/4.81371. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.79694/4.80964. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.79612/4.80506. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.79491/4.79845. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.79637/4.81694. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.79442/4.82402. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.79588/4.82498. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.79232/4.81588. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.79458/4.80371. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.78820/4.80785. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.80199/4.80678. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.79898/4.81672. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.78923/4.81865. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.79847/4.80675. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.79583/4.80647. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.78801/4.81834. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.80009/4.81577. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.78920/4.83440. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 4.79671/4.82051. Took 0.13 sec\n",
      "Epoch 83, Loss(train/val) 4.78398/4.82242. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.79269/4.82606. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.78536/4.83747. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.79670/4.83049. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.79244/4.82489. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.79197/4.81714. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.78781/4.83934. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.78483/4.82109. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.79435/4.82983. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.79374/4.82278. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.78580/4.84046. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.79341/4.82826. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.78674/4.83990. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.78409/4.82825. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.78593/4.82057. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.78773/4.82000. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.78803/4.83102. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.013980301652228746\n",
      "Epoch 0, Loss(train/val) 4.84792/4.86542. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.83666/4.86312. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.83158/4.88320. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.83056/4.88306. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.83414/4.87533. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.83347/4.86478. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.83558/4.84983. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.82830/4.85144. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.82481/4.84686. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.82534/4.84623. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.82502/4.84629. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.82524/4.84818. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.82448/4.84374. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.82295/4.83956. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.82291/4.84095. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.82206/4.84145. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.82043/4.84032. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.81654/4.84269. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.81858/4.84189. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.81878/4.84783. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.81999/4.83784. Took 0.12 sec\n",
      "Epoch 21, Loss(train/val) 4.81504/4.84278. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.81519/4.83402. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.82015/4.84180. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.81315/4.85244. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.81334/4.84631. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.81009/4.83763. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.81567/4.84395. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.81529/4.83543. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.81471/4.84293. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.81080/4.84891. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.80802/4.84720. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.81321/4.84606. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.80578/4.84389. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.80832/4.83120. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.81200/4.84096. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.81097/4.84169. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.80855/4.83110. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.80792/4.84623. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.80699/4.83605. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.80764/4.84973. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.80863/4.83857. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.80410/4.83630. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.80279/4.84839. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.80806/4.83235. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.80531/4.83538. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.80650/4.85101. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.80706/4.84983. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.80150/4.84863. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.80223/4.83177. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.80368/4.83853. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.80360/4.83204. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.80305/4.83935. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.80331/4.83966. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.80615/4.84781. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.80144/4.84587. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.80534/4.84800. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.79574/4.84517. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.80288/4.84769. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 4.80364/4.85284. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.79599/4.83965. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.79521/4.83632. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.80115/4.82818. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.80325/4.85007. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.80405/4.86392. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.80324/4.84773. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.79218/4.83417. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.79860/4.83244. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.80721/4.84267. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.82231/4.84678. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.80802/4.86273. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.80295/4.85546. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.80016/4.86800. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.80904/4.85283. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.80213/4.87025. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.80343/4.87817. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.80281/4.85880. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.81122/4.87839. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.80558/4.87563. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.80157/4.86701. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.80616/4.87161. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.80163/4.86776. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.79721/4.87223. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.80049/4.86906. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.79801/4.87593. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.79601/4.86991. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.80120/4.87954. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.79845/4.88354. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.80261/4.86325. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.81955/4.86694. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.81549/4.83125. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.81395/4.84003. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.80194/4.87250. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.81026/4.84279. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.80291/4.84457. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.79698/4.84589. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.80067/4.84849. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.81498/4.81191. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.81927/4.82441. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.81876/4.82895. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.03202563076101743\n",
      "Epoch 0, Loss(train/val) 4.99809/4.93340. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.91591/4.90697. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.90708/4.90546. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.90770/4.90846. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.91027/4.91234. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.90754/4.91598. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.90776/4.91416. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.90651/4.91302. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.91014/4.91364. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.90432/4.91393. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.90761/4.91440. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.90641/4.91276. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.90652/4.91159. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.90512/4.90781. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.90530/4.90876. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.90025/4.90596. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.90100/4.90565. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.90359/4.90837. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.90356/4.90569. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.90103/4.90554. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.90292/4.89794. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.90023/4.89833. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.89750/4.90298. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.89689/4.93494. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.90149/4.89715. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.89827/4.89820. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.89480/4.90262. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.89212/4.90731. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.89492/4.90070. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.89280/4.90079. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.89196/4.89753. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.88955/4.91209. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.89222/4.92003. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.89078/4.90822. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.89115/4.91917. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.88957/4.91820. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.89094/4.90626. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.88837/4.90383. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.88682/4.91615. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.89084/4.91552. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.88738/4.90632. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.89004/4.91153. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.88268/4.91060. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.88303/4.91694. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.88341/4.91650. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.88247/4.92922. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.88518/4.93160. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.88105/4.90346. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.88160/4.92113. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.88183/4.91859. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.87892/4.91961. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.87971/4.91981. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.87573/4.92644. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.88216/4.90490. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.88403/4.89994. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.87757/4.92735. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.87904/4.94237. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.88124/4.91520. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.87626/4.93584. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.87516/4.91905. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.87772/4.92843. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.86957/4.92435. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.88235/4.88898. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.87836/4.89675. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.87635/4.94514. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.87338/4.90880. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.86896/4.92235. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.86703/4.93426. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.86723/4.93548. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.87115/4.95471. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.87216/4.93924. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.86727/4.88302. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.87178/4.90512. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.87235/4.91660. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.86957/4.90754. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.87309/4.93261. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.86874/4.89872. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.86641/4.95352. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.87105/4.89859. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.87272/4.88439. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.87362/4.88506. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.88433/4.88559. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.87692/4.96322. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.90922/4.90309. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.89544/4.92028. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.89190/4.90470. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.88649/4.90950. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.88529/4.91200. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.88792/4.91182. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88667/4.92638. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.88622/4.90167. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.87248/4.91377. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.88291/4.88581. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.87157/4.90695. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.88393/4.89298. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.88327/4.89401. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.87746/4.88433. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.87389/4.87886. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.88046/4.86474. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.87539/4.89078. Took 0.08 sec\n",
      "ACC: 0.640625, MCC: 0.2771507137113173\n",
      "Epoch 0, Loss(train/val) 5.01163/5.01219. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.98751/4.99252. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.98325/4.99446. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.98305/4.98940. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.98535/4.98666. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.98239/4.98573. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.98250/4.98637. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.98535/4.98851. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.98482/4.98696. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.98369/4.98932. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.98321/4.99301. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.98101/4.99560. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.97568/4.99877. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.97630/4.99895. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.97864/4.99815. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.97452/4.99845. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.97395/5.00240. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.97692/4.99420. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.97372/5.00081. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.97666/5.00061. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.97172/5.00956. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.97074/5.00598. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.97295/5.00750. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.97118/5.01550. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.97094/5.00836. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.97580/5.03031. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.97568/5.00992. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.97392/5.01739. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.97650/5.01307. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.98096/4.99950. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.97724/5.00634. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.97410/5.00439. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.97115/5.00709. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.97246/5.00809. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.97763/5.00062. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.97433/4.99723. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.97075/5.00502. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.97545/5.00144. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.97080/5.00261. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.97304/4.99987. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.97091/5.00111. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.97089/4.99854. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.96976/4.99630. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.96952/5.02391. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.97066/4.99466. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.97039/5.00642. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.97181/5.00209. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.96749/5.01870. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.96486/5.00431. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.97048/5.01270. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.96466/5.03059. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.96386/5.02011. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.96495/5.02470. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.96430/5.04465. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.96338/5.01435. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.96631/5.01325. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.96450/5.01425. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.96528/5.02318. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.96101/5.03757. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.96417/5.03515. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.96248/5.01632. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.96206/5.02938. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.95691/5.03903. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.95963/5.04005. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.96225/5.01989. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.95609/5.03537. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.95919/5.04738. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.95957/5.01952. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.95542/5.03060. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.95846/5.03839. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.95524/5.03865. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.95985/5.01731. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.95341/5.03915. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.96170/5.01908. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.95551/5.05022. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.95943/5.03435. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.95591/5.04058. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.96862/5.01560. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.97480/4.99946. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.96988/5.01471. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.97071/5.01864. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.97006/5.01659. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.97126/5.02230. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.96471/5.03701. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 4.97275/5.00015. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.97080/4.99883. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.97035/4.99078. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.97011/4.99538. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.97095/4.99933. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.96748/4.99863. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.96128/5.01985. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.95876/5.01517. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.95964/5.02182. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.95918/5.02028. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.96436/5.02480. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.96934/5.01734. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.96183/5.01434. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.96152/5.02364. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.96508/5.00776. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.96023/5.01873. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.025552902682603722\n",
      "Epoch 0, Loss(train/val) 4.87438/4.82189. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.82752/4.80971. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.82165/4.83744. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.80927/4.81983. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 4.80094/4.81750. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.80290/4.82104. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.80241/4.82310. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.80299/4.81823. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.80501/4.82004. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.80067/4.82078. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79864/4.82294. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.79866/4.81827. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.79943/4.81846. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.79832/4.82236. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.79916/4.82306. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.79584/4.82329. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.79694/4.82565. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.79582/4.82859. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.79319/4.82702. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.79417/4.83776. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.79412/4.83204. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.79607/4.83745. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.79440/4.81891. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.78981/4.83420. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.79103/4.83216. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 4.78766/4.84052. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.79116/4.82890. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.79118/4.83967. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.79009/4.84486. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.78831/4.84636. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.78641/4.86308. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78655/4.84024. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.78739/4.85627. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78812/4.84968. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.78654/4.85066. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.77817/4.86138. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.78569/4.85151. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.78292/4.83365. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.78953/4.85149. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.78585/4.83583. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.78185/4.84920. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.78319/4.83698. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.78106/4.85717. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.77962/4.83607. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.78217/4.84995. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.78174/4.85968. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77747/4.87393. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77633/4.85232. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77873/4.88310. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.77911/4.84325. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77421/4.88516. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77194/4.87750. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 4.77236/4.86202. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.77891/4.88114. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.77810/4.85776. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.77361/4.87195. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.77003/4.86205. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.77490/4.86579. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76756/4.87795. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.76899/4.86585. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.76461/4.88026. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.77159/4.85709. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76851/4.86737. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.76436/4.88744. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.76642/4.85908. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.77255/4.87337. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.76922/4.87509. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.76473/4.87014. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.76267/4.88851. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76748/4.84957. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.76714/4.88216. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.76143/4.87652. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.75638/4.89837. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.76551/4.88654. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.75660/4.87580. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.76173/4.85316. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.75479/4.88151. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.76854/4.87446. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.75821/4.87485. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.76465/4.84364. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.75821/4.90628. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.75814/4.84689. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.76238/4.87380. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76164/4.87904. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.76307/4.86639. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.75282/4.89347. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.75532/4.87923. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.75169/4.87528. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.74576/4.88866. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.75199/4.86847. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.75880/4.88408. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.75735/4.88201. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 4.75519/4.89467. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.75250/4.88330. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.74987/4.89785. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.74224/4.92017. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.75008/4.92590. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.74938/4.87575. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.74290/4.94140. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.75351/4.91865. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.0778902520624173\n",
      "Epoch 0, Loss(train/val) 4.80958/4.71231. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.76481/4.71140. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.76029/4.71321. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.76024/4.71846. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.75789/4.72582. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.75518/4.73378. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.75068/4.73724. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.74835/4.73445. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.75274/4.72845. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.75560/4.73493. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.75031/4.74064. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.75205/4.74658. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.74878/4.74763. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.74753/4.75103. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.74574/4.75252. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.74578/4.75480. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.74717/4.75656. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.74444/4.75560. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.74479/4.76384. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.74363/4.75159. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.74124/4.76550. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.74705/4.75030. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.74136/4.77499. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.73905/4.77705. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.74046/4.77302. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.73482/4.78147. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.74283/4.75248. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.73904/4.79519. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.73443/4.77000. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.73562/4.78449. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.73442/4.78359. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.73426/4.78658. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.73883/4.77628. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.73655/4.78634. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.73596/4.78792. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.73528/4.77926. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.73421/4.79843. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.73206/4.79335. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.73606/4.79082. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.73052/4.80247. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.72380/4.80848. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.73377/4.77963. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.73011/4.80510. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.73329/4.78858. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.73337/4.79844. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.73152/4.79637. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.72739/4.80688. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.72971/4.79898. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.72997/4.80675. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.73028/4.80620. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.72873/4.81339. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.73345/4.77398. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.74916/4.77422. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.73713/4.78036. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.73288/4.80017. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.73243/4.79402. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.73211/4.79853. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.73046/4.79349. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.73071/4.80254. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.72698/4.81064. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.73041/4.80139. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.73054/4.81525. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.72703/4.80918. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.72587/4.80833. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.72591/4.81191. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.72422/4.81242. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.72769/4.81597. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.72164/4.81827. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.72827/4.78737. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.72181/4.81466. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.72330/4.82905. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.72154/4.80615. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.71962/4.81532. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.72062/4.82304. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.72121/4.81304. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.72137/4.81667. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.72953/4.78902. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.72037/4.80991. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.72298/4.80966. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.71873/4.82387. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.71952/4.82655. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.71860/4.83333. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.72362/4.81866. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.71924/4.82028. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.71782/4.78705. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.72272/4.79428. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.72491/4.82503. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.71973/4.84475. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.71623/4.81390. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.71010/4.83256. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.71950/4.80785. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.72035/4.82868. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.71151/4.82887. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.72218/4.82203. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.71027/4.82501. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.71670/4.81402. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.71217/4.80945. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.72044/4.78922. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.72145/4.81978. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.71224/4.83189. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.09847634407689815\n",
      "Epoch 0, Loss(train/val) 4.94645/4.94112. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.87764/4.90807. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.88089/4.88209. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.87584/4.87456. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.86799/4.87906. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.87085/4.88414. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.86623/4.89436. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.86992/4.89560. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.86808/4.88874. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.86342/4.88718. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.86246/4.87712. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.86477/4.87501. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.86532/4.87128. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.86093/4.87479. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.85776/4.86715. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.85891/4.89109. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.85980/4.86876. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.85726/4.87358. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.86001/4.86139. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85705/4.85924. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.85918/4.86377. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.85295/4.86656. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.84976/4.85620. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.85427/4.86307. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.85487/4.86318. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.85353/4.87046. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.85221/4.86229. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.85259/4.86588. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.84736/4.87036. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84520/4.87365. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.84415/4.87694. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.84882/4.86935. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84615/4.87173. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.84180/4.88770. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.84779/4.88513. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84934/4.86639. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84355/4.87378. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84180/4.88290. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.84440/4.88908. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84769/4.87376. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84172/4.88797. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.84395/4.88576. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.83877/4.89512. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.83817/4.89310. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.84147/4.89786. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.83481/4.89431. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.84289/4.89052. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.84182/4.89470. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84099/4.88566. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.84487/4.88303. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.83666/4.89020. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.83696/4.89763. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.83099/4.90418. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.83464/4.90602. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.83679/4.89663. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.83176/4.89981. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.83450/4.91776. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.83599/4.89627. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.83568/4.92260. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 4.83161/4.91269. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.83570/4.93533. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83182/4.91553. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.82789/4.95219. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.82632/4.93499. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.83589/4.93564. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.83075/4.92434. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.83061/4.93141. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.82872/4.93809. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.82854/4.93380. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83648/4.96244. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.86342/4.87924. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.85645/4.89502. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.85470/4.89545. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.85019/4.90317. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.85041/4.89561. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.84106/4.90998. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.84213/4.90702. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.83552/4.90373. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 4.84294/4.90413. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.84393/4.89472. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.83864/4.91412. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.84259/4.88755. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.83453/4.91476. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.84138/4.90088. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83612/4.91485. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83554/4.91870. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83879/4.89806. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.83636/4.90996. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.84026/4.91395. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.83673/4.91626. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.83006/4.89720. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 4.83384/4.90023. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83521/4.92926. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.83542/4.91932. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.83422/4.91694. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.83320/4.91433. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.83124/4.92439. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.82858/4.89861. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.82741/4.91495. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.82695/4.94677. Took 0.09 sec\n",
      "ACC: 0.671875, MCC: 0.34654956599385545\n",
      "Epoch 0, Loss(train/val) 4.94262/4.91717. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.89398/4.88719. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.88797/4.89227. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.88969/4.89043. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.89124/4.89413. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.88758/4.89401. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.89004/4.89320. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.88837/4.89391. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.88670/4.89580. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.88770/4.89687. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.88561/4.90380. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.88211/4.90606. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.88281/4.91477. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.88121/4.91943. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.88212/4.92123. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.88014/4.92588. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.87991/4.92298. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.87654/4.92689. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.87542/4.93225. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.87942/4.92691. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.87254/4.93775. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.87510/4.93360. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.87142/4.95158. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.87464/4.92500. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.87353/4.92240. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.87617/4.94465. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.87592/4.93294. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.87602/4.93908. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.87570/4.93903. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.87568/4.93178. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.87175/4.93589. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.86709/4.94431. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.86740/4.95386. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.87314/4.93379. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.87211/4.94904. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.86593/4.95795. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.87029/4.95285. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.86936/4.94340. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86582/4.95558. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.86915/4.95706. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.86885/4.94696. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.86724/4.96356. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.86632/4.96480. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.87377/4.95093. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.86724/4.94242. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.86892/4.96517. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.86314/4.95911. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.86251/4.97038. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.86469/4.96207. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.86858/4.94534. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.86076/4.97084. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.86766/4.95007. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.86552/4.95892. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.86614/4.97694. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.86177/4.95186. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.86426/4.95955. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.86184/4.97565. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.86446/4.95763. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.85994/4.97249. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.86309/4.96990. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.86150/4.95883. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.86335/4.97845. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.85560/4.99569. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.86038/4.97352. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.86053/4.97002. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.85716/4.98300. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.85883/4.96183. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.86475/4.97302. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.85404/4.97709. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.85606/4.98980. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.86502/4.93164. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.86173/4.94036. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.85981/4.96747. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.86359/4.93623. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.86315/4.95217. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.86488/4.93504. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.86548/4.94276. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.86103/4.93870. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.85935/4.94787. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.85940/4.94107. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.85883/4.95344. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.86024/4.94175. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.85439/4.97176. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.85921/4.95401. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.86240/4.95471. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.85551/4.94590. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.85404/4.95489. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.85944/4.95227. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.85063/4.96886. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.85643/4.95295. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.85562/4.95524. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.85679/4.97443. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.85901/4.94922. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 4.85556/4.95621. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.84982/4.98787. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.85195/4.97784. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.85808/4.96409. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.85084/4.98286. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.85574/4.98972. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.85515/4.96801. Took 0.08 sec\n",
      "ACC: 0.640625, MCC: 0.2847473987257497\n",
      "Epoch 0, Loss(train/val) 4.97009/4.84176. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.82553/4.84406. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.82907/4.82948. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.82874/4.82537. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.82430/4.82510. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.82493/4.82477. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.82487/4.82599. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.82431/4.82578. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.82544/4.82516. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.82248/4.82480. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.82619/4.82387. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.82417/4.82471. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.82195/4.82241. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.82152/4.82371. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.82123/4.82416. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.82066/4.82363. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.82196/4.82070. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.82087/4.82035. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.82232/4.82212. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.81767/4.82196. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.82205/4.82375. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.82358/4.82233. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.82045/4.82204. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.81921/4.82387. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.81977/4.82596. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.82003/4.82605. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.81587/4.83147. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.81742/4.82817. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.81509/4.83175. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.81656/4.83284. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.81634/4.83018. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.81765/4.83303. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.81703/4.83413. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.81674/4.83428. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.81283/4.83810. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.81625/4.83505. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.81405/4.83941. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.81626/4.81326. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.81889/4.82605. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.81730/4.82536. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.81497/4.82678. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.81616/4.82953. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.81248/4.83779. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.81318/4.83433. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.81584/4.83331. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.80945/4.84475. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.81605/4.84274. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.81657/4.83654. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.80884/4.84799. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.81253/4.84842. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.80841/4.85178. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.81172/4.85272. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.80678/4.85018. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.81002/4.85042. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.81021/4.85149. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.81302/4.85991. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.81459/4.84688. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.80628/4.85563. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.80788/4.83596. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.80908/4.84028. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.80680/4.82859. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.81489/4.82093. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.81746/4.81271. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.81525/4.81666. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.81334/4.81679. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.81415/4.82075. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.81430/4.82143. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.81126/4.82263. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.81203/4.82238. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.81310/4.80978. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.80999/4.82151. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.81362/4.82674. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.80811/4.83557. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.80303/4.82177. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.80561/4.82579. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.80782/4.82832. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.80634/4.83563. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.80365/4.82923. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.80515/4.83455. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.80796/4.84516. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.80207/4.84377. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 4.79989/4.84993. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.80467/4.82720. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.80334/4.83318. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.79805/4.85018. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.80086/4.84379. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.80366/4.84286. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.79918/4.83631. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.80787/4.84435. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.80744/4.84347. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.80360/4.85519. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.80448/4.84755. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.80771/4.84901. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.80832/4.84645. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.80729/4.84960. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.80801/4.85814. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.80294/4.85900. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.80014/4.85962. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.80030/4.87234. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.79567/4.86555. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.013980301652228746\n",
      "Epoch 0, Loss(train/val) 5.01427/4.94763. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.97121/4.96002. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.97431/4.96282. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.97305/4.95318. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.97437/4.94665. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.97128/4.95390. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.96540/4.95045. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.96303/4.94562. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.96298/4.94700. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.96362/4.94972. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.96146/4.95073. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.96614/4.95127. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.96441/4.95479. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 4.96349/4.95167. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.96439/4.95240. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.96199/4.95026. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.96164/4.95102. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.96208/4.94885. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.96019/4.94966. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.96164/4.94891. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.96300/4.95214. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.95802/4.95189. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.95741/4.94798. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 4.96033/4.94890. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.95892/4.95155. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.95801/4.95310. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.95533/4.95806. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.95493/4.96109. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.95480/4.95150. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.95240/4.96012. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.95294/4.95497. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.95048/4.96139. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 4.94946/4.95990. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.94852/4.95806. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.95153/4.95475. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.94925/4.95524. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.95226/4.95388. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.95435/4.95022. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.95225/4.94895. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.94857/4.94775. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.94882/4.95536. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.95012/4.95383. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.94855/4.95455. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.94945/4.94862. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.94747/4.94903. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.95027/4.94874. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.94333/4.95628. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.94444/4.94923. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.94777/4.94975. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.94393/4.95241. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.94469/4.95045. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.94514/4.96247. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.94906/4.94711. Took 0.11 sec\n",
      "Epoch 53, Loss(train/val) 4.94694/4.94307. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.94551/4.95037. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.94353/4.94883. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.94515/4.95435. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.94620/4.95132. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.93845/4.95477. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 4.94195/4.95007. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.94354/4.94593. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.94928/4.95586. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.94211/4.95044. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.94682/4.95719. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.94112/4.94695. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.94493/4.95138. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.94115/4.94917. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.94581/4.95314. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.94623/4.96100. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.94391/4.95212. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.94416/4.95269. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.94746/4.95788. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.94361/4.94632. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.94256/4.95408. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.94047/4.95757. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.93836/4.95220. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.93902/4.94510. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.94269/4.93892. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.94139/4.95046. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.95738/4.95971. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.95256/4.95927. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.94638/4.95964. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.94515/4.94936. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.94549/4.95611. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.94109/4.94782. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.94703/4.95763. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.94070/4.97084. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.94360/4.94609. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.94057/4.95611. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.94270/4.95274. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.94307/4.94738. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.94393/4.95236. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.94281/4.95283. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.94334/4.94632. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.94138/4.95162. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.94370/4.94944. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.93845/4.96108. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.94101/4.95151. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.94830/4.94826. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.94411/4.96053. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.02404930351219409\n",
      "Epoch 0, Loss(train/val) 4.97860/4.93281. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.91520/4.91569. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.90591/4.92055. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.90818/4.91642. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.90665/4.91324. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.90815/4.91230. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 4.90726/4.91159. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.90783/4.90911. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.90883/4.92455. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.91103/4.92170. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.90910/4.92462. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.90826/4.92297. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.90677/4.92471. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.90751/4.92688. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.90677/4.92933. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.90770/4.92925. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.90622/4.92664. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.90936/4.92100. Took 0.11 sec\n",
      "Epoch 18, Loss(train/val) 4.90470/4.92427. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.90608/4.93131. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.90662/4.92900. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.90530/4.92379. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.90668/4.92004. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.90376/4.92042. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.90429/4.92149. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.90483/4.92570. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.90379/4.92073. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.90347/4.94110. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.90508/4.92372. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.90323/4.92364. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.89922/4.92632. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.90275/4.91708. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.90210/4.91750. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.90176/4.91556. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.89934/4.91949. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.89742/4.91413. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.90101/4.92441. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.90754/4.90657. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.90209/4.90268. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.90050/4.90391. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.90329/4.91388. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.90346/4.91580. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.90315/4.91803. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.89867/4.92667. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.89774/4.91868. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.90000/4.90975. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.89466/4.90981. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.89604/4.91802. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.90029/4.92070. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.89950/4.92659. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.89811/4.91842. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.89718/4.91917. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.89755/4.91908. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.89837/4.90936. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.89942/4.91861. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.89201/4.92194. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.88938/4.92533. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.89392/4.93166. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.89442/4.91585. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.89379/4.93159. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.89020/4.92371. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89041/4.92657. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.89238/4.91316. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.89308/4.92726. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.89070/4.93445. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.89605/4.92498. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.89536/4.91815. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.88651/4.92590. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.89324/4.90905. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.88453/4.91857. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.88685/4.91822. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.89106/4.92399. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.89129/4.90123. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88770/4.91572. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.88759/4.92274. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.88107/4.92871. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.89116/4.92454. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.88707/4.93528. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.88832/4.92300. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.89029/4.91206. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.89069/4.91175. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.88648/4.92252. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.88617/4.93881. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.88541/4.91905. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.88623/4.92071. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.88650/4.91980. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.88253/4.92282. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.88006/4.94192. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.88410/4.92260. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88117/4.92817. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.87748/4.93893. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.87524/4.92705. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.88365/4.90761. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.87717/4.92917. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.87714/4.93204. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.87519/4.91618. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.88173/4.90945. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.87247/4.91149. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.87385/4.90792. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.88415/4.91401. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.049929648757959806\n",
      "Epoch 0, Loss(train/val) 4.60353/4.57926. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.56562/4.58459. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.55872/4.57383. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.55744/4.57038. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.55896/4.56792. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.55485/4.56785. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.55333/4.56030. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.55102/4.56376. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.54842/4.55875. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.54678/4.56793. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.54269/4.55736. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.54874/4.55678. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.53842/4.55991. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.54424/4.56800. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.53671/4.57152. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.54058/4.57043. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.53538/4.54643. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.53692/4.56368. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.53457/4.56858. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.53603/4.57301. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.53643/4.56916. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.53503/4.56606. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.53609/4.56610. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.52876/4.57372. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.53593/4.57024. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.53932/4.58625. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.53111/4.59762. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.53323/4.58119. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.53212/4.59338. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.53585/4.57483. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.53230/4.58721. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.53535/4.57432. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.53098/4.58140. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.53539/4.57393. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.52928/4.58105. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.53347/4.58326. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.53099/4.57770. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.53284/4.56905. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.53266/4.58020. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.53079/4.58127. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.53109/4.56879. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.52667/4.58888. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.52559/4.59082. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.53090/4.57159. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.52294/4.59135. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.52442/4.58687. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.52265/4.59784. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.52208/4.59231. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 4.53216/4.57192. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.52270/4.59328. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.52311/4.58023. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.52688/4.58641. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.52692/4.57426. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.52862/4.57448. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.51816/4.58418. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.52983/4.56772. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.52125/4.57706. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.53034/4.57320. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.52336/4.56588. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.51903/4.58011. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.52128/4.58473. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.52309/4.58144. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.53020/4.56821. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.53401/4.56848. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.52840/4.56855. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.53039/4.61363. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.54790/4.59232. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.53785/4.59685. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.54389/4.59029. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.53706/4.59128. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.53510/4.59207. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.53831/4.57683. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.53190/4.59257. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.53393/4.58966. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.53137/4.58662. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.53063/4.59247. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.53370/4.58440. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.52980/4.59406. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.53408/4.58607. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.53523/4.57588. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.53494/4.58482. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.53351/4.57663. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.52959/4.58276. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.53624/4.57639. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.52801/4.56418. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.53273/4.56704. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.53155/4.55767. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.53127/4.57226. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.52932/4.56077. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.53225/4.55491. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.53170/4.56674. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.52980/4.57594. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.52847/4.56721. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.52121/4.57481. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.52557/4.56825. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.53185/4.57866. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.52520/4.57221. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.52702/4.57712. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.52958/4.58132. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.52930/4.57558. Took 0.08 sec\n",
      "ACC: 0.390625, MCC: -0.224179415327122\n",
      "Epoch 0, Loss(train/val) 4.90658/4.88593. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.90114/4.87669. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.89754/4.88679. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.89344/4.89250. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 4.88253/4.87989. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.88048/4.88164. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.88232/4.88607. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.88349/4.89088. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.88073/4.89580. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.87805/4.89875. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.87697/4.89941. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.87890/4.89554. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.88033/4.89918. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.87673/4.90089. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.87527/4.90173. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.87650/4.89704. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.87486/4.89828. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.87312/4.90218. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.87228/4.90456. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.87275/4.90055. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.87321/4.90135. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.87302/4.89488. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.87297/4.90278. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.87078/4.90588. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 4.87390/4.89354. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.86948/4.91064. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.87277/4.89662. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.87184/4.90275. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.87090/4.89830. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.86766/4.89217. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.86692/4.89906. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.87189/4.89492. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.86980/4.92988. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.87257/4.88278. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.86645/4.90829. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.86458/4.89791. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.86818/4.88848. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.87075/4.89555. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86857/4.89275. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.86088/4.89909. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.86625/4.90267. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.86363/4.89444. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 4.85867/4.90253. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.86113/4.91539. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.86183/4.90016. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.85667/4.89417. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.85737/4.91363. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.85962/4.90957. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.85202/4.92818. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.86055/4.91779. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.85222/4.91936. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.85660/4.90562. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.85259/4.93043. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.85314/4.93433. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.85550/4.90719. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.85033/4.91548. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.86595/4.88429. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.86440/4.89711. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.85868/4.94904. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.86279/4.88729. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.86317/4.90806. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.85338/4.91295. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.85259/4.92355. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.85076/4.92915. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.85378/4.89695. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.84677/4.91196. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.84754/4.96876. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.84738/4.90762. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.84171/4.93084. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.84568/4.92402. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.85104/4.92552. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.85083/4.89990. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.84050/4.94626. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.84153/4.93765. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.84691/4.92060. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.84560/4.93158. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.84334/4.92987. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.84311/4.91286. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.84569/4.93523. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.84599/4.92189. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.84201/4.93575. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.84580/4.90440. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.84249/4.91682. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.83894/4.95356. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83652/4.92823. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83772/4.94809. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.84192/4.93554. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.83659/4.93960. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.83374/5.00251. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.84563/4.94755. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.84284/4.92449. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.83759/4.95495. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83908/4.93262. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.84034/4.91657. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.83120/4.93351. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.83743/4.92899. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.84505/4.91309. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.83255/4.92906. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83496/4.93250. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.84023/4.92065. Took 0.09 sec\n",
      "ACC: 0.578125, MCC: 0.1777495495783369\n",
      "Epoch 0, Loss(train/val) 5.00279/4.96353. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.96916/4.95180. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.97739/4.95626. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.96352/4.95504. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.95318/4.94877. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.95060/4.95099. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.95095/4.95429. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.95044/4.95762. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.95262/4.96216. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.95163/4.96859. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.94754/4.97896. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.95350/4.97098. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.94214/4.96997. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.94679/4.97015. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.94775/4.97155. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.94530/4.97143. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.94397/4.98486. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.94567/4.98296. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.94362/4.98790. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.94201/4.98613. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.94012/4.99689. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.94514/4.97279. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.94109/4.96645. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.94025/4.97201. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.94105/4.97343. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.94228/4.95741. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.94246/4.95963. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.94275/4.96689. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.94024/4.97026. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.93690/4.97315. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.94367/4.96877. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.93803/4.97163. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.93903/4.97476. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.93867/4.97380. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.93643/4.97416. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.93988/4.96693. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.94054/4.96592. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.93632/4.97055. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.93525/4.97396. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.93662/4.97520. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.93761/4.97226. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.93449/4.97210. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.93398/4.96984. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.93910/4.97821. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.93629/4.97667. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.93449/4.98151. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.93946/4.97947. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.93514/4.97918. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.93498/4.97870. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.93577/4.98054. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.93334/4.98908. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.93580/4.98121. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.93548/4.97792. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.92919/4.97779. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.93260/4.98704. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.93378/4.98481. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.93655/4.97134. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.93298/4.97385. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.92917/4.97231. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.92690/4.97991. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.93316/4.98186. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.93826/4.97991. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.93529/4.97176. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.92931/4.96954. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.92758/4.98063. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.93261/4.97739. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.93276/4.98404. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.92619/4.98390. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.94007/4.95313. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.93668/4.95515. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.93205/4.95755. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.93029/4.96676. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.92667/4.97026. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.92931/4.96730. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.93133/4.96724. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.92454/4.96922. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.92514/4.96299. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.93135/4.97478. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.92261/4.97784. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.92920/4.97572. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.92510/4.98139. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.92759/4.97768. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.92231/4.98503. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.93067/4.97173. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.92709/4.96995. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.93202/4.97147. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.92341/4.98303. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.92306/4.96300. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.92997/4.99920. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.93103/4.95486. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.92324/4.97009. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.92454/4.99140. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.92972/4.96138. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.92663/4.95979. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.92130/4.97709. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.92548/4.97330. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.92604/4.96373. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.92297/4.98129. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.92935/4.97998. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.92488/4.96396. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.03734655007457467\n",
      "Epoch 0, Loss(train/val) 5.00748/4.94393. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.92621/4.91676. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.92130/4.91377. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.92449/4.91749. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.92477/4.92756. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.92494/4.92774. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.92549/4.92645. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.92208/4.92540. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.91915/4.92953. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.91515/4.93319. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.91672/4.93455. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.91435/4.93954. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.91538/4.94156. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.91420/4.94090. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.91491/4.93724. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.91066/4.94304. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.90946/4.94466. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.91293/4.94482. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.90978/4.95013. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.90824/4.95495. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.90814/4.94919. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.90708/4.95464. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.90337/4.95702. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.90333/4.96017. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.91001/4.95316. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.90144/4.97231. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.90234/4.96404. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.90072/4.96756. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.90082/4.96340. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.89857/4.97285. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.90444/4.96882. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.89933/4.97488. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.89768/4.98825. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.89646/4.97104. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.89855/4.98143. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.89856/4.97421. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.90177/4.99144. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.89198/4.98221. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.89366/4.99736. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.89612/4.96306. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.89280/4.99045. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.88889/5.00264. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.89714/4.96537. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.89371/4.99691. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.89793/4.97025. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.89325/4.99368. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.88836/4.98544. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.88719/4.98916. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.88312/4.99984. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.89025/4.97327. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.90347/4.95723. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.90810/4.94482. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.90156/4.95939. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.90249/4.96921. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.89865/4.97632. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.89631/4.97668. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.89581/4.97313. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.89173/4.97545. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.89237/4.97635. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.88850/4.98312. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.89089/4.98949. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89275/4.96741. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.89400/4.96830. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.89226/4.96860. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.89249/4.98742. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.88941/5.00781. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.88922/4.99908. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.89170/4.97268. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.88808/4.99600. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.89382/4.96668. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.88637/5.00628. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.89096/4.97027. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88737/5.00381. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88559/4.99681. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.88856/4.98052. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.88366/4.97695. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88639/4.97677. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.88037/5.00387. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.88352/4.98978. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88802/4.97312. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.89108/4.97500. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.89469/4.96353. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.89196/4.98282. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.88456/5.01462. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.88196/5.03061. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.89059/4.98498. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.88676/4.98683. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.88964/4.99596. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.88673/4.98161. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88882/4.99419. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.88199/5.02302. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.88103/4.98310. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.88510/4.98538. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.88434/5.01662. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.88125/4.99152. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.87629/5.00897. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.88135/4.97745. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.88556/5.01031. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.87881/4.99868. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.87874/5.01400. Took 0.08 sec\n",
      "ACC: 0.40625, MCC: -0.20321305705762227\n",
      "Epoch 0, Loss(train/val) 4.88343/4.81650. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.82246/4.81323. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.81846/4.81340. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.81744/4.81495. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.81728/4.81337. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.81576/4.81565. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.81352/4.81266. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.81123/4.81120. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.81210/4.80943. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.80718/4.81292. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.80940/4.81692. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.81089/4.81803. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.80589/4.81945. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.80328/4.82433. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.80370/4.82890. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.80246/4.82807. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.80347/4.83270. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.80226/4.82685. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.80409/4.82750. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.80137/4.83543. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.79969/4.84122. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.79836/4.83656. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.80165/4.83461. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.79893/4.83842. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.79640/4.83699. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.79850/4.82900. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.79710/4.83079. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.79909/4.82729. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.79466/4.82995. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.80215/4.82213. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.79556/4.83198. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.79336/4.83194. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.79418/4.83388. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.79426/4.83638. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.79463/4.83316. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.79715/4.82625. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.79118/4.83622. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.79686/4.83161. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.79319/4.83885. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.79766/4.82666. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.79047/4.83329. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.79350/4.83009. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.79338/4.83090. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.78969/4.83882. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.79204/4.83507. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.79214/4.82432. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.78552/4.84821. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.79224/4.82998. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.79065/4.83935. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.78949/4.82759. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.78591/4.85563. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.79516/4.82528. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.78432/4.85575. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.78268/4.84709. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.79165/4.83756. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.78439/4.86802. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.79052/4.84657. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.78547/4.86848. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 4.79255/4.83368. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.78735/4.84036. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.78482/4.84220. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.79033/4.83294. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.78547/4.86276. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.79254/4.82791. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.78712/4.84995. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.78396/4.84319. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.78160/4.85088. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.79081/4.83341. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.78927/4.84583. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.78320/4.86270. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.78357/4.85717. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78789/4.83599. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.78077/4.85270. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.78100/4.85504. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.77952/4.86109. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.77875/4.83639. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.77852/4.85141. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.78513/4.86512. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.78399/4.83452. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.78090/4.84451. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.77872/4.87403. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.78070/4.86224. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.77650/4.85911. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.77827/4.86333. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.78535/4.84143. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.77072/4.86677. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.77792/4.84596. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.77539/4.86176. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.77691/4.87628. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.77630/4.85437. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.77419/4.86511. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.77660/4.87355. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.77501/4.85958. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.78000/4.84040. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.77574/4.84291. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.76851/4.87875. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.77337/4.86996. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.77553/4.84948. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77274/4.86573. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.77964/4.84745. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.019772733298509516\n",
      "Epoch 0, Loss(train/val) 5.11130/5.03603. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.04146/5.04378. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 5.03838/5.05484. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.03646/5.05336. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.03625/5.05295. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.03819/5.05542. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.03650/5.05747. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 5.03816/5.06292. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.03446/5.06605. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 5.02996/5.07922. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.03949/5.06083. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 5.03148/5.07742. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.02897/5.07516. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 5.02723/5.07834. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.03051/5.06178. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 5.02853/5.05870. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.02742/5.06034. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.02708/5.04467. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 5.02847/5.04830. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.02319/5.05638. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 5.02999/5.05561. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.02113/5.06467. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.02410/5.06088. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.02259/5.08173. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 5.02482/5.06388. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 5.02447/5.07128. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.01983/5.07884. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.02298/5.06387. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.01901/5.07113. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.02014/5.07742. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.01823/5.07398. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.02008/5.07524. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 5.01639/5.07909. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.02128/5.06999. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.01695/5.09125. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.01988/5.08974. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.01967/5.08467. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.01947/5.10051. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 5.01289/5.11582. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.01610/5.10039. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.01442/5.10812. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 5.01512/5.11247. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.01086/5.11416. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.01683/5.11215. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 5.01575/5.13415. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.02207/5.11190. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.01236/5.12257. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.01449/5.10460. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.01312/5.13860. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 5.01808/5.11043. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 5.01414/5.10877. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.00996/5.11063. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.01282/5.11581. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.01137/5.11956. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.01124/5.12324. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 5.01447/5.11192. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.01190/5.13446. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.01671/5.11836. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.01506/5.11796. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.02450/5.08595. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 5.02675/5.08694. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.01859/5.08919. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.02016/5.09562. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.01482/5.12367. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.01326/5.11575. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.01539/5.11707. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 5.01329/5.11411. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.01674/5.10904. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.01227/5.11623. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.01961/5.09393. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 5.01760/5.11398. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.01534/5.08527. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.01040/5.12673. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.01593/5.10483. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.01575/5.11365. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 5.01333/5.11168. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.01262/5.11426. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.00760/5.12893. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 5.00807/5.10535. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.01348/5.11059. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.01033/5.12870. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.01052/5.12312. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.01084/5.11703. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.00706/5.13179. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 5.00678/5.12134. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.01389/5.10013. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.00864/5.11841. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 5.00816/5.13422. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.01400/5.09538. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 5.01905/5.08136. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.01310/5.08829. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.01459/5.09656. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.01118/5.09751. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 5.01087/5.07058. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.00887/5.09187. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 5.01161/5.09155. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.01023/5.09405. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 5.00671/5.10836. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.01179/5.10645. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.01048/5.10947. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.09593745047287525\n",
      "Epoch 0, Loss(train/val) 4.80653/4.87110. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.79245/4.78310. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.78194/4.78578. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.77899/4.79664. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.78025/4.80014. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.77673/4.80163. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.77570/4.79913. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.77788/4.79174. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.77653/4.79905. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.77678/4.79363. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.77775/4.79000. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.77662/4.79491. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.77658/4.80127. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.77489/4.79916. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.77326/4.80151. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.77403/4.79847. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.77059/4.80298. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.77104/4.80345. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.77337/4.79840. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.77137/4.80054. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.77057/4.80319. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.77030/4.80208. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.77100/4.81087. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.76935/4.80226. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.77287/4.79872. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.77279/4.79025. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.77067/4.80314. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.76654/4.80219. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77132/4.80173. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.76914/4.82131. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.76976/4.80831. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.76936/4.79705. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.76978/4.80549. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.76996/4.81515. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.76837/4.80372. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.76623/4.82147. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.76295/4.83319. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.76777/4.82648. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.76687/4.80884. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.76733/4.81647. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.76655/4.81937. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.76942/4.81837. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.76660/4.82280. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.76705/4.82149. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.76685/4.82515. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.76620/4.82849. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.76267/4.82423. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.76505/4.82996. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.76381/4.84006. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.76243/4.83515. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.76152/4.82432. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.76361/4.82545. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.76434/4.83457. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.76010/4.83473. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.76335/4.83785. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.76348/4.83354. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.76275/4.81636. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.76096/4.83074. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.75954/4.84097. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.75586/4.83733. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.75985/4.85340. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.76109/4.84189. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.75827/4.84012. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.76033/4.84335. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.76031/4.84346. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.75814/4.85432. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.76079/4.84100. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.75964/4.84083. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.75789/4.84769. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76326/4.85093. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.75470/4.85596. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.76154/4.84073. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.75744/4.84365. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.76309/4.85521. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.75652/4.84806. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.76165/4.82774. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.76131/4.83471. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.75996/4.82905. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.75753/4.84516. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.75659/4.83568. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.76003/4.85254. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.76246/4.82932. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.75705/4.83452. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.75600/4.84503. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75865/4.83583. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.76088/4.85249. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.76114/4.84410. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.75851/4.83818. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.75642/4.83745. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.76296/4.86251. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.75749/4.84405. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.75706/4.86562. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.75718/4.84895. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.75698/4.85169. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.75760/4.84741. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.75287/4.84968. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.75243/4.84764. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.76196/4.81939. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.75959/4.86238. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.75548/4.85302. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.09416472308615167\n",
      "Epoch 0, Loss(train/val) 5.09304/5.07665. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.06871/5.07330. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.05494/5.07401. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.05611/5.06484. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.05628/5.06281. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.05327/5.06361. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.04930/5.06411. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.04652/5.06299. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.04991/5.06508. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.05008/5.06588. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.04787/5.06549. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.04661/5.06511. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.04780/5.06776. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.04722/5.06918. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.04414/5.07216. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.04376/5.07319. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.04405/5.07501. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.04322/5.06839. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.04244/5.07273. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.04248/5.06830. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.04176/5.06929. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.03815/5.07107. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.03903/5.07012. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.03605/5.07455. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.03885/5.06591. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.04345/5.06293. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.04150/5.06810. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.03827/5.07591. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.04009/5.06942. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.03649/5.07680. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.03631/5.07073. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.03795/5.07576. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.03763/5.08219. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.03985/5.06833. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.03500/5.06934. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.03565/5.06305. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.03532/5.06304. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.03353/5.07047. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.03495/5.06712. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.03176/5.06588. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 5.03706/5.05921. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.02925/5.06951. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.02914/5.06791. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.03785/5.05451. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.03126/5.05699. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.03285/5.07505. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.03788/5.06418. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.03374/5.06665. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.03310/5.06930. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.03201/5.05838. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 5.03221/5.06097. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.03730/5.06164. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.03200/5.06254. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.03070/5.06646. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.02579/5.07217. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.03274/5.05741. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.03341/5.06029. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.03137/5.07651. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.02578/5.06015. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 5.02876/5.05934. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.03020/5.06150. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.02928/5.06412. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 5.02858/5.05741. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.02367/5.06354. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.02484/5.06001. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 5.03247/5.04743. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.02852/5.05527. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.02509/5.05792. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.02630/5.05590. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.02256/5.06543. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.02310/5.06346. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.03005/5.05427. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 5.02898/5.06886. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.02788/5.07233. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.02555/5.06745. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.02916/5.06151. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.03039/5.06299. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.02719/5.06534. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 5.02428/5.08460. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.03092/5.05043. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.02674/5.06743. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.02541/5.06941. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.02699/5.06907. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.02817/5.06956. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.02897/5.06477. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.02281/5.07251. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.02614/5.06152. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.01975/5.08256. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.02595/5.06845. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.02462/5.08718. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.03112/5.06969. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.02447/5.07405. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 5.01981/5.09444. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.02875/5.07334. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.02358/5.08832. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.02778/5.07583. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.02795/5.07564. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.01814/5.06483. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.02819/5.06273. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.02414/5.07843. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.03779786830012622\n",
      "Epoch 0, Loss(train/val) 4.67016/4.54830. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.59088/4.60471. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.59173/4.64840. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.59548/4.66309. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.59621/4.64677. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.59266/4.61214. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.58926/4.60235. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.58438/4.61142. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.58646/4.61526. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.58508/4.60968. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.58554/4.61132. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.58629/4.61149. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.58586/4.60893. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.58445/4.59757. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.58121/4.59961. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.58216/4.60226. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.57812/4.59687. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.57809/4.60258. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.57905/4.60178. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.57798/4.59455. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.57936/4.60017. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.57803/4.58800. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.57527/4.59381. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.57517/4.59293. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.57396/4.58687. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.57190/4.59911. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.57644/4.58621. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.57316/4.59368. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.57337/4.59278. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.57157/4.58432. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.57361/4.59569. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.57215/4.58933. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.56997/4.58885. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.56826/4.59426. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.56828/4.60904. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.56965/4.58959. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.56786/4.59010. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.56749/4.59896. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.57049/4.58964. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.56353/4.59480. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.56457/4.59858. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.56623/4.59994. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.56650/4.60497. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.56378/4.59978. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.56592/4.59557. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.56301/4.60258. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.56214/4.60358. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.56002/4.60758. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.55830/4.61023. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.56566/4.58662. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.56138/4.61237. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.56299/4.60444. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.55706/4.59905. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.56342/4.59137. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.55858/4.59966. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.55796/4.59814. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.55725/4.60320. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.55820/4.59953. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.56029/4.59672. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.55746/4.60039. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.55712/4.60730. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.55682/4.58388. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.55820/4.60220. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.55492/4.60177. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.55896/4.59590. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.55855/4.58156. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.55686/4.59901. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.56054/4.60636. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.55917/4.60605. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.55834/4.59651. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.55378/4.61518. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.55216/4.61601. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.55702/4.61387. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.55600/4.60494. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.55248/4.60890. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.55728/4.59748. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.55355/4.60476. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.55393/4.60863. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.54966/4.59165. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.55217/4.59959. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.54895/4.60350. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.55647/4.60832. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.55271/4.58432. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.55029/4.61206. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.55263/4.59302. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.54854/4.61402. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 4.55539/4.58782. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.54840/4.60615. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.55331/4.60824. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.55077/4.60540. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.55063/4.61032. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.55340/4.60014. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.54572/4.62383. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.55116/4.60118. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.54537/4.63396. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.54923/4.59697. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.55038/4.60429. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.55106/4.61122. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.54702/4.60042. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.55125/4.60529. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.035451483867683334\n",
      "Epoch 0, Loss(train/val) 4.83996/4.76956. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.78170/4.77452. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.78055/4.76862. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.78237/4.76674. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.78063/4.76611. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.78211/4.76660. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.78013/4.76448. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.78060/4.76200. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.77726/4.76245. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.77564/4.76180. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.77394/4.76325. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.77408/4.76702. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.77685/4.76032. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.77447/4.76334. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.77185/4.76283. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.77626/4.77188. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.77697/4.76719. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.77653/4.76210. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.77385/4.76167. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.77068/4.76042. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.77041/4.75924. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.76851/4.75569. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.76866/4.75618. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.76834/4.76805. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.76981/4.76554. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.76682/4.76613. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.76467/4.76906. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 4.76325/4.76470. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77043/4.76702. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.76745/4.76916. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.76497/4.77492. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.76609/4.77056. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.75654/4.77032. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.77137/4.77448. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.76759/4.76462. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.76276/4.77649. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.75941/4.77236. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.75898/4.78143. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.76179/4.77927. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.75809/4.77590. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.76127/4.77099. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.75609/4.79681. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.76406/4.77586. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.75786/4.78520. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.75587/4.78857. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.75633/4.78736. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.75981/4.78613. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.76963/4.79474. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77189/4.78943. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.76336/4.78117. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.76501/4.78305. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.75661/4.80081. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.76044/4.77855. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.76059/4.78516. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.75843/4.79796. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.76240/4.78717. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.76235/4.78934. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.75569/4.79512. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.75772/4.80419. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.76153/4.78777. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.75753/4.77770. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.75958/4.81911. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.75898/4.78968. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.75706/4.77086. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.75880/4.80226. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.76129/4.78505. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.76471/4.77701. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.75211/4.77794. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.75030/4.76764. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76189/4.76604. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.75691/4.79022. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.76002/4.78174. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76188/4.79400. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.75467/4.77039. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.75129/4.81731. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.75883/4.78989. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.75825/4.80501. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.75169/4.79776. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.75434/4.78198. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.75344/4.78778. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.75390/4.81463. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.75484/4.77877. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.75954/4.79570. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.76152/4.79869. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.74892/4.79599. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.75232/4.78758. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.75273/4.79788. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.75474/4.80005. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.74892/4.79862. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.75629/4.78968. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.75123/4.79781. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.74851/4.77293. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.75320/4.79547. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.75028/4.78636. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.74843/4.79195. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.75453/4.80622. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.75464/4.79271. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.75457/4.80525. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.74986/4.78855. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.75312/4.81784. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.08952126702661474\n",
      "Epoch 0, Loss(train/val) 5.01697/4.98288. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.99752/5.00262. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.99223/5.02713. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.99207/5.04561. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.99719/5.04224. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.99299/5.00888. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.99199/4.99033. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.98800/4.99574. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.98710/4.99966. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.98729/4.99828. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.98413/4.99899. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.98774/4.99604. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.98491/4.99268. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.98631/4.99793. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.98176/4.98747. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.98361/4.98636. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.97786/4.98699. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.97989/4.97977. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.97975/4.97857. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.97789/4.98315. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.97508/4.97875. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.97012/4.97510. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.97363/4.98877. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.97591/4.98278. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.97450/4.98735. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.97037/4.97929. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.97270/4.97388. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.97247/4.97590. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.96687/4.97669. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.97026/4.97186. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.97108/4.97302. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.96602/4.98375. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.96286/4.97395. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.97302/4.97474. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.96817/4.98035. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.96508/4.97134. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.96235/4.97446. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.95959/4.97392. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.96204/4.97233. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.96118/4.99354. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.96339/4.97233. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.96339/4.96573. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.96146/4.97028. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.95815/4.97033. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.96695/4.97695. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.96106/4.97766. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.95782/4.97793. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.95728/4.97579. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.96425/4.96986. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.96125/4.96371. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.96511/4.95055. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.96386/4.98541. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.96118/4.96713. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.96164/4.96526. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.96192/4.97383. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.95446/4.96479. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.96061/4.96031. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.96241/4.97434. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.96550/4.97326. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.95661/4.96843. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.95683/4.96316. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.95978/4.97353. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.95173/4.95762. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.95364/4.96925. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.94851/4.96514. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.95555/4.96347. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.96064/4.96802. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.95723/4.96834. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.95325/4.96761. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.95817/4.96705. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.95588/4.96477. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.95814/4.96926. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.95280/4.95415. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.95861/4.97615. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.95203/4.97982. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.95417/4.99141. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.95693/4.98267. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.95495/4.97893. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.95370/4.97009. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.95126/4.98064. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.95589/4.97020. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.95295/4.96122. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.95204/4.96499. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.95683/4.99028. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.94405/4.98035. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.95445/4.97608. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.95663/4.98626. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.95719/4.97142. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.95327/4.98722. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.94848/4.97523. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.95568/4.96161. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.94856/4.97411. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.95048/4.95249. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.95225/4.95719. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.95077/4.97072. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.94864/4.97263. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.94913/4.96284. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.94507/4.97455. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.94827/4.97311. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.95059/4.95906. Took 0.09 sec\n",
      "ACC: 0.46875, MCC: -0.08544439848099883\n",
      "Epoch 0, Loss(train/val) 5.24487/5.14741. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.16087/5.13336. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.16142/5.13512. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.16425/5.13802. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.16005/5.13991. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.15686/5.13691. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.15871/5.13611. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.15533/5.13723. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.15733/5.13766. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 5.15616/5.13825. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.15596/5.13862. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.15295/5.14108. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.15204/5.14163. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.15299/5.14250. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.15206/5.14495. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 5.15210/5.14214. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.15136/5.14426. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.15054/5.14996. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.14775/5.15067. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.14516/5.14617. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.15002/5.14826. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.14535/5.14770. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.14364/5.15024. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 5.14230/5.14696. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.14185/5.13974. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.14444/5.14153. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 5.14669/5.16465. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.14869/5.15899. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.14666/5.14098. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.14512/5.16386. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.14711/5.16562. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.14121/5.14961. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 5.14482/5.17503. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.14051/5.14806. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.13939/5.15266. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.14068/5.15231. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.13813/5.15807. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.13866/5.14002. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.13882/5.15238. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.13849/5.15241. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.14237/5.15841. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.13623/5.15209. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.13280/5.17724. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.13633/5.14628. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.13331/5.15816. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 5.13511/5.17768. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.12774/5.15789. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.13236/5.16568. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.13090/5.15101. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 5.13183/5.15188. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.13103/5.18112. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.13526/5.16651. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.12908/5.16742. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.13366/5.17170. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 5.12733/5.16589. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.13258/5.17215. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.12452/5.18494. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.12767/5.16901. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.12658/5.17462. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.12791/5.18457. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.13014/5.17312. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 5.12635/5.16978. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.13604/5.16471. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.12835/5.16035. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 5.12579/5.18033. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.13052/5.16938. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.12082/5.16629. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.12464/5.17792. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.12213/5.17399. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.12130/5.19192. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.12266/5.17110. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.12252/5.17996. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.11876/5.18970. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.12158/5.18111. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.12741/5.18121. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.12552/5.17122. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.12616/5.17704. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.12070/5.18001. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 5.12080/5.18481. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.11933/5.18192. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.11982/5.17089. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 5.11920/5.15961. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.12621/5.17361. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.12098/5.17235. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 5.12150/5.16968. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.11020/5.17348. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 5.12193/5.18025. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.11203/5.18883. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.12216/5.15970. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.11798/5.17696. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.12006/5.17754. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 5.12152/5.18850. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.11780/5.17144. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.11802/5.16875. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.11608/5.17872. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.11738/5.16946. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.11777/5.17774. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.11836/5.17030. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.11250/5.18238. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.11663/5.17567. Took 0.09 sec\n",
      "ACC: 0.578125, MCC: 0.15694120514358612\n",
      "Epoch 0, Loss(train/val) 4.92824/4.98920. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.89637/4.86876. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.88294/4.86160. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.86365/4.86380. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.86504/4.86768. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.86554/4.87371. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.86644/4.87395. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.86520/4.87463. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.86356/4.87689. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.86366/4.87749. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.86141/4.88043. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.86178/4.87999. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.86056/4.87704. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.85977/4.87734. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.86097/4.87675. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.86068/4.87818. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.85752/4.88311. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.85922/4.88543. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.85936/4.88341. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85838/4.88955. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.85547/4.89387. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.85532/4.89360. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.85841/4.89594. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.85405/4.89511. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.85115/4.90743. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.85200/4.90115. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.85000/4.90067. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.85597/4.88767. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.85231/4.89120. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84943/4.88876. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.85433/4.87973. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.85289/4.88545. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.84775/4.89403. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.85115/4.89623. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.85304/4.89015. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84359/4.91399. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.84588/4.91670. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84842/4.90945. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84696/4.91337. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.84596/4.91659. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84558/4.90477. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.84405/4.92317. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.84508/4.92515. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.83962/4.93263. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.84494/4.91495. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.84220/4.92639. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.84631/4.89951. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.84145/4.92618. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.84017/4.92221. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.83694/4.92894. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.83972/4.92557. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.84345/4.93250. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.83855/4.92204. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 4.83883/4.91314. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.83844/4.92454. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.83005/4.92383. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.83549/4.93979. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.83899/4.91619. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.83822/4.91426. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83339/4.91831. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.84869/4.88302. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.85470/4.87760. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.85118/4.89922. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.84526/4.90833. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.85192/4.89829. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.85000/4.91666. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.84670/4.91525. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.84312/4.92280. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.84274/4.93642. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.84724/4.93587. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.84907/4.88082. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.85988/4.87526. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.85057/4.86978. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.84684/4.89153. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83848/4.91989. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.84521/4.92558. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.83538/4.93737. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.83104/4.98521. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.84173/4.94454. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.83775/4.94580. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83454/4.97540. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.83354/4.96934. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.83668/4.93415. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.84059/4.92072. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83277/4.96855. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.82868/4.98147. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.83641/4.92278. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.83343/4.93843. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83215/4.96510. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.82985/4.95616. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.82860/4.95803. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.83242/4.94426. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83043/4.95840. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83149/4.96247. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.82557/4.93808. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.83025/4.94030. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.82925/4.94600. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.82949/4.95236. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.82857/4.95459. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.82879/4.95597. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.09416472308615167\n",
      "Epoch 0, Loss(train/val) 4.75441/4.71827. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.71867/4.71466. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.72888/4.71895. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.73079/4.73249. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.72777/4.71668. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.71355/4.72137. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.71171/4.71875. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.71117/4.71994. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.71416/4.71751. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.71316/4.72371. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.71279/4.71887. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.71172/4.72353. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.71191/4.72068. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.71191/4.72259. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.71030/4.73122. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.70969/4.72347. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.70990/4.72617. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.71159/4.72300. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.70964/4.72659. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.71076/4.72527. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.70977/4.72590. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.70982/4.72743. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.70703/4.73046. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.70926/4.72602. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 4.70991/4.72291. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.70846/4.71663. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.70739/4.72861. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.70870/4.72408. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.71010/4.72044. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.70662/4.73445. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.70527/4.72915. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.70887/4.72865. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.70779/4.72850. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.70542/4.73027. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.70585/4.72732. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.70719/4.72067. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.70621/4.73925. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.70576/4.73179. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.70291/4.72742. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.70361/4.73138. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.70096/4.74910. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.70408/4.73100. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.70081/4.74611. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.69996/4.73269. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.70156/4.73392. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.69676/4.73434. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.70146/4.72256. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.69790/4.72858. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.69972/4.72526. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.70390/4.73376. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.69707/4.73978. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.70107/4.73445. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.70020/4.73406. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.69464/4.72636. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.70384/4.71790. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.69419/4.73253. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.69437/4.74283. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.69133/4.73324. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.70084/4.73485. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.69533/4.72632. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.69515/4.74086. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.69933/4.74132. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.68886/4.75347. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.69201/4.75377. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.69296/4.74196. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.69468/4.75909. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.69878/4.74139. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.68919/4.75159. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.69707/4.74375. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.69233/4.74485. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.69120/4.75015. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.69490/4.72951. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.69139/4.74608. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.69100/4.74762. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.69051/4.74337. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.69707/4.74775. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.69042/4.73636. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.69054/4.75221. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.69092/4.74914. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.68899/4.73581. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.68624/4.74692. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.68620/4.77641. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.69680/4.73834. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.68685/4.76854. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.68657/4.77175. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 4.69049/4.76522. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.69123/4.75493. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.68815/4.76752. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.68472/4.75299. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.67641/4.77572. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.69251/4.74765. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.67925/4.76337. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.68853/4.75739. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.68164/4.75916. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.68095/4.77600. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.67650/4.79413. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.67479/4.78599. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.68378/4.79019. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.68512/4.75525. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.67179/4.78044. Took 0.09 sec\n",
      "ACC: 0.484375, MCC: -0.07539645724831788\n",
      "Epoch 0, Loss(train/val) 4.91404/4.88347. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.85969/4.86264. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.85538/4.85797. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.85847/4.85923. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.85915/4.85953. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.86060/4.86103. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.85646/4.86237. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.85648/4.86164. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.85949/4.86214. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.85768/4.86285. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.85598/4.86322. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.85711/4.86582. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85542/4.86810. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.85198/4.87085. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.85469/4.87106. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.85377/4.87372. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.85375/4.87579. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.85579/4.86551. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 4.85864/4.86721. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85496/4.87226. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.85242/4.88127. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.85111/4.88511. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.85175/4.88462. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.85246/4.88762. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.85116/4.89201. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.85305/4.88310. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 4.85072/4.88396. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.84724/4.89849. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.85014/4.88588. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.85111/4.88978. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.84828/4.88854. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.84736/4.89631. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84859/4.89934. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.84467/4.90710. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.84683/4.88966. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84834/4.89849. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.84232/4.92357. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84288/4.89703. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84481/4.92697. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.84943/4.89930. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.83979/4.95790. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.84779/4.89565. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84011/4.96492. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.84994/4.89420. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.84091/4.93999. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.84021/4.93601. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.84070/4.93723. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.83663/4.96359. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84357/4.93478. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.83587/4.95129. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.83648/4.95112. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.84294/4.94262. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.83884/4.96004. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.83379/4.98650. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.83930/4.95522. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.83603/4.96742. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.84317/4.92495. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.82812/4.99387. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.84022/4.95641. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83398/4.98318. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.83097/4.97635. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.82770/5.01367. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.82587/5.01475. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.83680/4.94090. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.83094/5.00658. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.83149/4.99601. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.83400/5.00815. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.82306/5.01945. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.82961/4.97683. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.82339/4.99046. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.82289/5.02428. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.83719/4.94039. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.82818/4.98158. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.83246/4.95015. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.82279/4.98427. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.81968/5.01086. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.83093/5.00224. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.82033/5.00752. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.85139/4.87356. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.84687/4.87805. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.84549/4.89961. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.84415/4.91590. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.84155/4.91189. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.84176/4.91626. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83729/4.93882. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.84181/4.92954. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83584/4.91786. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.84325/4.91851. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83178/4.94294. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.83686/4.94384. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.83800/4.91131. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 4.83421/4.92236. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83680/4.92476. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83367/4.91817. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.83642/4.93463. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.83618/4.91332. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.83509/4.92970. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.83015/4.94487. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83108/4.95157. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.83932/4.91022. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.04350001325672093\n",
      "Epoch 0, Loss(train/val) 4.92829/4.89625. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.88292/4.87095. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.87670/4.87141. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.87298/4.87585. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.87008/4.87701. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.87240/4.87834. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.87308/4.87995. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.87329/4.88118. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.87315/4.88089. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.87391/4.88062. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.86333/4.87922. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.87498/4.88055. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.87194/4.88505. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.87105/4.88515. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.86902/4.88486. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.87376/4.88763. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.87256/4.88487. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.86982/4.88536. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.86755/4.88369. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.86748/4.89248. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.86993/4.88833. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.86760/4.89511. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.86656/4.89060. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.86572/4.89163. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.86501/4.89155. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.86489/4.89127. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.86290/4.89653. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.86373/4.89049. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.86555/4.88527. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.86066/4.90788. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.86607/4.88832. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.86386/4.91477. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.86300/4.89875. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.86376/4.90132. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.85467/4.91863. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.85606/4.91177. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.86117/4.91024. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.85631/4.90755. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.85723/4.89388. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.85992/4.90416. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.85935/4.90078. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.85799/4.91433. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.86175/4.88808. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.86123/4.90150. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.85981/4.90856. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.85611/4.92385. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.86771/4.88262. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.86455/4.89517. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.87170/4.88866. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.86487/4.89433. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.86347/4.90184. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.86400/4.94272. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.86808/4.89186. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.86820/4.89409. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.87126/4.90025. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.86382/4.91550. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.86994/4.89212. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.86524/4.89255. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.86414/4.90903. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.86474/4.91950. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.86466/4.91278. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.85970/4.92913. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.86081/4.91892. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.85929/4.93726. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.85918/4.91891. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.85540/4.93262. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.86364/4.89350. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.85564/4.92179. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.85962/4.89886. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.85521/4.91664. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.85663/4.90300. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.85397/4.90902. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.84925/4.93764. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.85904/4.90534. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.84917/4.91943. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.85519/4.93019. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.84981/4.92641. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.85632/4.89927. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.86267/4.87625. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.86495/4.88983. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.85769/4.88618. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.85961/4.88364. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.85610/4.89272. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.86508/4.89350. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.86090/4.89634. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.85882/4.89770. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.85833/4.95080. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.86215/4.91007. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.85394/4.92472. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 4.85757/4.89776. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.85032/4.92997. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.85742/4.91803. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 4.85661/4.90867. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.85315/4.91814. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.85745/4.90523. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.84843/4.91730. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.85538/4.92144. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.84837/4.94542. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.85666/4.92111. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.84976/4.93126. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.02757337634153148\n",
      "Epoch 0, Loss(train/val) 4.81267/4.80612. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.79611/4.79938. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.79808/4.79816. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.79232/4.80376. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.79343/4.80960. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.79285/4.81386. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.79512/4.81529. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.80128/4.80785. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.80541/4.81598. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79667/4.82562. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79080/4.81558. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.78877/4.82224. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.79296/4.82067. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.78894/4.82976. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.78981/4.82587. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.79044/4.82751. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.78948/4.83898. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.78986/4.83223. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.79057/4.82957. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.78781/4.83753. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.79097/4.83101. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.78641/4.84018. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.78742/4.84437. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.78500/4.85545. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.78910/4.84480. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.78341/4.87245. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.78649/4.85666. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.78442/4.86534. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.78675/4.86914. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.78008/4.88587. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.78468/4.85440. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78066/4.86881. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.77878/4.86647. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.78661/4.86824. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.77740/4.89984. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.78189/4.90876. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.77847/4.86862. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.77992/4.89165. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.77343/4.87506. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.77986/4.89342. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.77403/4.91903. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.77123/4.90061. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.76810/4.86530. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.77455/4.93019. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.78152/4.87180. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.76702/4.89970. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.76939/4.93033. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.76750/4.92524. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.78192/4.86262. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.77041/4.90055. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.76918/4.90755. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.76960/4.92138. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.76783/4.91631. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.76842/4.92140. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.76038/4.93145. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.76982/4.90817. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.76380/4.90823. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.76670/4.93290. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76197/4.91339. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.75923/4.89477. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.76378/4.91540. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.76491/4.93226. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.76308/4.92559. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.76313/4.91903. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.75771/4.90818. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.76271/4.89613. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.76136/4.90379. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.76566/4.91611. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.75931/4.91110. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76275/4.89925. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.75599/4.94562. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.75787/4.94816. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76373/4.90430. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.75785/4.90441. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.76146/4.92754. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.75508/4.91982. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.75616/4.94974. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.76745/4.89376. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.75428/4.89894. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.75154/4.95308. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.76252/4.93905. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.75748/4.92655. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.75243/4.91734. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.75824/4.90558. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75361/4.91184. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.77246/4.88836. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.76041/4.88896. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.76510/4.90747. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.75911/4.87072. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.75646/4.86873. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.76666/4.87655. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.75899/4.90831. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.76178/4.90592. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.76259/4.90272. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.76332/4.89040. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.75842/4.89341. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.81012/4.81697. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.78779/4.81692. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.78521/4.83128. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.78103/4.84555. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.0625\n",
      "Epoch 0, Loss(train/val) 4.74076/4.68011. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.68990/4.67331. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.68843/4.66550. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.68569/4.66700. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.68459/4.66697. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.68546/4.66740. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.68450/4.66733. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.68454/4.66723. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.68465/4.66925. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.68391/4.66968. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.68886/4.67252. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.68447/4.67621. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.68347/4.67784. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.68322/4.67597. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.68307/4.67533. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.68083/4.67599. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.68248/4.67587. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.68258/4.67652. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.68308/4.67927. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.68386/4.67979. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.67705/4.68310. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.67956/4.68048. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.67947/4.67873. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.67804/4.68189. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.68147/4.67975. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.67928/4.68185. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.67909/4.67906. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.67943/4.67865. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.67777/4.67825. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.67528/4.68041. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.67469/4.67480. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.67539/4.67677. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.67657/4.68465. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.67604/4.68119. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.67497/4.68063. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.67565/4.67628. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.67568/4.68007. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.67675/4.67627. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.67634/4.67889. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.67600/4.68086. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.67233/4.68369. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.67188/4.68114. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.67724/4.68020. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.67243/4.68138. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.67200/4.68207. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.67542/4.68199. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.67971/4.69678. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.68059/4.67246. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.67675/4.67621. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.67413/4.68097. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.67177/4.68344. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.67339/4.68754. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.67054/4.68287. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.67099/4.68714. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.66784/4.67998. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.67296/4.69027. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.67085/4.68524. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.66955/4.69084. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.67659/4.68710. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.67216/4.68142. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.67278/4.68663. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.67144/4.68276. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.67051/4.68380. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.67736/4.68488. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.67945/4.68411. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.67532/4.68155. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.67152/4.69065. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.67318/4.68311. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.66902/4.67685. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.66941/4.68032. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.66557/4.67591. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.67359/4.67453. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.66728/4.67403. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.66727/4.67811. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.66668/4.68085. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.66658/4.68415. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.67441/4.67720. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.66718/4.69712. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.66581/4.68938. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.66852/4.68985. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.66782/4.68463. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.66142/4.69049. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.66721/4.68032. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.66640/4.68458. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.66619/4.68996. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.66963/4.69937. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.66520/4.68457. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.67072/4.69254. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.66674/4.70291. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.66239/4.69491. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.68049/4.69566. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.68049/4.68053. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.67833/4.68598. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.67607/4.69066. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.67189/4.69962. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.67112/4.71156. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.67175/4.70540. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.66542/4.71589. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.67186/4.70967. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.66864/4.70320. Took 0.09 sec\n",
      "ACC: 0.421875, MCC: -0.191246010612082\n",
      "Epoch 0, Loss(train/val) 4.92066/4.96266. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.91901/4.88996. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.89973/4.89086. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.88923/4.88688. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.88888/4.88638. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.88797/4.88632. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.88886/4.88602. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.89065/4.88493. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.88832/4.88476. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.88837/4.88389. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.88930/4.88374. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.88803/4.88280. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.88753/4.88298. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.88781/4.88088. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.88528/4.88285. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.88378/4.88690. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.88198/4.89235. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.88155/4.89451. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.87969/4.89427. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.88020/4.89313. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.87911/4.89538. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.88328/4.89402. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.88771/4.88117. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.88236/4.88541. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.88605/4.88409. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.88151/4.89088. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.88041/4.89342. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.88429/4.89006. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.87914/4.89146. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.87619/4.90337. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.88390/4.89034. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.87788/4.89124. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.88304/4.89058. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.87444/4.89217. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.87776/4.88721. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.87462/4.89095. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.87887/4.88999. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.87712/4.88955. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.87725/4.89999. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.87621/4.90183. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.88046/4.89624. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.87616/4.89696. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.87743/4.89738. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.87499/4.90258. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.87569/4.89939. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.87258/4.90530. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.87817/4.89742. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.87504/4.89942. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 4.87501/4.89385. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.87442/4.89538. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.87181/4.89878. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.87205/4.89882. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.87294/4.89462. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.86956/4.90081. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.87531/4.89328. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.86950/4.89158. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.86640/4.89284. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.86979/4.89182. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.86864/4.89382. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.86723/4.89982. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.86728/4.89642. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.86760/4.89735. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.86681/4.89325. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.86667/4.89941. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.86389/4.90368. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.86198/4.88653. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.87116/4.88253. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.86138/4.88881. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.86655/4.88810. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.85910/4.89574. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.86571/4.88470. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.86290/4.89610. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.86140/4.89464. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.86513/4.88438. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.86554/4.88614. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 4.85539/4.89256. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.85565/4.88214. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.85426/4.89840. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.85833/4.89261. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.85990/4.88647. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.85633/4.89710. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.85819/4.88098. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.85086/4.89222. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.85178/4.90884. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.85735/4.87990. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.85058/4.89332. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.85600/4.90562. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.85374/4.88955. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.85792/4.89166. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.87095/4.92811. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.87784/4.89246. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.87346/4.89630. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.86857/4.90942. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.86887/4.91719. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.86588/4.90543. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.86715/4.90893. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.86085/4.91223. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.86463/4.90929. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.86359/4.90121. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.85737/4.91224. Took 0.08 sec\n",
      "ACC: 0.34375, MCC: -0.314970394174356\n",
      "Epoch 0, Loss(train/val) 4.87479/4.82459. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.83780/4.82232. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.84055/4.82913. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.84002/4.83826. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.84220/4.82818. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.83874/4.81902. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 4.83237/4.81368. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.82866/4.81250. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.83292/4.81627. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.83408/4.81436. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.82937/4.81470. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.82657/4.81547. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.82940/4.82004. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.82342/4.81346. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.82948/4.82255. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.82271/4.82527. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.82752/4.81959. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.82231/4.82472. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.82252/4.82234. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.82160/4.83441. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.81884/4.83305. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.82198/4.83236. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.81984/4.83786. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.81946/4.81776. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.82348/4.82515. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.81812/4.83233. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.81374/4.83463. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.81551/4.83628. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.81742/4.83951. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.81404/4.83690. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.81592/4.83468. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.81343/4.84174. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.82058/4.84373. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.81409/4.83851. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.81457/4.84294. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.81368/4.83892. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.81626/4.83711. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.81685/4.84156. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.81327/4.83441. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.81366/4.83976. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.81188/4.83972. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.81123/4.83340. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.81515/4.83950. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.81410/4.83367. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.81243/4.84672. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.80895/4.84788. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.81011/4.85113. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.81173/4.84759. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.81304/4.84024. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.80703/4.84907. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.80750/4.84997. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.81240/4.84371. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.80878/4.84657. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.81277/4.84286. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.81062/4.85261. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.80749/4.85056. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.80695/4.85205. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.81364/4.84431. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.80895/4.85096. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.80680/4.84334. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.80934/4.85121. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.80932/4.84983. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.80640/4.84237. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.81196/4.85031. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.80882/4.84935. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.80914/4.85391. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.80455/4.84771. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.80416/4.85855. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.81099/4.85520. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.80918/4.85210. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 4.81050/4.84510. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.80381/4.84852. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.80794/4.84738. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.80109/4.85722. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.80499/4.85888. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.79945/4.86085. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.81371/4.85292. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.80323/4.86509. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.81014/4.84735. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.80627/4.84952. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.80562/4.84594. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.80324/4.85762. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.80660/4.84681. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.80240/4.84990. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.80064/4.84803. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.80622/4.85083. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.80484/4.84149. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.80511/4.86090. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.80346/4.85248. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.80453/4.85025. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.80140/4.84780. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.80109/4.86110. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.80378/4.84366. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.80388/4.84344. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.79888/4.86833. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.79852/4.84989. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.80181/4.86376. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.79901/4.86766. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.80593/4.84967. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.80070/4.85480. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.08818077954471167\n",
      "Epoch 0, Loss(train/val) 4.88543/4.86545. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.84241/4.83796. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.83564/4.84239. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.83265/4.84629. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.83502/4.84150. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.83659/4.84131. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.83623/4.84116. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.83267/4.84197. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.83236/4.83788. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.82909/4.84162. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.83074/4.84114. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.82889/4.83772. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.82795/4.83960. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.82794/4.84137. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.82902/4.84218. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.82470/4.85321. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.83038/4.84829. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.82515/4.84672. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.82542/4.84253. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.82871/4.83742. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.82225/4.84459. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.82219/4.84001. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.81797/4.84553. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.81831/4.84721. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.81739/4.84879. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.81539/4.85247. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.82141/4.84446. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.81978/4.85993. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.82124/4.86215. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.82262/4.85866. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.81247/4.85954. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.82118/4.84965. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.82065/4.85813. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.82072/4.84845. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.81213/4.85028. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.81687/4.83742. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.81001/4.84686. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.81336/4.84281. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.80462/4.84939. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.80595/4.85244. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.81203/4.84469. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.81379/4.84115. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.80939/4.84617. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.80788/4.84845. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.81200/4.84421. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.80398/4.84627. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.80679/4.83556. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.80819/4.84349. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.80771/4.84585. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.81289/4.83709. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.80069/4.84434. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.81093/4.84560. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.80321/4.85613. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.80961/4.84181. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.80509/4.83562. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.80440/4.84612. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.80211/4.85001. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.79903/4.85763. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.79576/4.84502. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.79666/4.84031. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.79635/4.85578. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.79971/4.85266. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.80486/4.83343. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.79811/4.83747. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.80459/4.84093. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.81311/4.83872. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.80602/4.83332. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.80640/4.84844. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.80873/4.83036. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 4.80020/4.83389. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.80398/4.83713. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.80579/4.84684. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.79996/4.84843. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.80129/4.84950. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.79318/4.86829. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.79628/4.84269. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.79364/4.83463. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.79272/4.85013. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.79186/4.84834. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.79458/4.84793. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.79525/4.84592. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.78996/4.84743. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.79632/4.84457. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.79096/4.85084. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.79955/4.85221. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.79343/4.84745. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.80602/4.83364. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.79791/4.83145. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.79144/4.85375. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.79268/4.85136. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.78992/4.84859. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.78978/4.83656. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.78567/4.88507. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.78467/4.85692. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.78559/4.86884. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.79010/4.87128. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.78228/4.86360. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.78180/4.86609. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.78574/4.85407. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.78631/4.86576. Took 0.08 sec\n",
      "ACC: 0.609375, MCC: 0.2710196976763859\n",
      "Epoch 0, Loss(train/val) 4.99103/4.90978. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.91319/4.90939. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.91313/4.90818. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.90970/4.91322. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.91645/4.91231. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.91381/4.90903. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.91431/4.90504. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.90816/4.89977. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.90913/4.90434. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.91138/4.89897. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.90933/4.89673. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.90425/4.89656. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.90214/4.89819. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.90435/4.89744. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.89957/4.90173. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.90008/4.90044. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.90146/4.90008. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.89928/4.89826. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.90255/4.89811. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.89965/4.90020. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.89737/4.90049. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.90262/4.89837. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.90123/4.90452. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.89621/4.89903. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.89876/4.89727. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.89743/4.89647. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.89557/4.89963. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.89522/4.90184. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.89584/4.90019. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.89489/4.89575. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.89515/4.90199. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.89619/4.89994. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.89689/4.90703. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.89366/4.89554. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.89596/4.90292. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.89385/4.90992. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.89190/4.90452. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.89429/4.90287. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.89050/4.90863. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.89062/4.90654. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.89127/4.90607. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.89031/4.91373. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.89046/4.91198. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.88996/4.91482. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.88458/4.90847. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.88928/4.90540. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.88643/4.92159. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.88911/4.92624. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.89317/4.90517. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.88647/4.91138. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.88736/4.92423. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.88043/4.91581. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.88694/4.91481. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.88632/4.92280. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.88271/4.91823. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.88111/4.91678. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.88375/4.91714. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.88385/4.91834. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.88144/4.92998. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.88376/4.92733. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.88445/4.92700. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.88535/4.90919. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.88827/4.92740. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.88760/4.92383. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.88924/4.92031. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.88569/4.92565. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.88318/4.94361. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.87948/4.94294. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.88118/4.93692. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.87702/4.92956. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.88045/4.94941. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.87868/4.94824. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.87671/4.95444. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88148/4.93275. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.87401/4.93962. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.88230/4.94536. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88099/4.92643. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 4.87115/4.95544. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.87791/4.93557. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88012/4.93443. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.87210/4.97116. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.89195/4.91542. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.87702/4.94271. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.87880/4.93587. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.87018/4.95910. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.88032/4.93610. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.87572/4.94367. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.87019/4.95425. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.87175/4.94971. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.86757/4.96245. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.87029/4.94333. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.87691/4.93289. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.87650/4.93974. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.86796/4.97334. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.87780/4.92132. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.87158/4.95681. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.86888/4.94981. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.87798/4.94772. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.87364/4.94941. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.87022/4.96119. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.07195062246738243\n",
      "Epoch 0, Loss(train/val) 4.85081/4.83316. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.84067/4.86456. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.83663/4.83831. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 4.83049/4.83976. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.83218/4.84790. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.83217/4.84616. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 4.83465/4.83728. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.83136/4.83470. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.83179/4.82644. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 4.83280/4.82287. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.83328/4.81932. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.83226/4.81977. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.83354/4.82455. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.83235/4.82513. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.83124/4.82521. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.82804/4.82559. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.82858/4.82540. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.83229/4.82464. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.83061/4.82430. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.82909/4.82587. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.82990/4.82363. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.82871/4.82306. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.82963/4.82217. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.82797/4.82259. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.82607/4.81983. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.82976/4.81729. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.82701/4.81532. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82646/4.81519. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.82510/4.81431. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.82264/4.81448. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.82583/4.81984. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.82143/4.81393. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.82508/4.82817. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.82479/4.83082. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.82530/4.83058. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.81838/4.82328. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.81929/4.84657. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.81941/4.83313. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.82140/4.84876. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.81929/4.83992. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.81479/4.85383. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.81900/4.83803. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.81611/4.84417. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.81725/4.84670. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.81455/4.85142. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.81243/4.84391. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.81060/4.85138. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.80936/4.85156. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.81269/4.84972. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.81208/4.84501. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.81229/4.85248. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.81372/4.84779. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.81154/4.84812. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.80817/4.84716. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.80468/4.85507. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.81062/4.85808. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.81163/4.86406. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.81245/4.84683. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.80570/4.85301. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.81005/4.85650. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.80852/4.85327. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.81300/4.84966. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.80919/4.85387. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.80370/4.86149. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.80720/4.86496. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.81042/4.84984. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.80444/4.87210. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.80540/4.87688. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.80672/4.85458. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.81054/4.86947. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.80687/4.84869. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.80461/4.87402. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.80248/4.86031. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.80673/4.87331. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 4.80465/4.86859. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.80332/4.89203. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.80291/4.86178. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.80729/4.84681. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.80005/4.89358. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.80695/4.86130. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.80332/4.87706. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.79831/4.87648. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.80332/4.86107. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.80389/4.86575. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.79865/4.89574. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 4.80077/4.88353. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.80345/4.86599. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.79900/4.87521. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.79796/4.88057. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.80475/4.86708. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.80554/4.87167. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.80219/4.87442. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.80610/4.87317. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.79731/4.87818. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.80009/4.86634. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.80264/4.86469. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.80302/4.87285. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.80347/4.88478. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.80317/4.89735. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.80033/4.88118. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.17004550636718183\n",
      "Epoch 0, Loss(train/val) 5.22173/5.12235. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.14380/5.15431. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.14173/5.12891. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 5.13208/5.12548. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.13257/5.12897. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 5.13108/5.13400. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.13338/5.13501. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.13130/5.13638. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.12938/5.14123. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 5.12924/5.13654. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.12852/5.13370. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.12516/5.14949. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 5.12664/5.14803. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.12541/5.14256. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.12344/5.15423. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 5.12225/5.14094. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.11684/5.14424. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.12467/5.14675. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.11904/5.14320. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.11996/5.12069. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.12084/5.13814. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.11657/5.14317. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 5.11713/5.14805. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.11664/5.15055. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 5.11968/5.14557. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.11084/5.16887. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.12323/5.13247. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.11236/5.14211. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 5.11047/5.16157. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.11075/5.16116. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.11932/5.14141. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.11481/5.14433. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.11401/5.15687. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.11279/5.16780. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 5.11303/5.14613. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.11101/5.14403. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.10920/5.14619. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.10855/5.15244. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.10745/5.16202. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.10510/5.16138. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 5.10305/5.15965. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.10608/5.14719. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.10434/5.15667. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 5.10226/5.16992. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.10378/5.16598. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.10306/5.16013. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 5.10563/5.16051. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.09492/5.18239. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.10047/5.17309. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 5.09691/5.16989. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.10040/5.15979. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.09273/5.17714. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 5.10039/5.16465. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.09848/5.16684. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.09418/5.17713. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 5.09414/5.16632. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.09473/5.16690. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.08994/5.17946. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.09859/5.16991. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.09528/5.16666. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.09393/5.17590. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.08908/5.19481. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.09753/5.17866. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.09611/5.17294. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 5.09375/5.17152. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 5.09446/5.17826. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 5.09110/5.17804. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 5.09540/5.17942. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.09522/5.16159. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 5.09064/5.16863. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.08438/5.18374. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.08724/5.18346. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.08772/5.17376. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.08398/5.19956. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 5.08435/5.17937. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.07958/5.19471. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 5.07537/5.19690. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 5.08722/5.18283. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.08716/5.17822. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.08312/5.18758. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.08513/5.19652. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.08129/5.20320. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 5.08118/5.18616. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.08321/5.21647. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 5.08336/5.19400. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.08278/5.19698. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.07983/5.20079. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.07376/5.17813. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 5.07793/5.19428. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.07389/5.18791. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.07842/5.19513. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.06779/5.20235. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 5.06963/5.22883. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.07736/5.19682. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.07543/5.19023. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 5.07528/5.20103. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 5.07816/5.20388. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.07339/5.18488. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.07785/5.23041. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.06789/5.22452. Took 0.09 sec\n",
      "ACC: 0.578125, MCC: 0.191246010612082\n",
      "Epoch 0, Loss(train/val) 5.06977/4.98620. Took 0.19 sec\n",
      "Epoch 1, Loss(train/val) 4.99710/4.98751. Took 0.17 sec\n",
      "Epoch 2, Loss(train/val) 4.99580/5.00517. Took 0.12 sec\n",
      "Epoch 3, Loss(train/val) 4.99685/5.01145. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.99581/5.00543. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.99406/5.00948. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.99356/5.01576. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.99504/5.01332. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.99655/5.00807. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.99462/4.99872. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.99102/4.99085. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.99222/4.99219. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.99148/4.98745. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.98980/4.98542. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.98706/4.99217. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.98902/4.99178. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.98815/4.99035. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.98656/4.99823. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.98664/4.99583. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.98701/4.99297. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.98496/4.99163. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.98504/4.99524. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.98584/4.98533. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.98255/4.99134. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.98506/4.99768. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.98659/4.98938. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.98954/4.97729. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.98470/4.98512. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.98346/4.98993. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.98307/4.99400. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.98198/4.99756. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.98422/4.99327. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.98308/4.98928. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.98342/4.99743. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.98236/4.99022. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.98188/4.98593. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.98029/4.99623. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.97673/4.99955. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.98302/5.00132. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.98070/4.98895. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.98141/4.99046. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.98013/4.99726. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.98065/4.99205. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.97452/5.00829. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.97460/5.01167. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.97515/5.00810. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.97797/5.00301. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.97508/5.01353. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 4.97413/5.01317. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.97811/5.01260. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.98403/4.99542. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.97550/4.99753. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.97406/5.01482. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.97324/5.00102. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.97263/5.00414. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.97219/5.00563. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.97213/5.00539. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.96918/5.02202. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.98078/4.99366. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.98230/4.99676. Took 0.12 sec\n",
      "Epoch 60, Loss(train/val) 4.97603/5.01229. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.97883/5.00319. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.97985/4.99325. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.97579/5.00139. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.97260/5.01897. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.97123/5.00984. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.97091/5.01289. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.97507/5.01342. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.97078/5.00646. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.97130/5.00071. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.97014/5.02550. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.96974/5.00294. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.97084/5.01231. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.96739/5.01723. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.96448/5.01220. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.96930/5.02787. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.96485/5.02601. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.98408/4.96636. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.97928/4.99491. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.97551/4.99818. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.99359/5.02832. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.98491/5.01761. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.98491/5.00596. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.98267/5.01241. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.98203/5.00963. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.98416/5.00737. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.98081/5.00234. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.98286/5.00923. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.98279/5.00611. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.98051/5.00111. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.98509/4.99811. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.98186/5.00577. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.98202/5.00285. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.97830/5.00512. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.97720/5.01140. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.97645/5.01125. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.97914/5.00502. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.97598/5.01625. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.97365/5.03183. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.97550/5.02227. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.10677275981195139\n",
      "Epoch 0, Loss(train/val) 4.92640/4.95692. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.86608/4.88075. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.86244/4.87028. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.86556/4.87092. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.86820/4.87331. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.86233/4.87516. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.86613/4.87403. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.85874/4.88195. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.86063/4.88531. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.86010/4.88184. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.85772/4.88318. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.85438/4.89449. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85349/4.89925. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.85453/4.90358. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.85115/4.90451. Took 0.11 sec\n",
      "Epoch 15, Loss(train/val) 4.85144/4.90234. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 4.85159/4.90183. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.85238/4.90400. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.85340/4.90551. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85167/4.90966. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.84869/4.90420. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.84894/4.92374. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.84690/4.91521. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.84846/4.91317. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.85603/4.90910. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 4.84664/4.93348. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.84815/4.92360. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.84524/4.92263. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.84417/4.93098. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84557/4.92774. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.83952/4.92954. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.84282/4.91916. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84404/4.91872. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.84353/4.92843. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.84642/4.92956. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84739/4.93475. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.84952/4.92988. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84607/4.93029. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.84055/4.94942. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.84249/4.94395. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84419/4.93413. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.84034/4.94427. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.84220/4.94810. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.84338/4.93158. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.84155/4.93422. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.83557/4.95341. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.84022/4.94759. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.84055/4.95292. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.83954/4.94855. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.83598/4.96525. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.84057/4.94200. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.84012/4.94029. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.83487/4.96924. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.83586/4.92561. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.83810/4.97244. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.84080/4.93436. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.84372/4.94562. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.83643/4.94926. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.84483/4.95584. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.83591/4.95803. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.83791/4.94799. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.83470/4.94163. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.83880/4.94958. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.83508/4.95610. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 4.83855/4.94853. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.83503/4.95997. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.83535/4.95783. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.84300/4.93546. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.83799/4.95958. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83379/4.96131. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.83092/4.96556. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.83323/4.94140. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83798/4.92899. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.84108/4.95757. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83486/4.96539. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83575/4.96295. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.83566/4.97843. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.83061/4.96336. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.83401/4.95000. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.83152/4.96151. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83001/4.97160. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.83341/4.96513. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.83464/4.94816. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.83029/4.97033. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.82719/4.97026. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83087/4.97666. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.82974/4.95645. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 4.82903/4.97063. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83017/4.94385. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.83022/4.93645. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.82512/4.97376. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.82876/4.95445. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.83093/4.97362. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83364/4.94521. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.82467/4.95487. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.83097/4.95980. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.82930/4.97084. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.82156/4.97002. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.82813/4.96600. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.82391/4.97956. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.05707352953442433\n",
      "Epoch 0, Loss(train/val) 4.96516/5.01902. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.94579/5.02524. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.94924/5.03290. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.96230/4.95454. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 4.94606/4.93438. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.93025/4.94321. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.93158/4.94587. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.93257/4.94556. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.92854/4.93960. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.93122/4.95750. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.92869/4.94865. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.92818/4.95884. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.93268/4.93216. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.92459/4.94885. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.92787/4.94971. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.92327/4.96044. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.92315/4.95527. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.92202/4.96158. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.92215/4.95944. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.92000/4.96781. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.92267/4.97120. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.92064/4.97303. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.92075/4.96439. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.92291/4.97733. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.91681/4.96951. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.92056/4.98322. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.91682/4.97535. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.91848/4.98113. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 4.91775/4.99284. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.91609/4.96624. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.91910/4.97749. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.91621/4.99183. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.91675/4.97289. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.91696/4.99539. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.91119/4.98956. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.91326/4.97947. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.91635/4.98613. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.91769/4.98396. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 4.91451/4.99439. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.91452/4.98954. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.91233/4.98316. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.91544/4.98397. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.91383/4.99231. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.91259/4.98338. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.91443/5.00316. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.91186/4.98551. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.91172/5.00669. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.91283/4.96056. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.91740/5.00171. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.91721/4.97799. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.91482/4.98669. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.91302/4.99126. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.91226/4.99592. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.91284/4.99087. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.91307/4.98878. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.91245/5.00147. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.91262/4.99309. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.91134/4.99499. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.91249/4.99292. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.91022/4.99186. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.90996/5.00770. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.91272/4.99133. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.91095/5.00733. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.90939/4.99326. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.91057/4.99726. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.91214/4.99279. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.91124/4.99168. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.90534/5.01041. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.91000/4.99491. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.91243/5.00361. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.90951/5.00615. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.90945/4.99081. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.91377/4.99874. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.91021/4.99295. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.90768/4.99900. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.91126/4.98803. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.91017/5.00057. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.90699/5.00969. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.91187/4.99777. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.91152/4.99791. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.90714/5.00001. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.90817/5.00980. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.91247/4.98023. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.90898/5.00477. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.90656/5.02492. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.90902/5.00761. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.90805/5.00235. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.90611/5.01730. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.90794/5.00809. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.91018/4.99714. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.91252/4.95026. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.92349/4.93208. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.92235/4.94564. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.91933/4.94831. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.91462/5.00850. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.91272/4.99449. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.90496/5.00146. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.90453/5.01227. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.90308/5.01103. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.90616/5.00954. Took 0.08 sec\n",
      "ACC: 0.40625, MCC: -0.18786728732554486\n",
      "Epoch 0, Loss(train/val) 4.69075/4.68664. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.66622/4.67182. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.66026/4.67670. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.66324/4.67446. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.66246/4.67593. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.66591/4.67858. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.66182/4.68222. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.66284/4.68489. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.66046/4.68527. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.66083/4.68520. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.66220/4.69062. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.65956/4.69084. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.65915/4.68814. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.65705/4.68910. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.65708/4.69316. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.66170/4.68832. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.65772/4.69418. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.65826/4.69435. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.65686/4.68934. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.65905/4.68921. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.65528/4.69034. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.65631/4.69560. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.65638/4.69036. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.65548/4.69718. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.65535/4.70824. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.65660/4.70397. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 4.65396/4.71416. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.65584/4.70458. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.65517/4.70905. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.65373/4.70564. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.65623/4.69794. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.65226/4.70382. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.64996/4.72197. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.65590/4.68557. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.65842/4.67869. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.65287/4.70676. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.65640/4.69525. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.65448/4.69101. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.65739/4.69247. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.65847/4.69199. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.65814/4.69192. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.65570/4.69925. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.65786/4.69018. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.65905/4.69163. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.65753/4.69101. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.65821/4.68811. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.65466/4.69193. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.65338/4.69102. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.65393/4.69881. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.65571/4.69354. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.65338/4.69167. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.65660/4.69443. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.65421/4.70068. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.65586/4.70074. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.65452/4.69927. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.65563/4.70757. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.65352/4.69370. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.65333/4.70133. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.65853/4.67572. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.65554/4.68366. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.65559/4.69833. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.65475/4.69653. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.65203/4.69544. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.65323/4.70221. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.65076/4.69258. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.65108/4.69066. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.65202/4.70267. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 4.65040/4.70426. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.65104/4.71449. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.65301/4.70112. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.65339/4.71969. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.65286/4.69925. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.64867/4.70812. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 4.64820/4.70900. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.64988/4.69690. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.65027/4.70480. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.65247/4.70199. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.64859/4.71338. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.64861/4.70093. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.64853/4.68735. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.64868/4.71671. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.64831/4.71695. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.64588/4.71125. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.64944/4.69691. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.65182/4.69698. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.64859/4.70136. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.65454/4.70472. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.64825/4.72416. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.64664/4.72398. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.64571/4.71370. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.64659/4.70900. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.65074/4.72666. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.64726/4.70908. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.64660/4.71109. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.64846/4.71110. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.64683/4.71294. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.64722/4.69717. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.64584/4.71470. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.64477/4.72331. Took 0.12 sec\n",
      "Epoch 99, Loss(train/val) 4.64515/4.72983. Took 0.09 sec\n",
      "ACC: 0.53125, MCC: 0.07570682517832987\n",
      "Epoch 0, Loss(train/val) 4.85103/4.77657. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.80315/4.77793. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.80683/4.79490. Took 0.11 sec\n",
      "Epoch 3, Loss(train/val) 4.80737/4.78708. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.79823/4.77432. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.78886/4.77277. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.79019/4.77281. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.79202/4.77265. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.79183/4.77195. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 4.78951/4.77387. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.78733/4.77440. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.78869/4.77212. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.78730/4.77077. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.79021/4.77180. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.78623/4.77375. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.78619/4.77445. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78576/4.77145. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.78839/4.77145. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.78426/4.76764. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.78500/4.76978. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.78274/4.76851. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78564/4.77506. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 4.78374/4.77002. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.78255/4.77362. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.78294/4.77327. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.78136/4.77953. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.77964/4.77176. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.78419/4.77631. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.78237/4.77452. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.78001/4.77686. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.77693/4.77944. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.77760/4.78258. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.78043/4.78173. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.77575/4.77448. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.77687/4.77813. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.77875/4.78690. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.77152/4.77949. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.77295/4.78104. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.77429/4.80301. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.76844/4.78046. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.76887/4.78831. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.76879/4.81216. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.77153/4.78737. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.76912/4.77835. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 4.76747/4.79791. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.76760/4.78503. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.76649/4.78736. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.76577/4.80337. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.76611/4.78199. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.75856/4.77620. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.76519/4.78922. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.76814/4.80961. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.76759/4.76986. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.76306/4.79763. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.75812/4.78023. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.75997/4.78411. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.75940/4.80414. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.76961/4.78179. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76553/4.79819. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.76157/4.78629. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.76140/4.78409. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.76210/4.77338. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76156/4.78458. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.75816/4.79910. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.76023/4.79413. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.76836/4.77694. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.75822/4.78863. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.76184/4.78165. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.75862/4.77240. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.76301/4.76265. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.75707/4.78820. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.75208/4.78208. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.75938/4.78222. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.75165/4.80547. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.75459/4.78187. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.75828/4.78555. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.76045/4.77232. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.75462/4.78887. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.74931/4.79044. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.75601/4.78738. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.75065/4.77910. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.75986/4.76727. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.75561/4.77725. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 4.75057/4.79143. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.75323/4.77607. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 4.75422/4.78948. Took 0.11 sec\n",
      "Epoch 86, Loss(train/val) 4.75340/4.78026. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 4.74750/4.78183. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.75278/4.78762. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.74963/4.79392. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.75309/4.79072. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.74758/4.78184. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 4.75153/4.79096. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.74333/4.78728. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.75050/4.77590. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.74821/4.79491. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.75245/4.78720. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.74501/4.82183. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.75060/4.80796. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.74625/4.79498. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.0\n",
      "Epoch 0, Loss(train/val) 4.97210/4.86258. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.87478/4.85846. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.87567/4.86144. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.87972/4.86363. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.87921/4.86820. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.88055/4.87216. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.88017/4.87495. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.87882/4.87316. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.87574/4.87680. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.87322/4.87497. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 4.87310/4.87896. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.87288/4.87526. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.87410/4.87138. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.87490/4.87390. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.87338/4.87764. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.87250/4.87870. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.86959/4.88445. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.87332/4.87690. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.87088/4.88143. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.87201/4.87865. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.87000/4.88367. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.86936/4.88199. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 4.86844/4.87065. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.87235/4.87788. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.87141/4.87321. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.86964/4.87318. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.86909/4.87606. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.87016/4.87436. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 4.86631/4.89298. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.86681/4.87972. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.86678/4.87918. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.86869/4.87718. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.86578/4.88422. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.86494/4.87990. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.86505/4.88130. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.86253/4.88918. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.86846/4.87063. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.86489/4.87112. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.86188/4.87005. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.86094/4.87355. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 4.86279/4.87945. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.86155/4.86260. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.86287/4.88610. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.86424/4.86712. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.86349/4.89179. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.86255/4.87115. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 4.85849/4.87884. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.85973/4.87508. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.86104/4.88521. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.85954/4.87555. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.86285/4.86711. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.85893/4.85687. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 4.86645/4.87145. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.85795/4.87865. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.85718/4.87003. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.86287/4.85360. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.86350/4.87212. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.85759/4.87186. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.85686/4.87392. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.85516/4.88417. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.85744/4.86324. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.85369/4.87233. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.85799/4.85526. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.85826/4.86769. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.85530/4.88036. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.85549/4.85392. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.85526/4.86912. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.84881/4.86245. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.85579/4.85417. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.85758/4.85428. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.85615/4.85452. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.85493/4.86566. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.85705/4.85962. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.84864/4.86436. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.84977/4.86124. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.85231/4.86509. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.85156/4.85312. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.85302/4.88425. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.85161/4.86387. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.84897/4.86310. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.85177/4.85421. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.85206/4.87156. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.84649/4.86448. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.85210/4.86296. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.84959/4.87279. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.84786/4.84942. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.85350/4.88587. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.85420/4.87652. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.85137/4.85730. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.84835/4.86763. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.84781/4.89071. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.84492/4.87037. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.84508/4.87409. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.84603/4.86946. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.84136/4.87706. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 4.84402/4.90206. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.84721/4.89609. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.84628/4.88773. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.84648/4.87063. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.84471/4.89213. Took 0.08 sec\n",
      "ACC: 0.375, MCC: -0.2545139051903111\n",
      "Epoch 0, Loss(train/val) 4.90318/4.81547. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.79797/4.79860. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.79407/4.79742. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79604/4.80083. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.79273/4.80286. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.79339/4.80257. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.79227/4.80102. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.79226/4.79955. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.79291/4.79761. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79118/4.80323. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.79325/4.80172. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.79350/4.80597. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.78945/4.80634. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.78932/4.80790. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 4.78992/4.80865. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.78735/4.80809. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.78382/4.80717. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.78565/4.81102. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.78730/4.80881. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.78653/4.81089. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.78615/4.81467. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78202/4.81958. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.78339/4.82652. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.78278/4.83115. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.78366/4.81391. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.78280/4.82038. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.77948/4.82444. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.78032/4.82608. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77887/4.81023. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.77468/4.81005. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.78149/4.85267. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78651/4.81304. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.77915/4.82620. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.77544/4.82171. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.77596/4.81989. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.76704/4.83148. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.78066/4.83397. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.77621/4.81700. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.77134/4.83621. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.77498/4.81832. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.77233/4.82234. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.77568/4.83932. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.76910/4.83833. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.77243/4.81369. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 4.77425/4.81056. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.77222/4.81756. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.76793/4.82758. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.77179/4.81850. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77316/4.81525. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.77292/4.79804. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77587/4.79518. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.76553/4.79575. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.77300/4.80893. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.76888/4.79294. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.77116/4.80615. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.76404/4.79350. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.76180/4.79290. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.77364/4.80010. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76701/4.82020. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.76612/4.81635. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.76386/4.79931. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.76987/4.83242. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76402/4.78987. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.76245/4.83111. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.76252/4.80591. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.76523/4.81064. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.75690/4.78054. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.76453/4.80969. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.76239/4.78975. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76155/4.81374. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.76178/4.81740. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.75929/4.80900. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.76819/4.80745. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.75946/4.80329. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.75853/4.83393. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.76902/4.80599. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.75420/4.80980. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 4.76293/4.79293. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.75379/4.80701. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.75876/4.83012. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.75921/4.82713. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.75938/4.80931. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.75889/4.79866. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76299/4.80794. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.75559/4.79468. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.75358/4.82627. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.74838/4.80169. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.75618/4.77800. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.76041/4.75581. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.75470/4.79415. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.75731/4.80604. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.75885/4.79667. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.75122/4.77190. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.75493/4.76558. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.75128/4.77560. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.75668/4.81008. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.77416/4.84307. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.77765/4.81080. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.76755/4.79412. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.76884/4.80464. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.16012815380508713\n",
      "Epoch 0, Loss(train/val) 4.90331/4.81296. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.80191/4.80162. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.79640/4.80296. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79608/4.80676. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79880/4.80768. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.79366/4.80923. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.79662/4.80964. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.79535/4.81122. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.79106/4.81427. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79414/4.81573. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79399/4.82134. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.79198/4.82129. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.79229/4.82104. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.79383/4.82614. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.79261/4.82620. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.79025/4.83297. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.79201/4.82495. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.78953/4.83074. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.78610/4.83849. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.78757/4.83252. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.79050/4.82886. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78859/4.83193. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.78672/4.83275. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 4.78414/4.83377. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.78833/4.82638. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.78372/4.83446. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.78678/4.82599. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.78837/4.82561. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.78612/4.81999. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.78100/4.82499. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 4.78213/4.82067. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.78093/4.82215. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.78553/4.81848. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78174/4.82212. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.78162/4.82620. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 4.78030/4.81969. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.77866/4.82563. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.77830/4.82092. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.77924/4.81862. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.78403/4.81155. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.77882/4.81682. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.78008/4.82156. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.78127/4.81979. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 4.78216/4.81401. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.77812/4.82430. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.77750/4.81845. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.77831/4.82219. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.77531/4.81895. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.77669/4.81958. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.77522/4.81247. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77374/4.81287. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77943/4.82690. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.78028/4.82473. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.77547/4.81820. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.77344/4.82209. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.77558/4.81971. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.77898/4.81510. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.77022/4.82497. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76800/4.82324. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.77677/4.81694. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.77518/4.82000. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.76691/4.82759. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.77614/4.81422. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.77268/4.81339. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.77175/4.80068. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.77135/4.82557. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.76746/4.81984. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.77003/4.80955. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.76670/4.81338. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.77195/4.81518. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.76700/4.83079. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.77154/4.82535. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76980/4.81814. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.77028/4.82227. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.76581/4.81779. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.77451/4.82405. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78136/4.81744. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78036/4.81462. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.77950/4.84064. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.77451/4.83855. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.77860/4.82259. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.76715/4.85615. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.77070/4.82187. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.76732/4.84081. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.77029/4.82758. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.77102/4.83890. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.76564/4.84744. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.76952/4.84020. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.76517/4.85501. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.77246/4.83871. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.76732/4.83775. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.75854/4.84071. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.77719/4.82794. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.77254/4.82728. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.76865/4.83526. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.76556/4.84447. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.76359/4.83150. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.77073/4.82922. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.76183/4.81661. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.76150/4.82422. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.019772733298509516\n",
      "Epoch 0, Loss(train/val) 4.93889/4.86160. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.86651/4.84493. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.85152/4.84759. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.84854/4.84791. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.84631/4.84823. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.84468/4.85065. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.84461/4.85162. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.84498/4.85315. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.84192/4.85563. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.83965/4.86838. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.83943/4.86385. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.83993/4.86901. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.83692/4.87285. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.83478/4.87680. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.83638/4.87005. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.83513/4.87643. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.83550/4.87262. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.83200/4.87122. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.83044/4.86892. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.82814/4.88034. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.83028/4.85993. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.82939/4.86788. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.82908/4.86676. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.82636/4.87584. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.82697/4.85927. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.82590/4.85653. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.82896/4.85615. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82521/4.87442. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.82066/4.87721. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.82395/4.86588. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.82298/4.86014. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 4.82253/4.87071. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.82533/4.86223. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.82596/4.86734. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.82421/4.87170. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.81869/4.86777. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.81945/4.86872. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.82170/4.86020. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.81839/4.86981. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.82248/4.86038. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.82285/4.86554. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.81604/4.86966. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.82835/4.86010. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.81915/4.86748. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.82331/4.86593. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.82515/4.86764. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.82089/4.86255. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.81184/4.88109. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.81911/4.87554. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.82057/4.87694. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.82207/4.88952. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.82445/4.87678. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.81960/4.87317. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.81964/4.88193. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.82040/4.87613. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.82098/4.87482. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.81858/4.87436. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.81956/4.86502. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.81991/4.87828. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.81525/4.87449. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.82071/4.86659. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.81621/4.86925. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.81448/4.86068. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.81824/4.86086. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.81764/4.88149. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.81144/4.87374. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.82001/4.86901. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.80801/4.88296. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.80615/4.89678. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.82445/4.85717. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.83175/4.86774. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.82403/4.87460. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.82633/4.87800. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.81751/4.87027. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.80912/4.86878. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.81655/4.87510. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.81070/4.87174. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.81543/4.87531. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.80765/4.87344. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.80797/4.86998. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.81262/4.88181. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.80700/4.87676. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.81224/4.87380. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.81001/4.89205. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.81511/4.87452. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.80779/4.88966. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.81100/4.90427. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.80361/4.86946. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.80577/4.88687. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.80098/4.87883. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.80714/4.88083. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.81133/4.89672. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.81118/4.86396. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.80790/4.86662. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.80625/4.86175. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.80375/4.87911. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.80209/4.90568. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.80299/4.88744. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.80347/4.87803. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.80577/4.89712. Took 0.09 sec\n",
      "ACC: 0.34375, MCC: -0.3427828221414188\n",
      "Epoch 0, Loss(train/val) 4.78542/4.70049. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.70416/4.70636. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.69953/4.71187. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.70029/4.71718. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.69856/4.71621. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.70142/4.71673. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.70140/4.72886. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.69917/4.72425. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 4.70085/4.72522. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.69800/4.72786. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.69646/4.73193. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.70026/4.72103. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.70225/4.72487. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.69848/4.71989. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.69728/4.73102. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.69927/4.73229. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.69482/4.73764. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.69699/4.73379. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.69299/4.74217. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.69435/4.72995. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.69396/4.75815. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.69214/4.74099. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.69432/4.75172. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.69419/4.74296. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.69314/4.75272. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.69132/4.75563. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.69331/4.75845. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.69347/4.74811. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.69082/4.76092. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.68929/4.77000. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.68508/4.78088. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.68930/4.76386. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.68561/4.78056. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.68816/4.77187. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.68468/4.76699. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.68438/4.79424. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.68360/4.78552. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.68055/4.80280. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.68087/4.79411. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.68383/4.80260. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.67665/4.81777. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.68055/4.79151. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.67596/4.80742. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.67729/4.80118. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.67987/4.78852. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.67977/4.81366. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.67380/4.79823. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.67745/4.81430. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.67363/4.79573. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.66962/4.82154. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.67843/4.80012. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.69456/4.75939. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.68835/4.79366. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.68525/4.77855. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.68587/4.78584. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.68010/4.79805. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.67916/4.80484. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.67904/4.79518. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.67734/4.80091. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.67639/4.78477. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.68024/4.78653. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.67379/4.79246. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.67778/4.79637. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.67494/4.78979. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.67230/4.79900. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.66790/4.81055. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.67661/4.79984. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.67423/4.79194. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.68865/4.74048. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.67957/4.76196. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.68742/4.75622. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.67773/4.77081. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.67658/4.75601. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.67593/4.78817. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.67623/4.78831. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.67254/4.78412. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.66549/4.81102. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.67066/4.79885. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.67192/4.80744. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.67372/4.80016. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.66917/4.78813. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.67344/4.79763. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.67988/4.74724. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.67027/4.79752. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.66741/4.79543. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.66538/4.81771. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.67202/4.77572. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.67271/4.78870. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.67214/4.77788. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.66562/4.81184. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.67158/4.78218. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.66740/4.82455. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.67170/4.78653. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.66773/4.81944. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.66754/4.81882. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.66719/4.80969. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.66692/4.81226. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.66229/4.83647. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.66705/4.82513. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.66649/4.83020. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.15318083468998522\n",
      "Epoch 0, Loss(train/val) 4.78011/4.74418. Took 0.18 sec\n",
      "Epoch 1, Loss(train/val) 4.76359/4.73548. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 4.75437/4.73678. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.75732/4.74416. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.76553/4.74289. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.76750/4.74380. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.76882/4.77458. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.76325/4.77368. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.75560/4.76011. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.75596/4.75803. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.75818/4.76049. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.75551/4.76008. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.75563/4.76086. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.75597/4.76359. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.75360/4.76148. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.75243/4.75819. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.75396/4.76573. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.75285/4.75515. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.74924/4.76272. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.75279/4.76686. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.74931/4.78341. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.74803/4.77886. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.74876/4.80328. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.74985/4.79722. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.74492/4.81518. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.74658/4.79608. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.74180/4.82226. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.74539/4.80626. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.74199/4.81693. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.74101/4.81970. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.73786/4.81369. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.73526/4.82526. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.73355/4.84957. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.73738/4.81869. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.74479/4.76111. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.75268/4.75810. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.74301/4.76520. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.73464/4.78512. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.74905/4.79455. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.73945/4.79946. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.73558/4.80434. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.73285/4.83826. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.73299/4.79709. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.72622/4.81021. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.72880/4.83397. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.72208/4.82364. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.72837/4.82070. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.72596/4.82288. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.72682/4.78638. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.72434/4.87341. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.72595/4.78487. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.72415/4.86894. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.73474/4.79799. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.74788/4.79566. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.73639/4.85900. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.72997/4.83038. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.73164/4.85471. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.73003/4.82212. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.72489/4.81271. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.72433/4.83978. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.74949/4.75367. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.75282/4.76743. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.74494/4.76346. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.74733/4.77166. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.74285/4.78393. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.74373/4.78322. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.74105/4.78953. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.74431/4.78583. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.74234/4.78227. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.73714/4.80012. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.74301/4.78595. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.73543/4.81788. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.74108/4.78657. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.73577/4.81421. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.74363/4.79316. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.73159/4.81263. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.73964/4.80278. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.73473/4.80529. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.73639/4.80874. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.73147/4.79861. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.73826/4.81009. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.72576/4.81979. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.73985/4.79846. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.73718/4.79421. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.73091/4.82537. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.73810/4.79618. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.72965/4.83118. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.73548/4.79715. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.73373/4.82403. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.73732/4.79322. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.73544/4.80606. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.73294/4.81912. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.73039/4.82243. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 4.73220/4.83416. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.73157/4.82485. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.72858/4.81823. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.72276/4.84921. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.72678/4.81284. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.73157/4.86481. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.72338/4.81095. Took 0.08 sec\n",
      "ACC: 0.5, MCC: -0.015873015873015872\n",
      "Epoch 0, Loss(train/val) 5.00326/4.90295. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.90415/4.92544. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.90137/4.91825. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.89927/4.91009. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.89823/4.90656. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.89373/4.90599. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.89968/4.90675. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.89393/4.91328. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.89617/4.91712. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.89818/4.91987. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.89216/4.92153. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.89480/4.93518. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.89018/4.93395. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.89471/4.92746. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.88989/4.93818. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.88534/4.92195. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.88324/4.93248. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.88351/4.93571. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.88412/4.93349. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.88197/4.94771. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.88413/4.94424. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.88594/4.94170. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.88028/4.95993. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.87609/4.96287. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.88610/4.93563. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.87666/4.92419. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.86923/4.93934. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.87425/4.94595. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.87476/4.91751. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.87803/4.95582. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.87507/4.92564. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.87599/4.93474. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.87402/4.96719. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.87471/4.92963. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.87406/4.98430. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.87181/4.92627. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.87154/4.95729. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.86785/4.93974. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86980/4.98297. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.87228/4.94651. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.86413/4.97914. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.86138/4.96020. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.88743/4.90754. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.87512/4.95778. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.87225/4.93147. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.86960/4.95371. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.86526/4.92844. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.86567/4.97768. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.86973/4.95250. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.86997/4.93541. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.86942/4.94306. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.86487/4.94946. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 4.86337/4.96438. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.86199/4.94805. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.87009/4.92954. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.86266/4.96100. Took 0.11 sec\n",
      "Epoch 56, Loss(train/val) 4.86991/4.93908. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.86573/4.93391. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.86323/4.94792. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.85574/4.97169. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.85305/4.97230. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.85941/4.96264. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.85883/4.95119. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.86308/4.93156. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 4.86194/4.96952. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.86006/4.96074. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.85770/4.95328. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.86127/4.97439. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.86316/4.93468. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.85767/4.96228. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.85886/4.94044. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.86033/4.95768. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.85346/4.96154. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.85835/4.92723. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.85768/4.94853. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.86386/4.94796. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.85779/4.96094. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.85564/4.97718. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.85511/4.96249. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.85401/4.96300. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.86410/4.98580. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.85540/4.97972. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.85214/4.98382. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.85316/4.97808. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.85348/5.00357. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.85081/4.96702. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.85126/4.98718. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.85002/4.97877. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.85264/4.97195. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.84339/5.01118. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.85303/4.98165. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.84891/4.97573. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.85641/4.95993. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.86595/4.95845. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.85392/4.97439. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.85323/4.96722. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.84906/4.96770. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.84821/4.99555. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.85023/4.97636. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.86072/4.95033. Took 0.10 sec\n",
      "ACC: 0.546875, MCC: 0.09379580992210836\n",
      "Epoch 0, Loss(train/val) 4.78000/4.74977. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.76063/4.75000. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.75901/4.74157. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.75373/4.74303. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.74842/4.74230. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.74850/4.74092. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.74990/4.73994. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.74953/4.74406. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.74998/4.74407. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.74700/4.74401. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.74621/4.74479. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.74553/4.74563. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.74498/4.74653. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.74861/4.74527. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.74310/4.74668. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.74472/4.74620. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.74267/4.74960. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.74271/4.75103. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.74246/4.75357. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 4.74047/4.75652. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.74226/4.75711. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.73963/4.75952. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.74128/4.76684. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.74062/4.77087. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.74097/4.76876. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.73789/4.76961. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.73963/4.77880. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.74047/4.75061. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.74288/4.76322. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.74016/4.76064. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.73805/4.77294. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.73827/4.76783. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.73638/4.77270. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.73579/4.76899. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.73333/4.77815. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.73434/4.79359. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.73532/4.79020. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.73626/4.76895. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.73883/4.78432. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.73949/4.77785. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.73928/4.76722. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.73493/4.77568. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.73420/4.78521. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.73329/4.78863. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.73678/4.78266. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.73689/4.79429. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.73295/4.79951. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.73481/4.78441. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.73475/4.79131. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.73327/4.79614. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.73184/4.80610. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.73712/4.80661. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.74055/4.76394. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.73939/4.76271. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.73490/4.77137. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.73188/4.79533. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.73649/4.77731. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.73377/4.79030. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.73105/4.78739. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.73819/4.77492. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.73540/4.78684. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.73320/4.79046. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.73401/4.79256. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.72914/4.79676. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.73416/4.80032. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.73673/4.79176. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.73384/4.79549. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.73223/4.80277. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.72924/4.79024. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.73251/4.79556. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.73033/4.79598. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.73055/4.80589. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.73340/4.79658. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.72895/4.81629. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.72875/4.80886. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.72877/4.80193. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.73021/4.79637. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.73060/4.78981. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.72785/4.80770. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.72807/4.80406. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.73291/4.79658. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.73121/4.81258. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.73120/4.81654. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.71990/4.81960. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.72194/4.81581. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.72924/4.80018. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.73039/4.81841. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.72908/4.80097. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.72583/4.82793. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.72387/4.81795. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.72977/4.81236. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.72269/4.82604. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.72216/4.81524. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.72221/4.83190. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.72921/4.80916. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.72172/4.83705. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.73945/4.77702. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.73262/4.80500. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.72673/4.80999. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.72141/4.83853. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.03163859985841663\n",
      "Epoch 0, Loss(train/val) 5.01347/4.97560. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.98256/4.98095. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.98435/4.98406. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.98516/4.99261. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.98605/5.01527. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.98730/4.99563. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.98404/4.97493. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.97668/4.97632. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.97533/4.98263. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.97853/4.98833. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.97966/4.98450. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.98009/4.98301. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.97883/4.98012. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.97444/4.97926. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.97735/4.98174. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.97793/4.98207. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.97660/4.97974. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.97596/4.98013. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.97771/4.98187. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.97595/4.98012. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.97462/4.98106. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.97412/4.98307. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.97350/4.98249. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.97377/4.98593. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.97534/4.97969. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.97303/4.98261. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.97302/4.98398. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.97280/4.98264. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.97242/4.97778. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.97046/4.98432. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.97153/4.98217. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.97135/4.97845. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.97010/4.98206. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.96848/4.98199. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.97093/4.98196. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.96981/4.97662. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.96983/4.98449. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.96927/4.98700. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.96691/4.97906. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.96680/4.98577. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.96695/4.98527. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.96865/4.98291. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.96606/4.98449. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.96739/4.97680. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.96490/4.98998. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.96756/4.98617. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.96680/4.98032. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.96313/4.99468. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.96189/4.99203. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.96301/4.99252. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 4.96651/4.99380. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.96287/4.98717. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.96223/4.99447. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.96426/4.99663. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.96671/4.98377. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.96281/4.99853. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.96314/4.99221. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.95802/4.99909. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.96065/5.00249. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.96220/4.99968. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.95854/4.99697. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.95822/4.99758. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.96115/4.99066. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.96047/4.99589. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.95939/5.00727. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.95802/5.01438. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.95534/5.01313. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.95515/5.01072. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.95819/5.02905. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.96185/5.00565. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.95940/5.01058. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.95802/5.00809. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 4.95596/5.01035. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.95455/5.01264. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.95569/5.02815. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.95295/5.01457. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.95583/5.02308. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.95396/5.02997. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.95980/5.00740. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.95097/5.03289. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.95864/5.01873. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.95335/5.03232. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.95433/5.01529. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.95487/5.02681. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.95055/5.04306. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.95009/5.01835. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.95293/5.01640. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.95065/5.01934. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.95471/5.03246. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.94819/5.02153. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.95547/5.01666. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.94695/5.03555. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.94828/5.03670. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.95505/5.01729. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.95260/5.04428. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.95994/5.00952. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.95768/5.02560. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.95750/4.99833. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.95123/5.03238. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.96731/4.98867. Took 0.09 sec\n",
      "ACC: 0.4375, MCC: -0.12725695259515554\n",
      "Epoch 0, Loss(train/val) 4.69425/4.65306. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.67185/4.65420. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.66884/4.65969. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.67213/4.66174. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.67898/4.64575. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.66579/4.64858. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.66084/4.64455. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.66102/4.64230. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.66302/4.64195. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.66030/4.63973. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.65946/4.63824. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.65969/4.63698. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.65870/4.63643. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.65848/4.63642. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.65943/4.63495. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.65638/4.63334. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.65680/4.63479. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.65500/4.63316. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.65670/4.63251. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.65417/4.63646. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.65629/4.64008. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.65430/4.64017. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.65126/4.63889. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.65082/4.64229. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.65482/4.64199. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.65299/4.64204. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.65232/4.64180. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.65601/4.65375. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.65409/4.64954. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.65310/4.64909. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.65190/4.64573. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.65347/4.64536. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.65322/4.65078. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.65190/4.64988. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.65058/4.64037. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.65113/4.64220. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.64904/4.64125. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.65178/4.64501. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.64792/4.63925. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.65510/4.64711. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.64869/4.64309. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.64744/4.64752. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.64987/4.64591. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.65322/4.64594. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.64747/4.64073. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.64563/4.64958. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.64723/4.64519. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.64528/4.64138. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.64755/4.65263. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.65013/4.63844. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.65105/4.64341. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.64830/4.64483. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.64751/4.64548. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.64524/4.63981. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.64805/4.64511. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.64237/4.64500. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.64523/4.63580. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.64487/4.64890. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.65039/4.64354. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.64648/4.63964. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.64889/4.64172. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.64496/4.64343. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.64818/4.63363. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.64788/4.63770. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.64475/4.64475. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.64318/4.64449. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.64493/4.65266. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.64114/4.64868. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.64769/4.63916. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.64580/4.64313. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.64969/4.65039. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.64342/4.65272. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.64427/4.65335. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.64022/4.65657. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.63924/4.65860. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.64225/4.67099. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.64047/4.65148. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.64159/4.66017. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.64320/4.64456. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.63568/4.65538. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.64344/4.66422. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.64280/4.66313. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.64463/4.65637. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.63997/4.65130. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.64708/4.66896. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.63885/4.64748. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.63541/4.65421. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.64366/4.65815. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.63574/4.66443. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.64512/4.66324. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.64192/4.65569. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.64038/4.66594. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.63436/4.66213. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.64043/4.66362. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.63987/4.65529. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.63945/4.65457. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.64237/4.66732. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.63126/4.66415. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.64144/4.65203. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.63882/4.66173. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.06052275326688024\n",
      "Epoch 0, Loss(train/val) 5.07031/5.03858. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.01375/5.00911. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 5.00528/5.00127. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.00660/4.99822. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 5.01174/4.99513. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.00745/4.99648. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.00654/4.99768. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 5.01054/5.00007. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.00451/4.99835. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.00248/4.99660. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.00386/4.99769. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.00342/4.99787. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 5.00161/4.99678. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.00100/4.99200. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.00095/4.99100. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 5.00351/4.99451. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.00377/5.00078. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.99848/4.99960. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.99778/4.99682. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.00130/4.99767. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.00180/5.00019. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.99850/4.99750. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.99956/4.99945. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.99825/5.00107. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.99885/5.00088. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.99898/4.99673. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.00144/4.99652. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.99933/4.99408. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.99782/4.99316. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.99839/4.99008. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.99512/4.99729. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.99935/4.98949. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.99832/4.99161. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.99809/4.99193. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.99611/4.98400. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.99911/4.99761. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 5.00014/4.99454. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.00128/4.99651. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.99603/4.99610. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.99481/4.99222. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.99427/4.99325. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.99309/4.99568. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.99333/4.98423. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.00215/4.99695. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.99740/4.99765. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.99506/4.99977. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.99350/4.99954. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.99640/4.99296. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.99432/4.98489. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.99468/4.98921. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.99266/4.98896. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.99285/4.99262. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.99134/4.98900. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.99002/4.98669. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.98829/4.98485. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.99057/4.98912. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.98658/5.00337. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.98878/4.98670. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.98532/4.98949. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.98365/4.98846. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.00915/4.99178. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.00222/4.99574. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.99789/4.99680. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.99710/4.99859. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.99384/4.98442. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.99833/4.98470. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.99516/4.98792. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.99387/4.98670. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.99591/4.98496. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.99317/4.98646. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.99156/4.98309. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.99205/4.98614. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.99333/4.98343. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.99307/4.98417. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.99322/4.98746. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.99089/4.98973. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.99143/4.98492. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.98728/4.98371. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.98537/4.97823. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.98590/4.98820. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.99165/5.00194. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.98775/4.98458. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.98802/4.98510. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.98549/4.97257. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.98524/4.98511. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.98410/4.97939. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.98931/4.97791. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.98693/4.97128. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.98004/4.97374. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.98756/4.98897. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.98548/4.98711. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.98848/5.00578. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.98661/4.99649. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.98366/4.97502. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.99240/4.97694. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.98767/4.97820. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.98382/4.97163. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.98249/4.96550. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.98364/4.96586. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.98528/4.97715. Took 0.09 sec\n",
      "ACC: 0.578125, MCC: 0.1308525335766877\n",
      "Epoch 0, Loss(train/val) 4.94491/4.87238. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.88986/4.90064. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.90090/4.91999. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.90365/4.92829. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.89141/4.90703. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.87650/4.91271. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.88097/4.91719. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.88100/4.91558. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.88003/4.90955. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.87630/4.91507. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.87379/4.91237. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.87597/4.91781. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.87399/4.91902. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.87568/4.91936. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.87341/4.91931. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.87193/4.91409. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.87128/4.90693. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.86872/4.91748. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.87069/4.91199. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.86931/4.92031. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.87435/4.91227. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.86586/4.91146. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.87400/4.89938. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.86496/4.90038. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.86367/4.90088. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.86911/4.91524. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.86221/4.91059. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.86446/4.90661. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.86054/4.89609. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.86450/4.90589. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.86115/4.90196. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.86891/4.90881. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.86297/4.89096. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.85969/4.90283. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.86197/4.89674. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.85708/4.89986. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.86333/4.89793. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.86235/4.88176. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.85723/4.89926. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.85910/4.88341. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.85075/4.90414. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.85731/4.89579. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.85395/4.89774. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.85324/4.90077. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.85370/4.90167. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.84652/4.90092. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.85509/4.87832. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.85690/4.90213. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84920/4.90264. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.85018/4.90493. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.85407/4.89491. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.85292/4.87482. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.85178/4.90048. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.84964/4.89011. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.85623/4.91221. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.87354/4.89872. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.86722/4.90262. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.86513/4.89095. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.86518/4.90210. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.86453/4.91464. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.86218/4.90983. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.85745/4.89185. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.85684/4.89759. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.86204/4.89169. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.85577/4.90526. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.85814/4.89706. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.85292/4.90625. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.85464/4.91804. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.85678/4.87761. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.85059/4.91203. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.84860/4.91738. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.84941/4.90248. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.85007/4.87793. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.84755/4.88431. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.84651/4.90542. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.84965/4.88261. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.84998/4.90348. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.84661/4.88858. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.84927/4.89057. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.84701/4.89048. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.84496/4.90758. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.84555/4.88034. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.84295/4.88793. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.83997/4.90046. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.84156/4.89129. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83968/4.90225. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.84184/4.90698. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.84284/4.90513. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.84117/4.91266. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.83730/4.92905. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.84117/4.93179. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.83610/4.92154. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.84249/4.92184. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83666/4.91899. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.83855/4.91117. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 4.83041/4.92536. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.83828/4.91411. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.84580/4.91184. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83309/4.93829. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.83619/4.91372. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.03546361409014303\n",
      "Epoch 0, Loss(train/val) 4.89391/4.81298. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.83734/4.83027. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.83434/4.84757. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.82844/4.83010. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.82539/4.81339. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.81868/4.82201. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.82018/4.82774. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.82024/4.82911. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.82101/4.82964. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.81961/4.82906. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.81654/4.82651. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.81673/4.82717. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.81412/4.82706. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.81378/4.82990. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.81608/4.81964. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.81521/4.82319. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.81133/4.83420. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.81789/4.83437. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.81035/4.83656. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.81225/4.83605. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.81378/4.83647. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.81454/4.83401. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.81054/4.83570. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.81186/4.82934. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.80935/4.83222. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.81161/4.82833. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.81182/4.82913. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.80761/4.82485. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.80808/4.83159. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.80928/4.82233. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.80743/4.81630. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.80647/4.82143. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.80798/4.82924. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.80498/4.83220. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.80995/4.83539. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.80572/4.81370. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.80084/4.83581. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.81034/4.82487. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.80558/4.82224. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.80418/4.82252. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.80700/4.83014. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.80285/4.83507. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.80260/4.82529. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.80345/4.82160. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.80125/4.81983. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.80567/4.82326. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.80466/4.83093. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.80393/4.82036. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.79940/4.82355. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.79956/4.83167. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.80398/4.83331. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.80082/4.82948. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.80094/4.83751. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.80296/4.84386. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.80512/4.82981. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.80091/4.84597. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.80276/4.83562. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.80578/4.82605. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.80539/4.82717. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.79977/4.83841. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.79657/4.83272. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.80077/4.84395. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.81026/4.84228. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.80701/4.83313. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.80605/4.83705. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.80073/4.84046. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.80051/4.84485. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.80096/4.85033. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.80019/4.85367. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.80028/4.84201. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.79948/4.86225. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.79759/4.84983. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.80335/4.83787. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.80155/4.85261. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.79521/4.84041. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.80015/4.82215. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.79443/4.84940. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.79901/4.82401. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.79750/4.83438. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.79543/4.85550. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.79642/4.85199. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.79935/4.85890. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.79589/4.83093. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.79511/4.85335. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.79503/4.82485. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.80178/4.84502. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.80173/4.82951. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.79674/4.85843. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.79610/4.83204. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.79503/4.84226. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.79757/4.83350. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.79907/4.83850. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.79521/4.85081. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.80023/4.85472. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.79593/4.86983. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.80309/4.84614. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.80322/4.85848. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.79692/4.86125. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.79026/4.85703. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.78966/4.88113. Took 0.08 sec\n",
      "ACC: 0.59375, MCC: 0.18029556650246306\n",
      "Epoch 0, Loss(train/val) 4.92695/4.93245. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.93423/4.91370. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.91601/4.94165. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.92499/4.93244. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.92697/4.90625. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.91560/4.90556. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.91115/4.90619. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.91494/4.90559. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.91173/4.90489. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.91281/4.90427. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.90980/4.90394. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.91066/4.90378. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.91074/4.90329. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 4.90958/4.90339. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.90989/4.90339. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.90936/4.90373. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.90726/4.90453. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.90805/4.90559. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.90685/4.90520. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.90480/4.90988. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.90432/4.91523. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.90547/4.90977. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.90336/4.91094. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.90461/4.91140. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.90552/4.91474. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.90406/4.91638. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.90138/4.92255. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.89997/4.91525. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.90644/4.90725. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.89930/4.91401. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.89955/4.91482. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.90021/4.91676. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.89900/4.92111. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.89982/4.91476. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.90102/4.91399. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.89760/4.92147. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.89713/4.91553. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.89978/4.92248. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.89734/4.92149. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.89449/4.92158. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.89446/4.91939. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.89656/4.92291. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.89891/4.90550. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.89437/4.91934. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.89680/4.91225. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.89817/4.91797. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.89356/4.92405. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.89364/4.92924. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.89543/4.91187. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.89046/4.91480. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.89389/4.90894. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.89146/4.92358. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.89173/4.92141. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.90666/4.87236. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.90470/4.89650. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.90018/4.89119. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.90298/4.89266. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.89819/4.90070. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.89556/4.90744. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.89354/4.90184. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.89419/4.90394. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.88898/4.90162. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.89098/4.90867. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.88879/4.90573. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.88498/4.90525. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.89724/4.89891. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.89008/4.90786. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.88873/4.91430. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.89384/4.90497. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.89031/4.90498. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.88975/4.91974. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.88696/4.90673. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88834/4.91671. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88903/4.91774. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.88478/4.92033. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.89017/4.91692. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88506/4.92156. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.89023/4.91778. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.90810/4.90065. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.90022/4.89403. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.90152/4.88512. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.89946/4.87804. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.89548/4.88622. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.90065/4.89460. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.89929/4.90110. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.89691/4.90057. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.89283/4.89588. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.89572/4.90449. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.89185/4.89584. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 4.89434/4.89673. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.89224/4.93686. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.90704/4.90247. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.90805/4.90123. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.89977/4.90403. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.89659/4.92635. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.89630/4.90677. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.89294/4.92174. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.89409/4.94248. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.89169/4.92392. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.89256/4.93374. Took 0.09 sec\n",
      "ACC: 0.625, MCC: 0.25417271391119695\n",
      "Epoch 0, Loss(train/val) 4.95725/4.92296. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.91630/4.92702. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.91493/4.91116. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.90656/4.90513. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.90860/4.90664. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.90880/4.90997. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.90639/4.90810. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.90677/4.90452. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.90714/4.90209. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.90383/4.90203. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.90368/4.90762. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.90033/4.90880. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.89960/4.90834. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.89889/4.91355. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.90350/4.91493. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.89843/4.91297. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.89555/4.91796. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.89261/4.92616. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.90343/4.93145. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.90055/4.92811. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.89636/4.93318. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.89557/4.92909. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.89929/4.92967. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.89689/4.92487. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.89789/4.93192. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.89173/4.92650. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.89175/4.93344. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.89450/4.93334. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.89114/4.94408. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.91230/4.94483. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.90281/4.94277. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.89325/4.95614. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.89506/4.94763. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.88947/4.95451. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.89691/4.95301. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.89643/4.95689. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.89263/4.95796. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.88715/4.93366. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.89786/4.93916. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.89894/4.93508. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.88784/4.93588. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.89751/4.92601. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.89048/4.94098. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.88821/4.94565. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.88606/4.95287. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.88434/4.93921. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.89999/4.98129. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.91333/4.97165. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.90730/4.96217. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.90354/4.93794. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.89258/4.94366. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.89087/4.94771. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.88653/4.95647. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.88746/4.95410. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.88091/4.94654. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.90125/4.95193. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.90360/4.96754. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.89762/4.99750. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.90154/4.98473. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.90034/4.97966. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.89970/4.97462. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89587/4.97817. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.89792/4.94947. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.89090/4.94441. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.88595/4.94967. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.88675/4.95126. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.88298/4.94439. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.88254/4.95022. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.89592/4.93075. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.88989/4.96129. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.88303/4.96199. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.88540/4.95717. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88944/4.93694. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88392/4.95986. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.88667/4.94683. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.89029/4.92298. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.88536/4.94850. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.87920/4.95358. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.88090/4.96310. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.88418/4.95257. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.88385/4.95070. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.88562/4.93787. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.88679/4.94040. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.87713/4.96045. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.88003/4.95489. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.87649/4.96666. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.88534/4.96596. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.88525/4.98130. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.88530/4.97715. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88256/4.97382. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.87991/4.98147. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.87815/4.97412. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.88530/4.95287. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.87916/4.94896. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.87645/4.96492. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.87780/4.97242. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.88317/4.96118. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.88062/4.95979. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.87789/4.97752. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.87925/4.97005. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.07116958850708793\n",
      "Epoch 0, Loss(train/val) 4.80415/4.78772. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.76208/4.77555. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.75688/4.77324. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.75441/4.77408. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.75726/4.77134. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.75535/4.77094. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.75357/4.77335. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.75220/4.77928. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.75187/4.78152. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.75350/4.77947. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.75256/4.78688. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.74850/4.78967. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.75240/4.77696. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.74681/4.77817. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.74759/4.78749. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.74754/4.77989. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.74597/4.77969. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.74431/4.78701. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.74768/4.78128. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.74853/4.77494. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.74223/4.78108. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.74141/4.78230. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.74482/4.77857. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.74802/4.78349. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.75124/4.78010. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.75065/4.78081. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.74486/4.79128. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.74454/4.79857. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.74174/4.79341. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.73807/4.80649. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.73732/4.81349. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.74064/4.80360. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.73784/4.80843. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.75005/4.77633. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.75161/4.76647. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.74292/4.77836. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.74014/4.77052. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.74351/4.79308. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.74950/4.77562. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.74768/4.77469. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.74069/4.79726. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.74109/4.80784. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.74853/4.77190. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.73827/4.79269. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.74088/4.79299. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.74302/4.77518. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.73868/4.78655. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.73911/4.78163. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.73459/4.79484. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.73331/4.79054. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.73877/4.78001. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.73431/4.79500. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.73946/4.78818. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.73255/4.79314. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.73733/4.79218. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.73286/4.78206. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.73845/4.79257. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.74066/4.78674. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.73123/4.79761. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.73145/4.80004. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.73466/4.79028. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.73066/4.80645. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.73316/4.81909. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.73207/4.81457. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.73413/4.81455. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.73171/4.81481. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.72946/4.80641. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.72369/4.82093. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.73196/4.81425. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.72455/4.82942. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.72550/4.81248. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.72697/4.82917. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.72235/4.83433. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.73396/4.79873. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.73166/4.81585. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.72747/4.82730. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.72651/4.83124. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.72437/4.83291. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.71802/4.84379. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.72452/4.82997. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.72343/4.83148. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.72582/4.83085. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.72288/4.82686. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.72040/4.84058. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.72571/4.83134. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.72149/4.85003. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.71546/4.84964. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.72941/4.81414. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.72948/4.83370. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.74616/4.79080. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.74062/4.81355. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.73354/4.83503. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.74068/4.81988. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.73459/4.81847. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.73215/4.82755. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.73267/4.81702. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.73071/4.82226. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.73610/4.81162. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.73002/4.84232. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.73341/4.80175. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.08818077954471167\n",
      "Epoch 0, Loss(train/val) 4.80146/4.78540. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.77679/4.78085. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.77848/4.77665. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.78108/4.76078. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.77069/4.76235. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.76394/4.76145. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.76299/4.76228. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.76555/4.76363. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.76437/4.76443. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.76175/4.76551. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.76247/4.77040. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.76196/4.77194. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.76370/4.76942. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.75985/4.77906. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.76126/4.77709. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.75846/4.78180. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.75475/4.78446. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.75651/4.78953. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.75808/4.79284. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.75746/4.79632. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.75291/4.80612. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.75128/4.82500. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.74878/4.79908. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.75260/4.80337. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.75146/4.82962. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.75026/4.81483. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.74562/4.81864. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.74937/4.82175. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.74468/4.84322. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.74909/4.82209. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.75125/4.81559. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.74839/4.83494. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.74612/4.83687. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.74704/4.82051. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.74392/4.82300. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.74268/4.84067. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.74320/4.84332. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.74503/4.82949. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.74204/4.82859. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.73521/4.85371. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.74798/4.81495. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 4.74097/4.81575. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.73911/4.84337. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.73988/4.82782. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.73591/4.81909. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.74106/4.81291. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.73700/4.84082. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.73593/4.83157. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.74481/4.82686. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.73482/4.84042. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.74411/4.82748. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.73768/4.82845. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.73765/4.82950. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.73884/4.84889. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 4.73793/4.84059. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.73998/4.81999. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.73593/4.86107. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.73557/4.84247. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.73421/4.83691. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.73303/4.88591. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.74347/4.83800. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.72746/4.85819. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.73672/4.87323. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.74079/4.83572. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.73352/4.87026. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.73836/4.85680. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.73075/4.84921. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.72984/4.87475. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.73237/4.85690. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.73717/4.84399. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.73461/4.85949. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.73687/4.87843. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.72941/4.87006. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.72645/4.84898. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.73061/4.86979. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.73173/4.87058. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.73309/4.86530. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.73157/4.87164. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.72901/4.86844. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.72559/4.88347. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.73138/4.87511. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.72498/4.88202. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.72973/4.88468. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.73136/4.88632. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.72742/4.87806. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.72719/4.88726. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 4.72615/4.89748. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.72226/4.89454. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.72565/4.85005. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.72937/4.85247. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.72599/4.89463. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.72856/4.86646. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.72342/4.86486. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.72606/4.87665. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.72694/4.86996. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.72766/4.86236. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.73105/4.85958. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.72228/4.87971. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.71857/4.90730. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.73114/4.87513. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.002944085489318884\n",
      "Epoch 0, Loss(train/val) 4.88315/4.99578. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.83050/4.92606. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.82144/4.90055. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.81842/4.89123. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.81556/4.88793. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.81662/4.89686. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.81696/4.89333. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.81249/4.89963. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.81070/4.91122. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.81471/4.91428. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.81356/4.90861. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.81099/4.89761. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.80884/4.88430. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.80693/4.91035. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.81043/4.89681. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.80840/4.88546. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.80258/4.87756. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.80035/4.88326. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.80195/4.89057. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.79945/4.89817. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.79838/4.91196. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.80238/4.92009. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.80321/4.87816. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.80156/4.88505. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.80131/4.90146. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.80014/4.89377. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.79821/4.90161. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.80063/4.89238. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.80173/4.88667. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.79774/4.89191. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.79573/4.91274. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.79942/4.91541. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.79713/4.91196. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.79910/4.91182. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.79780/4.91021. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.79482/4.92406. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.79336/4.91605. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.80034/4.90453. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.79419/4.91229. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.80008/4.91034. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.79532/4.90960. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.79277/4.93245. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.79683/4.86208. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.79884/4.88000. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.79730/4.92548. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.79286/4.91305. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.79508/4.91693. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.79585/4.89868. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.79236/4.92153. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.79073/4.93573. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.79563/4.92482. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.79160/4.91831. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.78848/4.91793. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.78918/4.93555. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.79144/4.93463. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.78737/4.96678. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.79025/4.94111. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.78821/4.92991. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.78865/4.94855. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.78314/4.92864. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.79151/4.92782. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.78990/4.89606. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.79131/4.90902. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.78707/4.93558. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.78878/4.93025. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.78730/4.92983. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.78563/4.95772. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.78993/4.92063. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.78484/4.97224. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.78739/4.93433. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.78638/4.93372. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78408/4.92857. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.78969/4.92620. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.78745/4.91412. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.78663/4.94108. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.78478/4.93110. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78244/4.95365. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78459/4.95243. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.77996/4.93783. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.78099/4.97247. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.78358/4.94035. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.78420/4.95469. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.78508/4.95032. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.78814/4.92814. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.78415/4.94384. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.78376/4.94448. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.78120/4.95900. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.78182/4.94643. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.78376/4.97423. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.78562/4.91832. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.78309/4.96851. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.78629/4.92858. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.77913/4.96280. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.78159/4.96396. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.78224/4.93729. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.78783/4.94859. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.78219/4.94244. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 4.77817/4.96479. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.78006/4.97427. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.77854/4.93947. Took 0.09 sec\n",
      "ACC: 0.5625, MCC: 0.12725695259515554\n",
      "Epoch 0, Loss(train/val) 4.98047/4.96736. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.97562/4.97785. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.96153/4.99088. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.96114/4.98320. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.96124/4.96962. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.95414/4.96719. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.95556/4.96782. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.95465/4.97416. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.95536/4.97336. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.95494/4.97159. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.95232/4.97354. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.95638/4.97398. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.95233/4.97111. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.95502/4.96608. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.95378/4.96455. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.95260/4.96757. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.95273/4.96732. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.95396/4.96732. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.95194/4.96740. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.95112/4.96967. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.95067/4.96462. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.95143/4.97114. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.95309/4.96936. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.95247/4.96941. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.95107/4.97310. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.95149/4.97149. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.94812/4.97159. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.94999/4.97382. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.95099/4.96970. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.94940/4.97045. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.95142/4.97327. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.94737/4.96969. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.94885/4.97757. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.94788/4.97610. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.94315/4.96349. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.94685/4.97964. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.94831/4.98022. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.95150/4.98237. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.94392/4.97873. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.94368/4.98730. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.94552/4.97896. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.94657/4.97794. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.94204/4.97732. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.94290/4.97642. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.93989/4.97693. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.94385/4.97599. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.94339/4.97295. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.93991/4.97583. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.94031/4.99136. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.94187/4.98800. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.94192/4.96872. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.93769/4.99089. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.93575/4.99046. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.93864/5.00178. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.94245/4.98038. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.93648/4.97518. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.93972/4.99338. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.93477/4.98994. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.93785/4.96978. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 4.93781/4.98843. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.93702/4.99909. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.93677/4.97898. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.93201/4.99994. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.93641/5.01248. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.93147/4.98903. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.93523/4.99385. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.93673/4.98336. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.93487/4.99313. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.93522/4.98951. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.93079/4.98731. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.93213/5.00373. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.94005/4.99063. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.93828/4.98843. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.93763/4.96724. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.93899/4.97036. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.94262/4.96918. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.93839/4.99112. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.93856/4.97773. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.93311/4.97617. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.92503/5.03458. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.94145/4.96977. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.93884/4.97061. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.93439/4.97422. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.93642/4.97693. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.93141/4.99329. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.92918/5.00229. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.93513/4.97137. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.93196/4.98603. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.93431/4.99788. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.92805/4.98848. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.93233/4.98700. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.92979/5.00508. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.92708/4.98315. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.93200/4.98054. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.92483/4.99970. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.93095/4.99851. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.92952/4.99063. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.92578/5.00643. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.92489/4.99786. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.93024/5.00291. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.06240659293657412\n",
      "Epoch 0, Loss(train/val) 4.95884/4.90769. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.92050/4.90991. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.91691/4.91512. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.91626/4.92010. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.91985/4.92003. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.91753/4.91981. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.91647/4.91822. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.91852/4.91167. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.91439/4.90543. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.91436/4.90398. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.91301/4.90477. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.91287/4.90805. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.91166/4.90808. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.91260/4.90763. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.90908/4.91475. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.90848/4.91656. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.90919/4.91396. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.90578/4.91650. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.90942/4.92074. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.90687/4.91716. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.90556/4.91260. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.90856/4.90824. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.90631/4.91220. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.90560/4.91377. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.90450/4.92911. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.90400/4.91549. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.90482/4.91421. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.90210/4.93288. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.90401/4.91276. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.90520/4.91952. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.90105/4.91835. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.89816/4.93320. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.90178/4.92572. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.90091/4.92034. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.90225/4.91716. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.90004/4.92499. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.89917/4.92707. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.89945/4.92868. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.90084/4.92536. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.90267/4.91840. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.90166/4.91732. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.89658/4.92264. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.89901/4.92361. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.89638/4.88802. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.91242/4.89577. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.91081/4.89405. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.90628/4.89665. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.90436/4.89999. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.90267/4.91508. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.90116/4.91331. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.89933/4.91564. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.89976/4.91232. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.89876/4.92365. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.89812/4.91591. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.89756/4.91847. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.89845/4.91911. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.90167/4.91135. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.89726/4.91220. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.89789/4.91964. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.89765/4.91526. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.89794/4.90968. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.89668/4.92047. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.89740/4.93457. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.90454/4.92444. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.90191/4.92522. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.90409/4.91628. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.90264/4.91438. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.90014/4.92064. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.89949/4.92867. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.90308/4.91523. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.90310/4.92719. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.89745/4.92070. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.90174/4.92210. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.90153/4.92486. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.90015/4.92691. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.89958/4.92879. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.90016/4.93203. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.90031/4.92916. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.90005/4.92975. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.89901/4.92963. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.89706/4.93300. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.89565/4.92817. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.89973/4.94272. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.89727/4.91837. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.89747/4.93364. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.89510/4.92262. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.89723/4.92431. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.89615/4.92175. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.89887/4.93259. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.89583/4.91732. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.89803/4.90829. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.89191/4.92744. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.89714/4.92435. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.89417/4.92306. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.89480/4.91978. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.88938/4.91849. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.89645/4.90153. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.89696/4.92259. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.89130/4.92234. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.89381/4.92330. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.019772733298509516\n",
      "Epoch 0, Loss(train/val) 5.06834/4.93873. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.95748/4.94337. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.94786/4.94610. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.94826/4.94480. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.94330/4.93930. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.93945/4.93571. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.93731/4.93413. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.93721/4.94326. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.93322/4.94406. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.93369/4.95243. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.93514/4.94824. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.93401/4.93874. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.93618/4.94350. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.93612/4.94572. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.93353/4.94660. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.93216/4.94407. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.93151/4.94820. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.92889/4.94852. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.92827/4.94141. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.92796/4.94039. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.92862/4.93977. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.92961/4.93670. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.92980/4.94093. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.92866/4.93648. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.92815/4.93817. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.92849/4.94061. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.92466/4.93607. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.92655/4.94131. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.92701/4.93589. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.92659/4.94065. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.92456/4.93955. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.92344/4.93850. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.92637/4.93825. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.92144/4.93968. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.92283/4.93470. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.91938/4.93769. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.92166/4.94318. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.91987/4.94685. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.92059/4.94563. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.92093/4.94267. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.92299/4.93496. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.91924/4.92673. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.93678/4.93880. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.93261/4.93216. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.92902/4.92755. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.92470/4.93015. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.92096/4.93022. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.92465/4.93052. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.91973/4.94132. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.92324/4.93573. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.92107/4.93057. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.92501/4.93463. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.92079/4.93428. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.91832/4.92628. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.91735/4.92901. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.91632/4.93385. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.91812/4.93390. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.91716/4.93282. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.91317/4.92378. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.91325/4.92813. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.91251/4.93774. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.91365/4.93756. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.92803/4.95292. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.91951/4.94572. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.91630/4.94168. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.91624/4.94597. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.91265/4.94861. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.91423/4.93343. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.91113/4.94784. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.91668/4.94553. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.91961/4.94954. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.91526/4.93773. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.90846/4.94110. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.91955/4.93705. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.90975/4.94238. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.90719/4.96241. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.91527/4.93849. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.90988/4.95083. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.91451/4.93621. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.90689/4.94486. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.91051/4.93788. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.90798/4.95232. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.91077/4.93238. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.91437/4.94918. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.90909/4.94312. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.91246/4.95328. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.90798/4.95722. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.90980/4.95051. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.91160/4.93271. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.90315/4.95786. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.90986/4.92848. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.90602/4.93692. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.90782/4.94055. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.90483/4.93077. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.90003/4.95390. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.90597/4.94698. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.90369/4.95181. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.90304/4.93298. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.90610/4.95604. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.90201/4.94480. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.03779786830012622\n",
      "Epoch 0, Loss(train/val) 4.98448/4.88435. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.88697/4.88244. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.88679/4.89008. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.88971/4.89725. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.88766/4.89916. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.88580/4.89858. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.88646/4.90033. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.88647/4.89502. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.88975/4.89723. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.89053/4.89374. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.89073/4.89202. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.88986/4.88664. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.88856/4.88884. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.88666/4.88999. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.88295/4.89304. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.88417/4.88994. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.88344/4.88862. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.88441/4.88958. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.88311/4.89002. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.88232/4.89089. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.88113/4.90360. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.88075/4.89956. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.88115/4.89850. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.87946/4.90718. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.87844/4.90180. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.87968/4.90701. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.87882/4.90364. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.87569/4.90702. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.87560/4.89813. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.87616/4.91105. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.87601/4.90587. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.87545/4.91161. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.87611/4.89772. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.87141/4.91601. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.87190/4.90323. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.87052/4.89579. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.86878/4.91454. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.87051/4.90289. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.86636/4.90380. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.87091/4.90882. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.87708/4.88990. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.86895/4.89232. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.86850/4.89656. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.86954/4.90205. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.86633/4.89493. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.86514/4.88524. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.86635/4.88877. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.86083/4.89157. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.87286/4.89449. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.87274/4.88110. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.86870/4.90201. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.86656/4.88660. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.86296/4.88258. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.86154/4.88190. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.86186/4.91186. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.86338/4.88808. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.86349/4.89361. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.86587/4.89972. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.85974/4.89979. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.86304/4.90985. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.86606/4.87992. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.85634/4.89409. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.85897/4.90330. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.86729/4.88174. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.86178/4.88447. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.85494/4.86934. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.86100/4.86582. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.86512/4.88202. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.86118/4.89004. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.86181/4.87237. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.85798/4.87076. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.85507/4.89061. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.85990/4.92662. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.86344/4.87604. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.85516/4.89067. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.85147/4.90042. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.85591/4.87020. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.85674/4.91107. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.86325/4.88832. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.85865/4.88959. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.85338/4.87767. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.85370/4.86730. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.85975/4.88295. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.84691/4.88385. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.85564/4.86093. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.85157/4.86124. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.87813/4.87567. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.87346/4.87063. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.86725/4.89155. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.85226/4.88551. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.85485/4.90582. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.85740/4.87561. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.85838/4.88998. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.85024/4.88137. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.85425/4.88057. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.84666/4.88215. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.84770/4.86881. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.85275/4.88292. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.85392/4.89348. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.84440/4.86074. Took 0.08 sec\n",
      "ACC: 0.390625, MCC: -0.19770766067180878\n",
      "Epoch 0, Loss(train/val) 4.90892/4.79316. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.83068/4.78116. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.82917/4.78552. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.81596/4.79594. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.81382/4.78876. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.81546/4.78509. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.82000/4.78371. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.81603/4.78733. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.81329/4.78883. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.81433/4.78715. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.81287/4.78572. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.81479/4.78495. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.81180/4.78279. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.81279/4.78343. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.80888/4.78689. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.80903/4.76755. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.80844/4.77582. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.80812/4.77292. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.80671/4.77399. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.79980/4.77592. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.80406/4.77614. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.80068/4.78449. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.79639/4.79181. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.80033/4.79467. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.81373/4.79771. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.81497/4.78820. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.81375/4.79456. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.80849/4.79005. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.80258/4.79335. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.80282/4.78304. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.80584/4.79528. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.79836/4.78794. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.79768/4.78268. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.80644/4.78831. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.80139/4.79105. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.79580/4.79427. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.79575/4.80008. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.79553/4.79342. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.79469/4.79794. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.79538/4.81639. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.78942/4.82036. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.78325/4.80826. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.79671/4.79987. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.79488/4.80629. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.79106/4.80567. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.78978/4.81337. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.79007/4.80548. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.79470/4.80901. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.79289/4.81847. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.78360/4.80908. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.78630/4.80812. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.78595/4.81593. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.79182/4.81421. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.78311/4.82048. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.78253/4.84536. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.78475/4.84902. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.78685/4.82217. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.80488/4.79326. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.81068/4.78844. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.80082/4.80036. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.79807/4.80078. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.79786/4.80431. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.79563/4.80264. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.79291/4.81001. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.79514/4.81178. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.79083/4.82370. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.79790/4.81548. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.79645/4.81642. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.79406/4.83021. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.79247/4.82715. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.79605/4.82370. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78982/4.81941. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.78676/4.83450. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.79203/4.84003. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.79006/4.83143. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.79004/4.83429. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.79396/4.84544. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.78999/4.83727. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.78687/4.84754. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.79055/4.83377. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.78529/4.84740. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.78521/4.85058. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.78574/4.86247. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.78639/4.85971. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.78074/4.84430. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.77989/4.88095. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.80856/4.82697. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.79808/4.84183. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.79522/4.83804. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.78787/4.82760. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.79119/4.83074. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.78815/4.82410. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.78247/4.83825. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.78107/4.84198. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.78172/4.84068. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.78312/4.83985. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.78002/4.83579. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.78064/4.84053. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77606/4.84079. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.78098/4.82800. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.025552902682603722\n",
      "Epoch 0, Loss(train/val) 4.96585/5.00079. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.91720/4.95804. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.91103/4.91873. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.90576/4.90482. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.90229/4.90998. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.90296/4.91704. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.90312/4.91733. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.90279/4.91866. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.90396/4.91083. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.89807/4.90784. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.89975/4.91283. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.89878/4.91093. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.89508/4.91385. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.89248/4.92690. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.89752/4.91787. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.89625/4.91185. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.89392/4.90398. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.88896/4.90297. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.88978/4.91567. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.89661/4.90297. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.89227/4.89767. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.89069/4.90112. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.88734/4.91254. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.88596/4.91895. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.88750/4.90642. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.88923/4.90159. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.88516/4.91813. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.90206/4.91488. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.89388/4.90353. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.88836/4.91492. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.89113/4.90012. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.88902/4.89506. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.88569/4.90111. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.88975/4.90165. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.87982/4.91749. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.88357/4.90591. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.88401/4.90155. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.88552/4.89522. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.88844/4.89362. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.88325/4.90568. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.88220/4.90329. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.88672/4.89447. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.88063/4.90334. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.88791/4.92031. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.88388/4.91028. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.88333/4.90352. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.88096/4.90567. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.88014/4.89867. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.87735/4.91005. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.87895/4.89504. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.87975/4.89604. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.87773/4.89997. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.88143/4.89027. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.89676/4.90172. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.89649/4.89004. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.89018/4.89212. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.89107/4.88810. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.89007/4.89786. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.88821/4.89971. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.88895/4.89910. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.88314/4.90836. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.88151/4.90354. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.90092/4.90241. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.89467/4.89135. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.89258/4.88696. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.89016/4.90338. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.89123/4.90212. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.88671/4.90715. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.88343/4.90503. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.88570/4.90671. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.87660/4.92930. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.87977/4.92426. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.87766/4.92183. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.87860/4.93212. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.88342/4.92028. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.87652/4.93760. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.87189/4.93377. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.88066/4.93133. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.87377/4.94088. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.87158/4.94617. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.87598/4.95951. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.87631/4.92579. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.87294/4.93516. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.87003/4.94408. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.87109/4.94209. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.86805/4.94733. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.87873/4.92385. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.87124/4.93477. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.87591/4.92031. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.87041/4.93369. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.87453/4.92331. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.87143/4.93649. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.86904/4.94565. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.86141/4.94908. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.87280/4.94027. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.86857/4.93491. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.86940/4.96169. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.89620/4.89747. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.89088/4.88534. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.88689/4.88946. Took 0.09 sec\n",
      "ACC: 0.65625, MCC: 0.3110917000380287\n",
      "Epoch 0, Loss(train/val) 5.17002/5.10885. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.09922/5.09706. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 5.09300/5.11714. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.09397/5.12218. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.09191/5.11802. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.08722/5.12092. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.09113/5.11897. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 5.09212/5.11609. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.09136/5.11515. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.09133/5.11753. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 5.08998/5.12195. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.08821/5.12130. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.09107/5.12810. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 5.08888/5.13064. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.08976/5.13526. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.08700/5.13148. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.08755/5.13386. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.09008/5.13788. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 5.08522/5.13974. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.08650/5.13602. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.08802/5.13875. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.08553/5.10848. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.08649/5.14207. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.08498/5.14729. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.08352/5.12051. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.08674/5.13614. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.08501/5.14065. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.08273/5.15580. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.08301/5.15278. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 5.08903/5.16212. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.08267/5.15518. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.08620/5.16214. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 5.08462/5.14750. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.08357/5.14507. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.08432/5.14154. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 5.08169/5.15327. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.08035/5.15012. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.08298/5.15183. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 5.08325/5.15241. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.08306/5.16794. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.07919/5.16021. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.08149/5.15905. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.07987/5.14607. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.08071/5.16588. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.08084/5.15743. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.07963/5.17229. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.07601/5.16855. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.07905/5.15686. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 5.08125/5.15460. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.08161/5.15413. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.08219/5.15050. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.08008/5.15986. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.07770/5.16762. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 5.07799/5.16017. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.07857/5.15787. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.07782/5.16162. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.07735/5.14735. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.07491/5.16032. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.07595/5.16696. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 5.07275/5.17726. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.07501/5.15252. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.07246/5.14967. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 5.07388/5.14860. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.07624/5.16504. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.07017/5.15354. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 5.07484/5.14829. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.07408/5.14463. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.07364/5.15128. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.06625/5.17660. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.07450/5.14246. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.06953/5.13874. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 5.07256/5.13794. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.07099/5.12476. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.07001/5.18305. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.06713/5.16208. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.06773/5.16551. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.06754/5.13948. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 5.07206/5.16711. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.06645/5.14444. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.07039/5.15888. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 5.06679/5.16307. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.06498/5.15295. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 5.06724/5.15819. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.06597/5.17359. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.06878/5.17097. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.06740/5.17513. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.06649/5.17831. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 5.06086/5.16148. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.06316/5.16599. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.06257/5.17922. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.06543/5.14393. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.06187/5.15205. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.06488/5.18351. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 5.06368/5.15615. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.06319/5.18723. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.05825/5.18168. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.06371/5.14562. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.05814/5.16192. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 5.06090/5.18187. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.06336/5.19055. Took 0.08 sec\n",
      "ACC: 0.5, MCC: -0.011953709238683663\n",
      "Epoch 0, Loss(train/val) 4.69829/4.70065. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.66093/4.66300. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.65719/4.67895. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.65927/4.69460. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.65906/4.69774. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.65926/4.68949. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.66366/4.66636. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.66007/4.65004. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.65901/4.64955. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 4.65461/4.65277. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.65394/4.65580. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.65553/4.64951. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.65601/4.64848. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.65533/4.65082. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.65126/4.65245. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.65337/4.64956. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.65483/4.64958. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.65205/4.65371. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.65391/4.65136. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.65216/4.65409. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.65043/4.64990. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.64984/4.65181. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.64844/4.65375. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.65035/4.65086. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.64758/4.64975. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.64833/4.65351. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.64858/4.65276. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.64592/4.65624. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.64512/4.65380. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.64633/4.65986. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.64457/4.66771. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.64510/4.65751. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.64280/4.66507. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.64509/4.66369. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.64234/4.66214. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.64037/4.66544. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.64376/4.65848. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.63627/4.67623. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.64412/4.67434. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.64512/4.66055. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.64924/4.65816. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.64476/4.66072. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.64384/4.66233. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.64126/4.67235. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.64209/4.66890. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.63858/4.67789. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.63966/4.66922. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.63742/4.66998. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.64366/4.65912. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.63365/4.66691. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.63797/4.67213. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.64534/4.67971. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.64409/4.66535. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.63763/4.67233. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.64041/4.67921. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.63823/4.67433. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.64244/4.66759. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.63792/4.66295. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.63345/4.67740. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.64537/4.66705. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.63344/4.66879. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.63527/4.67266. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.63318/4.68309. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.63584/4.66641. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.63542/4.67163. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.63463/4.67135. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.63496/4.66649. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.63224/4.67976. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.63187/4.67355. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.63527/4.67006. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.63702/4.67805. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.62983/4.67405. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.62979/4.66989. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.62988/4.67515. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.62869/4.68856. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.63447/4.66857. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.63302/4.67734. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.62927/4.68454. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.63295/4.67921. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.62719/4.68855. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.62937/4.69279. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.63133/4.66747. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.63317/4.67383. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.62572/4.67397. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.62726/4.68094. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.62543/4.67721. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.63069/4.67413. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.62342/4.69021. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.62899/4.68271. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.62680/4.68795. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.62605/4.68531. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.65169/4.67029. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.63938/4.68579. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.63546/4.68288. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.62771/4.69060. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.63252/4.67122. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.63809/4.68010. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.63136/4.68371. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.63266/4.67310. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.63690/4.67930. Took 0.08 sec\n",
      "ACC: 0.5, MCC: 0.007889684472185849\n",
      "Epoch 0, Loss(train/val) 5.14909/5.09063. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.12080/5.08233. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.11647/5.08115. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.11891/5.09081. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 5.10521/5.10546. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.10055/5.10285. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.10162/5.10666. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 5.09964/5.11188. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.10066/5.11527. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.10140/5.11142. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 5.10028/5.11812. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.09862/5.12659. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.09591/5.12556. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.09832/5.11789. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.09445/5.12885. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.09598/5.12999. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 5.09316/5.13594. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.09260/5.13440. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.09300/5.13481. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 5.09475/5.13653. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.09400/5.13675. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.09131/5.13986. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.08921/5.14042. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.09752/5.13214. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.09093/5.15481. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 5.09010/5.14658. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.09313/5.14471. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.09343/5.16216. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 5.09262/5.13918. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.09315/5.14130. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.09085/5.14658. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.09330/5.13640. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.09116/5.14585. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.08796/5.14346. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 5.09227/5.14876. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.08935/5.15839. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.08648/5.15322. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.08759/5.16833. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.08481/5.16937. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.08776/5.17037. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.08307/5.17912. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.08721/5.16529. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 5.08699/5.18929. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.08340/5.17271. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.08143/5.18184. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.08184/5.17792. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.08164/5.17741. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.08635/5.17379. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.08190/5.16105. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.07982/5.20086. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 5.08154/5.17486. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.07674/5.17944. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.08728/5.16186. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.07565/5.19063. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.08154/5.18441. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.07640/5.19116. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.07557/5.17685. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.07603/5.21116. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 5.07646/5.17327. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.08906/5.12796. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 5.09158/5.17931. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.07850/5.19935. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.07934/5.20972. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 5.08252/5.17905. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.07767/5.19476. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.07697/5.19298. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 5.07791/5.19502. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.08242/5.20877. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.07912/5.19428. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.07162/5.22859. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.07725/5.19495. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.07345/5.21495. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 5.07646/5.20521. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.08111/5.17534. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.08130/5.18962. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.08644/5.16289. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.07905/5.18142. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.07800/5.17823. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 5.08566/5.16259. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.07670/5.19190. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.08165/5.17192. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.07887/5.19124. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.07925/5.18607. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.07794/5.17140. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 5.07313/5.18552. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.08368/5.16493. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.07576/5.17959. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 5.08022/5.19343. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.07200/5.18727. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.07693/5.18556. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 5.07521/5.20581. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.07646/5.20924. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.07428/5.19548. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.07794/5.20317. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.06844/5.21209. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 5.07864/5.18533. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.07699/5.21106. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.07114/5.20417. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 5.07469/5.19518. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 5.07324/5.20293. Took 0.08 sec\n",
      "ACC: 0.359375, MCC: -0.2875509247045426\n",
      "Epoch 0, Loss(train/val) 5.19420/5.23921. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 5.12972/5.20564. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.12927/5.20118. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.13567/5.20254. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 5.13095/5.19548. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.13442/5.17812. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 5.13216/5.15995. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 5.13164/5.14992. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.13112/5.14952. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.12973/5.14790. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 5.12986/5.15429. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.12758/5.14444. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.12636/5.14348. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.12646/5.15198. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.12505/5.14819. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.12663/5.14927. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.12718/5.15598. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.12175/5.16172. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 5.12327/5.15524. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.12508/5.15745. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.12270/5.14873. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.12319/5.14712. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.11991/5.15226. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.11929/5.16576. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.12133/5.15960. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 5.11940/5.15983. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.11970/5.16413. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.11857/5.17183. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.11863/5.16617. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.12006/5.15609. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.11849/5.15756. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 5.11876/5.16658. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.11678/5.17436. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.11334/5.19287. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 5.11981/5.16332. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.11759/5.17111. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.11617/5.18339. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.11777/5.17231. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.11745/5.16759. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.11804/5.16915. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.11199/5.18339. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 5.11312/5.19387. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.11774/5.17896. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.11259/5.17152. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.11273/5.18759. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.11504/5.19339. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 5.11243/5.18820. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.11095/5.18903. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.10780/5.19565. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 5.11408/5.18488. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.10969/5.18945. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.11069/5.20300. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 5.11025/5.19609. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.10863/5.19541. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 5.10995/5.19463. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.11105/5.19677. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.11394/5.18542. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 5.10706/5.19918. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 5.10480/5.20651. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.10712/5.20164. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 5.11099/5.19278. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.11109/5.18909. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 5.11027/5.18730. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.11752/5.19397. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 5.11028/5.19565. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.11636/5.20286. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.11257/5.20487. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.11148/5.20379. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.10808/5.20998. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 5.10816/5.20334. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.11201/5.19082. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.11125/5.20139. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.11158/5.21319. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.10737/5.20212. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.11065/5.19415. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 5.10789/5.20955. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.10797/5.21090. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.10459/5.21793. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 5.10588/5.21185. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.10564/5.20439. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.10666/5.20540. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 5.10521/5.20555. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.10696/5.18994. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.10870/5.19808. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.10621/5.20795. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.10057/5.20701. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 5.10574/5.20972. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.10489/5.20505. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.10331/5.19997. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 5.10283/5.20193. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.10292/5.21824. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.10145/5.21062. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 5.10468/5.20586. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.10422/5.22800. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 5.10803/5.21294. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.09855/5.22132. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 5.10706/5.21345. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.10380/5.20771. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 5.10594/5.21401. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.10255/5.21939. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.06262242910851495\n",
      "Epoch 0, Loss(train/val) 4.83530/4.79738. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.78211/4.79280. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.78382/4.78972. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.78727/4.79295. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.78792/4.80173. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.78845/4.80945. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.78202/4.80646. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.78066/4.80047. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.77695/4.80106. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.77836/4.79604. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.78495/4.79919. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.78454/4.79633. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.77965/4.79336. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.78053/4.79201. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.78028/4.79305. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.78134/4.79282. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.77783/4.79464. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.78035/4.79494. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.77602/4.80006. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.77561/4.80530. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.77767/4.80861. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.77733/4.81391. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.77679/4.80825. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.77281/4.81354. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 4.77365/4.82254. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.77537/4.81897. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.77236/4.82030. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.77211/4.82399. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.77316/4.82595. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.77088/4.83610. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.76862/4.84526. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.77356/4.81674. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.76714/4.83397. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.77025/4.82694. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.77023/4.82096. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.76829/4.83588. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.76871/4.83399. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.77113/4.82635. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.76774/4.83156. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.76489/4.84378. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.77863/4.80292. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.77847/4.80345. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.77763/4.80999. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.77596/4.80915. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.77707/4.80667. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.77649/4.81462. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.77402/4.81697. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.77571/4.81605. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.77315/4.82091. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.77252/4.81457. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.77456/4.82091. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.77075/4.83738. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.77149/4.81380. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.77386/4.81974. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.76895/4.82990. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.77153/4.81525. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.76805/4.84776. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.77143/4.80732. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76796/4.83775. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.76995/4.79962. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.76823/4.82673. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 4.76765/4.81556. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76854/4.82639. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.76588/4.81925. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.76279/4.83463. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.76706/4.82270. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.76585/4.83057. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.76766/4.81947. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.76215/4.83537. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76121/4.83334. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.76191/4.82652. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.75893/4.83680. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76246/4.82223. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.76530/4.82911. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.76044/4.83707. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.75808/4.86757. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.76489/4.81519. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.75988/4.83819. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.75426/4.84440. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.75621/4.86323. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.75778/4.84176. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.75956/4.83686. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.75928/4.85452. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.75848/4.85588. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75865/4.85384. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.75549/4.85869. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.75652/4.85070. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.75594/4.83741. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.75165/4.85312. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.75363/4.84151. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.75498/4.86637. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.75206/4.83589. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.75842/4.83437. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.75953/4.84194. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.75131/4.86805. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.76188/4.83007. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.75322/4.86698. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.76139/4.82162. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.74961/4.88297. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.75292/4.84540. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.06240659293657412\n",
      "Epoch 0, Loss(train/val) 4.85012/4.83919. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.81079/4.81552. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.80760/4.81698. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.80610/4.81683. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.81035/4.81705. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.80943/4.81821. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.80572/4.81831. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.80744/4.81564. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.80604/4.81129. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.80435/4.80469. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.80677/4.81255. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.80464/4.81056. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.80461/4.80641. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.80499/4.80088. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.80180/4.79833. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.80128/4.79479. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.80112/4.79392. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.80064/4.80083. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.79877/4.80419. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.80286/4.82039. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.80516/4.81533. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.80477/4.81336. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.80261/4.80992. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.80073/4.80958. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.80099/4.81003. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.79932/4.80893. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.80098/4.80624. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.79478/4.81124. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.79747/4.80117. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.79884/4.80023. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.79516/4.78999. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.79889/4.79001. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.79187/4.79561. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.79346/4.78353. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.79412/4.78641. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.79348/4.79634. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.79274/4.79286. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.79447/4.78238. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.79139/4.77567. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.79272/4.77646. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.78903/4.77552. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.79052/4.78370. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.78818/4.76939. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.78669/4.77693. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.78968/4.77988. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.78952/4.77879. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.78743/4.78098. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.78480/4.76187. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.78570/4.76801. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.78774/4.76638. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.78941/4.79147. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.79236/4.77178. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.78699/4.77189. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.78275/4.77639. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.78401/4.77041. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.78368/4.77571. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.78247/4.77044. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.77965/4.76829. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.77809/4.77109. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.78204/4.75590. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.78089/4.77569. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.77751/4.77912. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.77520/4.77463. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.77968/4.78379. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.77616/4.77229. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.77885/4.77559. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.78177/4.78152. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.77433/4.78161. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.77847/4.79617. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.77586/4.77975. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.77784/4.78032. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.77244/4.78063. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.77663/4.78139. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.77764/4.78071. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.77291/4.77045. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.76949/4.78458. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.77491/4.77651. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.77475/4.77631. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.77308/4.77100. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.77610/4.76362. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.77028/4.76840. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.77270/4.77260. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.77387/4.76224. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.77264/4.79093. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.77384/4.77712. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.76705/4.78254. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.77271/4.77658. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 4.76807/4.76122. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.77492/4.78195. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.77000/4.77905. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.76656/4.77937. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.76857/4.77479. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.77180/4.76895. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.76661/4.76925. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.76907/4.78437. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.76632/4.81240. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.76854/4.78199. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.77241/4.76119. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 4.76433/4.77363. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.76002/4.77252. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.11834526708278773\n",
      "Epoch 0, Loss(train/val) 4.94495/4.86001. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 4.85826/4.85052. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.85886/4.84893. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.86064/4.84438. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.86043/4.84384. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.86091/4.84655. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.86009/4.84819. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.85879/4.85556. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.85804/4.86149. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.85480/4.86766. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.85523/4.86904. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.85210/4.86900. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85241/4.86946. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.85380/4.87005. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.85393/4.86952. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.85330/4.87497. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.85093/4.87661. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.85152/4.87512. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.84950/4.87247. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.84994/4.87296. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.84954/4.87127. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.84797/4.86986. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.84799/4.87167. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.84933/4.87943. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.84736/4.88215. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.84625/4.87458. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.84162/4.87736. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.84726/4.87186. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.84241/4.88782. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84450/4.88228. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.83995/4.87320. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.84530/4.86997. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84169/4.86837. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.83926/4.86177. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.84227/4.86542. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84719/4.87118. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.84306/4.86213. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84284/4.86120. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84707/4.84298. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.84503/4.84616. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.84380/4.85443. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.84033/4.85550. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.83599/4.85350. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.83672/4.86887. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.84066/4.85339. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.83989/4.86461. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.83691/4.86912. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.83708/4.86570. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.83704/4.86820. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.83559/4.87054. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.83290/4.87261. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.83975/4.85222. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.84092/4.86512. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.84053/4.87977. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.84774/4.86276. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.84221/4.86717. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.83547/4.86815. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.83567/4.87165. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.83653/4.86516. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83433/4.88405. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.83649/4.87278. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83887/4.86501. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.83700/4.87022. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.84243/4.85548. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.84940/4.85586. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.84042/4.86052. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.84104/4.86969. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.83627/4.88149. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.83740/4.86689. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.83796/4.87761. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.83668/4.87349. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.83389/4.89256. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.83549/4.87110. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.83653/4.87821. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.83172/4.88852. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.83640/4.86942. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.83675/4.86318. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.83533/4.87602. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.83057/4.89175. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.84035/4.87916. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.83292/4.86567. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.83637/4.86921. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.82885/4.87844. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.83404/4.87145. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.83010/4.88190. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.83391/4.86864. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.83241/4.88344. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.83582/4.88054. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83400/4.87370. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.82841/4.86980. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.82379/4.86153. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.83110/4.86747. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.83444/4.86491. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83543/4.86470. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.83274/4.85888. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.82865/4.86814. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.82905/4.86072. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.82892/4.86537. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83211/4.86716. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.82718/4.86405. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.06643282473893375\n",
      "Epoch 0, Loss(train/val) 4.78243/4.81378. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.77409/4.77083. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.76290/4.79328. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.76975/4.78685. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.76619/4.79232. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.76867/4.79634. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.76736/4.78686. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.77125/4.78153. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.77138/4.78009. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.76295/4.78650. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.75920/4.79006. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.75855/4.79033. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.75845/4.79150. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.76064/4.78963. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.75723/4.78865. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.75635/4.78955. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.75596/4.78638. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.75221/4.79602. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.75333/4.78924. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.75058/4.79314. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.75329/4.79371. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.75388/4.78875. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.74596/4.79416. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.75096/4.77963. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.74753/4.79089. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.75291/4.77779. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.75049/4.78408. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.74665/4.79014. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.74787/4.78510. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.74853/4.78846. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.74542/4.79080. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.74632/4.78494. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.74779/4.77365. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.75303/4.77775. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.74897/4.78083. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.74195/4.79080. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.74850/4.78117. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.74021/4.78932. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.74289/4.78421. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.74494/4.77730. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.74888/4.77499. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.74134/4.78229. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.74203/4.77919. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.74568/4.77457. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.73963/4.78648. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.74279/4.77635. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.73932/4.77685. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.74112/4.78318. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.73843/4.77186. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.73919/4.77583. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.74239/4.77287. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.73984/4.78035. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.73670/4.77728. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.74340/4.76782. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.73893/4.78091. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.73904/4.77273. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.73888/4.77416. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.73488/4.78064. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.73912/4.77757. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.73875/4.78628. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.74251/4.77947. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.74027/4.77201. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.73887/4.77423. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.73482/4.77426. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.73672/4.77887. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.73895/4.78460. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.73346/4.78222. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.74145/4.77197. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.73783/4.77912. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.73519/4.77956. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.73224/4.78325. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.73092/4.79077. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.73588/4.77295. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.73759/4.77509. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.73713/4.77762. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.73246/4.77411. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.73321/4.76833. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.73300/4.77432. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.73429/4.77113. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.73432/4.77677. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.73201/4.77024. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.73171/4.77847. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.72898/4.78302. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.73412/4.77137. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.73485/4.77282. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.72989/4.78011. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.73271/4.77252. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.73086/4.77955. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.73805/4.76830. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.73473/4.77736. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.72581/4.78219. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.73188/4.78194. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.72795/4.78401. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.73110/4.78001. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.73053/4.76943. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.73600/4.77234. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.72853/4.78664. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.72819/4.76909. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.72997/4.77069. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.72408/4.77953. Took 0.09 sec\n",
      "ACC: 0.453125, MCC: -0.08222643447147887\n",
      "Epoch 0, Loss(train/val) 4.96000/4.89857. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.88211/4.88357. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.88009/4.87464. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.87971/4.87202. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.87773/4.87092. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.88067/4.87188. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.87707/4.87554. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.87671/4.87564. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.87331/4.88098. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.87254/4.88170. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.87103/4.88333. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.87380/4.87992. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.86692/4.89538. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.86997/4.89177. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.86422/4.90074. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.86815/4.90479. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.86419/4.88970. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.86282/4.90001. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.86875/4.89326. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.86600/4.87273. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.86579/4.89988. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.86378/4.93150. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.86908/4.89213. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.86170/4.88983. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.85648/4.94777. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.87081/4.88829. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.86276/4.88340. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.86239/4.90717. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.86254/4.90454. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.85713/4.92025. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.86404/4.89289. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.85800/4.92637. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.85994/4.89531. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.85817/4.90696. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.85642/4.92636. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.86063/4.90144. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.85631/4.92268. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.85662/4.92271. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.85938/4.92576. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.85993/4.92125. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.85453/4.95392. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.85339/4.92442. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.85800/4.89234. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.85478/4.96087. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.85813/4.92231. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.85148/4.93844. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.85753/4.89713. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.85122/4.92336. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.85491/4.93512. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.85582/4.92427. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.85797/4.92493. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.85250/4.92154. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.85298/4.91948. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.85257/4.94506. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.85538/4.96152. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.85932/4.91729. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.85403/4.87418. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.85450/4.92054. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.85567/4.90585. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.86307/4.90332. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.86593/4.93481. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.86189/4.92555. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.86195/4.92085. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.86538/4.95427. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.86273/4.93112. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.86523/4.93098. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.85673/4.91244. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.86180/4.93430. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.85605/4.94225. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.85686/4.93171. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.86055/4.94819. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.86164/4.93305. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.85972/4.93926. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.85916/4.92873. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.85685/4.93858. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.85716/4.92743. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.85185/4.93999. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.85103/4.93029. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.85092/4.94926. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.85656/4.93035. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.86352/4.92726. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.85991/4.91326. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.85598/4.93297. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.85464/4.92741. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.85310/4.93543. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.85919/4.94474. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.85408/4.92781. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.85427/4.92326. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.84849/4.94114. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.84863/4.95092. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.85186/4.92543. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.84933/4.94735. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.85079/4.94231. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.84832/4.94880. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.85572/4.95390. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.85139/4.96430. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.84526/4.96780. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.84911/4.93181. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.85039/4.94881. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.84609/4.93647. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.0657512425947884\n",
      "Epoch 0, Loss(train/val) 4.86085/4.84357. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.85638/4.88387. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.85023/4.90927. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.85628/4.89756. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.86846/4.83952. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.85122/4.83847. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.84235/4.84603. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.84599/4.84674. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.84554/4.84656. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.84390/4.84894. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.84464/4.85196. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.84025/4.85789. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.84654/4.85658. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.84364/4.85465. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.84569/4.85483. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.84406/4.85428. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.84130/4.85386. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.84154/4.85431. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.84066/4.85405. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.84061/4.85696. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.83880/4.85217. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.84144/4.85287. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.84000/4.85040. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.83480/4.85131. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.83983/4.85225. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.83793/4.85311. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.83580/4.85863. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.83249/4.85975. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.83375/4.86433. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.83822/4.85028. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.83135/4.86054. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.83408/4.85579. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.82885/4.86667. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.83079/4.86071. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.83194/4.86707. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.83131/4.86165. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.82704/4.87656. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.83499/4.85396. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.83169/4.85273. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.83448/4.87235. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.83644/4.87093. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.83205/4.87581. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.83131/4.86475. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.83071/4.86098. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.82839/4.86605. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.83262/4.84887. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.82979/4.85577. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.83053/4.86930. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.82837/4.87748. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.83003/4.87400. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.83004/4.86724. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.83393/4.86414. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.82757/4.88493. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.82702/4.86735. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.82782/4.87800. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.82569/4.87389. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.82542/4.87342. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.82861/4.86823. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.82264/4.87864. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.82693/4.88288. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.82205/4.90388. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.81733/4.85621. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.82571/4.86757. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.82899/4.85607. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.82645/4.87256. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.82691/4.87127. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.82232/4.89465. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.82346/4.89763. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.82140/4.88722. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.82146/4.89779. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.82467/4.89283. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.82145/4.88394. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.82370/4.87903. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.82622/4.88570. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.82017/4.88494. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.81878/4.90311. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.82462/4.90084. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.81818/4.90610. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.82239/4.90071. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.81750/4.90321. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.81640/4.91584. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.81831/4.92161. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.81753/4.90188. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.81069/4.91206. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.81795/4.89539. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.81391/4.91705. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.81344/4.89978. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.82319/4.83794. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.83180/4.85334. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.82768/4.85848. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.82351/4.86396. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.82645/4.86935. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.82435/4.87123. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.82367/4.87389. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.82601/4.87783. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.82120/4.88283. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.81973/4.87394. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.81991/4.88542. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.82137/4.87877. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.81616/4.87971. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.12855839970025792\n",
      "Epoch 0, Loss(train/val) 4.87934/4.86071. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.78271/4.82209. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.78467/4.77890. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.77859/4.78144. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.77793/4.78607. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.77668/4.79656. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.77472/4.80306. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.77726/4.80721. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.77574/4.80549. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.77631/4.80407. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.77426/4.79621. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.77400/4.80232. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.77380/4.78910. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.77327/4.78550. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.77173/4.79364. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.77075/4.78551. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.77064/4.78869. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.76781/4.79676. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.76923/4.79802. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.77080/4.80085. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.76745/4.80147. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.76904/4.79309. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.76971/4.78328. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.77360/4.79637. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.76943/4.80558. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.76973/4.79481. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.77028/4.79228. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.76777/4.79964. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.76697/4.79651. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.76657/4.79552. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.76449/4.81119. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.76970/4.79381. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.76804/4.79035. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.76765/4.79877. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.76587/4.79663. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.76460/4.79755. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.76460/4.80290. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.77026/4.81775. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.76696/4.80542. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.76362/4.82239. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.76523/4.83098. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.76478/4.80569. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.76213/4.80999. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.76190/4.83064. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.76306/4.81573. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.76075/4.81788. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.75906/4.82799. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.76409/4.81862. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.75982/4.82267. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.76216/4.81747. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.76143/4.81516. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.76191/4.81032. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.76605/4.82950. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.76720/4.81513. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.77139/4.81453. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.76985/4.82487. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.75886/4.82531. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.76124/4.83426. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.76248/4.81086. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.76901/4.80974. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.76913/4.81118. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.76770/4.80700. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.76554/4.80990. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.76731/4.81205. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.76532/4.81017. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.76499/4.81110. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.76144/4.81799. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.76296/4.82409. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.76265/4.83198. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.76240/4.82313. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.76782/4.80917. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.76715/4.79980. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.76489/4.80740. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.76302/4.81891. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.76327/4.84394. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.76130/4.81953. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.76031/4.83654. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.76628/4.82142. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.76215/4.82539. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.76005/4.81330. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.76580/4.82296. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.76172/4.80504. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.76162/4.82571. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.75886/4.82572. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.75849/4.83552. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.75707/4.80070. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.76637/4.81531. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.75930/4.82107. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.75422/4.82961. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.75544/4.81384. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.75421/4.82240. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.75380/4.83290. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.75903/4.81543. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.75992/4.82173. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.75846/4.82103. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.75420/4.82421. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.76059/4.82193. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.75881/4.80634. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.75615/4.81277. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.75529/4.82585. Took 0.08 sec\n",
      "ACC: 0.5625, MCC: 0.0931174089068826\n",
      "Epoch 0, Loss(train/val) 4.61548/4.52054. Took 0.18 sec\n",
      "Epoch 1, Loss(train/val) 4.52118/4.51939. Took 0.17 sec\n",
      "Epoch 2, Loss(train/val) 4.50655/4.49904. Took 0.14 sec\n",
      "Epoch 3, Loss(train/val) 4.50513/4.50156. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.50952/4.50419. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.50801/4.50579. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.50654/4.50436. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.50770/4.50530. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.50790/4.50572. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.50342/4.50601. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.50711/4.50749. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.50310/4.51021. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.50396/4.51296. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.50313/4.51862. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.50158/4.51291. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.50047/4.51814. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.49573/4.51959. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.49794/4.52313. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.49799/4.51955. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.49917/4.52367. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.49798/4.51474. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.49464/4.52266. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.49887/4.52310. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.49643/4.52189. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.49665/4.52264. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.49418/4.52867. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.49417/4.52574. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.49311/4.53923. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.48913/4.53539. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.49578/4.52765. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.49580/4.52607. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.49208/4.52973. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.48936/4.53503. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.48818/4.54178. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.49177/4.53880. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.48765/4.53932. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.49344/4.53127. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.48792/4.53695. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.48616/4.54534. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.48729/4.53781. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.48827/4.53842. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.48774/4.53962. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.48534/4.54343. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.48912/4.53703. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.48572/4.54068. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.48920/4.52216. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.48443/4.54493. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.48706/4.55151. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.48547/4.54097. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.48203/4.55199. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.47865/4.55797. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.47954/4.55792. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.48448/4.54866. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.48379/4.53550. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.48101/4.55409. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.47976/4.54923. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.47903/4.55850. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.47683/4.55209. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.47847/4.56656. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.48621/4.53750. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.47677/4.57218. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.48131/4.54818. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.47663/4.54832. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.47646/4.58287. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.47575/4.54463. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.47727/4.54692. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.48271/4.55667. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.47312/4.55940. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.47808/4.54570. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.47606/4.57354. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.47716/4.56945. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.47607/4.60807. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.47860/4.54769. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.48176/4.55307. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.47134/4.59173. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.47073/4.57064. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.47448/4.54978. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.47684/4.55881. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.47497/4.54392. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.47043/4.56941. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.47694/4.53840. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.47950/4.54658. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.46903/4.59976. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.47334/4.58059. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.47297/4.58498. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.46502/4.57680. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.47237/4.56065. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.47671/4.59470. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.46723/4.59452. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.47707/4.54331. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.46663/4.59657. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.46508/4.59140. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.47012/4.56271. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 4.47309/4.58795. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.47156/4.56343. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.46085/4.58508. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.46720/4.56369. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.46949/4.58179. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.46269/4.57860. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.46262/4.58502. Took 0.08 sec\n",
      "ACC: 0.578125, MCC: 0.13951311175942296\n",
      "Epoch 0, Loss(train/val) 5.12359/5.13409. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.07720/5.08426. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 5.07383/5.07081. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.07498/5.06551. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.07657/5.06470. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 5.07463/5.06108. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.07927/5.05931. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.07459/5.06113. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.07312/5.06292. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.07303/5.06115. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.07193/5.05802. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.07189/5.05565. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 5.07147/5.05778. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.07161/5.05954. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 5.07152/5.06083. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.07205/5.05842. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.07225/5.05251. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.07315/5.06648. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.06972/5.07143. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.07101/5.06909. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 5.07028/5.06945. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.06930/5.06711. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.07065/5.06938. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.06772/5.06994. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.06740/5.07097. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.07175/5.06739. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 5.06929/5.07210. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.06597/5.07404. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.06856/5.07466. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.06665/5.07423. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.06844/5.07258. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.06841/5.07352. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 5.06755/5.07461. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.06650/5.07600. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.06798/5.07989. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.06575/5.07836. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.06664/5.07725. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 5.06615/5.08318. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.06320/5.08480. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.06382/5.07623. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.06412/5.07893. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 5.06249/5.07832. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.06227/5.08126. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.06360/5.08584. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.06262/5.08593. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.06124/5.08987. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 5.05891/5.08414. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.05847/5.08464. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.05514/5.09461. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.06800/5.07664. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 5.06047/5.09136. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.05952/5.08284. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.06212/5.08623. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.05544/5.10691. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.06065/5.09875. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.05769/5.08985. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 5.06139/5.08998. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.05635/5.10621. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.05624/5.10612. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.05288/5.10528. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.05611/5.10832. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.05304/5.11405. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.04801/5.09707. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.05267/5.09545. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 5.05831/5.09092. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.06268/5.07180. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.05197/5.12161. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.05890/5.10383. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.04889/5.11394. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.04924/5.08410. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.05176/5.15219. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.05415/5.10828. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.05528/5.10257. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.05307/5.12797. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.05069/5.10382. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 5.05722/5.10944. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.05282/5.12914. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.04793/5.13527. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.04835/5.13559. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.04778/5.14201. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.05073/5.12556. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.05733/5.11743. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.05759/5.10859. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.05479/5.12241. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.05239/5.11851. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.05165/5.12466. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 5.05335/5.12857. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.05329/5.08118. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.05882/5.13574. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.04809/5.11546. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.05030/5.15891. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.04873/5.12872. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.04824/5.14403. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.04737/5.14303. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.04785/5.13863. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.04219/5.16016. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.04874/5.15803. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 5.04731/5.12092. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.04712/5.14818. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.04950/5.11770. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.12855839970025792\n",
      "Epoch 0, Loss(train/val) 4.78442/4.74466. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.73634/4.75020. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.73652/4.75747. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.73674/4.76154. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.73382/4.76530. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.73312/4.77073. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.73162/4.77873. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.73121/4.78901. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.73373/4.79726. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.73268/4.79326. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.73162/4.77865. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.72859/4.76129. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.73151/4.75384. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.72755/4.76853. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.72621/4.76815. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.72478/4.78731. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.72776/4.77552. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.72648/4.76921. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.72442/4.78541. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.72553/4.77222. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.72632/4.77736. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.72683/4.77424. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.72194/4.79145. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.72577/4.75798. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.72919/4.75940. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.72626/4.76989. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.73580/4.72916. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.73220/4.73563. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.73087/4.74586. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.73083/4.74457. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.72878/4.74756. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.72843/4.74786. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.72965/4.74977. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.72944/4.74979. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.72690/4.75376. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.72714/4.75746. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.72859/4.74872. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.73053/4.74214. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.72962/4.74562. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.72930/4.74767. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.73030/4.75163. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.72731/4.75425. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.72865/4.76131. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.73059/4.75460. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.72827/4.75672. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.72702/4.77244. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.72780/4.76234. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.72879/4.76914. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.72960/4.76409. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.72696/4.76801. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.72636/4.77025. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.72834/4.77054. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.72500/4.77570. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.72548/4.77717. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.72786/4.77755. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.72488/4.77919. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.72471/4.77974. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.72358/4.78744. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.72503/4.78770. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.72792/4.78426. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.72434/4.78058. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 4.72637/4.78425. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.72342/4.77534. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.72373/4.79725. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.72029/4.80288. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.72331/4.79065. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.71935/4.81240. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.72226/4.79930. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.72300/4.79952. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.72151/4.80032. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.72111/4.80823. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.72168/4.80826. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.71805/4.82367. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.72031/4.80291. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.71922/4.82626. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.71909/4.81625. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.72001/4.79846. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.71895/4.79251. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.72268/4.79327. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.71471/4.81229. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.71802/4.81473. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.71486/4.82406. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.71644/4.81520. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.71547/4.82380. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.71057/4.83664. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.71427/4.83043. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.71919/4.80350. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.71054/4.80181. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.71310/4.81234. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.70701/4.85906. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.71178/4.79945. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.71256/4.81729. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.70961/4.81018. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.71467/4.82596. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.70884/4.85231. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.70911/4.82282. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.70654/4.85591. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.71219/4.83251. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.71106/4.82933. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.70840/4.81705. Took 0.09 sec\n",
      "ACC: 0.4375, MCC: -0.12725695259515554\n",
      "Epoch 0, Loss(train/val) 4.98276/4.95601. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.96588/4.95363. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.96899/4.95658. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.96720/4.96743. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.98238/4.95402. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.97965/4.96859. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.96038/4.95753. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.96517/4.95747. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.96284/4.96117. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.96289/4.96185. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.96083/4.96468. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.96336/4.96492. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.96001/4.96809. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.95842/4.97742. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.96007/4.96643. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.95654/4.97352. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.95451/4.97068. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.95666/4.97017. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.95411/4.97729. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.95058/4.98245. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.95851/4.97525. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.95388/4.97463. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.95367/4.97478. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.94833/4.97971. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.95658/4.97437. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.95108/4.98004. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.95490/4.97007. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.95241/4.97209. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.94855/4.97196. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.95085/4.97645. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.95143/4.97512. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.94961/4.98037. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.95419/4.96964. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.95115/4.97609. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.94937/4.97267. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.94537/4.97790. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.94747/4.98274. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.94710/4.98546. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.94644/4.97734. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.94968/4.98199. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.94606/4.97701. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.95028/4.98200. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.94320/4.97128. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.94889/5.00311. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.94483/4.97224. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.94467/4.99306. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.94844/5.00998. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.94141/4.97869. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.94955/5.00885. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.94172/4.98291. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.94724/5.00537. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.94168/4.99258. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.94170/4.98371. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.94359/4.98900. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.93873/4.99255. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.94316/4.98488. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.94325/4.99350. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.94140/4.99452. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.94116/4.99875. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.94350/4.98881. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.94855/4.97752. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.94027/5.00593. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.94044/4.99610. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.94104/4.98846. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.94363/4.98746. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.94222/5.00866. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.93942/5.00756. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.93438/5.00989. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.94220/4.97598. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.93998/4.99300. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.93689/5.00509. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.93557/5.03444. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.94029/4.96987. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.93779/4.99127. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.94411/4.99109. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.93881/4.98184. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.93285/5.03261. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.93649/4.99965. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.93487/5.01637. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.93996/5.01167. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.93685/4.96953. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.93984/5.00301. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.93936/5.00008. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.93750/4.98743. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.93700/5.01201. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.93675/5.01960. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.93229/5.02772. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.93275/5.01588. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.93510/5.02414. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.93121/5.03138. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.94104/5.00422. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.92991/5.02462. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.93085/4.99829. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.93600/5.02940. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.93246/5.01299. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.93975/5.00070. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.92503/5.03382. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.93592/5.01902. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.93166/5.00874. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.93600/5.02844. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.1014574359634967\n",
      "Epoch 0, Loss(train/val) 4.91075/4.84101. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.82846/4.83801. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.83341/4.83959. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.83823/4.84078. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.84077/4.84428. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.83984/4.85330. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.83446/4.84773. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.83056/4.84328. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.83098/4.83864. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.83167/4.83815. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.82774/4.83927. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.83044/4.83646. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.83124/4.84239. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.82846/4.83820. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.82761/4.83067. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.82683/4.83601. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.82830/4.83456. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.82687/4.83263. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.82484/4.82826. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.82577/4.83134. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.82546/4.82988. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.82194/4.82462. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.82389/4.82625. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.82382/4.82712. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.82321/4.82491. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.82497/4.81816. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.82433/4.82120. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.82641/4.82088. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.82070/4.81806. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.82024/4.81546. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.82212/4.82470. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.81980/4.82188. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.81792/4.81697. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.82090/4.82544. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.82073/4.82387. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.82025/4.81967. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.82152/4.81397. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.82016/4.81374. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.81811/4.81215. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.81561/4.81338. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.81753/4.82275. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.82314/4.81377. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.81890/4.82983. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.82386/4.82075. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.81871/4.80042. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.81529/4.80481. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.82178/4.81898. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.81663/4.81034. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.81511/4.82025. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.81845/4.81396. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.82140/4.83929. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.81975/4.82807. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.82178/4.81813. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.82027/4.81444. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.81966/4.80571. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.81385/4.80336. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.81424/4.80336. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.81581/4.80198. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.81551/4.80271. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.81342/4.79644. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.81174/4.80536. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.81317/4.79616. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.81393/4.79623. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.81338/4.78619. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.81205/4.82238. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.81975/4.83668. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.82322/4.83128. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.81601/4.83917. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.82005/4.81230. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.81431/4.79787. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.81233/4.82029. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.81299/4.81044. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.81673/4.82320. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.81156/4.82738. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.81315/4.80955. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.81169/4.80718. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.81131/4.80966. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.81436/4.79621. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.81393/4.80922. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.80754/4.80822. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.80943/4.80470. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.80242/4.80228. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.80206/4.79956. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.80662/4.80620. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.80243/4.80593. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.80656/4.79549. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.80194/4.80174. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.81020/4.80981. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.80777/4.80987. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.80983/4.81589. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.80764/4.79384. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.80231/4.78054. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.79915/4.80099. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.80416/4.79938. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.80373/4.79455. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.80363/4.78686. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.80199/4.77768. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.80423/4.78770. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.80163/4.79915. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.80550/4.80522. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.09847634407689815\n",
      "Epoch 0, Loss(train/val) 4.90966/4.88484. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.88205/4.87812. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.87909/4.87218. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 4.87671/4.86874. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.87464/4.86261. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.87295/4.85837. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.87264/4.85535. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.87244/4.85670. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.86993/4.85693. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.86950/4.85814. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.86635/4.85464. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.86656/4.85517. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.86734/4.85517. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.86833/4.85706. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.87040/4.86044. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.86275/4.85353. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.87097/4.86061. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.86284/4.85436. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.86276/4.85514. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.86444/4.85575. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.86422/4.85397. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.86890/4.85218. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.86169/4.85249. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.86567/4.85385. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.86278/4.85320. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.86400/4.85453. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.86680/4.85706. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.86605/4.85149. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.86193/4.85363. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.86178/4.85372. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.85912/4.85781. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.86305/4.86144. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.86311/4.85461. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.86292/4.85419. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.86023/4.85853. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.85998/4.85533. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.86188/4.85795. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.85843/4.85074. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.85987/4.85602. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.86123/4.85200. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.86136/4.85388. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.86094/4.85612. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.85753/4.85705. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.86044/4.85727. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.86037/4.85860. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.85776/4.86154. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.85682/4.86029. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.85712/4.85920. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.85803/4.85571. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.85844/4.86416. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.85386/4.86132. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.85348/4.85737. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.86169/4.86444. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.85904/4.86309. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.85487/4.86557. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.85075/4.85424. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.85940/4.85398. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.85592/4.85844. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.85689/4.85424. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.85807/4.85838. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.85250/4.86401. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.85113/4.86565. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.85503/4.85970. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.85068/4.85758. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.84982/4.86292. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.84842/4.86329. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.85319/4.85281. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.86064/4.85674. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.85165/4.85932. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.84647/4.85850. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.84809/4.85880. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.84991/4.85420. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.85386/4.85240. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.85041/4.83972. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.85513/4.87179. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.87154/4.87077. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.86249/4.86369. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.86052/4.86404. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.85427/4.86422. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.85808/4.87028. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.84994/4.86128. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.85125/4.87338. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.85495/4.86054. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.84914/4.86835. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.85303/4.87093. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.85048/4.85655. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.84952/4.85675. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.85042/4.87529. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.84485/4.86059. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.84542/4.87074. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.84465/4.87187. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.84819/4.86814. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.84762/4.86548. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.84605/4.86645. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.85286/4.85824. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.84831/4.85534. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.84534/4.86671. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.84790/4.85741. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.84249/4.87354. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.83912/4.86079. Took 0.10 sec\n",
      "ACC: 0.515625, MCC: 0.03779786830012622\n",
      "Epoch 0, Loss(train/val) 5.00291/4.97330. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.97512/4.96407. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.96382/4.96755. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.96602/4.97064. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.96633/4.97184. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.96105/4.96734. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.96620/4.96565. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.96303/4.96507. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.96176/4.96475. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.96030/4.96207. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.95983/4.96132. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.96204/4.96463. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.95578/4.96065. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.96063/4.96327. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.95638/4.96294. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.95912/4.96131. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.95405/4.96729. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.95727/4.96548. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.95156/4.96739. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.95594/4.96323. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.95467/4.97099. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.95498/4.96514. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.95446/4.97020. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.95726/4.97538. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.95322/4.97786. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.94868/4.98199. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.95397/4.98303. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.95058/4.97137. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.94940/4.97652. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.94560/4.98105. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.94901/4.98135. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.95032/4.98047. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.94441/4.97598. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.95010/4.98317. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.94964/4.97066. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.94566/4.98887. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.94784/4.97512. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.94719/4.97797. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.94616/4.97666. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.95288/4.96835. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.93960/4.98253. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.94495/4.98357. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.94731/4.97971. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.94317/4.98738. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.94870/4.98467. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.94101/4.97648. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.94359/4.97555. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.94361/4.98626. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.94330/4.98515. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.94383/4.98831. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.94155/4.98422. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.94527/4.98202. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.94397/4.97968. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.94030/4.97718. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.94512/4.98292. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.93906/4.99028. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.94313/4.99112. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.93788/4.99118. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.94256/4.96578. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.94995/4.97617. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.94712/4.97804. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.94048/4.97656. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.93406/5.00616. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.94027/4.99392. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.94456/4.99155. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.93907/5.00099. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.94091/4.98446. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.94230/4.98006. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.94234/4.97001. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.93464/4.97069. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.94177/4.98131. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.93515/4.97817. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.94028/4.97909. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.93449/4.99310. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.93162/4.98035. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.93996/4.98643. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.93485/5.04555. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.93849/4.98126. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.93977/4.98823. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.94027/4.99692. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.93662/4.99281. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.93719/4.97794. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.93766/4.98726. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.94101/4.99481. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.93999/4.97954. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.92795/4.99101. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.93773/4.97869. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.94014/4.97687. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.93827/4.97561. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.93336/4.98155. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.93494/4.98438. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.93652/4.97915. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.93528/4.98400. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.93391/4.97514. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.92975/4.97676. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.93642/4.97745. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.92882/4.98923. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.93783/4.99341. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.92796/4.99060. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.92943/4.98034. Took 0.08 sec\n",
      "ACC: 0.59375, MCC: 0.21411361031120882\n",
      "Epoch 0, Loss(train/val) 4.70212/4.59394. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.62062/4.59418. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.61287/4.59118. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.60762/4.59100. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.60572/4.58739. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.60913/4.58775. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.60998/4.58849. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.60735/4.58828. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.60370/4.58627. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.60234/4.58479. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.60342/4.58228. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.60098/4.58629. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.60115/4.58271. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.60325/4.58032. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.60370/4.58143. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.60180/4.59050. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.59498/4.58904. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.59566/4.58762. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.59510/4.59742. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.59318/4.58199. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.59670/4.58783. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.59596/4.59058. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.59339/4.59309. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.59281/4.58964. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.58701/4.59664. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.58836/4.58800. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.58884/4.60216. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.58770/4.59356. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.59424/4.59576. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.59229/4.59880. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.59815/4.57331. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.61269/4.58827. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.60495/4.58993. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.59890/4.58210. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.59504/4.58572. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.59377/4.59201. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.59180/4.59238. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.58838/4.59834. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.60394/4.58084. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.60190/4.58409. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.60068/4.58233. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.59306/4.57493. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.59147/4.59079. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.58972/4.58902. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.59305/4.57859. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.59276/4.58054. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.58819/4.58124. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.59283/4.59297. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.58828/4.59512. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.58479/4.59499. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.58615/4.59690. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.58261/4.58938. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.58293/4.58815. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.58131/4.60626. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.58277/4.60314. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.58232/4.59928. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.58392/4.60064. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.58492/4.56374. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.58587/4.56167. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.58932/4.58878. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.58740/4.59745. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.58490/4.59418. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.58330/4.61496. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.58249/4.61379. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.57748/4.61898. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.58252/4.60712. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.58017/4.61172. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.57812/4.61649. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.57748/4.59900. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.58015/4.61608. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.57735/4.60329. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.57612/4.60187. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.57502/4.61815. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.57839/4.61572. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.57271/4.62412. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.57533/4.63907. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.57611/4.61347. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.57408/4.61894. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.57074/4.63027. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.57083/4.62752. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.57231/4.61844. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.57733/4.62098. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.57236/4.62801. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.57096/4.62359. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.57064/4.62764. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.57113/4.61737. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.57109/4.64050. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.56632/4.60661. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.57276/4.61259. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.57412/4.64084. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.57460/4.62850. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.57137/4.61330. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.57289/4.63247. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.57362/4.62483. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.57403/4.63890. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.56784/4.61485. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.56603/4.64674. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.57244/4.62009. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.56571/4.63334. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.56993/4.61987. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.09416472308615167\n",
      "Epoch 0, Loss(train/val) 5.16580/5.12632. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.07783/5.09810. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 5.07542/5.09534. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.07732/5.09497. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.07488/5.09178. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.07383/5.08748. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.07165/5.08839. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.07306/5.09112. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.07297/5.08752. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.07019/5.08371. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.06833/5.08270. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.06654/5.08332. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 5.06788/5.08829. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.06745/5.08433. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.06654/5.07633. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 5.06743/5.07322. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.06390/5.07012. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.06254/5.07003. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 5.06277/5.06821. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.06282/5.06901. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 5.06172/5.07408. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.06336/5.06821. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.05984/5.07162. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 5.05985/5.06274. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.05891/5.06688. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.05388/5.07749. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.05760/5.07615. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.05995/5.07147. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 5.05585/5.06782. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.05490/5.06732. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.05487/5.07373. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.05417/5.06852. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.05267/5.07856. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.04686/5.08399. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 5.05487/5.06996. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.04526/5.09958. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.05210/5.08640. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 5.05408/5.08844. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.05229/5.07732. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.05033/5.07317. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.04421/5.07370. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.05108/5.07498. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 5.05115/5.08316. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.04678/5.08219. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.05091/5.08300. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.04612/5.08302. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.05040/5.08566. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.04375/5.09299. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.04773/5.08328. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.05020/5.06819. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.05325/5.07438. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.04619/5.07880. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 5.05224/5.09528. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.05265/5.09833. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 5.05008/5.09180. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.04786/5.09145. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.04202/5.10292. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.05085/5.09771. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 5.05488/5.10676. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 5.04814/5.09840. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.04372/5.09973. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.04425/5.10602. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.04036/5.12074. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.04324/5.10418. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 5.04301/5.08973. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.03936/5.10318. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.04348/5.09170. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.04393/5.09656. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.04143/5.09780. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.04417/5.10251. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.04878/5.09398. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.04046/5.09852. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.04528/5.09845. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.04042/5.10669. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.04246/5.10172. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 5.03983/5.12158. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.03963/5.10717. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.04341/5.10528. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.03265/5.10954. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.03980/5.09263. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 5.03596/5.10233. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.03578/5.12279. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.03360/5.10595. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.03839/5.10917. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.03122/5.13953. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.04365/5.10393. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 5.03421/5.13163. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.04237/5.11143. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.02794/5.15238. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.04269/5.11022. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.03355/5.11661. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 5.03255/5.12635. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 5.03145/5.13333. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.03561/5.10174. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.03175/5.10371. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.03117/5.09720. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 5.03475/5.11035. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.03282/5.12519. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.02802/5.12926. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.03426/5.11569. Took 0.08 sec\n",
      "ACC: 0.46875, MCC: -0.07845331985520247\n",
      "Epoch 0, Loss(train/val) 4.78949/4.85362. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.77544/4.80277. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.77318/4.76429. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.77089/4.75978. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.76669/4.76357. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.76012/4.76668. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.76187/4.76307. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.76066/4.76310. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.76317/4.76447. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.76118/4.76563. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.76143/4.76693. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.76056/4.77640. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.76096/4.77902. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.75906/4.77259. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.75387/4.77493. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.75575/4.77845. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.75469/4.77961. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.75341/4.77993. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.75411/4.76916. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.75330/4.77863. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.75075/4.77952. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.75213/4.77951. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.74690/4.79061. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.74981/4.78114. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.74954/4.79771. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.74262/4.79968. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.74772/4.78540. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.74930/4.78409. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.74385/4.80989. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.74280/4.79182. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.74342/4.79833. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.74380/4.80231. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.74183/4.82213. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.74297/4.78860. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.74005/4.79637. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.74002/4.80180. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.74235/4.81658. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.73679/4.81810. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.73820/4.82260. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.73792/4.83692. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.73842/4.81798. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.74111/4.80040. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.73165/4.82471. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.74051/4.79036. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.74143/4.78371. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.74244/4.78323. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.74318/4.79257. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.73401/4.81856. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.73829/4.81978. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.73737/4.81259. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.73936/4.81549. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.73428/4.82801. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.73555/4.84064. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.74069/4.80471. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.73261/4.83066. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.73674/4.83228. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.73418/4.83846. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.73309/4.82808. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.73763/4.82796. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.73214/4.84105. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.73378/4.85065. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.73600/4.83553. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.73223/4.84105. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.73581/4.83427. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.73322/4.84190. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.73110/4.82397. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.73244/4.85274. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.73519/4.82929. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.73018/4.83720. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.73275/4.82848. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.73799/4.83676. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.72630/4.83979. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.73395/4.83270. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.73318/4.83156. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.73131/4.81949. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.73183/4.83938. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.73057/4.82116. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.72816/4.84001. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.72859/4.81688. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.73529/4.81361. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.73275/4.83288. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.73167/4.81910. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.73321/4.84176. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.72712/4.83924. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.72944/4.84607. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.73204/4.83152. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.72991/4.84646. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.72970/4.81708. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.73246/4.83708. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.73393/4.82226. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.72523/4.83515. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.72629/4.85457. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.73608/4.82203. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.72657/4.83807. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.72973/4.83521. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.72771/4.84175. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.73233/4.83347. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.73421/4.83198. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.73050/4.84419. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.72915/4.85205. Took 0.09 sec\n",
      "ACC: 0.484375, MCC: -0.06240659293657412\n",
      "Epoch 0, Loss(train/val) 5.06667/4.94630. Took 0.17 sec\n",
      "Epoch 1, Loss(train/val) 4.96111/4.96931. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 4.95094/4.94984. Took 0.11 sec\n",
      "Epoch 3, Loss(train/val) 4.94647/4.94556. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.94484/4.94674. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.94486/4.94863. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.94827/4.94769. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.94477/4.95264. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.94521/4.95236. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.94551/4.95434. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.94863/4.94757. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.94615/4.94633. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.94193/4.94508. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.94336/4.94485. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.94431/4.94235. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.94329/4.94393. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.94168/4.94487. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.94119/4.94694. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.94148/4.94396. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.94096/4.94446. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.93924/4.94725. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.94111/4.94619. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.94126/4.94753. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.93724/4.94769. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.93721/4.94659. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.93660/4.94731. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.93742/4.95478. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.93680/4.94842. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.93485/4.94873. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.93295/4.95430. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.93125/4.95555. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.93131/4.95896. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.93005/4.96484. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.93159/4.96551. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.93242/4.96547. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.93117/4.96263. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.92992/4.96394. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.92668/4.95753. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.92924/4.96208. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.94022/4.95663. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.93332/4.95319. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.92974/4.95852. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.92907/4.96822. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.92894/4.95423. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.93724/4.94811. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.92762/4.95298. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.92690/4.95059. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.92677/4.96136. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.92599/4.95732. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.92385/4.96608. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.92066/4.96470. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.91947/4.97009. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.91767/4.96566. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.92630/4.96398. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.91952/4.96781. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.92004/4.95216. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.91736/4.99110. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.91886/4.97404. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.92028/4.97609. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.91894/4.97652. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.91503/4.99956. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.91404/4.99938. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.91951/4.98249. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.91565/4.98138. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.91042/4.98808. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.92004/4.97556. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.91442/4.99374. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.92049/4.97173. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.92855/4.95941. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 4.93870/4.95984. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.91879/4.98794. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.91054/5.00255. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.91167/5.00873. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.91546/4.98728. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.90984/4.99389. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.90996/4.98376. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.90395/5.01753. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.91520/4.98570. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.91058/4.98964. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.92213/4.97571. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.93671/4.95455. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.93700/4.95127. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.93056/4.95506. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.93025/4.96558. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.93180/4.94745. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.92882/4.95686. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.92918/4.96658. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.93435/4.95466. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.93558/4.95306. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.92756/4.96124. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.92785/4.97422. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.92204/4.97132. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.93024/4.97178. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.92802/4.96851. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.92342/4.97091. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.92527/4.96820. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.91908/4.96364. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.92416/4.97449. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.92636/4.95981. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.93661/4.95080. Took 0.08 sec\n",
      "ACC: 0.578125, MCC: 0.1463182264089704\n",
      "Epoch 0, Loss(train/val) 4.85634/4.76933. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.78549/4.77561. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.78144/4.78027. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.77757/4.77451. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.77407/4.77085. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.77192/4.76926. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.77678/4.77024. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.77319/4.76966. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.77201/4.77170. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.77055/4.77271. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.77127/4.77664. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.77177/4.77663. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.76884/4.77633. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.76669/4.77987. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.76612/4.78307. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.76678/4.78495. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.76649/4.78807. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.76392/4.79439. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.76574/4.79509. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.76363/4.79855. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.76446/4.79850. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.76076/4.79669. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.76076/4.80436. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.75704/4.79870. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.76342/4.81047. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.76306/4.80504. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.75826/4.80869. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.75894/4.80489. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.75554/4.80920. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.75605/4.81369. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.75815/4.81684. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.75627/4.81722. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.75618/4.81199. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.75636/4.80545. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.75831/4.80719. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.75736/4.81741. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.75929/4.81817. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 4.75405/4.81929. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.75448/4.81123. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.74846/4.82305. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.74906/4.82366. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.75348/4.81734. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.75161/4.81703. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.75089/4.84010. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.75120/4.83272. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.74835/4.82479. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.74960/4.82388. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.74789/4.82997. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.75051/4.81931. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.74732/4.83552. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.75747/4.82291. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.75155/4.82033. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.75159/4.82616. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.75322/4.81739. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.74489/4.84437. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.75268/4.82370. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.74671/4.82208. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.75098/4.81915. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.74720/4.84607. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.74111/4.84276. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.74879/4.82475. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.74917/4.83834. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.74522/4.84883. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.74543/4.83991. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.74289/4.83597. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.74377/4.85747. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.74733/4.84878. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.74624/4.84094. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.74306/4.85067. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.73885/4.84995. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.74127/4.84717. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.74106/4.86866. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.74746/4.85251. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.74112/4.85114. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.74004/4.87296. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.74678/4.84848. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.74394/4.86156. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.74775/4.85733. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.74017/4.85898. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.74033/4.88707. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.74188/4.86052. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.74561/4.85592. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.74122/4.86626. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.73965/4.88774. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.74694/4.85606. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.73779/4.87399. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.74116/4.89106. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.74054/4.87497. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.73954/4.87306. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.74010/4.88555. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.73797/4.89752. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.73960/4.86544. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.73809/4.87404. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.74113/4.87124. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.74280/4.87125. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.73937/4.87387. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.73912/4.89858. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.73211/4.87675. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.73448/4.89024. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.73936/4.89019. Took 0.08 sec\n",
      "ACC: 0.453125, MCC: -0.09202163616785992\n",
      "Epoch 0, Loss(train/val) 4.93048/4.91506. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.89584/4.91366. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.89092/4.93294. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.88846/4.93882. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.89040/4.93941. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.89082/4.94802. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.88950/4.95292. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.88807/4.96072. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.88764/4.95702. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.88868/4.95032. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.88417/4.94697. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.88187/4.95234. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.88094/4.95212. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.88328/4.94582. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.88171/4.95478. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.88018/4.94473. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.87738/4.95791. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.88209/4.94967. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.87729/4.95061. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.87512/4.95920. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.88025/4.93850. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.87797/4.95170. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.87787/4.95653. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.87663/4.93403. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.87723/4.97726. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.88148/4.93567. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.88226/4.93107. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.87735/4.93428. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.88033/4.91661. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.88571/4.91721. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.88388/4.92588. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.88220/4.92477. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.88144/4.93729. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.87802/4.94883. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.88101/4.92655. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.87784/4.96518. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.88467/4.93200. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.88150/4.94833. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.87865/4.93948. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.87954/4.95483. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.87771/4.94217. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.87938/4.94956. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.88289/4.94131. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.87875/4.95172. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.88062/4.94581. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.87855/4.93345. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.87614/4.94096. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.87763/4.94757. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.87839/4.96915. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.87970/4.93135. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 4.87556/4.93798. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.87519/4.97651. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.87420/4.92085. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.87486/4.94788. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.87450/4.95589. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.87507/4.95747. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.87519/4.96108. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.87714/4.94169. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.86966/4.98579. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.87857/4.92957. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.87368/4.96906. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.87101/4.95733. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.86989/4.97806. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.87103/4.95284. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.87262/4.98632. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.88475/4.89976. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.89033/4.90807. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.88906/4.89720. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.88683/4.90088. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.88584/4.90250. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.88923/4.89996. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.88557/4.90261. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88275/4.90785. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.88322/4.91274. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.88139/4.91482. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.88433/4.92227. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88441/4.92259. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.88060/4.92794. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.87720/4.93547. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.88036/4.93151. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.88027/4.93658. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.87983/4.94990. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.87325/4.97859. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.87747/4.94540. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.87961/4.94658. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.87852/4.95830. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.88036/4.95750. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.87544/4.95475. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.86961/4.97441. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.87847/4.94341. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.87676/4.94783. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.87194/4.96990. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.87303/4.96759. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.87477/4.98084. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.87254/4.97348. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.87282/4.97031. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.86797/5.01310. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.87138/5.01583. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.87209/4.93385. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.86883/5.01297. Took 0.08 sec\n",
      "ACC: 0.390625, MCC: -0.21971768720102058\n",
      "Epoch 0, Loss(train/val) 4.72072/4.66898. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.67037/4.67751. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.66645/4.66881. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.66772/4.67556. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.66309/4.68709. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.66389/4.69275. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.66627/4.68770. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.66826/4.68769. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.66652/4.66917. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.66754/4.67388. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.66468/4.67242. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.66605/4.65943. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.66590/4.66096. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.66355/4.65922. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.66270/4.65297. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.66403/4.64936. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.66341/4.64985. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.66374/4.64755. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.66406/4.65187. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.66189/4.64946. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.66323/4.65554. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.66385/4.66376. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.66331/4.66629. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.65794/4.67100. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.66040/4.67086. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.65931/4.67849. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.66109/4.67531. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.65731/4.69406. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.65639/4.69211. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.66078/4.68479. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.65625/4.67866. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.65697/4.68596. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.65478/4.68408. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.65554/4.68726. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.65372/4.68677. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.65562/4.67734. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.65322/4.68905. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.65276/4.69702. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.65361/4.68287. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.65757/4.68976. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.65424/4.69015. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.65325/4.69628. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.65553/4.69420. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.65335/4.69642. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.65616/4.69801. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.65466/4.69692. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.65248/4.69729. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.65578/4.68259. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.65258/4.68986. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.65616/4.67868. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.65324/4.69716. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.65215/4.69886. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.65150/4.69576. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.65121/4.70636. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.65047/4.70223. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.64454/4.72208. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.65023/4.70088. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.64791/4.69816. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.65160/4.71011. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.64698/4.70971. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.64963/4.69804. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.64517/4.70641. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.64853/4.69561. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.65168/4.71476. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.64517/4.72542. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.64855/4.69226. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.64798/4.72064. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.65376/4.69674. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.64642/4.71593. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.64577/4.71402. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.64509/4.70962. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.64514/4.70899. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.64944/4.69964. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.64330/4.75049. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.63894/4.72519. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.64337/4.72217. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.64281/4.72849. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.64549/4.72231. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.64240/4.71518. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.64497/4.70606. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.63833/4.71951. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.64537/4.69876. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.64045/4.72229. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.64498/4.71250. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.64006/4.71310. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.64174/4.74094. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.64549/4.69546. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.64061/4.71703. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.63998/4.71914. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.64187/4.70969. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.64256/4.71926. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.64077/4.73777. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.64663/4.71156. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.64154/4.73617. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.63804/4.72239. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.63765/4.71499. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.64153/4.71326. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.63877/4.72632. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.64101/4.72103. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.64052/4.73007. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.049929648757959806\n",
      "Epoch 0, Loss(train/val) 4.97756/4.93715. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.93521/4.94330. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.92775/4.94495. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.93083/4.94040. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.92719/4.93974. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.92974/4.94095. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.93142/4.93984. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.92637/4.93737. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.92638/4.93519. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.93057/4.93518. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.92826/4.93360. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.92354/4.93225. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.92426/4.95124. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.92351/4.93998. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.92389/4.94420. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.92165/4.94239. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.92148/4.94087. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.92153/4.93898. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.92302/4.93846. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.92217/4.94068. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.92041/4.93783. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.92024/4.93752. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.91966/4.93502. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.92549/4.93777. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.92477/4.93027. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.92037/4.93285. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.92528/4.92384. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.92188/4.92528. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.91840/4.92788. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.91745/4.92495. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.91985/4.93346. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.91729/4.93121. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.91583/4.92665. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.91494/4.93730. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.91633/4.93293. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.91407/4.94770. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.91490/4.94810. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.91680/4.94321. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.91392/4.94124. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.92062/4.93789. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.91309/4.93936. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.90816/4.94469. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.91410/4.96086. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.92015/4.95106. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.91831/4.93243. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.91735/4.94181. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.91727/4.94411. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.92211/4.94421. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.91746/4.94728. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.91703/4.93334. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.91463/4.95239. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.91357/4.93209. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.91361/4.94409. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.91305/4.93686. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.90801/4.93811. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.91010/4.93355. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.90709/4.93293. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.90641/4.93730. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.91030/4.94199. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.91103/4.93542. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.90500/4.94697. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.90578/4.93844. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.90401/4.93873. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.90706/4.94597. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.90468/4.94542. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.90949/4.94276. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.90347/4.93755. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.90411/4.93577. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.90297/4.94675. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.90654/4.94844. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.90630/4.93976. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.90322/4.93702. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.90008/4.94250. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.90275/4.94041. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.89823/4.95306. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.89510/4.94553. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.90831/4.95337. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.90006/4.94002. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.90331/4.93318. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.89752/4.94139. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.90563/4.92999. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.91030/4.92080. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.90213/4.92957. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.90239/4.93360. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.89998/4.93986. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.89997/4.91942. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.89596/4.93555. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.89953/4.93473. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.89853/4.93469. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.90706/4.94837. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.91740/4.92887. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.90561/4.93396. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.89842/4.92165. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.90217/4.91666. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.90222/4.91627. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.90363/4.92213. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.90080/4.91042. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.89615/4.90235. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.91024/4.93051. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.91123/4.93367. Took 0.08 sec\n",
      "ACC: 0.515625, MCC: 0.02757337634153148\n",
      "Epoch 0, Loss(train/val) 5.11386/5.08674. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.07452/5.07565. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 5.07199/5.08367. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.07990/5.07448. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 5.08597/5.06198. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.07769/5.06827. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.06890/5.06377. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.06353/5.06165. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.06462/5.06353. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.07317/5.06010. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 5.06987/5.05834. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.06366/5.05476. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 5.06139/5.05361. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.06488/5.05638. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 5.06519/5.05688. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 5.06406/5.05614. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.06533/5.05939. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.06194/5.06333. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.06095/5.06203. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.06140/5.06199. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 5.06206/5.05832. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.05879/5.05948. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.06081/5.05798. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 5.06013/5.05948. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.06032/5.05638. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 5.06423/5.05163. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.06249/5.06417. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.05838/5.06266. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 5.05993/5.06081. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 5.05809/5.06283. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.05654/5.06293. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.05863/5.05819. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 5.06048/5.05939. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.05592/5.05875. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.05746/5.06082. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 5.05752/5.05806. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.05446/5.05802. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 5.05631/5.05921. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.06164/5.06060. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.05537/5.06931. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.05413/5.06596. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.05413/5.06504. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.05595/5.07379. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 5.05457/5.07342. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.05418/5.07766. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 5.05267/5.07822. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 5.04949/5.07084. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.05477/5.06688. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.05570/5.07809. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 5.05061/5.09252. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.05577/5.08423. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.04986/5.07888. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.06060/5.09023. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 5.05753/5.07052. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.04977/5.08606. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.05605/5.06103. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.05212/5.07413. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.05062/5.07671. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.05518/5.08005. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 5.06191/5.07230. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.05645/5.06952. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.05820/5.06996. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.05557/5.07226. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 5.05604/5.06445. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 5.05605/5.07021. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 5.05336/5.07693. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 5.05789/5.07963. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.05264/5.08109. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.05744/5.06552. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 5.05387/5.07974. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.05406/5.08335. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.05196/5.08785. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 5.05504/5.08992. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.05274/5.07975. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.05650/5.08822. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 5.04731/5.09448. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.05364/5.09000. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.05962/5.07281. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.05933/5.07937. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.05224/5.07358. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 5.05113/5.07094. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.05317/5.08059. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.05348/5.08847. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.05100/5.07322. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 5.04929/5.08000. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.04829/5.07844. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.05109/5.08112. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 5.04945/5.08695. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.04461/5.08979. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.04682/5.09269. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.04653/5.09351. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 5.05837/5.08430. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.05905/5.08924. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.05045/5.08363. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 5.04224/5.09728. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.04905/5.09678. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.04771/5.08418. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 5.04366/5.09848. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.04829/5.08895. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.04765/5.08558. Took 0.08 sec\n",
      "ACC: 0.40625, MCC: -0.19088542889273333\n",
      "Epoch 0, Loss(train/val) 4.72068/4.69579. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.69545/4.70200. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.69397/4.71143. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.69417/4.70013. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.69157/4.68558. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.69566/4.67062. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.69297/4.67335. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.68999/4.68479. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.68869/4.68412. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.68610/4.67742. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.68800/4.67962. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.68551/4.67465. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.68287/4.67043. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.67971/4.65543. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.68524/4.66910. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.68194/4.66569. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.68313/4.66531. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.67761/4.66982. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.68014/4.66855. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.67718/4.67346. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.67643/4.67376. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.67618/4.67604. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.67640/4.67422. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.67492/4.66678. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.67903/4.67360. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.67928/4.67885. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.67447/4.67868. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.67645/4.68296. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.67393/4.68365. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.67905/4.67104. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.67762/4.66924. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.68037/4.67845. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.67737/4.67615. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.67882/4.68069. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.67600/4.68480. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.67807/4.68440. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.67976/4.67724. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.67777/4.67863. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.67258/4.68245. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.67748/4.68290. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.67449/4.68372. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.67379/4.68694. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.67446/4.68175. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.67142/4.67613. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.67633/4.67609. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.67326/4.67302. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.67256/4.68159. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.67342/4.69084. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.67096/4.68164. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.67243/4.67821. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.67562/4.68513. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.66960/4.67692. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.66810/4.68738. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.66665/4.67312. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.66414/4.69269. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.66513/4.68415. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.66820/4.67859. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.67006/4.68174. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.66451/4.68841. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.66666/4.67756. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.66407/4.67320. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.66385/4.67006. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.66443/4.67855. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.65753/4.68204. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.66561/4.68972. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.66272/4.69109. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.65525/4.67937. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.65836/4.67785. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.65910/4.67097. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.65830/4.67127. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.67727/4.68023. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.68099/4.69114. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.67954/4.67636. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.67384/4.69120. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.67316/4.67955. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.67177/4.67201. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.67595/4.67529. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.67228/4.68166. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.67073/4.66645. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.66534/4.66157. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.66740/4.66716. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.66492/4.67927. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.66294/4.67461. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.66414/4.67691. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.66620/4.68905. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.66208/4.65703. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.66424/4.68304. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.66143/4.67054. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.66527/4.67280. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.66543/4.68264. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.65980/4.67908. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.66146/4.69769. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.65905/4.68240. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.66098/4.68143. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.66025/4.70139. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.67522/4.68132. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.66551/4.68450. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.66369/4.66689. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.65378/4.69504. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.66387/4.69014. Took 0.09 sec\n",
      "ACC: 0.46875, MCC: -0.1003113655692617\n",
      "Epoch 0, Loss(train/val) 4.99656/4.84479. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.87341/4.83513. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.86128/4.85070. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.85712/4.85816. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.85621/4.86087. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.85947/4.86256. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.85760/4.86808. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.85506/4.86833. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.85674/4.86570. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.85573/4.87165. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.85295/4.87323. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.85393/4.87267. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.85659/4.87334. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.85463/4.87756. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.85492/4.86185. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.85420/4.87333. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.85242/4.88449. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.85299/4.88686. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.85087/4.88693. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.85004/4.89296. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.85033/4.89010. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.85235/4.87666. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.85004/4.87629. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.85079/4.88658. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.85187/4.88714. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.84827/4.89216. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.84915/4.89292. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.84685/4.89437. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.84794/4.89223. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.84555/4.90447. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.84829/4.88936. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.84551/4.89478. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.84433/4.90038. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.84564/4.90105. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.84412/4.90136. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.84307/4.90696. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.84109/4.91274. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.84309/4.90301. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.84411/4.91844. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.84103/4.89955. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.84663/4.88099. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.84319/4.89110. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.85403/4.86809. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.84025/4.92258. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.83874/4.93459. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.83834/4.92834. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.83790/4.93888. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.84151/4.91236. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 4.83775/4.93853. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.84215/4.91043. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.83881/4.94103. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.83315/4.94186. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.83347/4.93145. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.83443/4.92649. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.83258/4.92917. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.83474/4.93878. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.83147/4.93965. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.82987/4.95584. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.83510/4.91373. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.83560/4.91671. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.83594/4.93166. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.83381/4.91827. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.83586/4.93152. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.83659/4.90790. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.82830/4.94395. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.83470/4.91348. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.82733/4.92724. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.82657/4.92771. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.84701/4.85369. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.84425/4.88078. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.83879/4.89123. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.83158/4.90982. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.82613/4.90725. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.83038/4.91670. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.82951/4.91960. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.82619/4.91072. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.82595/4.92434. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.82922/4.89329. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.82837/4.90996. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.82264/4.92126. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.82063/4.93388. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.82355/4.92136. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.82289/4.91378. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.82067/4.91822. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.82190/4.91125. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.81842/4.93109. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.82061/4.92476. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.81785/4.93561. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.81873/4.93333. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.81994/4.92398. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.82283/4.90679. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.82157/4.92490. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.81662/4.92027. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.81853/4.90911. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.82482/4.93474. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.81097/4.93349. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.81593/4.92143. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.81143/4.94591. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.81929/4.92169. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.81896/4.91907. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.032380592972966034\n",
      "Epoch 0, Loss(train/val) 4.84920/4.87517. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.82541/4.84328. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.81981/4.81227. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.81154/4.80578. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.80619/4.80985. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.80750/4.81166. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.80755/4.81206. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.80726/4.81247. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.80808/4.81039. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.80779/4.80618. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.80478/4.81001. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.80669/4.80684. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.80394/4.80838. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.80523/4.80729. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.80318/4.80989. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.80318/4.80386. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.80271/4.80198. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.80095/4.79928. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.79937/4.80288. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.80348/4.80562. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.79799/4.80603. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.79871/4.81137. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.79676/4.81608. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.79754/4.81591. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.79825/4.81222. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.79462/4.81768. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.79542/4.81858. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.79509/4.82403. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.79382/4.82345. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.79297/4.81773. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.79288/4.82002. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.79023/4.82066. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.78709/4.82987. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.79068/4.83230. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.79569/4.82154. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.79077/4.82218. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.79109/4.82890. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.78898/4.82448. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.79040/4.82113. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.79203/4.81963. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.78843/4.81454. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.80307/4.80096. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.79401/4.79747. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.79135/4.80804. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.79451/4.81009. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.79063/4.80703. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.79356/4.80911. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.78990/4.81052. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.78562/4.82550. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.78745/4.82461. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.78805/4.82458. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.78648/4.79482. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.79410/4.80279. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.79054/4.82057. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.78611/4.83852. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.79370/4.81545. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.79062/4.81860. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.78433/4.82399. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.78468/4.82576. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.78806/4.82563. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.78862/4.81761. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.78051/4.82958. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.79104/4.77808. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.78978/4.79824. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.77864/4.82751. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.79626/4.82321. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.79561/4.80232. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.79443/4.80772. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.79117/4.81415. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.79416/4.81132. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.79092/4.81493. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.78675/4.82834. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.79124/4.81964. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.78850/4.82321. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.79040/4.81894. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.78930/4.82067. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.78739/4.82049. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.78831/4.81380. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.78653/4.81851. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.79300/4.82157. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.79173/4.82330. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.78773/4.84884. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.79109/4.83014. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.78624/4.83152. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.78329/4.83339. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.78652/4.82116. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.78333/4.82595. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.78533/4.80309. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.79135/4.80699. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.78433/4.81085. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.78284/4.82689. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.78318/4.82174. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.78088/4.83805. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.78170/4.82216. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.78230/4.82866. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.77947/4.83801. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.78000/4.83396. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.77909/4.82775. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77829/4.81977. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.78262/4.83986. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.12267022872162016\n",
      "Epoch 0, Loss(train/val) 4.97579/4.95036. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.94605/4.93281. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.94643/4.93099. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.94940/4.92622. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.94779/4.93005. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.93927/4.93097. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.93741/4.93579. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.94147/4.93003. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.93845/4.93039. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.93626/4.92603. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.93704/4.92717. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.94212/4.94549. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.94169/4.94422. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.94094/4.94894. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.93772/4.94957. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.93997/4.94295. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.93776/4.94690. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.94079/4.94707. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.93905/4.94948. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.93786/4.94817. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.93833/4.95059. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.93688/4.94748. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.93961/4.94751. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.93704/4.95377. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.93496/4.95099. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.93380/4.94820. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.93295/4.95578. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.93440/4.94888. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.93210/4.95129. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.93314/4.95296. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.93103/4.96516. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.93135/4.96354. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.92926/4.96380. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.92852/4.96265. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.93329/4.95934. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.93595/4.95678. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.92859/4.94991. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.93159/4.95140. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.93155/4.96385. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.92701/4.96270. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 4.92712/4.95188. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.92899/4.94872. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.92545/4.95569. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.92463/4.95476. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.92577/4.94957. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.92447/4.94260. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 4.92461/4.95261. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.91862/4.96331. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.92462/4.94631. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.92244/4.96257. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.92079/4.95854. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.92329/4.95926. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.92043/4.94105. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.92153/4.95883. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.92449/4.95690. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.92224/4.96442. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.91696/4.98144. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.91814/4.96408. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.92143/4.96437. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.91668/4.98206. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.91606/4.97008. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.91809/4.96684. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.91656/4.95605. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.92007/4.96996. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.91978/4.95356. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.91491/4.97108. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.91490/4.96338. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.91597/4.96378. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.91726/4.96699. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.92348/4.96245. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.91885/4.96872. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.92029/4.97777. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.91800/4.96631. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.91840/4.97549. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.91640/4.96251. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.91764/4.97624. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.91864/4.95531. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.92002/4.96390. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.91077/4.97372. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.91334/4.97472. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.91421/4.97264. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.91466/4.97368. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.90779/4.98136. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.91482/4.96948. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.91265/4.95869. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.90934/4.97636. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.91470/4.95882. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.91299/4.96991. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.91148/4.97441. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.91041/4.98047. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.90984/4.96154. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.91530/4.96578. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.90888/4.98164. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.91185/4.95683. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.91177/4.96867. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.90946/4.97182. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.91259/4.95895. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.90956/4.96364. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.91129/4.96553. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.90852/4.96844. Took 0.08 sec\n",
      "ACC: 0.421875, MCC: -0.1308525335766877\n",
      "Epoch 0, Loss(train/val) 4.73411/4.71283. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.70946/4.71095. Took 0.17 sec\n",
      "Epoch 2, Loss(train/val) 4.70412/4.71538. Took 0.14 sec\n",
      "Epoch 3, Loss(train/val) 4.70814/4.71259. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.70685/4.71153. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.69881/4.71503. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.69707/4.71860. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.70019/4.71826. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.69860/4.72176. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.69527/4.72563. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.69689/4.72629. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.69520/4.73389. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.69802/4.73390. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.69785/4.72832. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.69352/4.74363. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.69681/4.73741. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.69515/4.74683. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.69204/4.73631. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.69356/4.74442. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.68886/4.74775. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.69246/4.74399. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.69349/4.74616. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.69076/4.73806. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.68928/4.75470. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.69178/4.75448. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.69178/4.74806. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.69139/4.75428. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.69180/4.74728. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.68884/4.75005. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.68650/4.76405. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.69360/4.74874. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.68772/4.75359. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.69234/4.74509. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.68540/4.76345. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.68982/4.74511. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.68502/4.75404. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.68805/4.75977. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 4.69241/4.74206. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.68270/4.74882. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.68592/4.75039. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.69006/4.73676. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.68162/4.76413. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.68582/4.74603. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.68569/4.76121. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.68643/4.73778. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.68179/4.80902. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.68982/4.73270. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.68709/4.74261. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.68514/4.73681. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.68311/4.72736. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.68012/4.74079. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.68693/4.72159. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 4.68035/4.77566. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.68244/4.75750. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.67989/4.74609. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.68395/4.73670. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.67745/4.74036. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.69141/4.72176. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.68906/4.72121. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.68387/4.73560. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.67628/4.74592. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.67663/4.74491. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.68543/4.74957. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.68107/4.76121. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.68229/4.76059. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.67957/4.77306. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.68105/4.75347. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.67966/4.81155. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.68321/4.73815. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.67697/4.75754. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.68008/4.77235. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 4.67773/4.75423. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.67351/4.75857. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.67559/4.78335. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.67347/4.75247. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.68092/4.76207. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.67641/4.76682. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.66796/4.81046. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.67074/4.77728. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.67429/4.73618. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.67431/4.79266. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.66821/4.76005. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.67313/4.77354. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.67281/4.76216. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.66925/4.78926. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.67266/4.77393. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.67057/4.77387. Took 0.13 sec\n",
      "Epoch 87, Loss(train/val) 4.66989/4.74432. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.67351/4.76043. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.67432/4.78293. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.67460/4.74444. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.66863/4.77955. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 4.66611/4.76801. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.67063/4.75054. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.66822/4.78710. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 4.66657/4.76478. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.66585/4.76666. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.66521/4.79033. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.66451/4.82844. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.66499/4.74234. Took 0.08 sec\n",
      "ACC: 0.546875, MCC: 0.09847634407689815\n",
      "Epoch 0, Loss(train/val) 5.16793/5.06805. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 5.07124/5.07541. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.07336/5.08879. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.08056/5.09374. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 5.08270/5.08635. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 5.07845/5.07900. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 5.06973/5.08325. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 5.06793/5.08959. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.06991/5.09931. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 5.06895/5.09984. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 5.06427/5.10137. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.06387/5.11233. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 5.06302/5.10575. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.06381/5.10827. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 5.05927/5.12984. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 5.06029/5.11754. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.06537/5.14080. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 5.05685/5.12128. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 5.06123/5.14172. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 5.05801/5.12435. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 5.05656/5.11925. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 5.05264/5.15726. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 5.05130/5.14712. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 5.05809/5.11866. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 5.05563/5.13703. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 5.05178/5.14099. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 5.04976/5.16300. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 5.04903/5.15542. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 5.05374/5.13461. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 5.05374/5.15294. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.04927/5.12852. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 5.04871/5.14124. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 5.04789/5.14718. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 5.04678/5.14188. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.04942/5.15430. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 5.04759/5.12817. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 5.04848/5.17762. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 5.04393/5.12947. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 5.06920/5.08198. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.05493/5.13904. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 5.04378/5.15757. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 5.04491/5.13260. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 5.06187/5.06638. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 5.06193/5.07385. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 5.05558/5.09719. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 5.05426/5.12142. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 5.04554/5.17530. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.04856/5.12987. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.04757/5.13909. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 5.04618/5.16045. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 5.04368/5.15875. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.04467/5.16516. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.04984/5.16253. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 5.05019/5.13644. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 5.04713/5.16656. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 5.04123/5.15890. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 5.04611/5.15247. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 5.04298/5.17938. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 5.04078/5.16672. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 5.04083/5.16682. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.04106/5.17387. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.03864/5.17679. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 5.04286/5.15720. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 5.03992/5.17627. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 5.03901/5.18357. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 5.03945/5.18586. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 5.04070/5.16368. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 5.03531/5.19949. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 5.04043/5.15186. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 5.03812/5.20153. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 5.04132/5.16894. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 5.03597/5.19221. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 5.04067/5.15667. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 5.03675/5.17418. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.03869/5.17724. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.03971/5.15812. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.03843/5.18567. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.03502/5.19567. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 5.03616/5.18343. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.03279/5.19056. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 5.03725/5.18746. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 5.03027/5.19479. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 5.04138/5.15476. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 5.03964/5.20009. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 5.03568/5.18089. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 5.03247/5.19909. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 5.03615/5.16481. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 5.03508/5.18678. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 5.03263/5.20421. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 5.03270/5.18060. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 5.02849/5.23657. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 5.03453/5.18612. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 5.03065/5.19566. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 5.02659/5.20666. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 5.03305/5.17266. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 5.03157/5.19696. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 5.02809/5.21852. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 5.03403/5.17508. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 5.02948/5.18402. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 5.03509/5.21498. Took 0.09 sec\n",
      "ACC: 0.4375, MCC: -0.1241446725317693\n",
      "Epoch 0, Loss(train/val) 5.08999/5.06350. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 5.03989/5.05327. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.02967/5.04130. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 5.03098/5.03835. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 5.02997/5.03675. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 5.02919/5.03757. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 5.03024/5.03199. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 5.02762/5.02895. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 5.02722/5.03121. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 5.02703/5.03157. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 5.02857/5.03449. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 5.02287/5.03952. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 5.02722/5.02636. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 5.02454/5.03922. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 5.02600/5.04578. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 5.02636/5.04482. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 5.02037/5.04687. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 5.02522/5.05379. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 5.02168/5.05505. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 5.02057/5.06966. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 5.01829/5.05622. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 5.01646/5.05985. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 5.01404/5.06154. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 5.01856/5.05473. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 5.01401/5.07535. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 5.01445/5.05994. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 5.01396/5.06287. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 5.00827/5.08199. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 5.01176/5.05846. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 5.00583/5.06868. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 5.00761/5.06260. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 5.00605/5.06015. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 5.01430/5.04659. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 5.00926/5.06564. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 5.00789/5.06815. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 5.00456/5.07103. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 5.00831/5.05364. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 5.00159/5.07641. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 5.00900/5.05339. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 5.00328/5.06943. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.99954/5.07685. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 5.00601/5.04859. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 5.00358/5.06354. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.99689/5.08990. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 5.00509/5.06500. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 5.00381/5.07055. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 5.00369/5.07588. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 5.00435/5.08496. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 5.00536/5.06206. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 5.00340/5.06139. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 5.00749/5.04973. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 5.00291/5.07199. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 5.00449/5.06476. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 5.00336/5.08150. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 5.00119/5.08104. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 5.00601/5.05471. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 5.00480/5.06665. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.99607/5.08530. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.99490/5.08293. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 5.00234/5.05558. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 5.00034/5.08843. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 5.00096/5.07304. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.99689/5.07150. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.99190/5.09658. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.99575/5.08084. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.99725/5.08839. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.99168/5.10345. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.99518/5.09304. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.99548/5.07846. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.99237/5.08470. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.99661/5.07640. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.99437/5.11915. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.99926/5.05708. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.99831/5.05823. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 5.00888/5.04469. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 5.00875/5.07341. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 5.00190/5.09032. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 5.00284/5.07027. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 5.00055/5.08109. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 5.00230/5.08319. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.99791/5.07892. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.99539/5.08802. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.99353/5.09353. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.99560/5.08876. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.99901/5.09309. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.99380/5.09732. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.99007/5.09943. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.99338/5.08726. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 4.99576/5.08648. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.99193/5.10958. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.99075/5.10107. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.98930/5.08320. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.99441/5.08295. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.99596/5.10321. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.99284/5.10879. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.99606/5.08660. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.99203/5.10663. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.99486/5.09204. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.99045/5.11184. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.98696/5.13742. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.0629940788348712\n",
      "Epoch 0, Loss(train/val) 4.97753/4.96965. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.93186/4.90293. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.92870/4.90969. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.92508/4.90416. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.91827/4.90303. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.91592/4.90167. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.91549/4.89988. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.91790/4.89696. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.91706/4.89681. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.91327/4.89645. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.91335/4.89574. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.91278/4.89369. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.91342/4.89210. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.91229/4.89373. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.91327/4.89379. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.91071/4.89534. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.90895/4.89427. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.91025/4.89395. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.91152/4.89652. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.90760/4.89561. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.90579/4.89860. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.90812/4.89588. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.91024/4.90035. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.91554/4.90177. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.90836/4.90334. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.90904/4.90918. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.90982/4.90898. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.90835/4.90641. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.90615/4.90314. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.90890/4.90502. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.90762/4.90676. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.90496/4.90712. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.90791/4.90649. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.90865/4.90882. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.90422/4.91307. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.90462/4.91511. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.90593/4.91335. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.90309/4.91847. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.90510/4.91303. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.90044/4.92371. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.90321/4.90954. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.90347/4.91588. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.90255/4.91347. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.89631/4.92307. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.89918/4.91205. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 4.90300/4.91358. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.89985/4.90739. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.89813/4.91418. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.89926/4.91091. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.89882/4.91130. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.89818/4.91658. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 4.89515/4.91607. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.89525/4.91426. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.89011/4.92075. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.89186/4.91488. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.89231/4.92520. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.89285/4.92070. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.89351/4.91404. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.89135/4.91628. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.88948/4.91731. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.89305/4.92092. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.88880/4.91285. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.88661/4.92276. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.89565/4.91528. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.88974/4.92186. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.88933/4.92031. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.88833/4.90361. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.88401/4.92835. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.88664/4.91597. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.88890/4.91751. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.89254/4.90798. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.88706/4.93022. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.88999/4.92045. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.88945/4.91149. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.88617/4.92997. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.88926/4.92385. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.88446/4.94177. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.88805/4.91574. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.89177/4.91851. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.89050/4.92301. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.88747/4.92305. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.88787/4.93143. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.88981/4.91706. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.88548/4.94023. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.89163/4.92935. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.89131/4.93643. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.88436/4.93394. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.88607/4.91498. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.89124/4.92374. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.88680/4.93263. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.88834/4.94027. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.88772/4.93990. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.88326/4.95828. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.88562/4.92488. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.88038/4.93443. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.88002/4.94202. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.88605/4.92757. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.88442/4.93101. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.88254/4.94174. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.88322/4.93196. Took 0.08 sec\n",
      "ACC: 0.484375, MCC: -0.013980301652228746\n",
      "Epoch 0, Loss(train/val) 4.85570/4.78072. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.79504/4.79961. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 4.80126/4.81436. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.79991/4.82234. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.79705/4.82153. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 4.80062/4.81620. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.80149/4.79557. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.80313/4.78380. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.79927/4.78233. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.79357/4.78743. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.79479/4.79216. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.79328/4.79185. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.79335/4.79376. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.79284/4.79708. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.78987/4.80017. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.79378/4.79344. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.79239/4.79264. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 4.79136/4.80100. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.79034/4.80360. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.78973/4.80850. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.78882/4.80831. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.78895/4.80209. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.78876/4.80560. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 4.78627/4.80915. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.78923/4.80199. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.78478/4.81464. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 4.78586/4.80908. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.78281/4.80521. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.78466/4.82615. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.79373/4.79394. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.79145/4.80845. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.78750/4.81581. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.78784/4.82035. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.78737/4.81039. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.78667/4.81948. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.78613/4.82242. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 4.78964/4.81356. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.78668/4.80913. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.78684/4.82555. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.78441/4.82221. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 4.78465/4.81518. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.78442/4.82306. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.78840/4.81292. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.78643/4.82199. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.78665/4.82776. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.78664/4.81551. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.78375/4.83077. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.78304/4.82600. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.78197/4.81502. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.78409/4.82165. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.78534/4.82228. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.78491/4.81535. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.78051/4.82913. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.78141/4.81924. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.78146/4.81990. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.78364/4.82034. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.77773/4.83275. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.78043/4.82664. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.77768/4.83542. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.77692/4.81794. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.77857/4.82197. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.78157/4.81278. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.77865/4.82332. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.77743/4.82201. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.78222/4.82301. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.77891/4.82294. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.77585/4.82359. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.77649/4.81378. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.77871/4.83761. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.77333/4.84635. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.77630/4.82633. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.77513/4.83546. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.77833/4.79651. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.77723/4.82891. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.77569/4.84035. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.77549/4.81245. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.77430/4.83150. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.77059/4.83621. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.77516/4.82784. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.77290/4.82095. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.77913/4.83613. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.77545/4.83380. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.77034/4.81098. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.77479/4.81297. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.77727/4.82117. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.76815/4.83638. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.77418/4.82339. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.77000/4.80907. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.77611/4.81932. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.77263/4.82006. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.77018/4.80673. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.77191/4.80845. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.77329/4.81929. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.77285/4.81009. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 4.77242/4.81682. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.76846/4.81735. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.76797/4.82593. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.77137/4.81936. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.77129/4.82603. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.77109/4.83305. Took 0.08 sec\n",
      "ACC: 0.53125, MCC: 0.07936507936507936\n",
      "Epoch 0, Loss(train/val) 4.77300/4.73239. Took 0.08 sec\n",
      "Epoch 1, Loss(train/val) 4.74135/4.72900. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 4.72993/4.73449. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.72953/4.73729. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.72953/4.73508. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.72721/4.73733. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.72577/4.73643. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 4.72648/4.74011. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.72832/4.73715. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.72367/4.73762. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.72523/4.73767. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.72518/4.72977. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.72137/4.73552. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.71949/4.73346. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.72012/4.72734. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.71595/4.73623. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.72132/4.73078. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.72151/4.72729. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.72070/4.72963. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.72069/4.72588. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 4.71846/4.72532. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.71428/4.72947. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.71570/4.73078. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.71370/4.72848. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.71228/4.72042. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.71631/4.71963. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 4.71376/4.72026. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.71945/4.73889. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.71716/4.72347. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 4.71784/4.71924. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.71649/4.71674. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.71569/4.71448. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.71504/4.71167. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.71410/4.71511. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.71234/4.71288. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.71324/4.71873. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.71198/4.71356. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.71281/4.71638. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 4.71251/4.71399. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.71028/4.71990. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.70752/4.71360. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.71317/4.71399. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.70975/4.71541. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.71227/4.73538. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.71737/4.73425. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.70893/4.72997. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.71138/4.72717. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.70927/4.72987. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.70685/4.72398. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.71615/4.72019. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.71171/4.71885. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.70861/4.72055. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.70524/4.71534. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 4.70537/4.71852. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.70504/4.71988. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.70266/4.71755. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.70583/4.71934. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.70316/4.71608. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.70483/4.72317. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.70350/4.71530. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.70530/4.71809. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.70798/4.71530. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.70417/4.71890. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.70227/4.72026. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.70167/4.72369. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.70606/4.71692. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.70294/4.72615. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 4.70037/4.72867. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 4.70685/4.72040. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.70184/4.72926. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.70465/4.71494. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.69922/4.71747. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.69599/4.71035. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 4.69296/4.73348. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.69947/4.70348. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.69606/4.71619. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 4.69657/4.70381. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.69781/4.71649. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.71057/4.75033. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.71154/4.73737. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.70819/4.72757. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.70341/4.73130. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.69475/4.73557. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.70372/4.72843. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.69423/4.73626. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.69697/4.72972. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.71332/4.75999. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.71477/4.72088. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.71789/4.73524. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.71699/4.71868. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.71459/4.72938. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.71183/4.73543. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 4.71044/4.73216. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.71397/4.71698. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.70827/4.71767. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.70557/4.72436. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 4.70734/4.74295. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.70866/4.72229. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.70198/4.73274. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.70915/4.72749. Took 0.09 sec\n",
      "ACC: 0.515625, MCC: -0.010188710438961876\n",
      "Epoch 0, Loss(train/val) 5.04686/5.04564. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 5.01437/5.03411. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 5.00554/4.99357. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.99681/4.98803. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 4.99382/4.98279. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.99267/4.98468. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 4.99448/4.99214. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.99345/4.99269. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 4.99111/4.98563. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.99318/4.98480. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.99040/4.98721. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 4.98937/4.98653. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.99012/4.98945. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.99051/4.98432. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.98577/4.98428. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.98819/4.98762. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 4.98854/4.98536. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.98820/4.98825. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.98643/4.98532. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.98405/4.98852. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.98081/4.99240. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 4.98509/4.98566. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 4.98221/4.99494. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 4.97967/4.99973. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.98240/5.00478. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.98520/4.99601. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 4.97728/4.99920. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.97402/4.99586. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.98052/4.99395. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.97557/4.99789. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 4.97319/5.00309. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 4.97549/4.99109. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.97455/4.99574. Took 0.11 sec\n",
      "Epoch 33, Loss(train/val) 4.97377/5.00704. Took 0.12 sec\n",
      "Epoch 34, Loss(train/val) 4.97326/5.00028. Took 0.11 sec\n",
      "Epoch 35, Loss(train/val) 4.97644/4.99155. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 4.97640/5.00622. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 4.97317/4.99642. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.97326/5.01097. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.97271/4.99575. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 4.98126/4.98702. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.98394/4.98100. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.97900/4.98166. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.97415/4.98900. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.97613/4.98869. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.97923/4.98352. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.97537/4.98907. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 4.97556/4.98918. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.97329/5.00476. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.97323/4.99991. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.97647/4.99663. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.97199/4.99995. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.96774/5.03038. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.97190/5.01336. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.97132/5.00979. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 4.97130/5.02730. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.96088/5.03300. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.96892/5.03866. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 4.96669/5.02585. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.96631/5.04274. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.96417/5.01812. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 4.97342/5.02032. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.97218/5.01659. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.96391/5.01725. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.96666/5.03351. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 4.96293/5.03365. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.97000/5.02047. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.96513/5.01520. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.96355/5.03744. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 4.98666/4.99307. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 4.98083/5.01849. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 4.97467/5.01633. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.97331/5.00847. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.97381/5.00276. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 4.97256/5.00754. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.96657/5.00896. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 4.97108/5.00842. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 4.96300/5.00774. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.97211/5.00927. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 4.96486/5.00106. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.96483/5.00845. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 4.97092/5.00230. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.96942/4.97694. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.96112/4.99544. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.96876/5.01511. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.97282/5.00530. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.97075/5.01386. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.96134/5.01703. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.96629/5.01368. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.96809/5.01028. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 4.95968/5.01333. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.96126/4.99135. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 4.96475/5.01302. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.96653/4.99127. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 4.96013/5.02512. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 4.96320/5.00552. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.96906/5.01105. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.96201/4.99546. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 4.96010/5.01960. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.96316/5.03155. Took 0.08 sec\n",
      "ACC: 0.5, MCC: -0.00700902994282404\n",
      "Epoch 0, Loss(train/val) 4.80627/4.73075. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.73321/4.72824. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 4.73072/4.72646. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.72236/4.73643. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.72124/4.74391. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 4.72393/4.74775. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.71931/4.75333. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 4.72035/4.75330. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.71914/4.75637. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.71763/4.76025. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 4.71596/4.75505. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 4.71864/4.75418. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 4.71626/4.76598. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 4.72295/4.74435. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 4.71533/4.75935. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.71765/4.75371. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.71603/4.76181. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.71853/4.76098. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.71459/4.76787. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.71259/4.76830. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.71608/4.76726. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.71717/4.75463. Took 0.12 sec\n",
      "Epoch 22, Loss(train/val) 4.71250/4.77576. Took 0.13 sec\n",
      "Epoch 23, Loss(train/val) 4.71294/4.76250. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.71561/4.77077. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.71654/4.76113. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.71298/4.76672. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.70946/4.77296. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.70839/4.77553. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.71209/4.76578. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.70591/4.78314. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.71116/4.76771. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.70793/4.77435. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.70874/4.77244. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.71256/4.76841. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.71149/4.76213. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.71140/4.76432. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.71029/4.76149. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.70734/4.76326. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.70685/4.75225. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 4.70845/4.75710. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 4.70712/4.76002. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.70769/4.76955. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 4.70827/4.76166. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 4.70173/4.77886. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.70401/4.77570. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.70296/4.77862. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.70526/4.77110. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 4.70254/4.78778. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.70288/4.76271. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.70191/4.78196. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.70731/4.76487. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.71244/4.75876. Took 0.11 sec\n",
      "Epoch 53, Loss(train/val) 4.70465/4.77545. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.70402/4.76032. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.70004/4.79379. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 4.70644/4.74564. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.70512/4.77015. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 4.70415/4.75954. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 4.70312/4.78375. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.70158/4.79235. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 4.70742/4.76863. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.69722/4.79443. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 4.70158/4.77394. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 4.70107/4.78007. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.69880/4.78040. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.69896/4.76696. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.70732/4.76480. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.70068/4.76788. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.70112/4.77403. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 4.69581/4.77515. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.69622/4.76629. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.69905/4.77201. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.69990/4.76568. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.69635/4.76437. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 4.69672/4.78494. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.68962/4.79459. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.69674/4.77269. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 4.69699/4.77391. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.69483/4.78126. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 4.70171/4.78007. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.69512/4.78008. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 4.69670/4.76738. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.69291/4.78888. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.69605/4.78296. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.69464/4.78854. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.68574/4.79603. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.68679/4.78945. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 4.69253/4.78968. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.69262/4.79485. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.69173/4.75094. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.69179/4.78660. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.69244/4.79084. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.69629/4.77319. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 4.68591/4.78961. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.69170/4.78014. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 4.68730/4.80088. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 4.68625/4.80171. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.68517/4.80434. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 4.68747/4.80976. Took 0.08 sec\n",
      "ACC: 0.4375, MCC: -0.11221803979288456\n",
      "Epoch 0, Loss(train/val) 4.92581/4.87583. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 4.87218/4.86721. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.87971/4.86717. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.88049/4.87891. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.87842/4.88932. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 4.87803/4.87796. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 4.86735/4.87032. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 4.86914/4.86980. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.86922/4.87007. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 4.86584/4.86669. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 4.86883/4.86505. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.86510/4.86367. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 4.86415/4.86484. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.86726/4.85552. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 4.86663/4.85921. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 4.86423/4.85694. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.86003/4.85561. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 4.87039/4.87053. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 4.86865/4.86881. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 4.86803/4.86908. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.86586/4.86380. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 4.86371/4.86209. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.86581/4.86130. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.86300/4.86378. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 4.86243/4.86578. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 4.86141/4.86316. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.85947/4.86477. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 4.85307/4.86300. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 4.86491/4.85861. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 4.85550/4.86241. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.85575/4.86596. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.85471/4.87749. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 4.85453/4.87269. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 4.85640/4.87021. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 4.85257/4.87692. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 4.85620/4.87382. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 4.86273/4.86680. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.85579/4.86582. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 4.85618/4.86456. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 4.85245/4.87137. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.85272/4.87164. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 4.85474/4.88285. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 4.85607/4.87972. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.85786/4.87737. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 4.85224/4.87617. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.85487/4.87775. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 4.85334/4.86924. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.85362/4.87078. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.84952/4.89353. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 4.85438/4.88426. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 4.85058/4.88893. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 4.84654/4.87924. Took 0.13 sec\n",
      "Epoch 52, Loss(train/val) 4.84914/4.88016. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 4.84822/4.88282. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 4.84661/4.88608. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.84886/4.89060. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 4.85114/4.88189. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 4.84564/4.88834. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.84811/4.87732. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.84249/4.89583. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 4.84851/4.88212. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 4.84065/4.88895. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 4.84908/4.88200. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 4.85038/4.88828. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 4.84118/4.91625. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 4.84875/4.87683. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 4.85452/4.87835. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 4.84743/4.87179. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 4.84985/4.89006. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 4.84290/4.88503. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.84620/4.88152. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 4.84974/4.88021. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 4.84338/4.88748. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 4.85199/4.87036. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.84326/4.88366. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 4.84215/4.89901. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.83483/4.91700. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.84223/4.89643. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 4.84407/4.88551. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 4.83493/4.91374. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.84711/4.87454. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.84127/4.88992. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.84861/4.91125. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 4.84940/4.90184. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 4.84537/4.88512. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 4.84128/4.90210. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 4.84339/4.88273. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 4.84122/4.89734. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 4.84012/4.88042. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 4.83628/4.89673. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 4.84065/4.88607. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 4.83564/4.88611. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 4.83936/4.90328. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 4.83730/4.89850. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.84065/4.90554. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.83454/4.87996. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 4.83008/4.90273. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.84005/4.88761. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.83191/4.90395. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 4.84177/4.89814. Took 0.09 sec\n",
      "ACC: 0.40625, MCC: -0.19136555680572745\n",
      "Epoch 0, Loss(train/val) 4.79238/4.73116. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 4.72292/4.72225. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 4.72140/4.72757. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 4.71948/4.74193. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 4.71601/4.74949. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 4.71558/4.76192. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 4.71569/4.74818. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 4.71533/4.77166. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 4.71554/4.74572. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 4.71235/4.75825. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 4.71357/4.74930. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 4.70888/4.72605. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 4.70805/4.73529. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 4.70672/4.77236. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 4.70327/4.72294. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 4.70755/4.75584. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 4.70272/4.76150. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 4.70516/4.75652. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 4.70120/4.75024. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 4.70203/4.76045. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 4.69892/4.74985. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 4.69946/4.76141. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 4.69513/4.77616. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 4.71400/4.70783. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 4.71027/4.74012. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 4.70932/4.74194. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 4.70477/4.71982. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 4.70852/4.72379. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 4.70656/4.72636. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 4.70428/4.73058. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 4.70600/4.73462. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 4.70447/4.72913. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 4.70339/4.75376. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 4.70291/4.74899. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 4.70370/4.74171. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 4.69904/4.78145. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 4.69804/4.76357. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 4.69684/4.76411. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 4.71435/4.73441. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 4.72057/4.73110. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 4.71140/4.72608. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 4.70945/4.72638. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 4.70142/4.75787. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 4.70440/4.76607. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 4.70332/4.78665. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 4.69572/4.80595. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 4.69252/4.76753. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 4.69543/4.79258. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 4.69610/4.81075. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 4.69640/4.79269. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 4.69397/4.79549. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 4.69192/4.81551. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 4.69130/4.79482. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 4.69145/4.80486. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 4.68906/4.79944. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 4.69103/4.81662. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 4.69378/4.81349. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 4.69195/4.79545. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 4.69255/4.81638. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 4.68764/4.82054. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 4.68699/4.80846. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 4.69075/4.81404. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 4.69411/4.81007. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 4.68747/4.80206. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 4.69338/4.80447. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 4.69460/4.79966. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 4.68302/4.83995. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 4.68702/4.81085. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 4.68645/4.79696. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 4.69109/4.81050. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 4.68670/4.82440. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 4.68777/4.81751. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 4.68981/4.80958. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 4.68469/4.82409. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 4.69003/4.79537. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 4.68746/4.80585. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 4.68363/4.80821. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 4.69274/4.79259. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 4.69856/4.77203. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 4.69249/4.79959. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 4.68733/4.83840. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 4.68633/4.82574. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 4.68599/4.81567. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 4.68394/4.83076. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 4.68541/4.81949. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 4.68772/4.82792. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 4.68160/4.83012. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 4.68575/4.81893. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 4.68564/4.81465. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 4.68469/4.81220. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 4.68392/4.82815. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 4.68346/4.83786. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 4.68829/4.81523. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 4.68142/4.84253. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 4.68334/4.83199. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 4.67889/4.83326. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 4.68113/4.81179. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 4.68271/4.82148. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 4.68487/4.81946. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 4.68188/4.82841. Took 0.09 sec\n",
      "ACC: 0.5625, MCC: 0.12725695259515554\n"
     ]
    }
   ],
   "source": [
    "## 실행 파일\n",
    "args.data_list = os.listdir(r\"C:\\Users\\lab\\Desktop\\DTML_NEW\\data\\kdd17\\ourpped\")\n",
    "\n",
    "\n",
    "with open(args.save_file_path + '\\\\' + 'DTML_result_t.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"model\", \"stock\", \"entire_exp_time\",  \"avg_test_ACC\", \"avg_test_ACC_std\", \"avg_test_MCC\"])\n",
    "\n",
    "    for data in args.data_list:\n",
    "        \n",
    "        stock = data.split('.')[0]\n",
    "\n",
    "        est = time.time()\n",
    "        setattr(args, 'symbol', stock)\n",
    "        args.new_file_path = args.save_file_path + '\\\\' + \"DTML_\" + args.symbol\n",
    "        os.makedirs(args.new_file_path)\n",
    "        \n",
    "        \n",
    "        csv_read = stock_csv_read(data,args.x_frames,args.y_frames)\n",
    "        split_data_list = csv_read.cv_split()\n",
    "\n",
    "\n",
    "        \n",
    "        with open(args.new_file_path + '\\\\'+ str(args.symbol)+'test_acc_list' +'.csv', 'w',newline='') as alist:\n",
    "            www = csv.writer(alist)\n",
    "            www.writerow([\"acc_list\"])\n",
    "        \n",
    "            ACC_cv = []\n",
    "            for i, data in enumerate(split_data_list):\n",
    "                args.split_file_path = args.new_file_path + \"\\\\\" + str(i) +\"th_iter\"\n",
    "                os.makedirs(args.split_file_path)\n",
    "                # 0번째에 index 1번째에 stock 1개가 input으로 들어감\n",
    "                trainset = StockDataset(data[0])\n",
    "                valset = StockDataset(data[1])\n",
    "                testset = StockDataset(data[2])\n",
    "            \n",
    "\n",
    "                partition = {'train': trainset, 'val': valset, 'test': testset}\n",
    "\n",
    "\n",
    "                att_LSTM = args.att_LSTM(args.input_dim, args.hid_dim, args.output_dim, args.num_layers, args.batch_size,\n",
    "                                        args.dropout, args.use_bn, args.attention_head, args.attn_size, activation=\"ReLU\")\n",
    "                transformer = args.transformer(args.trans_feature_size, args.trans_num_laysers,args.dropout, args.batch_size, args.x_frames, args.trans_nhead)\n",
    "                \n",
    "                \n",
    "                args.r = nn.init.xavier_normal_(torch.empty(args.batch_size, args.hid_dim)).to(args.device) \n",
    "                args.b = torch.zeros(1).to(args.device)\n",
    "                \n",
    "                att_LSTM.to(args.device)\n",
    "                transformer.to(args.device)\n",
    "\n",
    "                if args.optim == 'SGD':\n",
    "                    att_LSTM_optimizer = optim.SGD(att_LSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "                    transformer_optimizer = optim.SGD(transformer.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "                elif args.optim == 'RMSprop':\n",
    "                    att_LSTM_optimizer = optim.RMSprop(att_LSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "                    transformer_optimizer = optim.RMSprop(transformer.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "                elif args.optim == 'Adam':\n",
    "                    att_LSTM_optimizer = optim.Adam(att_LSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "                    transformer_optimizer = optim.Adam(transformer.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "                else:\n",
    "                    raise ValueError('In-valid optimizer choice')\n",
    "\n",
    "                # ===== List for epoch-wise data ====== #\n",
    "                train_losses = []\n",
    "                val_losses = []\n",
    "                # ===================================== #\n",
    "                for epoch in range(args.epoch):\n",
    "                    ts = time.time()\n",
    "                    att_LSTM, transformer, train_loss = train(att_LSTM, transformer,\n",
    "                                                            att_LSTM_optimizer, transformer_optimizer,\n",
    "                                                            partition, args)\n",
    "\n",
    "                    att_LSTM, transformer, val_loss = validation(att_LSTM, transformer, partition, args)\n",
    "\n",
    "                    te = time.time()\n",
    "\n",
    "                    ## 각 에폭마다 모델을 저장하기 위한 코드\n",
    "                    torch.save(att_LSTM.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_epoch' +'_att_LSTM' +'.pt')\n",
    "                    torch.save(transformer.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_epoch' +'_transformer' +'.pt')\n",
    "\n",
    "                    train_losses.append(train_loss)\n",
    "                    val_losses.append(val_loss)\n",
    "\n",
    "                    print('Epoch {}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "                        .format(epoch, train_loss, val_loss, te - ts))\n",
    "\n",
    "                ## val_losses에서 가장 값이 최소인 위치를 저장함\n",
    "                site_val_losses = val_losses.index(min(val_losses)) ## 10 epoch일 경우 0번째~9번째 까지로 나옴\n",
    "                att_LSTM = args.att_LSTM(args.input_dim, args.hid_dim, args.output_dim, args.num_layers, args.batch_size,\n",
    "                                        args.dropout, args.use_bn, args.attention_head, args.attn_size,\n",
    "                                        activation=\"ReLU\")\n",
    "                transformer = args.transformer(args.trans_feature_size, args.trans_num_laysers,\n",
    "                                            args.dropout, args.batch_size, args.x_frames, args.trans_nhead)\n",
    "                att_LSTM.to(args.device)\n",
    "                transformer.to(args.device)\n",
    "\n",
    "                att_LSTM.load_state_dict(torch.load(args.split_file_path + '\\\\' + str(site_val_losses) +'_epoch' +'_att_LSTM'+ '.pt'))\n",
    "                transformer.load_state_dict(torch.load(args.split_file_path + '\\\\' + str(site_val_losses) + '_epoch' + '_transformer' + '.pt'))\n",
    "\n",
    "                ACC,MCC = test(att_LSTM, transformer, partition, args)\n",
    "                print('ACC: {}, MCC: {}'.format(ACC, MCC))\n",
    "                www.writerow([ACC])\n",
    "\n",
    "                with open(args.split_file_path + '\\\\'+ str(site_val_losses)+'Epoch_test_metric' +'.csv', 'w') as fd:\n",
    "                    print('ACC: {}, MCC: {}'.format(ACC, MCC), file=fd)\n",
    "\n",
    "                result = {}\n",
    "\n",
    "                result['train_losses'] = train_losses\n",
    "                result['val_losses'] = val_losses\n",
    "                result['ACC'] = ACC\n",
    "                result['MCC'] = MCC\n",
    "\n",
    "\n",
    "                eet = time.time()\n",
    "                entire_exp_time = eet - est\n",
    "\n",
    "                fig = plt.figure()\n",
    "                plt.plot(result['train_losses'])\n",
    "                plt.plot(result['val_losses'])\n",
    "                plt.legend(['train_losses', 'val_losses'], fontsize=15)\n",
    "                plt.xlabel('epoch', fontsize=15)\n",
    "                plt.ylabel('loss', fontsize=15)\n",
    "                plt.grid()\n",
    "                plt.savefig(args.split_file_path + '\\\\' + str(args.symbol) + '_fig' + '.png')\n",
    "                plt.close(fig)\n",
    "                ACC_cv.append(result['ACC'])\n",
    "            # csv파일에 기록하기\n",
    "        ACC_cv_ar = np.array(ACC_cv)\n",
    "        acc_avg = np.mean(ACC_cv_ar)\n",
    "        acc_std = np.std(ACC_cv_ar)\n",
    "\n",
    "        wr.writerow([\"DTML\", args.symbol, entire_exp_time, acc_avg, acc_std, result['MCC']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4bb3f-2033-4b97-8031-19370a89d75b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('taewon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "013403e7ebf8f35ee0411721c7e4b108aa3c3f8cb903b89610d110413a68ec3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
