{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import import_ipynb\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train(Autoformer, Autoformer_optimizer,args,partition):\n",
    "    train_loader = DataLoader(partition[\"train\"],batch_size= args.batch_size,shuffle=False,drop_last=True)\n",
    "    \n",
    "    Autoformer.train()\n",
    "    train_loss = 0.0\n",
    "    for (x,y) in train_loader:\n",
    "        Autoformer.zero_grad()\n",
    "        Autoformer_optimizer.zero_grad()\n",
    "\n",
    "        x = x.float().to(args.device) # 64,10 40\n",
    "        y = y.float().squeeze().to(args.device)\n",
    "        \n",
    "\n",
    "\n",
    "        Autoformer_out = Autoformer(x)\n",
    "\n",
    "        loss = args.loss_fn(Autoformer_out, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        Autoformer_optimizer.step() ## parameter 갱신\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    return Autoformer, train_loss\n",
    "\n",
    "def validation(Autoformer, args, partition):\n",
    "    val_loader = DataLoader(partition[\"val\"],batch_size= args.batch_size,shuffle=False,drop_last=True)\n",
    "\n",
    "    Autoformer.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (x,y) in val_loader:\n",
    "\n",
    "            x = x.float().to(args.device) # 64,10 40\n",
    "            y = y.float().squeeze().to(args.device)\n",
    "\n",
    "            Autoformer_out = Autoformer(x)\n",
    "                        \n",
    "            loss = args.loss_fn(Autoformer_out, y)      \n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    return Autoformer, val_loss\n",
    "\n",
    "def test(Autoformer, args, partition):\n",
    "    test_loader = DataLoader(partition[\"test\"],batch_size= args.batch_size,shuffle=False,drop_last=True)\n",
    "    \n",
    "    Autoformer.eval()\n",
    "    ACC_metric = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (x,y) in test_loader:\n",
    "\n",
    "            x = x.float().to(args.device) # 64,10 40\n",
    "            y = y.float().squeeze().to(args.device)\n",
    "\n",
    "            Autoformer_out = Autoformer(x)\n",
    "\n",
    "            output_ = torch.where(Autoformer_out >= 0.5, 1.0, 0.0)\n",
    "\n",
    "            output_.requires_grad = True\n",
    "\n",
    "            perc_y_pred = output_.cpu().detach().numpy()     \n",
    "            perc_y_true =  y.cpu().detach().numpy()\n",
    "            acc = accuracy_score(perc_y_true, perc_y_pred)\n",
    "\n",
    "            ACC_metric += acc\n",
    "\n",
    "    ACC_metric = ACC_metric / len(test_loader)\n",
    "     \n",
    "    return ACC_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====== Argument initializtion ======#\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "#=============== Device ===============#\n",
    "args.device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#================ Path ================#\n",
    "\n",
    "args.save_file_path = \"D:\\\\Autoformer_result\"\n",
    "\n",
    "#========= Base Hyperparameter =========#\n",
    "args.batch_size = 32\n",
    "args.lr = 0.00005\n",
    "args.L2 = 0.00001\n",
    "args.epoch = 100\n",
    "args.dropout = 0.15\n",
    "args.loss_fn = nn.BCELoss()\n",
    "\n",
    "#===== Transformer Hyperparameter =====#\n",
    "from models.Autoformer import Model as Autoformer\n",
    "\n",
    "args.Autoformer = Autoformer\n",
    "args.input_feature_size = 40\n",
    "\n",
    "args.Autoformer_feature_size = 64\n",
    "\n",
    "args.nhead = 4\n",
    "\n",
    "args.nlayer = 1\n",
    "args.ts_len = 10\n",
    "args.target_len = 1\n",
    "# trans_feature_size / trans_nhead => int 필수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Stock_Dataset.ipynb\n",
      "Epoch 0, Loss(train/val) 0.72104/0.70229. Took 2.17 sec\n",
      "Epoch 1, Loss(train/val) 0.70832/0.68070. Took 0.54 sec\n",
      "Epoch 2, Loss(train/val) 0.69834/0.67846. Took 0.49 sec\n",
      "Epoch 3, Loss(train/val) 0.68866/0.67931. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68783/0.67646. Took 0.52 sec\n",
      "Epoch 5, Loss(train/val) 0.68915/0.65943. Took 0.50 sec\n",
      "Epoch 6, Loss(train/val) 0.69032/0.66870. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.67537/0.67149. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.67777/0.68436. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67901/0.69217. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67236/0.69436. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.66917/0.69533. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67144/0.69117. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.66177/0.69645. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67358/0.70038. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66180/0.70256. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.65576/0.69831. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.66139/0.70862. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.65427/0.73223. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.65537/0.73939. Took 0.47 sec\n",
      "Epoch 20, Loss(train/val) 0.66040/0.73325. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65632/0.73130. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64893/0.75262. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64046/0.77452. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64425/0.79291. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.63296/0.78602. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.63049/0.79878. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63226/0.82047. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62718/0.83040. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62700/0.85095. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61784/0.86761. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.62220/0.86587. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.61183/0.85998. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.61164/0.87349. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.62959/0.85298. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60966/0.86038. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.59229/0.85934. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60941/0.85959. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.59080/0.90378. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.59374/0.86592. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.59907/0.88733. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.58797/0.90605. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59328/0.88639. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58945/0.90208. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.58131/0.90529. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.57488/0.92563. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56947/0.91420. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.57176/0.92120. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56592/0.94378. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.57417/0.93614. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.55644/0.90741. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.59953/0.91485. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56306/0.93359. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56623/0.94627. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54496/0.92139. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.55786/0.97897. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56821/0.96845. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.54517/0.96324. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.55455/0.96972. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54491/0.95034. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.54556/0.99081. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.54846/0.95968. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.54113/0.97654. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.53574/0.98269. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.52734/0.95288. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.53272/1.00602. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.50239/1.03089. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.54482/1.01428. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.52580/0.97031. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.50902/0.95953. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.49228/1.00663. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.48075/1.01744. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.48483/1.03969. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.47073/1.05980. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.48610/1.02280. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.47424/1.07949. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.47490/1.09327. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.45377/1.19116. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44808/1.12642. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.47676/1.09803. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.49574/1.04248. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.46216/1.09410. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.44419/1.08514. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.44949/1.17936. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.43483/1.16061. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.42853/1.19575. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.45513/1.21545. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.42218/1.19043. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.42010/1.25401. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.40992/1.24898. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.37718/1.36560. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.41488/1.31177. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.38583/1.39970. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.42630/1.44002. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.41366/1.34058. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.40170/1.44744. Took 0.47 sec\n",
      "Epoch 96, Loss(train/val) 0.44424/1.41039. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.37492/1.47349. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.36461/1.49978. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.38508/1.39506. Took 0.44 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69826/0.71477. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.69218/0.69977. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69261/0.69260. Took 0.49 sec\n",
      "Epoch 3, Loss(train/val) 0.68794/0.68149. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.68145/0.69325. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68106/0.68843. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.67725/0.70205. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67631/0.68612. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.66984/0.69235. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68124/0.69846. Took 0.47 sec\n",
      "Epoch 10, Loss(train/val) 0.66969/0.69375. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.66691/0.69679. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.66845/0.71185. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66652/0.72530. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66396/0.72807. Took 0.48 sec\n",
      "Epoch 15, Loss(train/val) 0.65764/0.71060. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66289/0.71432. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65289/0.72271. Took 0.42 sec\n",
      "Epoch 18, Loss(train/val) 0.65280/0.72012. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.64909/0.72916. Took 0.48 sec\n",
      "Epoch 20, Loss(train/val) 0.64663/0.74948. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64841/0.75790. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63294/0.76772. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.63244/0.77190. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63237/0.75529. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.62859/0.76484. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62636/0.78928. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.61934/0.79496. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62560/0.79515. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61828/0.79732. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61630/0.81786. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.61307/0.79470. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.61715/0.82045. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61220/0.81326. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59824/0.84240. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61123/0.83873. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.61031/0.81396. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.60660/0.80595. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.59699/0.80158. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.59655/0.80226. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.59152/0.81530. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.58891/0.80262. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.58881/0.80469. Took 0.47 sec\n",
      "Epoch 43, Loss(train/val) 0.58781/0.80137. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.58766/0.81584. Took 0.48 sec\n",
      "Epoch 45, Loss(train/val) 0.58182/0.81624. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.56543/0.82832. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.56627/0.85245. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.56605/0.86443. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.56719/0.85782. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.56301/0.87308. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56199/0.88188. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.55976/0.88698. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54930/0.89963. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55602/0.87331. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.54942/0.90092. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.53651/0.92024. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.53223/0.95980. Took 0.52 sec\n",
      "Epoch 58, Loss(train/val) 0.53931/0.93236. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53645/0.93687. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52736/0.98964. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.52574/0.97909. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51882/1.01102. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51997/0.94568. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.51962/0.95479. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.51316/0.95102. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.50836/1.02201. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.50936/0.99055. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.48288/1.01647. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.49027/1.00363. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.49898/0.94064. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.48300/0.99569. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.49835/0.99968. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46973/1.02388. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.47905/1.03534. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.48455/1.04213. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.48330/1.00408. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46987/0.95942. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.47995/1.04423. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.49456/1.00359. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.47230/1.00654. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.45490/0.99931. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.43661/1.04917. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.44733/1.05566. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.46148/1.01603. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.43606/0.96038. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.41939/1.08086. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.45174/1.10315. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.47894/1.03757. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.43127/1.11876. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.42622/1.09088. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.42923/1.03814. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.41329/1.09383. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.41613/1.11113. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.41200/1.13407. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.41465/1.13673. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.39901/1.16985. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.39659/1.13369. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.41292/1.05485. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.40966/1.09104. Took 0.43 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69186/0.72801. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68968/0.72162. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68725/0.72284. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68208/0.72826. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.67846/0.71923. Took 0.48 sec\n",
      "Epoch 5, Loss(train/val) 0.67926/0.71668. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.67626/0.71391. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.67369/0.71877. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.66999/0.73566. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67441/0.74540. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67336/0.74315. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66789/0.75287. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67212/0.74737. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66306/0.76452. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.65905/0.77726. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66648/0.76875. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66204/0.77007. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65621/0.78528. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65777/0.79175. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65101/0.82166. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64784/0.82344. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.65009/0.82979. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64311/0.85898. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63129/0.88316. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63470/0.86117. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62010/0.89751. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.62802/0.91433. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62080/0.87738. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.61751/0.90230. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60605/0.93503. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60531/0.91231. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61440/0.93793. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59900/0.94198. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58997/0.96745. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59667/0.98246. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.59053/0.97593. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59839/0.97956. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.58429/0.95119. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.58322/0.98326. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.56986/0.99250. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57707/1.02268. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.57753/0.98717. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57637/1.01108. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56822/1.01474. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.55860/1.02094. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.55318/1.03969. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.54898/1.01600. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55849/1.00591. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.54418/0.99104. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.53375/1.00806. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53809/1.02096. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.54380/1.00027. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.51588/1.04810. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54055/1.01315. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52197/1.03645. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.52633/1.03720. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.49892/1.07201. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.50759/1.08311. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.49948/1.10865. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.48490/1.10029. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49080/1.07760. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.49523/1.12401. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48805/1.17959. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.46205/1.19849. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.44901/1.19408. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.45954/1.24842. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.46939/1.19356. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.45572/1.16308. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.45006/1.20310. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45144/1.18600. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.46009/1.18128. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.44498/1.16098. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.42092/1.25639. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.42258/1.20595. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.39980/1.25916. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.39994/1.23291. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43685/1.21159. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.39843/1.20860. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.39242/1.19149. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.39371/1.18982. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.38426/1.24386. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.38523/1.27572. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.38151/1.26719. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.37905/1.19869. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.38769/1.21068. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.38823/1.18875. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.36436/1.20751. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.34145/1.22000. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.34750/1.22727. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.33131/1.24542. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.33243/1.30391. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.33358/1.27572. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.33724/1.33026. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.35071/1.21928. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.35060/1.27014. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.33915/1.23668. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.32815/1.22955. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.32360/1.25038. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.35098/1.29840. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.32483/1.36681. Took 0.44 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.72084/0.68040. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.71346/0.68713. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.71479/0.69908. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.71395/0.69346. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.70540/0.69443. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.70450/0.68753. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.70449/0.68430. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.70325/0.68446. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69536/0.69267. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.69068/0.67805. Took 0.47 sec\n",
      "Epoch 10, Loss(train/val) 0.69900/0.68235. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.69127/0.68607. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.69128/0.68566. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.69135/0.68620. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68805/0.69155. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.68901/0.70020. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.68227/0.69725. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.68502/0.68768. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67493/0.69312. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67223/0.69804. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67718/0.69519. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.67542/0.69662. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.67095/0.69873. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.68754/0.69741. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.67939/0.71034. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.68127/0.71048. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.66969/0.72163. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.66759/0.73220. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.67581/0.74716. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.67082/0.74498. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.65852/0.75228. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.66451/0.77116. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.66713/0.76358. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.66512/0.77625. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.65915/0.77831. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.66166/0.78299. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.65462/0.78740. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.65493/0.78012. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.65559/0.80404. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.65778/0.80731. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.64690/0.81890. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.63775/0.83726. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.63883/0.83683. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.64300/0.84498. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.64109/0.82680. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.63997/0.85654. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.63298/0.86531. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.63741/0.87170. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.62530/0.85267. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.62388/0.87923. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.61750/0.88760. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.63280/0.85830. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.61848/0.87012. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.61027/0.90474. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.60947/0.90552. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.61188/0.87599. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.61174/0.86220. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.59144/0.89530. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.58622/0.92395. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.59587/0.91582. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.57518/0.93389. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.57176/0.91615. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.56624/0.97622. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.58493/0.97406. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.59257/0.96913. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.57242/0.97067. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.56442/0.94880. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.56800/0.96206. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.56357/0.94921. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.54567/1.04166. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.54832/1.07973. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.53780/1.04379. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.53540/1.03859. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.54528/0.99115. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.53056/1.04250. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.52032/1.08007. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.52082/1.07805. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.52803/1.07105. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.50901/1.06701. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.51024/1.09013. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.52131/1.07731. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.50298/1.13578. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.49852/1.20517. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.50668/1.10903. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.51317/1.11709. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.50735/1.03950. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.49313/1.04185. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.50129/1.13486. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.47830/1.04680. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.47971/1.08261. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.52139/1.01986. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.49503/1.02577. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.49697/1.06332. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.45822/1.09047. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.48898/1.09791. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.47162/1.04814. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.46516/1.09240. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.47168/1.09820. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.46128/1.07563. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.45962/1.13357. Took 0.44 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69936/0.69021. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69870/0.69065. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69154/0.70793. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69088/0.72239. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69114/0.71784. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68554/0.72224. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68222/0.72167. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67860/0.74175. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67863/0.72899. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68420/0.73208. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67752/0.72754. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68142/0.73957. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67688/0.74078. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67791/0.74862. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66575/0.71130. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67907/0.73562. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67276/0.73793. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66775/0.74074. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66965/0.74785. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66756/0.74436. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65992/0.74680. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66376/0.74859. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65322/0.74428. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65917/0.76079. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65296/0.75700. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64375/0.78072. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64376/0.78346. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64129/0.79025. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63828/0.82523. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63390/0.79702. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63989/0.79493. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62216/0.83992. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63795/0.80754. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.61833/0.83553. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.60845/0.85945. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62308/0.83375. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61348/0.85413. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.61060/0.86283. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61162/0.87805. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.60582/0.87024. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60708/0.86534. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.59539/0.90250. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.59359/0.97031. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59977/0.90894. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58786/0.92153. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56628/0.92629. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.58164/0.91830. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.59495/0.86414. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59279/0.90302. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.57610/0.94483. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.57398/0.92739. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56216/0.93971. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56036/0.88411. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56255/0.90006. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56261/0.92054. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.55089/0.93420. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53701/0.92693. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53705/0.92254. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.53800/0.92979. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52838/0.93043. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52448/0.94551. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.53422/0.91505. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.54216/0.95205. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.54252/0.92446. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.54421/0.93375. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.52891/1.01359. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.52736/0.96595. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.51777/0.99689. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.50533/1.08347. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.50602/1.03953. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.50354/1.00194. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.49840/0.98321. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.50593/0.92858. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.49736/0.97822. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.48400/0.96082. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.49383/0.95456. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.47525/1.01639. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.46207/1.06612. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.46184/1.02050. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.47105/1.03222. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.47642/1.02019. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.45798/0.98598. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.46176/1.05892. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.43382/1.15509. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.46335/1.08286. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.45366/1.15750. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.45690/1.09168. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.44835/1.12368. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.44538/1.10647. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.41767/1.09515. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.43442/1.11741. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.43712/1.08894. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.42986/1.06029. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.43558/1.09420. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40043/1.19108. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.40945/1.17234. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.40299/1.21945. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.40436/1.16019. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.39769/1.25014. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.40890/1.23239. Took 0.45 sec\n",
      "ACC: 0.3958333333333333\n",
      "Epoch 0, Loss(train/val) 0.69560/0.70898. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69627/0.69773. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68821/0.69369. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68640/0.69116. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68080/0.70037. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68474/0.69783. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67999/0.69743. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68167/0.70434. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68631/0.71653. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68293/0.70814. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68168/0.71159. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67356/0.71267. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67301/0.70466. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67662/0.70521. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67154/0.70187. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66488/0.69842. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66552/0.70470. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66512/0.70469. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66120/0.70459. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66827/0.69267. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65662/0.69492. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65518/0.68530. Took 0.47 sec\n",
      "Epoch 22, Loss(train/val) 0.65267/0.68542. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64870/0.69492. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64539/0.68716. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64731/0.70067. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64708/0.70274. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63628/0.72250. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.63679/0.71807. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63164/0.71217. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.62995/0.72468. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64054/0.72289. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63975/0.73461. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.62691/0.74405. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.62033/0.74366. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62279/0.77554. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62711/0.79569. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61141/0.80980. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61275/0.76762. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.62046/0.83965. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61081/0.77821. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60970/0.82487. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60582/0.83001. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61224/0.80513. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60064/0.82641. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.60886/0.81573. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.60473/0.78696. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.60534/0.81359. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.59363/0.81865. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.60070/0.83865. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.60229/0.79790. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.59408/0.84447. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.57874/0.86539. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.57712/0.86488. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.57590/0.88230. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.56455/0.89806. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56110/0.95900. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.57146/0.93020. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.55488/0.99517. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.56173/0.93243. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.54205/0.98332. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.54647/0.99471. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.54850/0.94230. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.53655/0.97458. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.52869/1.01666. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.53066/1.02065. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.52532/1.05448. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.52495/1.04918. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49226/1.11195. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.51366/1.08873. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48996/1.06491. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.48133/1.08700. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48630/1.01425. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.46923/1.11701. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.48397/1.18570. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48198/1.06511. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.46905/1.09280. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.45674/1.13148. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.45667/1.03347. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.45185/1.12997. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.44404/1.10486. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.45738/1.04801. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.45447/1.12925. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.44561/1.17731. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43041/1.13573. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.41367/1.15365. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.40545/1.17079. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.41329/1.14662. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.38899/1.22408. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.41162/1.15371. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.39264/1.17485. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.37743/1.22476. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.37447/1.24186. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.36289/1.27988. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.42285/1.29805. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.38774/1.31808. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.35926/1.29664. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.36770/1.30283. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.35377/1.36667. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.35516/1.33022. Took 0.43 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.72306/0.66586. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.70759/0.69025. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.70746/0.68832. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69512/0.67740. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.70406/0.67874. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.70338/0.68688. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69417/0.67852. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69385/0.67826. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69235/0.68824. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.69634/0.69834. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68437/0.71057. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.68640/0.70218. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.68539/0.70739. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67828/0.69202. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67241/0.70848. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67013/0.70408. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66711/0.72909. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66813/0.74637. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66475/0.75891. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65969/0.74982. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65910/0.75797. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66061/0.74917. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66388/0.74737. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.65507/0.75488. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65137/0.75670. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65460/0.74512. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65146/0.75475. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.65422/0.74752. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.65001/0.75257. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.64417/0.74937. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.64890/0.75400. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.63575/0.76089. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.63958/0.77449. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63790/0.77988. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.63456/0.76902. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.63336/0.79021. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.63630/0.78719. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.62563/0.80688. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.62577/0.80579. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.63789/0.80207. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62982/0.79987. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.62126/0.81128. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.61738/0.79185. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.61786/0.80747. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.61521/0.79693. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.61659/0.79171. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.60478/0.81910. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.60764/0.83023. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.61210/0.82766. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.59512/0.83695. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.60555/0.86311. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.60069/0.85645. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.60220/0.85186. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.60483/0.84372. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.59329/0.86036. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.60467/0.86055. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.59477/0.83682. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.58666/0.83194. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.59485/0.82206. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.57969/0.82653. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.58886/0.80291. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.59997/0.80987. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.57607/0.81878. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.57628/0.81031. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.57182/0.81455. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.58188/0.81493. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.58122/0.81623. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.57658/0.82113. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.56975/0.84362. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.56437/0.85858. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.55936/0.84768. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.56469/0.86117. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.56817/0.85522. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.55853/0.83797. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.55542/0.87926. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.55054/0.87311. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.54791/0.87365. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.54641/0.88671. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.53518/0.86737. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.54141/0.88112. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.52408/0.92133. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.52792/0.92318. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.52934/0.90298. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.52107/0.91017. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.51819/0.95416. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.52798/0.93641. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.51007/0.94084. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.51750/0.92650. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.51168/0.93739. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.51084/0.96106. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.51027/0.96366. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.49390/0.98825. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.50191/1.00821. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.47720/0.99827. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.50168/1.03590. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.48777/1.00538. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.49497/1.03463. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.47698/1.03034. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.47266/1.04641. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.48153/0.99592. Took 0.43 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.70470/0.69391. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70121/0.70422. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69439/0.71150. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.70020/0.70928. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69375/0.71273. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68982/0.70872. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.69287/0.70330. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.69289/0.70205. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69444/0.73066. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68306/0.71679. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68345/0.72810. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68758/0.72755. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67756/0.72527. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.68249/0.74590. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67308/0.76358. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67059/0.76940. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66511/0.77784. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66572/0.79837. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66728/0.83459. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65885/0.85600. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64947/0.86584. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.65859/0.89590. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64310/0.92976. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64247/0.95412. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63917/0.95640. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63838/0.93702. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63217/0.95204. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.62540/0.96606. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62664/0.98451. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62833/0.97509. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62234/1.00174. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61292/1.02639. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.60695/1.01880. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61050/1.04336. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.60783/1.05155. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.59643/1.06299. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.60963/1.04975. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.60280/1.05099. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.59500/1.06886. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.58505/1.10460. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57525/1.13638. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58737/1.11773. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58017/1.10212. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.56649/1.13803. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58186/1.17134. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.56677/1.17233. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56924/1.15657. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.56451/1.21609. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.55226/1.20717. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.55320/1.23797. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.54652/1.24337. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.54595/1.26208. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54892/1.20441. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.54717/1.28672. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.54079/1.25050. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51759/1.33252. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52858/1.36135. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51573/1.44031. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.49956/1.53005. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.51442/1.50513. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.49098/1.43112. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.51804/1.51180. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.49482/1.55151. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48774/1.57069. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.48887/1.57055. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.48217/1.64280. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48504/1.58005. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.48612/1.63700. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.46964/1.72182. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.45776/1.72127. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.45838/1.74744. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.44913/1.72475. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.43457/1.79747. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.44091/1.84505. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.43628/1.74868. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.42465/1.78513. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.44468/1.70619. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46712/1.73631. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.41778/1.74113. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.38932/1.89842. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.40685/1.91299. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.40650/1.89802. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.41210/1.83260. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.38984/1.92393. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43664/1.90366. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.43576/1.82434. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.39767/1.81707. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43929/1.79634. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.38892/1.86110. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.38637/1.86237. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36478/2.03669. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.37126/1.88381. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.35828/2.01107. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.36468/1.96593. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.39689/1.90617. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.37253/1.95982. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.36637/1.97871. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.33059/2.02341. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.34585/2.08996. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.35795/2.02904. Took 0.44 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.70068/0.70899. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69820/0.69006. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69501/0.68420. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69264/0.67741. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.68902/0.66912. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.68718/0.66743. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.68713/0.67137. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68510/0.68672. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67920/0.68126. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68097/0.68092. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67546/0.67445. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67593/0.66791. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67718/0.66365. Took 0.47 sec\n",
      "Epoch 13, Loss(train/val) 0.67354/0.68043. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67024/0.67559. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68101/0.67234. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66544/0.67599. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66728/0.68479. Took 0.42 sec\n",
      "Epoch 18, Loss(train/val) 0.67190/0.69347. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66747/0.69416. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66308/0.68626. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.66043/0.69113. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65605/0.69917. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.65945/0.69855. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65904/0.70426. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.65424/0.72514. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64407/0.70541. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64963/0.70019. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.64599/0.68864. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63197/0.70779. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64772/0.72521. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64233/0.73763. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.64132/0.73760. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63230/0.73599. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61346/0.76365. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61962/0.77605. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.61973/0.79620. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62342/0.82443. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.61341/0.81398. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.62125/0.83895. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61643/0.81697. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61932/0.84685. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.60765/0.87194. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60409/0.88972. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59410/0.92477. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.60002/0.96654. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59906/0.95395. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.59911/0.96712. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.59610/1.00222. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.58141/0.98784. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57326/1.02447. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.58201/1.09220. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.57818/1.04527. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.57963/1.10569. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.58124/1.06602. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.57578/1.09384. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.56511/1.05198. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.57244/1.06193. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.57006/1.03152. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54558/1.06449. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.56959/1.07641. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.54853/1.15867. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.55312/1.11474. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.55192/1.13354. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.54843/1.13638. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.53841/1.15569. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.53373/1.13835. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.54523/1.15195. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.52022/1.14246. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.52395/1.18165. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.52650/1.16344. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.52255/1.10433. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.52081/1.10886. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.53112/1.11105. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.51554/1.07973. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.50880/1.13179. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.51587/1.12468. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.49001/1.24558. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.51187/1.16933. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.50292/1.17332. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.49255/1.17222. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.49390/1.11439. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.49164/1.21174. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.48889/1.21813. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.47871/1.23602. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.48187/1.26457. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.48509/1.25235. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.47705/1.33599. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.48454/1.22054. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.49021/1.19518. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.48853/1.12828. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.47609/1.28201. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.49512/1.26896. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.47245/1.23170. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.45543/1.22431. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.45321/1.26123. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.45344/1.24758. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.43834/1.25138. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.43398/1.27323. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.44298/1.27207. Took 0.44 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69580/0.71373. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69157/0.71062. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68826/0.71622. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68296/0.74189. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68589/0.74097. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68274/0.74624. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67913/0.75382. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68408/0.76556. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67490/0.76757. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67176/0.77902. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67289/0.77541. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66677/0.76569. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66371/0.77160. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66706/0.78114. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65865/0.78714. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65527/0.81037. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.64973/0.82796. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.64886/0.81766. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.64269/0.81180. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.64446/0.84758. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64579/0.84055. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63488/0.87352. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.63356/0.87066. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.63049/0.90768. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62517/0.91417. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.61691/0.94329. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62040/0.93023. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.61483/0.96244. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.60816/0.94851. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.60571/0.95899. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.59107/1.01533. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.59636/0.97685. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.58733/1.03947. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.58565/1.03739. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.57697/1.06931. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.57560/1.08208. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.57047/1.08191. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.56522/1.12975. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.56449/1.07442. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.56938/1.05877. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56133/1.08198. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.56298/1.07450. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.54948/1.06947. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.53270/1.15918. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.53459/1.15364. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.54653/1.16607. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.53174/1.14550. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.52867/1.14707. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.53668/1.14426. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52398/1.19793. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.51204/1.18636. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.51448/1.18422. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.50843/1.18994. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.50519/1.14359. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.49799/1.19400. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.49465/1.19710. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.48329/1.21400. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.47121/1.23348. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.46238/1.22915. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.48217/1.28763. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.47455/1.18093. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.44502/1.25013. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.45764/1.20925. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.42532/1.24594. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.43896/1.30780. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.41026/1.28200. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.43018/1.24513. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.43738/1.27878. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.42096/1.33203. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.41153/1.39303. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.40178/1.43874. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.40044/1.40108. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.41341/1.31139. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.39536/1.44618. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.39081/1.37420. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.37508/1.45171. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.36831/1.42603. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.36054/1.50178. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.34530/1.52988. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.37182/1.45239. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.34204/1.50602. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.33200/1.56937. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.32979/1.57281. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.33398/1.61989. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.33770/1.54689. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.33369/1.59779. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.32405/1.63739. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.32770/1.48420. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.29823/1.72087. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.28953/1.70433. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.28227/1.69589. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.28956/1.66771. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.28356/1.75154. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.25722/1.74149. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.27776/1.76632. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.27770/1.81974. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.26230/1.94620. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.26626/1.89844. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.26241/1.81010. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.27446/1.84680. Took 0.44 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70326/0.69560. Took 0.63 sec\n",
      "Epoch 1, Loss(train/val) 0.69299/0.68367. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.68998/0.70142. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69022/0.69890. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68446/0.69789. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68727/0.70771. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68276/0.70671. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67923/0.70738. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67768/0.71640. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67210/0.72541. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67055/0.71504. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66978/0.71606. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66752/0.70824. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66377/0.71163. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.65580/0.72476. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66684/0.73301. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66135/0.72790. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66280/0.72262. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66222/0.69617. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65676/0.71203. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66016/0.70966. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65460/0.71882. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64760/0.72142. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64241/0.72811. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.64946/0.72761. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64047/0.73207. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63363/0.74191. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63879/0.75945. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63363/0.77134. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63377/0.78498. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.62854/0.79908. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.61795/0.80542. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61830/0.82193. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.61702/0.82428. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61521/0.84075. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60547/0.83157. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.61276/0.81576. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.60720/0.83404. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.60274/0.83610. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58783/0.86693. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59207/0.86784. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.59478/0.85695. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.58308/0.88791. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.55998/0.92487. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.55837/0.96340. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.57695/0.89986. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.55712/0.93226. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56088/0.91501. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.55802/0.96100. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.54910/0.93864. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53841/0.94650. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.52706/0.92882. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.51504/0.95030. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.51500/0.94408. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51467/0.98453. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.52814/0.97690. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.50709/0.97384. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.49925/0.97008. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.47709/1.00807. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.51444/0.99002. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.48641/1.00793. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.47188/1.04749. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.48040/1.04853. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.46938/1.03408. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.45905/1.03845. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.45025/1.01813. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.43547/1.02419. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.43373/1.01805. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.42109/1.01239. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.42691/1.00271. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.41528/1.02704. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.40107/0.98044. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.38569/1.00918. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.41472/1.00222. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.39945/0.96342. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.38558/1.06623. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.39529/1.08778. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.40412/1.06413. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.38303/1.05918. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.36050/1.04055. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.38046/1.07152. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.35938/1.10596. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.34470/1.09964. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.35818/1.15967. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.35040/1.11622. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.33841/1.14842. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.36158/1.15489. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.35302/1.20157. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.33418/1.16013. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.31152/1.23710. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.34113/1.26104. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.32049/1.33641. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.31365/1.30381. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.29526/1.26780. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.29280/1.27110. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.32350/1.25792. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.33071/1.27141. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.29201/1.27266. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.28543/1.35918. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.29637/1.28904. Took 0.44 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.70946/0.68939. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70462/0.68370. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70297/0.68202. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69236/0.68651. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69285/0.68820. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68831/0.69311. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68912/0.69552. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68997/0.69109. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68443/0.70018. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68000/0.71311. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68806/0.72560. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68062/0.72888. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67912/0.71512. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66881/0.73266. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66451/0.73754. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66508/0.74717. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67391/0.76313. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66762/0.76124. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65930/0.76721. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.67220/0.74656. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65514/0.75229. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65523/0.75160. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64276/0.74637. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64583/0.77121. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64955/0.76125. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.65241/0.75975. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65519/0.76968. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64766/0.76460. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.64632/0.77448. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64890/0.77273. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64724/0.77765. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.63909/0.78340. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64602/0.78257. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63582/0.78674. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.62880/0.80536. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.63376/0.79918. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.61949/0.79689. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.62256/0.79615. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62015/0.77096. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.61119/0.80985. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.61816/0.79403. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60393/0.79137. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.61319/0.79128. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.60527/0.81259. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.59737/0.83540. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.59345/0.84578. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.59549/0.82378. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.59429/0.85117. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59029/0.83817. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.58400/0.89379. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57494/0.85375. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.58339/0.90078. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.57417/0.91649. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.57238/0.93507. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56795/0.93607. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.56547/0.95216. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.55560/0.98749. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.56742/1.00190. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.57155/0.95909. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.55818/0.97027. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.54088/0.99123. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.54060/1.02318. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53593/1.06836. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.53253/1.05886. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.52902/1.11255. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.53116/1.13777. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.52601/1.13897. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.52049/1.14922. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.53162/1.06054. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.50359/1.20594. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.50879/1.20914. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.49198/1.23700. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.50140/1.25538. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.49780/1.23686. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.49338/1.28390. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.49352/1.26409. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.48493/1.29567. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.47076/1.32428. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.48026/1.29253. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.47006/1.36066. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.47032/1.29110. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.44344/1.40452. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.45907/1.33886. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.44608/1.41046. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.44034/1.39402. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.45302/1.49943. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.44538/1.50676. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.45333/1.35530. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.43728/1.41487. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.42299/1.49127. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.42485/1.54861. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.42231/1.53595. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.41854/1.45567. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.42053/1.57070. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.41214/1.49641. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.40214/1.61683. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.41914/1.45481. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.43318/1.35894. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.40110/1.42396. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.38430/1.55623. Took 0.43 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.72114/0.75576. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.71500/0.77912. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.72375/0.74671. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.71894/0.74983. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.70562/0.73889. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.70204/0.74503. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.70183/0.74247. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68886/0.73471. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.69398/0.74591. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.70080/0.74522. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68031/0.74990. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.69350/0.76051. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67865/0.75691. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68699/0.74319. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.68978/0.75398. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.68241/0.75128. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.68041/0.74179. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67308/0.73032. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.67730/0.72786. Took 0.47 sec\n",
      "Epoch 19, Loss(train/val) 0.67297/0.73236. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66835/0.73385. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.67323/0.72268. Took 0.47 sec\n",
      "Epoch 22, Loss(train/val) 0.66813/0.72339. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66524/0.72806. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.66642/0.74973. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.66537/0.75827. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65628/0.76484. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.66258/0.77519. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.64463/0.77598. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.65505/0.78235. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.64881/0.79868. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.64219/0.81063. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.63653/0.82360. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64476/0.83688. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.63157/0.85136. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.63927/0.86545. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.63513/0.85382. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.63416/0.86206. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.63781/0.84639. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.63718/0.86045. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.62235/0.83904. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.63148/0.84580. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.61611/0.86252. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.62110/0.85671. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60196/0.88889. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.59651/0.93808. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.60483/0.90144. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.59595/0.95527. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59377/0.91905. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.59003/0.94013. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.58858/0.92767. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.58775/0.94250. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.57710/0.97524. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.57463/0.94506. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.57062/0.95314. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.56539/0.99181. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.57025/0.97013. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.55737/0.98975. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.56097/0.94534. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.54405/0.97112. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.54782/0.94585. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.54437/0.96810. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.54183/1.01001. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.54411/0.99335. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.53572/1.01174. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.52035/1.00951. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.51867/1.02607. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.50388/1.05478. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.52055/1.09811. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.50645/1.08208. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.50747/1.09645. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.50048/1.08907. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.48695/1.16003. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.49743/1.14230. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.50688/1.13798. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48410/1.17026. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.49676/1.10561. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.47514/1.13062. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.47301/1.13642. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.45441/1.22751. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.45586/1.21021. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.43672/1.25944. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.44755/1.18624. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.42701/1.32936. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.45379/1.22968. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.43354/1.23809. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.42627/1.34490. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.44838/1.24080. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.42494/1.23133. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.44590/1.18438. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.41667/1.24050. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.42158/1.20426. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.44234/1.17351. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.44312/1.07430. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.40925/1.17038. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.43289/1.19972. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.41206/1.20348. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.41182/1.23794. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.41301/1.29799. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.40033/1.30715. Took 0.44 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.70077/0.72002. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69626/0.72581. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69370/0.72647. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69194/0.73509. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68693/0.73926. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68550/0.75023. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68388/0.75231. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67830/0.75544. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67660/0.76880. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67329/0.76408. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66791/0.77225. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66661/0.78393. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66773/0.78534. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.65618/0.80772. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65580/0.82122. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.65135/0.83405. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65239/0.82885. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.64657/0.84352. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.64322/0.83996. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.64078/0.84779. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63259/0.84326. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.62339/0.85214. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.62028/0.87508. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.61653/0.89722. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.60857/0.94716. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.60593/0.93519. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.60426/0.95456. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.60452/0.96179. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.59388/0.96711. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.58952/0.98824. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.58161/1.02333. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.58684/0.99995. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.57252/1.01996. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.56903/1.03215. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.55800/1.02937. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.56511/1.01366. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.54609/1.03036. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.54210/1.02634. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.53951/1.05633. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.52595/1.04428. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.53742/1.09575. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.53669/1.03493. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.51411/1.07318. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.51773/1.06816. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.51333/1.07886. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.50972/1.06776. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.49899/1.04099. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.49065/1.07639. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.48151/1.08206. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.47357/1.10140. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.47292/1.04773. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.47498/1.13453. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.49175/1.06264. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.46419/1.09828. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.48733/1.07750. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.46784/1.06864. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.44761/1.08769. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.45127/1.08550. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.43338/1.06939. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.42964/1.12554. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.40084/1.14874. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.42209/1.11920. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.38857/1.19933. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.43005/1.15553. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.40179/1.15636. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.40364/1.13807. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.38645/1.20314. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.36300/1.16956. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.39260/1.19502. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.37349/1.18195. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.37002/1.18895. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.35719/1.25624. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.34811/1.28372. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.34703/1.21599. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.34675/1.32855. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.35877/1.25386. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.34016/1.23627. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.31751/1.29441. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.31463/1.32912. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.31383/1.34497. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.32245/1.35372. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.30547/1.38037. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.30216/1.38094. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.29127/1.38166. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.27405/1.37670. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.27706/1.48172. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.27911/1.48651. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.28384/1.48172. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.26687/1.53905. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.25306/1.48366. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.23628/1.62105. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.24413/1.61660. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.24373/1.64295. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.26832/1.52217. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.23428/1.71633. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.24845/1.71331. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.26975/1.75452. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.23256/1.63250. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.23288/1.74568. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.22889/1.62275. Took 0.43 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.70496/0.71148. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70792/0.69004. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.70312/0.70906. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69639/0.69343. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69934/0.70597. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69714/0.70714. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.69009/0.70310. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67798/0.72387. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68352/0.71842. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68233/0.72488. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.69010/0.74786. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68788/0.73619. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68496/0.74770. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.68139/0.75895. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.68060/0.76072. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67868/0.77188. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.68109/0.77766. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67319/0.80445. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.67977/0.80035. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.67204/0.79619. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.67300/0.79078. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.66798/0.80277. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65969/0.80732. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.66467/0.79875. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.66451/0.81976. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66172/0.81012. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65694/0.80998. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64768/0.83329. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.64795/0.86049. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64847/0.87966. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.64576/0.88240. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.63839/0.86910. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.64263/0.85895. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63352/0.86252. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.63416/0.87699. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.62864/0.84921. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62561/0.86521. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.63001/0.85896. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62175/0.86649. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.62648/0.83565. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62244/0.84656. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.61614/0.83332. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.59951/0.85712. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.60574/0.86409. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.59843/0.90446. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.59767/0.87434. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58970/0.85511. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58335/0.88829. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58726/0.87896. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57336/0.89866. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56802/0.91062. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56819/0.89141. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.58658/0.93026. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.57130/0.90155. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.54751/0.92056. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.56446/0.93454. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55374/0.92688. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53831/0.98245. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54173/0.96417. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.54743/0.93111. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52667/0.97155. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.52537/0.95109. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.51607/0.95121. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.52174/0.94425. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.52242/0.98590. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.53226/0.88956. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51312/0.96075. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.50768/0.95272. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50642/0.90413. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.51102/0.91868. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49928/1.00262. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.49618/0.96163. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.48324/0.96692. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.51106/0.91559. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.47254/0.98021. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.47648/0.98063. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.47456/1.02098. Took 0.42 sec\n",
      "Epoch 77, Loss(train/val) 0.47664/0.96832. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.47278/0.99674. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.47677/0.97530. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.45859/0.96777. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.46464/0.91708. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.46706/0.97836. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.45454/0.95686. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.45001/0.98317. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.46212/0.94742. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.45957/0.96532. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.42741/1.00012. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.44151/1.04092. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.44516/1.04487. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.44519/0.99420. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.45621/0.94018. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.44169/0.99310. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.45978/1.02230. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.41678/1.04291. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.41414/1.06534. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.45495/1.01173. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.44174/0.99660. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.40984/1.07521. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.41667/1.04737. Took 0.43 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70262/0.69416. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69486/0.69495. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69157/0.68917. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68658/0.69790. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68872/0.69864. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68591/0.69521. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68292/0.69855. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67984/0.71412. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68046/0.71734. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68119/0.71021. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67511/0.71421. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67437/0.72099. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67617/0.71198. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67397/0.70677. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67408/0.71240. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66678/0.72793. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66996/0.71748. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66279/0.71031. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66285/0.73543. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66624/0.72020. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66443/0.72541. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65653/0.74355. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.67148/0.72412. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66334/0.72328. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65744/0.73902. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65064/0.74158. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65528/0.74166. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64200/0.76393. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.65535/0.75729. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65027/0.76112. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.63956/0.78016. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64208/0.78011. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64145/0.77199. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63455/0.78195. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62569/0.78819. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.62332/0.79165. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.62089/0.80702. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.61020/0.80135. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60137/0.83772. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.60357/0.83786. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59067/0.84120. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58414/0.85361. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.58359/0.84934. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57535/0.85193. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57226/0.86384. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56562/0.86195. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56890/0.88168. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.55285/0.90320. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.54533/0.91607. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55494/0.92840. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53894/0.94961. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54260/0.96636. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.52615/0.97379. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.51641/1.00116. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51390/0.98528. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51287/0.98961. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.52856/0.93164. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.49897/1.00942. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.48253/1.04396. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.49213/1.04598. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50243/1.03041. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.48733/1.02384. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48299/1.01437. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48313/1.04280. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.46069/1.08575. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.45639/1.09074. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.45813/1.07194. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.44713/1.07793. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.43372/1.11204. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.45078/1.13762. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.42481/1.13377. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.43330/1.09522. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.43020/1.14734. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.42923/1.17915. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.40553/1.17190. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.43950/1.16121. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.44449/1.03310. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.43287/1.05668. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.40984/1.09882. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.39428/1.14411. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.39890/1.13237. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.37452/1.13360. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.39432/1.11673. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.37266/1.18256. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.36653/1.15173. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.36003/1.19989. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.37188/1.17918. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.37849/1.22981. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.35684/1.22981. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.35874/1.24011. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.34265/1.23272. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.35312/1.22938. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.33695/1.17091. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.34130/1.23016. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.32670/1.20759. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.34983/1.15764. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.32740/1.20477. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.31570/1.17486. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.30401/1.25338. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34231/1.25338. Took 0.43 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.71706/0.71108. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69969/0.70985. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.70163/0.69578. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69878/0.69916. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68671/0.69911. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69255/0.70142. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68938/0.70249. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68982/0.71185. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68382/0.71016. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68218/0.71738. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67567/0.72116. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67527/0.72759. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66716/0.73831. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66492/0.75481. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.65712/0.78782. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66161/0.77391. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.65349/0.75975. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66091/0.76851. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65247/0.77863. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65716/0.77629. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65084/0.75957. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64556/0.77042. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.63628/0.77627. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63870/0.76540. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62843/0.76431. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.63077/0.75628. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63195/0.76702. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62625/0.76176. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.61905/0.78674. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.60864/0.80226. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.60289/0.80198. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61174/0.81738. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.60524/0.82403. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.59930/0.83021. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59383/0.86366. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.58758/0.84151. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.59331/0.84120. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.58721/0.84394. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.59000/0.85062. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.58274/0.86441. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.58289/0.88782. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.57317/0.89182. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59245/0.92040. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.57199/0.88013. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.56353/0.89056. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.56517/0.89568. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55728/0.93829. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56337/0.87759. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.54285/0.92416. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.55606/0.91730. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54387/0.91802. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.53548/0.93148. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.53692/0.98492. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.53278/0.99996. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.53448/0.96173. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51832/0.94005. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.52422/0.98290. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52329/1.03271. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.51878/0.99831. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.50248/1.06794. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50729/1.04241. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.49754/1.01852. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.48466/1.09698. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48262/1.12821. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.47169/1.17061. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.48864/1.17871. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.46793/1.14297. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.45702/1.18429. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.48563/1.15534. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46448/1.14153. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.44951/1.15327. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.45136/1.12907. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.44833/1.21089. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.45791/1.16579. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.48154/1.13177. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.42011/1.24946. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.43562/1.22421. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.42662/1.24148. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43063/1.24910. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.42843/1.29012. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.42773/1.22895. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.40068/1.18678. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.40800/1.33145. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.37755/1.34537. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43210/1.30751. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.40329/1.26239. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.37165/1.35377. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.37660/1.38013. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.36347/1.33670. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.37804/1.35362. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.35556/1.40171. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.34205/1.43530. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.35194/1.47811. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.35712/1.32097. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.37547/1.43663. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.34200/1.45226. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.32144/1.46343. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.31025/1.58096. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.34369/1.50187. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.29949/1.63367. Took 0.43 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.70785/0.69433. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70118/0.69775. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69680/0.70158. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68917/0.70972. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68975/0.70787. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69256/0.70656. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68375/0.69690. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68353/0.71175. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68093/0.71287. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68065/0.72130. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67100/0.70970. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67870/0.72167. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67257/0.72306. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67535/0.70990. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66692/0.72443. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66894/0.73369. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66475/0.74614. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66203/0.75537. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66303/0.75956. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65737/0.79857. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66130/0.78127. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65802/0.81300. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65094/0.82467. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65588/0.81756. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65277/0.82037. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64807/0.84363. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63828/0.86585. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64389/0.85730. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64606/0.88262. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63409/0.87610. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64586/0.85310. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.63275/0.88085. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64278/0.82825. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.62662/0.85739. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.62678/0.87964. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.62684/0.87376. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.62044/0.87935. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.61862/0.88402. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.61666/0.90024. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.61599/0.88671. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61380/0.85791. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60293/0.95394. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.60933/0.90401. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61463/0.88037. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60764/0.90424. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.60237/0.89435. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59543/0.87697. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.58907/0.93790. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.58942/0.91378. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.59008/0.92800. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.58487/0.94489. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.58890/0.91885. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.58363/0.92718. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.58648/0.94540. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.58650/0.89817. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.57600/0.91219. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56809/0.95481. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.57024/0.98771. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.58212/0.94008. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.56788/0.93407. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55724/0.98041. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.55914/0.94528. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54974/0.96816. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.54159/0.99762. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.55173/0.96848. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.53337/1.04292. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.55308/0.95909. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.53539/0.98494. Took 0.42 sec\n",
      "Epoch 68, Loss(train/val) 0.53242/1.01481. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.53391/1.02482. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.52341/1.01972. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.53158/0.97835. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.51391/1.06054. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.52556/1.02952. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.50575/1.11074. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49736/1.13446. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.51422/1.03780. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.49539/1.09487. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.49483/1.10266. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.52094/1.05478. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.49418/1.11106. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.45284/1.20603. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.48358/1.08267. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.46507/1.11241. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.48251/1.11553. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.47457/1.07440. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.48966/1.07425. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.47019/1.11688. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.46341/1.05281. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.47016/1.07718. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.44758/1.09466. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.46551/1.15653. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.44948/1.17364. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.42550/1.16946. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.42147/1.09159. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.42208/1.14762. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.41246/1.18158. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.44614/1.20886. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.42278/1.13607. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.40325/1.27416. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70290/0.70349. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69797/0.69651. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69011/0.70125. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69091/0.69777. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68498/0.70829. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.67921/0.70071. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68485/0.70434. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68103/0.70559. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67646/0.71353. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67410/0.72662. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66845/0.72609. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66535/0.72530. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66717/0.72716. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.66042/0.73287. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.65871/0.74109. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.65308/0.75450. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.65914/0.76286. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65247/0.74631. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64290/0.75344. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.64321/0.74888. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64069/0.75039. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64048/0.73464. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.63722/0.73697. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63170/0.75136. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62261/0.76226. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.62455/0.77251. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.62021/0.76472. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61956/0.75872. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62129/0.75487. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.60709/0.74586. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.60756/0.77464. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60436/0.76962. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.60142/0.77260. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60496/0.78887. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.59765/0.76632. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.59585/0.79759. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.58895/0.81244. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58532/0.81321. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58868/0.83626. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.57293/0.85512. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.56960/0.83924. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.57570/0.87458. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.56325/0.86175. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.55329/0.86884. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.55881/0.89533. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56618/0.90037. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55887/0.87952. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.53546/0.90140. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.53043/0.90997. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.53678/0.91978. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.53576/0.88456. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.52509/0.92140. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.52096/0.90759. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.52944/0.88780. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.50877/0.96049. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.50485/0.95483. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.51973/0.94813. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.52723/0.90324. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.50688/0.92477. Took 0.42 sec\n",
      "Epoch 59, Loss(train/val) 0.49027/0.96004. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50847/0.98236. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.49759/0.91182. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.47757/0.95967. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.48445/0.96215. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.47046/0.96944. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.46866/1.02296. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.46903/0.99195. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.44059/1.00316. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.45476/1.01600. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45563/1.02462. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.43283/1.06858. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.43611/1.12673. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.40937/1.12053. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.44577/1.07979. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.43953/1.10782. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.40132/1.13615. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.42573/1.13499. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.40044/1.13773. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.40816/1.18794. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.38926/1.15542. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.39114/1.18666. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.38808/1.24993. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.35864/1.21506. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.36344/1.27364. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.37169/1.34582. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.36083/1.26664. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.33898/1.28501. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.37029/1.32970. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.33287/1.32021. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.31668/1.36324. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.32753/1.35000. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.37223/1.38372. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.32277/1.36099. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.30862/1.37301. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.28722/1.45121. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.32224/1.43218. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.32955/1.42052. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.28320/1.53503. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.31073/1.54947. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.29651/1.53079. Took 0.43 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.71275/0.70795. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70304/0.71159. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.70403/0.70785. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69097/0.70802. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69579/0.70104. Took 0.48 sec\n",
      "Epoch 5, Loss(train/val) 0.68670/0.70711. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68739/0.70872. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68508/0.72121. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68136/0.73822. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67635/0.72337. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68182/0.74130. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68132/0.73808. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66806/0.73730. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67376/0.75216. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67243/0.75866. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66734/0.75648. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67253/0.76483. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67580/0.74851. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66415/0.77435. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66736/0.77392. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66337/0.78524. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.66595/0.80048. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66358/0.80154. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.66032/0.79613. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65279/0.81070. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65699/0.81543. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65596/0.83015. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64888/0.83302. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.65214/0.80154. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.64789/0.82066. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.64381/0.83476. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.64478/0.85461. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63548/0.85007. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63504/0.88693. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.62985/0.90058. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.63206/0.90398. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62301/0.89315. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.62506/0.89960. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.61984/0.93702. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61905/0.93525. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.61999/0.95323. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.62240/0.95132. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60096/0.98957. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61402/0.97722. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59659/1.02435. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.59100/1.02700. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.59773/1.05173. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.59273/1.03571. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57676/1.09082. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.58381/1.07445. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.58039/1.12496. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.59064/1.08458. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.56899/1.11480. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55706/1.11452. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55977/1.11449. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.56565/1.13331. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56722/1.15400. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.57090/1.09182. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54563/1.14663. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.52732/1.19452. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.54638/1.16261. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.57542/1.08055. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53220/1.16262. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.53169/1.14518. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.52897/1.18197. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.51421/1.15368. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51528/1.14122. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.50842/1.19437. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50742/1.13457. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.52441/1.16893. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.52219/1.14432. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.52070/1.14095. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.52708/1.14504. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.50856/1.16671. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.48774/1.21615. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.47759/1.16211. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.48590/1.15553. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.48691/1.16672. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.47418/1.21327. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.47289/1.20637. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.47445/1.12217. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.46531/1.18797. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.45593/1.26494. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.46213/1.23306. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.43452/1.24032. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.44971/1.26743. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.42343/1.31073. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.43474/1.28177. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.42296/1.33608. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.41066/1.39120. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.41520/1.45127. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.42724/1.29667. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.42034/1.33114. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.40564/1.33176. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.39938/1.38622. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.39647/1.44818. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.39261/1.42989. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.40178/1.34966. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.38583/1.44610. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.37039/1.39209. Took 0.44 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.71107/0.73301. Took 0.62 sec\n",
      "Epoch 1, Loss(train/val) 0.70515/0.71392. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70193/0.70943. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69605/0.72046. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69502/0.72021. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69319/0.72723. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69783/0.72643. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68560/0.72917. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.69093/0.72681. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.69185/0.71944. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68434/0.72397. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.68021/0.72181. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.69140/0.72821. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.69019/0.73854. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.68377/0.73509. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68169/0.72843. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67580/0.74347. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67868/0.73537. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.67021/0.74319. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67304/0.74533. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.68080/0.75129. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.67399/0.75948. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66714/0.75833. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66660/0.77614. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.66424/0.77549. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.67229/0.78184. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65870/0.79110. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.66159/0.78636. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.65998/0.80557. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.65423/0.77808. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.65467/0.80933. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.64625/0.79903. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.64695/0.79083. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64025/0.79885. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.64411/0.81704. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.63665/0.80886. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.64420/0.83568. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.63457/0.84059. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62611/0.81542. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.62917/0.81471. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.63456/0.80304. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.63636/0.80887. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.62153/0.81803. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.60441/0.82593. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.61495/0.83855. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.60997/0.85493. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.60773/0.85637. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.60870/0.87877. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59585/0.85705. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.59366/0.89282. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.58596/0.89987. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.59239/0.88078. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.59482/0.88506. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.57819/0.93450. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56814/0.97781. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.57858/0.97574. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56941/0.95122. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.55483/0.98944. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.55327/0.98378. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54641/0.99582. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55539/1.03654. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.55179/1.01996. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54526/1.08507. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.53210/1.09920. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.53540/1.10450. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.52111/1.07603. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.51967/1.08992. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.52093/1.12915. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52395/1.09567. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.51994/1.08739. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.52021/1.16283. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.51296/1.12540. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.52177/1.15392. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.50235/1.17685. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.51436/1.14625. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49747/1.19987. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.49595/1.20499. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.49897/1.19271. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.47996/1.28124. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.49513/1.25915. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.48675/1.25368. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.50320/1.24135. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.46833/1.28299. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.47259/1.33081. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.47268/1.26633. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.45845/1.21801. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.44649/1.26270. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.44946/1.36370. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.44341/1.33093. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.45756/1.41243. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.43954/1.41934. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.43150/1.45567. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.44429/1.39349. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.44061/1.40693. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.44201/1.30654. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.43352/1.29585. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.44002/1.30474. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.41034/1.38178. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.42454/1.41104. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.42366/1.39641. Took 0.43 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.70289/0.70304. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69125/0.72119. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68872/0.72313. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68590/0.72531. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69345/0.73034. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68120/0.75366. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68626/0.73611. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68933/0.74108. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68163/0.73938. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68551/0.72672. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67966/0.74526. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68520/0.74923. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68202/0.74920. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67711/0.76255. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66737/0.76133. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66858/0.74385. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67067/0.73741. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66313/0.75989. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66022/0.78097. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67107/0.76379. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65060/0.78799. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65307/0.79196. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66463/0.77944. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64689/0.79309. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.64167/0.81472. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64282/0.80542. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63438/0.83461. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63264/0.84069. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.63900/0.85849. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62150/0.84653. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.61708/0.87362. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61001/0.88126. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61308/0.89140. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61665/0.86176. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60774/0.90883. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60892/0.90506. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59163/0.91850. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.59617/0.91130. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.60165/0.90017. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59751/0.89663. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58956/0.92485. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.58685/0.89615. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58278/0.94021. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59367/0.92832. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.57574/0.93557. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57113/0.94049. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57024/0.93122. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.56270/0.94923. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.55329/0.97029. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56392/0.96604. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.55603/0.95343. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.55347/0.95760. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.53854/0.99215. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.54908/0.98282. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.54184/1.00286. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.53589/1.00677. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.51705/1.00956. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53242/1.03422. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.51415/1.06724. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.51570/1.04110. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.50573/1.05204. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.50202/1.05423. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.52266/1.04209. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.49895/1.07716. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.50556/1.08546. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.48372/1.10000. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.50413/1.10624. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.49816/1.11759. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.47351/1.10451. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47425/1.13431. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48790/1.03793. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48267/1.12405. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.46164/1.16491. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.46320/1.16878. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.45740/1.20485. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.45003/1.21527. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.44544/1.25433. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.44091/1.27992. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.47119/1.17924. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.42954/1.30978. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.44395/1.25108. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.43155/1.28643. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.43659/1.21952. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.41554/1.35346. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.42482/1.34162. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.41560/1.34531. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.39427/1.45355. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.42468/1.33266. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.41151/1.41549. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.39280/1.47317. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.39290/1.34667. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.38664/1.42622. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.39522/1.40199. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.41356/1.47662. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.39351/1.32412. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.37707/1.41536. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.38404/1.36966. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.38293/1.47081. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.38602/1.36377. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36012/1.40580. Took 0.44 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.71081/0.68669. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70395/0.71376. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69792/0.70937. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69261/0.70014. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69333/0.71099. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69024/0.71729. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69405/0.72106. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68932/0.71905. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68767/0.71276. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68878/0.71505. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68149/0.70544. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.68577/0.71597. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67885/0.71132. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67964/0.72333. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67576/0.73224. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67782/0.74250. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67330/0.74804. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67830/0.76211. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67266/0.77104. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67325/0.78527. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66681/0.79095. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66556/0.80362. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66657/0.80304. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.66565/0.81446. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.66300/0.83739. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.66654/0.83037. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65074/0.85137. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64686/0.84049. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.64739/0.84123. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.64476/0.84576. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63584/0.85274. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.63673/0.85100. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63324/0.86459. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63469/0.85554. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.63798/0.86866. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.62521/0.87486. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62870/0.86995. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.62719/0.87540. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.63699/0.85324. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.62404/0.88121. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.63062/0.83293. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.62458/0.84424. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.62335/0.86566. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61412/0.85981. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.61025/0.83934. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.61458/0.86323. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.60999/0.84916. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.59483/0.87827. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.60249/0.83796. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.60344/0.86370. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.58944/0.90871. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.59441/0.86355. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.57829/0.92926. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.58034/0.89752. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.57895/0.95125. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.56203/0.90738. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.57910/0.88510. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.55478/0.95847. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.55331/0.93689. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.54583/0.94391. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.56502/0.97822. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.54113/0.94758. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.53846/1.00586. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.52036/1.01365. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.52670/0.99402. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.52654/0.97253. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.52772/1.05232. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.54147/1.02697. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.51659/1.00209. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.50790/1.04388. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.52087/1.03428. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.51957/1.03340. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.50319/1.08743. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.49424/1.15718. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.47676/1.10701. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.47942/1.13149. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.48860/1.15668. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.47685/1.18289. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.49326/1.22870. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.49263/1.15674. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.47854/1.18479. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.46273/1.27786. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.44503/1.19777. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.44009/1.25262. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43789/1.28075. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.43450/1.26868. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43088/1.34166. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.42063/1.29415. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.41624/1.39774. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.42318/1.31559. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.41700/1.32548. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.42275/1.51121. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.44015/1.27463. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.42544/1.41599. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40701/1.53186. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.38395/1.52534. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.39471/1.56746. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.39009/1.53962. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.38416/1.51374. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.38704/1.60946. Took 0.43 sec\n",
      "ACC: 0.375\n",
      "Epoch 0, Loss(train/val) 0.69986/0.69508. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69179/0.68874. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69221/0.68678. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.68960/0.69276. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68870/0.69811. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68656/0.69868. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68573/0.70618. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68690/0.70669. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68424/0.70767. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68363/0.70755. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68097/0.70737. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67866/0.70738. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67831/0.71802. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67429/0.70272. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67019/0.70537. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66886/0.70658. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66425/0.69984. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66185/0.70235. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65755/0.71378. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66188/0.73326. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65500/0.73022. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.65320/0.76569. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64979/0.76572. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64408/0.80034. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63283/0.82400. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.63450/0.85676. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63019/0.85731. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63576/0.86472. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.63326/0.84865. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61586/0.85284. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61284/0.89496. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.61801/0.89966. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.60804/0.91381. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62782/0.88002. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.62193/0.86618. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60356/0.87559. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.60587/0.86406. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.59631/0.88077. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.60264/0.87451. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.57979/0.87004. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.58139/0.87572. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.56592/0.90570. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.57911/0.90607. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57468/0.84710. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.58596/0.82378. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55977/0.87811. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.57116/0.86576. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.56459/0.83263. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.56305/0.91534. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.56005/0.86848. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.54867/0.82053. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.56746/0.84915. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56296/0.82196. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.55681/0.86542. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54784/0.87174. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.53646/0.85744. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.53153/0.85720. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.53268/0.85069. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.52624/0.89659. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.51800/0.95203. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51290/0.94511. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.51235/0.94248. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.51139/0.92009. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.50267/0.91747. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.49395/0.94748. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.50902/0.97020. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.48857/0.91584. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.49435/0.99025. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.47371/0.99100. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.47542/1.06762. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.45750/1.06045. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.49233/1.00163. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.45472/0.94916. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.47102/1.05267. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.45478/1.00651. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44823/1.01050. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.44115/1.07966. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.43976/1.06516. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.44942/1.06063. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.43739/1.00177. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.44255/1.00857. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.44386/1.11949. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.42871/1.04829. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.41567/1.18680. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.40629/1.07170. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.39873/1.10042. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.41072/1.03742. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.42963/1.07628. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.41853/1.03999. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.41164/1.17803. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.41465/1.13667. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.38707/1.14448. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.38756/1.17284. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.39215/1.13466. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.38135/1.12689. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.39288/1.16855. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.42263/1.07069. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.38682/1.08452. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.37801/1.16634. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.37975/1.17291. Took 0.43 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.70084/0.70398. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70054/0.70063. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69804/0.69549. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69029/0.69277. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69011/0.69422. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69435/0.70534. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68711/0.69940. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68890/0.71344. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68831/0.72252. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68589/0.72503. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68129/0.72164. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.68409/0.74391. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67963/0.71672. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67540/0.73481. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67867/0.72709. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67577/0.72752. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67212/0.74750. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67167/0.73822. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67142/0.74295. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66958/0.73931. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66460/0.75703. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.66337/0.73604. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66905/0.72660. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.66276/0.72161. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.67109/0.71258. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66372/0.72471. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.66064/0.74336. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65243/0.72861. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65354/0.74304. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.64557/0.73222. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.64662/0.73733. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64152/0.73220. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.65289/0.73813. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63698/0.71752. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.63230/0.74328. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62839/0.78095. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62842/0.77556. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62554/0.81182. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.62730/0.74945. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.62101/0.77802. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.61856/0.77073. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.61434/0.80598. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.61845/0.76396. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.60666/0.78196. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.59939/0.78005. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.59648/0.80538. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.59805/0.80300. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.58611/0.82390. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.57767/0.86855. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.58045/0.85529. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.56893/0.89435. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55505/0.91076. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.56928/0.84703. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.55165/0.91421. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55394/0.93109. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55592/0.90807. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.54235/0.93364. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.54034/0.95670. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.50789/1.02153. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.52890/0.98203. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.52440/1.01970. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50806/1.03163. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.50957/1.07936. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.50985/1.07410. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.48444/1.15829. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.50164/1.11389. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.47997/1.16605. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.48224/1.19594. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.47417/1.19619. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.46533/1.29172. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.47784/1.32646. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.46279/1.27469. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46963/1.25877. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46995/1.27042. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.45920/1.31097. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.46023/1.27250. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.42995/1.33456. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.44491/1.30382. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.43191/1.33202. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.41820/1.34098. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.41153/1.39717. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.42120/1.41814. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.44119/1.31331. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.48958/1.28082. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.44567/1.28704. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.41128/1.41550. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.42315/1.40182. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.38817/1.48081. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.38137/1.51553. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.38708/1.56206. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.39276/1.56400. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.40664/1.60323. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.38329/1.53815. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.37450/1.68358. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.42246/1.50369. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.37262/1.54179. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.37984/1.47177. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.37019/1.45006. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.36654/1.57115. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.34336/1.64120. Took 0.43 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.72444/0.71776. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69201/0.71148. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.70298/0.76611. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68174/0.75575. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.70127/0.73146. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69187/0.74951. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.69049/0.75630. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68834/0.75190. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68989/0.77303. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68613/0.75346. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67669/0.76729. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68803/0.77143. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68453/0.75858. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.68117/0.76378. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67739/0.75656. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67327/0.75390. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66838/0.76067. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67286/0.77720. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.67296/0.75902. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66194/0.75360. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66876/0.78873. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.66274/0.79643. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66019/0.80189. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66342/0.81199. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66084/0.79329. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65540/0.79874. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64587/0.80650. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65461/0.82658. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63633/0.84312. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.64294/0.83234. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63104/0.82758. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.64975/0.82711. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63226/0.86287. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.62661/0.85158. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.62722/0.85066. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.62626/0.87707. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62565/0.90494. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.63004/0.91001. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.62321/0.91573. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.62442/0.90324. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.61798/0.90695. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.62186/0.93057. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.63556/0.91999. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.62991/0.92121. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.61616/0.92602. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.61248/0.94847. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.61119/0.94939. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.61101/0.92834. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.60536/0.94810. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.59671/0.95701. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.58863/0.99968. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.58914/0.98436. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.58001/1.00572. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.58742/1.01381. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.58101/1.00091. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.57633/1.01987. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.58703/1.00190. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.57708/0.97536. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.56663/0.95514. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.56809/1.00119. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.56540/1.06914. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.55407/1.02841. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.55224/1.00925. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.55645/1.03841. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.54336/1.04679. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.54803/1.07795. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.54319/1.09845. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.54478/1.04326. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.53213/1.02420. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.53762/1.06746. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.54141/1.07771. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.51508/1.19559. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.52414/1.14214. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.54663/1.05495. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.52502/1.14215. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.52522/1.18031. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.50949/1.15017. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.51329/1.05652. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.52337/1.09933. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.50726/1.16151. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.50017/1.18421. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.49175/1.13515. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.50728/1.12922. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.48239/1.18822. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.48313/1.28619. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.50069/1.18586. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.50009/1.18772. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.50261/1.23329. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.47833/1.27882. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.47458/1.21331. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.47413/1.23236. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.47129/1.24907. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.46871/1.18074. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.46441/1.25989. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.46835/1.22177. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.45739/1.28363. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.47249/1.22064. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.45095/1.21728. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.45029/1.27731. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.42903/1.38828. Took 0.44 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.73118/0.76976. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.71870/0.78541. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.71705/0.75680. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.70415/0.74672. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.70109/0.76208. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.70393/0.74880. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69666/0.74721. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.70517/0.74072. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.68775/0.75778. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68672/0.75434. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68721/0.76345. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.69536/0.74973. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.68601/0.75426. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67672/0.76504. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67032/0.79178. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67166/0.79183. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67417/0.82199. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66620/0.81110. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66504/0.82044. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65619/0.81508. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65487/0.81777. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64432/0.84007. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65410/0.85381. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.65665/0.88826. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64116/0.86521. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63379/0.89242. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64142/0.91879. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63630/0.92005. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62374/0.95728. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62953/0.93890. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.61815/0.97446. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61155/0.98485. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61002/1.02572. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.62375/0.99777. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.60983/1.04528. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60515/1.05831. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.59936/1.09256. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60450/1.15045. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59482/1.12225. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.59856/1.12778. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.59138/1.15075. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.58072/1.13654. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58179/1.16536. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58622/1.18290. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.56591/1.22952. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57162/1.26090. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.57040/1.23467. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.57129/1.26322. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.56949/1.17364. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55934/1.16321. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56106/1.23777. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.55941/1.19533. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.55396/1.26964. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.55206/1.19042. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.54259/1.18661. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55377/1.22394. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.54403/1.20430. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53181/1.30220. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.54283/1.22029. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.54137/1.30938. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.52894/1.26654. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.51832/1.24971. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52436/1.25456. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.51916/1.35196. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.51060/1.32815. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.51285/1.29648. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.49558/1.28275. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.49518/1.29963. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.49972/1.28593. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.50198/1.36985. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.50530/1.26100. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.50054/1.33918. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.50173/1.37145. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.51981/1.18703. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.50021/1.31777. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.48000/1.32170. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.49133/1.33064. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.49356/1.30021. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.49862/1.23658. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.47505/1.36683. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.49293/1.34412. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.47323/1.34698. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.46707/1.39389. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.47936/1.36152. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.46453/1.39872. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.47164/1.44095. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.46150/1.42799. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.47861/1.36810. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.47370/1.42508. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.46134/1.35186. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.47716/1.37438. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.43584/1.40861. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.43034/1.49588. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.44038/1.59159. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.42997/1.53147. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.43646/1.50993. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.44024/1.56110. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.42273/1.56581. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.43682/1.59506. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.42398/1.55676. Took 0.43 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69864/0.68691. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69325/0.69030. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.68794/0.70882. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68478/0.70367. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.67739/0.70034. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68081/0.70264. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.67919/0.70690. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67456/0.70652. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67955/0.70655. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67232/0.70846. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67370/0.70278. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67021/0.70327. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66304/0.71377. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.65760/0.71556. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66000/0.71344. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.65817/0.72762. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.65989/0.72781. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65632/0.75931. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.64810/0.76831. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64184/0.77305. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64054/0.78712. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.63569/0.79550. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.63140/0.80347. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.62430/0.79535. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62150/0.79528. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.62840/0.81040. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.61959/0.81178. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61777/0.78650. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60714/0.81401. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.60592/0.83626. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.60472/0.84214. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60066/0.84684. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59321/0.83120. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58595/0.85446. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59612/0.84273. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.58166/0.82459. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.58154/0.86153. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57032/0.84935. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.57091/0.85082. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.56421/0.85159. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.56111/0.82497. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.55870/0.81508. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.54048/0.84108. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.54838/0.84703. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.54392/0.83978. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.52885/0.85720. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.52291/0.94562. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.52759/0.93926. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.52150/0.92751. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.51765/0.94929. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.49582/0.94752. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.51233/0.91671. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.49750/1.00057. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.50402/0.96507. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.50395/0.96204. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.48051/0.95559. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.47388/0.99283. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.48169/0.99136. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.49162/0.99536. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.50847/0.95878. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.46949/0.99696. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.47040/1.02849. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.46389/0.99437. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.46111/1.01672. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.45517/0.98239. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.47643/1.02301. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.42679/1.07304. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.45671/1.09299. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.44868/1.03426. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.43733/1.09158. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.44138/1.02991. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.44517/1.07314. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.43113/1.08065. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.43765/1.08958. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.41026/1.05564. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.40626/1.10833. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.41586/1.12763. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.42409/1.11667. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.38960/1.15257. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.39645/1.17059. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.38502/1.14284. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.39956/1.17403. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.39030/1.12924. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.38432/1.13840. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.37104/1.24386. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.38422/1.20298. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.36434/1.24224. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.37903/1.21394. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.39193/1.18730. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.39452/1.21074. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36522/1.20883. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.35684/1.23493. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.33540/1.29897. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.34204/1.36087. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.33484/1.31630. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.31783/1.33881. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.33356/1.41592. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.31809/1.40333. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.27988/1.42038. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.30918/1.48032. Took 0.43 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.70721/0.68146. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.70569/0.67599. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69845/0.68643. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69250/0.68999. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69703/0.69696. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69323/0.69888. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68444/0.70337. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68436/0.70275. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.69231/0.69586. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68434/0.70004. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68611/0.69300. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67357/0.68161. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67657/0.69279. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67757/0.69637. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67826/0.70392. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67082/0.69646. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66978/0.69371. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66912/0.69046. Took 0.42 sec\n",
      "Epoch 18, Loss(train/val) 0.66674/0.69510. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67217/0.70629. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66395/0.71039. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66290/0.70498. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66302/0.70333. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66247/0.71456. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65427/0.72917. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.65590/0.73049. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65036/0.73867. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65451/0.77432. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64933/0.73336. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.64021/0.74852. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.63837/0.77332. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63309/0.77680. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.64055/0.75182. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.63990/0.74767. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.64549/0.77259. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.64024/0.76055. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.64772/0.75743. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.63316/0.74978. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.63315/0.78258. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.62331/0.77899. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.62609/0.77353. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.62148/0.77089. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.62068/0.76617. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.61343/0.77792. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.60638/0.79620. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.60543/0.79161. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.60670/0.79190. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.58887/0.80823. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59933/0.79283. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.59305/0.78211. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.59418/0.79374. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.59082/0.79019. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.57946/0.81672. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.57552/0.81402. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.58453/0.82135. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.57970/0.79632. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.58011/0.77887. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.57579/0.78414. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.56640/0.80531. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.55850/0.82645. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.55588/0.80406. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.57370/0.82303. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.55817/0.81358. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.54634/0.83592. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.55329/0.86050. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.54394/0.84034. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.56255/0.82359. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.56588/0.82492. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.55471/0.79771. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.53984/0.84764. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.54104/0.81723. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.54765/0.82053. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.53685/0.82927. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.54448/0.84982. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.52993/0.83990. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.52970/0.82260. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.51727/0.87705. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.52610/0.86364. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.52983/0.86404. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.51814/0.93161. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.51033/0.91256. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.52145/0.96013. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.50394/0.89319. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.50445/0.95575. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.49231/0.95788. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.48215/0.94699. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.49695/1.00977. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.50284/0.99661. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.48370/0.97028. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.50707/0.96891. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.50399/0.98322. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.48222/0.99057. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.47942/1.02292. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.48755/0.96115. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.47717/1.04347. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.45972/1.08028. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.46193/1.04692. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.45946/1.08732. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.46607/1.08918. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.45822/1.02102. Took 0.44 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.71396/0.70675. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69807/0.69915. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69329/0.69551. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.68942/0.69711. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68458/0.68650. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.67259/0.69605. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68094/0.69438. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67845/0.69537. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67286/0.71666. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67176/0.71090. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66943/0.73320. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66525/0.71906. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66292/0.72360. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65938/0.73023. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65470/0.73075. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.64942/0.72384. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65198/0.73306. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64426/0.73677. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.63626/0.76190. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.63076/0.76713. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.62063/0.74615. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.62426/0.76480. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.61472/0.77929. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.62202/0.77527. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.61364/0.77923. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.60905/0.78754. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.60224/0.78271. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.60931/0.79553. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.59586/0.79132. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.59336/0.83148. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.58079/0.83143. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.56816/0.88452. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.57309/0.86824. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.56310/0.87140. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.55790/0.93005. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.55712/0.88915. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.55937/0.91419. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.55246/0.92664. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.54812/0.91011. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.54498/0.97204. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.52520/1.01709. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.52298/1.01484. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.53321/1.03629. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.51510/0.95661. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.52233/0.99926. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.50975/0.93491. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.50446/0.95887. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.51021/0.95851. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.49507/0.98903. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.50061/0.90137. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.48058/0.95687. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.49692/0.98551. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.49021/0.96372. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.47291/0.99601. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.46499/0.98372. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.45863/0.99519. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.45827/0.99503. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.47320/0.97923. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.46899/1.00640. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.44688/0.98200. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.44635/0.97936. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.46311/0.99330. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.44750/1.00165. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.42920/1.04594. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.43976/0.99897. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.45582/0.96707. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.43514/1.04376. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.42842/1.04605. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.39839/1.02962. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.42489/1.02992. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.39649/1.09201. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.42175/1.05214. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.42585/1.08137. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.44072/1.08590. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.43387/1.09867. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.41626/1.13687. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.42557/1.07084. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.40857/1.07003. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.38860/1.10751. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.36838/1.09941. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.39435/1.21942. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.40535/1.15276. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.38551/1.22284. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.38347/1.08625. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.38159/1.11248. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.37260/1.18906. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.38568/1.17856. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.37069/1.27132. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.36452/1.15235. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.36735/1.16579. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36282/1.29185. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.36919/1.26078. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.36260/1.22993. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.34751/1.23827. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.34446/1.23171. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.35671/1.31280. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.33097/1.32026. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.33627/1.45330. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.33489/1.36178. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.34514/1.28975. Took 0.43 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.71471/0.67767. Took 0.65 sec\n",
      "Epoch 1, Loss(train/val) 0.70134/0.65503. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69308/0.65471. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.68831/0.66683. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69339/0.66550. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69012/0.66389. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69377/0.65955. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68294/0.66007. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68392/0.66741. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68097/0.65807. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67529/0.67064. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.68070/0.66877. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67741/0.67755. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67315/0.68162. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67400/0.68881. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67126/0.69407. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67384/0.68808. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66325/0.70327. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66359/0.70764. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66260/0.71923. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66622/0.73884. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.67085/0.72887. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65630/0.75089. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66217/0.75238. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65676/0.74286. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65108/0.77178. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65509/0.78031. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64910/0.79040. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.64685/0.81049. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.65287/0.78606. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.64721/0.81896. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.64576/0.80815. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.63730/0.81846. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.63565/0.81875. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.63478/0.83402. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.63982/0.84285. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.63694/0.81845. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.63099/0.81387. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.62205/0.83261. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.61760/0.85573. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.61985/0.84152. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61353/0.86879. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60805/0.87916. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.62098/0.85630. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.60295/0.87337. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.60337/0.89264. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59954/0.90119. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.61328/0.91621. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.58952/0.90924. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.58233/0.92286. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.57362/0.95694. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.58341/0.92647. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.56804/0.95014. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.57466/0.96564. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55780/0.93998. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.55695/0.93113. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.55305/0.94442. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54453/0.94129. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.54996/0.98461. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53485/1.00819. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.54133/0.99376. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.54149/0.99743. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53829/0.97740. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.51916/1.01280. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.52188/1.04554. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.51528/1.06147. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.52277/1.05091. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.52582/1.06941. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.49959/1.04872. Took 0.42 sec\n",
      "Epoch 69, Loss(train/val) 0.49626/1.04169. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49845/1.10638. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.48665/1.12841. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.49840/1.06695. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.49378/1.11405. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.49897/1.08647. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.48859/1.06899. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.46556/1.19526. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.46987/1.08692. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.47064/1.23727. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.46611/1.16670. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.47276/1.15189. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.45186/1.17752. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.45421/1.20636. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.42036/1.24356. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43747/1.28259. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.42668/1.27976. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.41180/1.33162. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.44930/1.23928. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.45483/1.12038. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.44171/1.16863. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.43716/1.18336. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.42039/1.18385. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.41141/1.22124. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.47469/1.17395. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.44709/1.12081. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.44281/1.11291. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.41342/1.21300. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.39826/1.24568. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.41380/1.21657. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.39920/1.24216. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70286/0.69786. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69859/0.70258. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69648/0.70341. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69471/0.70675. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69356/0.70997. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68909/0.70205. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68588/0.68862. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.68275/0.69554. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68181/0.70350. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67920/0.71591. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67135/0.72785. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66992/0.72712. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66664/0.71662. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65763/0.70866. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.65792/0.71672. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.65315/0.72394. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.64897/0.72575. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.63742/0.74424. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.63550/0.76379. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.62656/0.78943. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63208/0.80892. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.62586/0.79749. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.60849/0.82611. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.61801/0.84770. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.61251/0.85217. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.60722/0.88038. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.58754/0.89943. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.58175/0.90228. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.57987/0.94474. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.56887/0.95050. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.57754/0.95327. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.58012/0.97438. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.56280/0.93420. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.54988/1.01822. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.55156/1.04286. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.53637/1.00272. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.55512/1.05305. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.53807/1.03473. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.52732/1.06250. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.51346/1.10567. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.52783/1.06685. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.50563/1.08391. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.50588/1.05794. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.50566/1.04514. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.49617/1.07618. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.48855/1.08175. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.48359/1.03697. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.45507/1.10433. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.47358/1.11835. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.48407/1.17698. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.45739/1.19491. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.44374/1.15796. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.46475/1.08467. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.45084/1.12344. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.45045/1.05239. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.44412/1.14724. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.41977/1.14026. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.44006/1.17147. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.43764/1.18337. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.41537/1.25289. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.43210/1.10317. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.40313/1.15429. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.40272/1.10479. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.38631/1.24281. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.36670/1.19144. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.38115/1.16856. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.38352/1.10929. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.35788/1.17881. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.38079/1.16349. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.38235/1.22836. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.37052/1.30253. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.36673/1.17507. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.36715/1.13699. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.34449/1.21325. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.35190/1.13205. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.35153/1.24341. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.32371/1.30493. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.31588/1.25424. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.30971/1.38503. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.28229/1.34210. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.32721/1.35135. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.31880/1.38492. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.29490/1.43541. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.30074/1.49343. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.30992/1.33482. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.29242/1.57134. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.31377/1.55301. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.32271/1.47216. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.26267/1.52217. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.27181/1.46030. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.27482/1.57318. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.29972/1.63909. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.27772/1.52304. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.25340/1.60273. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.28612/1.57667. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.26404/1.69202. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.26770/1.54405. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.24282/1.65253. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.27103/1.60841. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.23991/1.77496. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.71094/0.70580. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.70447/0.69587. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69746/0.70340. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69145/0.70276. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69419/0.70489. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68894/0.71568. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68843/0.70561. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68424/0.71422. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67791/0.71373. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68146/0.71971. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67465/0.73303. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67908/0.72442. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66538/0.73986. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66551/0.76713. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66369/0.81457. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66327/0.78649. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.65366/0.80730. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64490/0.81988. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64475/0.80678. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.63716/0.80179. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63654/0.83574. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63201/0.84663. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.63060/0.83441. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62686/0.82301. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.62411/0.81820. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.62829/0.82318. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61941/0.80365. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61893/0.83435. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.61556/0.81276. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.61070/0.81606. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.60265/0.82337. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.59068/0.81243. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.59982/0.81246. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59735/0.83148. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.59347/0.84600. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.57736/0.85031. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.57491/0.86774. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.58403/0.86768. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57553/0.88093. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.57872/0.88713. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58208/0.91062. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.56659/0.92073. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56871/0.91964. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.55794/0.91924. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.56523/0.88278. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57374/0.82860. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55828/0.86615. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56766/0.89389. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55035/0.89011. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54747/0.89259. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.55172/0.91617. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.52690/0.87798. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54101/0.88601. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.53444/0.91365. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.53992/0.92525. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.53627/0.90489. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.52278/0.91385. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52482/0.92772. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.52152/0.93391. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.51392/0.92145. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51169/0.93358. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.50368/0.91814. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51746/0.91498. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51191/0.89667. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.48688/0.88819. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.48250/0.93715. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.48409/0.90671. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.49483/0.92912. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.50801/0.92865. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.50009/0.90727. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.47507/0.95146. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.45086/0.92608. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.45708/0.92200. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.47568/0.88933. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.45166/0.91979. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.45555/0.94799. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.45347/0.97053. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.44444/0.91941. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.46614/0.93156. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.44862/0.91369. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.44259/0.91341. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.44337/0.88762. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.42623/0.93046. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.42173/0.90288. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.41739/0.91400. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.42981/0.95777. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.42138/0.95350. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.40172/0.97020. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.41574/0.97705. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.41748/0.96702. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.40390/0.95695. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.41291/0.88857. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.39645/0.91622. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.38852/0.95727. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.39189/0.94900. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.37476/0.98863. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.37453/0.93768. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.36065/0.99017. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.36830/0.97237. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.35443/1.00934. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.70140/0.71492. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.68665/0.70551. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.69252/0.71737. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68634/0.73327. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68468/0.73349. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68445/0.73121. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.67810/0.73448. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67401/0.74329. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67680/0.74464. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67819/0.75222. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68192/0.75718. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.68582/0.76276. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67452/0.76480. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67221/0.76971. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66904/0.77105. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66219/0.78110. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66305/0.76985. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65595/0.80196. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66554/0.78504. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65679/0.79282. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65383/0.79744. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64898/0.79231. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65970/0.79038. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64067/0.78807. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.64320/0.79449. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64276/0.82469. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63797/0.84905. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.62875/0.85849. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62777/0.85528. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.63124/0.87736. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.62478/0.86970. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.61146/0.89192. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61289/0.92258. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.61037/0.92485. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.60442/0.94462. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60418/0.96841. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60432/0.90864. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.59965/0.96714. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.59414/0.90310. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.58549/0.98386. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.57699/0.97458. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.57868/0.97009. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57646/0.95782. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57808/0.96522. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.56224/0.99925. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.56140/0.99731. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.55896/0.97531. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.53568/1.01573. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.55324/1.02218. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52930/1.03531. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.52748/1.07212. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.52991/1.07921. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.52380/1.10653. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52004/1.08581. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.51984/1.05339. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.49784/1.08732. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.50301/1.11666. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51044/1.09105. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.48559/1.08786. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.48546/1.11119. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50135/1.13566. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.47570/1.10887. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.46371/1.10544. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.46551/1.12399. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.46008/1.06599. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.46335/1.11850. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.45637/1.11964. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.44497/1.09337. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.45107/1.04190. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.42598/1.05415. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.42218/1.17482. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.40942/1.13307. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.43398/1.09828. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.42062/1.13008. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.40790/1.15126. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.40213/1.11092. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.41004/1.10510. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.38914/1.13588. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.41456/1.13388. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.38332/1.15203. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.36386/1.14971. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.38059/1.16066. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.36309/1.23193. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.39794/1.19432. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.37407/1.18830. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.34896/1.27028. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.35720/1.26560. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.34938/1.26553. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.34574/1.21968. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.32710/1.32042. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.34463/1.28214. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.31972/1.35251. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.32900/1.31078. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.32953/1.36268. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.31590/1.44179. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.30823/1.39789. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.30331/1.36210. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.30318/1.31140. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.29500/1.44281. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.30865/1.37729. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.71817/0.68982. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69701/0.68658. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.69289/0.69742. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69780/0.71098. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69154/0.69010. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.70043/0.70045. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.69439/0.70827. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69054/0.70611. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68178/0.70339. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67945/0.71100. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68074/0.69194. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.68374/0.69021. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67969/0.68751. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67731/0.70622. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67797/0.69346. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67680/0.69042. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66734/0.70432. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66485/0.71804. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66207/0.72030. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66287/0.72080. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65906/0.71750. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.65948/0.72863. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64551/0.74104. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65119/0.74696. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65951/0.73136. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64985/0.75910. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64620/0.78216. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64991/0.77400. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63239/0.78910. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63902/0.81311. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63181/0.81579. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.63435/0.80849. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62301/0.82356. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63678/0.82631. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62020/0.84078. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62219/0.83210. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62265/0.84188. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.61768/0.84271. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.61581/0.83712. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.61073/0.84511. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.62148/0.84123. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61716/0.83082. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.61615/0.83884. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.59913/0.84298. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.59503/0.85670. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59446/0.83170. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.60923/0.86054. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.61142/0.84849. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58566/0.85912. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.58422/0.87218. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.58541/0.90890. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.57907/0.88658. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.57169/0.89093. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.57045/0.88134. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.57516/0.89575. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.55933/0.92070. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.57374/0.92780. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.55366/0.92253. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.55908/0.92830. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.54685/0.93714. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.53426/0.97259. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.54592/0.99222. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.53345/0.97481. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.52759/1.02792. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.51119/1.04603. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.52280/1.05786. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.52831/1.04665. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.52767/1.06182. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50543/1.14124. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.50484/1.09383. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.50154/1.13440. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.49784/1.15408. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.50142/1.18044. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.51205/1.17166. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.49881/1.19739. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49136/1.23446. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.48848/1.19288. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.49939/1.19811. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.48438/1.19281. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.50674/1.25305. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.48243/1.27646. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.47288/1.30715. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.49254/1.15898. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.47723/1.24597. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.45795/1.32126. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.46769/1.28863. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.46083/1.24086. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43617/1.31054. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.45785/1.34795. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.43914/1.37808. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.43064/1.40596. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.43132/1.44932. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.42285/1.41349. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.42607/1.49648. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.42552/1.41220. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.46767/1.32507. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.46160/1.40304. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.43552/1.42175. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.43385/1.39650. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.42457/1.38195. Took 0.43 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69605/0.71190. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69284/0.70034. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.68728/0.70104. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68883/0.69308. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68602/0.68634. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.68523/0.68188. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.68476/0.68437. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68314/0.68668. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68164/0.68848. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68051/0.68955. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67820/0.69456. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.68308/0.69648. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67130/0.70577. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67944/0.73021. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66963/0.72945. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67331/0.71298. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67302/0.71084. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67364/0.70263. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66596/0.71013. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66838/0.70207. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65825/0.71343. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65330/0.73221. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65936/0.72221. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.65548/0.73754. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65162/0.73814. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64949/0.74625. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64490/0.73777. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64763/0.76195. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.63450/0.76002. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.63311/0.78472. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.62071/0.78279. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.62445/0.78598. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.62682/0.78919. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.62777/0.75746. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61628/0.78970. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61811/0.76462. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.61117/0.77702. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.59955/0.80631. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.59799/0.78068. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.59841/0.80366. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.58668/0.81016. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.59088/0.80729. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59776/0.81229. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.57689/0.79640. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57455/0.80086. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57019/0.82094. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56273/0.83284. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.55046/0.82319. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.55359/0.85154. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.54581/0.87554. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54495/0.89852. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.54629/0.87934. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.53712/0.95120. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.54919/0.86743. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.51171/0.91603. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.54223/0.87671. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.50178/0.91220. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.50247/0.96506. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.49892/0.93557. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.49805/0.96871. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50112/0.95401. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.48163/0.97956. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49255/0.95171. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.47657/1.01332. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.46834/1.01078. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.45611/1.01390. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.49193/0.97881. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.45868/0.96883. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.45137/0.97920. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.45710/0.99754. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.44103/0.99768. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45342/1.02534. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.45134/0.98870. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.40900/1.02560. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.41734/0.97907. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48378/0.92633. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.45472/0.94089. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.42861/1.10947. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.44236/1.00603. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.43119/1.07022. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.40635/1.11946. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.39560/1.11835. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.40695/1.08802. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.37450/1.14113. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.39490/1.12571. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.39300/1.16763. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.49284/0.94577. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.47014/0.99609. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.41046/1.10363. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.37584/1.14674. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.38045/1.09647. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.37182/1.10825. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.36873/1.09005. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.37790/1.11153. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.34781/1.15774. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.34731/1.18030. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.35374/1.21209. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.37493/1.23704. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.34997/1.27201. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.34797/1.14645. Took 0.43 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69958/0.69315. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69720/0.69898. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69502/0.70345. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69454/0.70686. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69243/0.70998. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68787/0.70921. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68695/0.71300. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68587/0.71960. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68081/0.72866. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68270/0.72522. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67619/0.72823. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67276/0.72868. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66718/0.73910. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.66329/0.75313. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66107/0.76597. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.65393/0.78141. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.65126/0.79712. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64453/0.80422. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64412/0.79468. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.63288/0.79541. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.62299/0.80492. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.62975/0.81187. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.62455/0.80016. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.61697/0.82042. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.61208/0.82902. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.60703/0.83449. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.59856/0.86304. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.59557/0.86530. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.59697/0.87966. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.60017/0.89465. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.57637/0.92445. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.57897/0.91982. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.57332/0.94558. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.55872/0.94635. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.55636/0.97283. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.55427/0.98589. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.54541/1.01363. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.54370/1.03457. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.53551/1.02510. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.53688/0.99041. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.53493/0.98788. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.51082/1.02860. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.51254/1.03208. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.51158/1.02399. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.51923/1.00789. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.50210/1.03286. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.50286/1.02213. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.49730/1.01844. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.49410/1.04889. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.48979/1.07708. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.47932/1.07962. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.49518/1.06431. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.46712/1.05686. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.46504/1.03609. Took 0.42 sec\n",
      "Epoch 54, Loss(train/val) 0.46207/1.06483. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.46961/1.08203. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.45579/1.06700. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.46599/1.03571. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.47825/1.05386. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.46182/1.04827. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.47107/1.01634. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.44556/1.03656. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.43072/1.03434. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.45275/1.01193. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.44429/1.03438. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.41028/1.01729. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.42121/1.02355. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.40678/1.04589. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.41105/1.05313. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.41487/1.07948. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.41045/1.07500. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.40536/1.03604. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.40730/1.02451. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.39719/1.02387. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.37566/1.07543. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.38774/1.09418. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.38976/1.09503. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.38363/1.12711. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.38147/1.06548. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.36693/1.10249. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.36511/1.14165. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.34680/1.12823. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.35693/1.08492. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.34504/1.12004. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.36210/1.17385. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.35603/1.15710. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.35592/1.12409. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.36274/1.16101. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.34577/1.28571. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.34114/1.27109. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.33271/1.15581. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.36754/1.15669. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.32336/1.17703. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.31929/1.19877. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.31171/1.21704. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.29966/1.21224. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.29910/1.19151. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.29058/1.19818. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.29680/1.21941. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.31470/1.15425. Took 0.43 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.70191/0.69005. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69836/0.68526. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.69456/0.69814. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68683/0.70332. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.67890/0.70631. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.67879/0.71367. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67728/0.71337. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67825/0.72022. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67619/0.71841. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67477/0.71595. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67727/0.70504. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66878/0.72245. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66583/0.72651. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.66043/0.74676. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66724/0.73781. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66445/0.73672. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65611/0.74810. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65306/0.74001. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65424/0.75224. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65229/0.73868. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65134/0.73682. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63679/0.74511. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65071/0.74017. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64524/0.75756. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63854/0.75207. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.62898/0.75095. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63006/0.75074. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.62162/0.74815. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62408/0.76022. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62483/0.74041. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.62199/0.74801. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61677/0.73802. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61464/0.74093. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.61540/0.74181. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.61237/0.74981. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61356/0.75603. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59819/0.76989. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.60941/0.76208. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.59831/0.78258. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59526/0.78014. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59309/0.78462. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58372/0.79846. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58275/0.81899. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.57633/0.83046. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57118/0.83375. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56627/0.84006. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55753/0.86123. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.56057/0.85624. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55118/0.83548. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.54546/0.86455. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53782/0.86514. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53469/0.88184. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.53833/0.92192. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52919/0.89740. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51911/0.92620. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.50543/0.93283. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.50081/0.96199. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51376/0.98001. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.49173/0.95620. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.49546/0.96089. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.49785/0.97384. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.48995/0.98614. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.49048/0.98486. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.46679/0.99097. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.47076/1.01779. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.46198/1.03213. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.46200/0.97479. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.46392/0.98138. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.45475/1.03728. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.44738/0.98987. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.43353/1.04578. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.44238/1.06177. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.45777/0.98064. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.43130/0.99051. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.43830/1.02779. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.40987/1.05836. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.42476/1.11966. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.39914/1.14242. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.40796/1.14704. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.41445/1.15558. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.39411/1.17191. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.39091/1.17274. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.38777/1.17491. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.38545/1.25153. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.37587/1.20162. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.37242/1.27855. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.35757/1.34464. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.36338/1.26500. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.35711/1.27132. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.37338/1.24587. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.35354/1.22184. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.33573/1.30373. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.32056/1.35543. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.33664/1.37529. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.29745/1.40347. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.32754/1.34532. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.32279/1.37899. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.29968/1.48706. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.30102/1.52243. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.28995/1.46537. Took 0.43 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.70155/0.71553. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69902/0.71775. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69531/0.72406. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69166/0.75179. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68561/0.74676. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69086/0.75891. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68744/0.75139. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.69208/0.76589. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68319/0.75447. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68476/0.74158. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67993/0.73048. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67457/0.72068. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68825/0.72876. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68167/0.72407. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67753/0.73078. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68178/0.74130. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67875/0.72017. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67365/0.73690. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67001/0.73000. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67070/0.74432. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67907/0.73811. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.67706/0.72859. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66658/0.73023. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66558/0.75020. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65974/0.78241. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.66171/0.76933. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.66557/0.78585. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.66854/0.77967. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.65218/0.78987. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.64536/0.82707. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.65360/0.81359. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.64810/0.81973. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64710/0.83362. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63020/0.87426. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.64657/0.87196. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.63529/0.86623. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.63360/0.87821. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.62125/0.88737. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.62571/0.90419. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.62869/0.93123. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61200/0.87883. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61181/0.89781. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.61008/0.89791. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59916/0.90551. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.59922/0.89318. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.59713/0.90742. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58462/0.94872. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57147/0.96650. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.58013/0.96111. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57579/0.91448. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57448/0.98685. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56985/1.01167. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.56032/0.94740. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56399/1.02545. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54476/1.04316. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.55989/1.04372. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.54150/1.08389. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54695/1.06638. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.53241/1.09051. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53345/1.10407. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.53639/1.08309. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.53492/1.09850. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.52415/1.15837. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.52432/1.09778. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.51129/1.15548. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50288/1.14899. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.51461/1.14194. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.50258/1.16473. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49233/1.14520. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.49890/1.12413. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48891/1.13728. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48356/1.17728. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.48675/1.15520. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46159/1.13656. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.49157/1.14390. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.46641/1.20582. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.47348/1.17742. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.45871/1.21291. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44671/1.27178. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.45088/1.23336. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.45416/1.27045. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.45601/1.19056. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.44418/1.26847. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.43998/1.27977. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.44299/1.32681. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.43437/1.29858. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43251/1.26058. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.42191/1.33467. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.41891/1.41636. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.42156/1.27740. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.39586/1.42035. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.39926/1.35084. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.43590/1.23551. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.39959/1.35732. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.39459/1.34051. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.40878/1.38672. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.39560/1.46994. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.39009/1.36203. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.40234/1.38522. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36390/1.45287. Took 0.44 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.70221/0.69771. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69914/0.70278. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69380/0.69902. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69330/0.69804. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68809/0.70554. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68729/0.70684. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68613/0.70572. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68328/0.70557. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68226/0.71118. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67933/0.71268. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67842/0.71784. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67620/0.72920. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67617/0.72927. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67007/0.73217. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66718/0.74159. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66858/0.74890. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66142/0.74612. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65824/0.75171. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65344/0.75691. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65268/0.76677. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64942/0.76044. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63961/0.76588. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63756/0.77396. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.63779/0.77883. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63698/0.79112. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63358/0.80221. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62776/0.80336. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.62288/0.80571. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62173/0.81814. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.61630/0.81134. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.60967/0.82815. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61695/0.84412. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61650/0.82511. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.59942/0.84107. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59368/0.85888. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59144/0.86933. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58814/0.86960. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.59189/0.89275. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.57159/0.88005. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.57456/0.90600. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57123/0.90055. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58566/0.88478. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.56515/0.88999. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56026/0.89081. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.56165/0.89350. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.54977/0.90351. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.54718/0.91067. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.54795/0.92010. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.54968/0.91624. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.53081/0.93629. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.52513/0.95328. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.53193/0.92356. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.51860/0.92900. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.50871/0.94848. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.50163/0.90482. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.48918/0.97559. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.50241/0.94745. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.50220/0.94956. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.49199/1.00516. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.48329/0.94478. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.48881/0.99491. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.47786/0.95615. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.47817/0.97431. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.45152/0.95153. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.45880/1.02936. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.46435/1.02459. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.44644/1.07294. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.44028/1.05388. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.45401/1.01039. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.43972/1.05221. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.43403/1.08998. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.42183/1.04639. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.43206/1.15903. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.40745/1.10747. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.40662/1.10672. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.40710/1.13787. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.39795/1.09198. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.38237/1.09117. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.37036/1.24263. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.38425/1.19622. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.37796/1.20895. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.36242/1.23186. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.33941/1.18849. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.34197/1.18986. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.36617/1.28829. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.33722/1.27725. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.34773/1.18138. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.35733/1.28277. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.34541/1.22310. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.33573/1.29068. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.36309/1.17528. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.34725/1.21810. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.32466/1.17761. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.30834/1.32453. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.31662/1.31948. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.32568/1.30591. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.32011/1.27758. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.31785/1.32979. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.32789/1.28143. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.27725/1.24223. Took 0.44 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69580/0.70900. Took 0.59 sec\n",
      "Epoch 1, Loss(train/val) 0.68746/0.69391. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68387/0.69465. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68162/0.68867. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.67845/0.69910. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67705/0.70206. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67301/0.70798. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67297/0.71067. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67317/0.71369. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.66939/0.73252. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66179/0.73173. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66778/0.73171. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66019/0.73562. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.66158/0.74841. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.65705/0.73579. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.65336/0.73936. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.65773/0.74742. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.64924/0.74682. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65583/0.73768. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65012/0.73507. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64942/0.76129. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64331/0.78178. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64228/0.77214. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64627/0.77306. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64689/0.77251. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64170/0.76467. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63803/0.76141. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.62759/0.78135. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62774/0.77397. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63460/0.77908. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63175/0.77862. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62160/0.78848. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61871/0.79545. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.61674/0.81377. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.60866/0.80984. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.59925/0.83436. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.59385/0.85713. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.60252/0.87503. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.59439/0.87548. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58873/0.88542. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.59630/0.87977. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.57584/0.91648. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58136/0.89978. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.57746/0.95715. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.56661/0.95087. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.56152/0.95098. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56256/0.93399. Took 0.42 sec\n",
      "Epoch 47, Loss(train/val) 0.55409/0.98027. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.55885/0.95342. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.55826/0.96985. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55130/0.96674. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.55142/0.99034. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54243/1.00378. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.53339/0.99212. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52314/1.00408. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.52586/1.01895. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.52140/1.01359. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.51484/1.03027. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.51897/1.03750. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.50201/1.03279. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49948/1.10726. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.49552/1.08724. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.48296/1.12230. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.48202/1.15201. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.49063/1.10915. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.47939/1.15854. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48603/1.15304. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.46568/1.12658. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.47614/1.16341. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.45676/1.17723. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.45902/1.20470. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45699/1.20400. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.43609/1.14386. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.44775/1.21282. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.42778/1.22441. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.41867/1.26024. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.43789/1.24926. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.42557/1.25684. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.42027/1.29713. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.41759/1.28273. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.40538/1.33344. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.40395/1.30216. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.38908/1.29606. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.37818/1.31581. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.39441/1.33212. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.37738/1.31601. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.37418/1.33846. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.36996/1.31556. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.36108/1.31959. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.36205/1.37702. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.35388/1.34281. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.34226/1.30066. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.35416/1.32474. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.33026/1.37001. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.33786/1.35242. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.34364/1.35114. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.31772/1.30930. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.32071/1.35294. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.31409/1.35915. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.33379/1.46214. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69667/0.70176. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68630/0.70827. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.68475/0.69999. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68589/0.71884. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68841/0.72376. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.67880/0.71435. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68223/0.71561. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67606/0.72132. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67789/0.73209. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67988/0.73997. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66821/0.74001. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66761/0.73521. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67134/0.73005. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66759/0.73136. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66513/0.72574. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66860/0.73089. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66770/0.72346. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66420/0.72518. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65823/0.72092. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66166/0.73234. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65073/0.73846. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.65087/0.72057. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64992/0.73469. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64411/0.72630. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65152/0.72820. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64236/0.73449. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64396/0.73933. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64688/0.73844. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64046/0.73842. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62884/0.72173. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.64007/0.74578. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64178/0.72639. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.64268/0.75131. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.63488/0.72904. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.61530/0.75009. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.61754/0.76998. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.62862/0.76311. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.62142/0.75377. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.61204/0.77793. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.60500/0.77459. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.60031/0.78130. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59970/0.77574. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59452/0.75886. Took 0.42 sec\n",
      "Epoch 43, Loss(train/val) 0.59102/0.78351. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.59232/0.77000. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.58279/0.77843. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59398/0.75355. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58281/0.80462. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.58216/0.79051. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.58727/0.79493. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57523/0.81995. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56519/0.85499. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56833/0.84677. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.58417/0.81136. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.58343/0.78753. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.56191/0.81313. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56711/0.82730. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.55749/0.80864. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.55355/0.84735. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.53729/0.86775. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.53484/0.86821. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.54212/0.91717. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54285/0.90416. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.52500/0.85032. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.53006/0.86451. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.52789/0.87696. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.51516/0.95520. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.52225/0.91710. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.51224/0.95032. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.50055/0.96566. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.49512/0.97612. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.50284/0.96762. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.50430/0.97428. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48125/1.06551. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.50024/1.02034. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48055/0.97977. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.47541/1.04964. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.47488/1.02704. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.46035/1.01964. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46745/1.07855. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.45723/1.08952. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.45184/1.07274. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.44636/1.10117. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.43558/1.06818. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43571/1.08378. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.43174/1.17745. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.41849/1.15805. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.40940/1.16569. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.41556/1.15722. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.42522/1.15830. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.40708/1.16924. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.40467/1.25349. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.42818/1.25110. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.40291/1.16737. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.38647/1.22925. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.40079/1.27810. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.39394/1.26779. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.37428/1.25565. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.39423/1.23789. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.38325/1.26181. Took 0.43 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70882/0.73350. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70793/0.72126. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.70281/0.71738. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69907/0.72218. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69611/0.76122. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69040/0.76400. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68710/0.74316. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.69457/0.76133. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.69090/0.77515. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67544/0.74758. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68785/0.75840. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67707/0.76569. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.68727/0.75038. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67423/0.76949. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67492/0.78669. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67675/0.75366. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67264/0.77293. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67766/0.76883. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67441/0.80069. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66627/0.79122. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67723/0.80143. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.67261/0.80164. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66479/0.80853. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.66713/0.81879. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65977/0.83508. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.66018/0.85163. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65822/0.85551. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.66375/0.82898. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65538/0.85403. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.64723/0.85299. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.64123/0.86349. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64690/0.88738. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63718/0.86709. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.62773/0.91111. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.63663/0.92564. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.62006/0.92918. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61778/0.96161. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.61859/0.94892. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.61808/0.96283. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.60692/0.98009. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.59673/1.03354. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.60949/0.97798. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.61067/0.95211. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.61798/0.94635. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.61056/0.93471. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.59502/0.95862. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.60088/0.97618. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.59207/0.98858. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59827/0.94146. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.59121/0.91926. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.57789/0.94563. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.58203/0.96975. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.58670/0.94629. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.56909/0.97245. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.57995/0.97464. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.56589/0.96122. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.55782/0.95181. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.56370/1.00105. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.56885/0.97737. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.55388/0.96858. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.55848/0.98428. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.55075/0.99548. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.56307/0.94782. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.54401/0.97356. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.55009/0.98677. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.54189/0.97228. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.54575/0.97384. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.53022/0.98482. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52466/0.99045. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.52076/0.99887. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.53484/0.99672. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.52843/1.01943. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.51429/0.99426. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.50809/1.05249. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.49649/0.99303. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.51176/1.01707. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.50425/0.97311. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.50995/1.03170. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.51060/0.99697. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.49758/1.04212. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.49574/1.02171. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.49157/1.01838. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.48147/1.02214. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.48641/0.97883. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.47354/1.02146. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.48571/0.99328. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.46810/1.06260. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.47865/0.98480. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.47727/0.97843. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.46677/1.01799. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.45669/1.06321. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.44508/1.05789. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.42584/1.10890. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.47037/1.06732. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.44984/1.09248. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.43934/1.08013. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.46951/1.04465. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.44867/1.05137. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.46918/1.11203. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.43870/1.09995. Took 0.43 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.73229/0.70624. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.72679/0.70130. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.72358/0.70282. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.70308/0.70623. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.71261/0.71109. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.70707/0.73257. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.71394/0.71924. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68768/0.71816. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69769/0.73635. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68908/0.72221. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.69378/0.72982. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.68944/0.74640. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.68809/0.73195. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68093/0.73041. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.68438/0.73678. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.68719/0.74582. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.68225/0.74431. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67402/0.77725. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.68467/0.76542. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67788/0.76366. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67236/0.77267. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.66624/0.77648. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.67782/0.79148. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.65605/0.80249. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65757/0.78441. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.65964/0.81014. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65714/0.83942. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64838/0.84109. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65319/0.85212. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65872/0.85234. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64757/0.85154. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.65281/0.86907. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.64236/0.88529. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.63873/0.89864. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.63439/0.91083. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.63903/0.91414. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.63205/0.94845. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.63636/0.94182. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62465/0.95118. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.62071/0.94896. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62967/0.91029. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.62407/0.98082. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.61903/0.94306. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.61502/0.93363. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60811/0.95437. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.60959/0.96053. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.61026/0.97984. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.59869/1.00745. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.60063/1.00283. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.60104/1.00669. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.58900/0.98410. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.59249/1.02215. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.58829/1.01211. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.57537/1.07534. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.58967/1.06335. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.58695/1.02863. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.59253/1.05394. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.57862/1.07837. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.57971/1.06544. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.57939/1.04578. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.57002/1.04769. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.57224/1.09365. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.55831/1.09611. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.55168/1.11191. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.55563/1.06818. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.54091/1.11289. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.55299/1.17916. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.55149/1.13458. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.54362/1.11220. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.54368/1.12407. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.52991/1.18254. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.53099/1.16918. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.51914/1.18498. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.51832/1.23721. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.52804/1.16988. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.51240/1.17947. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.51743/1.21120. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.51283/1.27063. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.50482/1.28753. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.49930/1.21150. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.49549/1.23439. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.48332/1.29081. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.48966/1.26755. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.48084/1.26631. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.48564/1.28827. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.47115/1.25872. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.46730/1.32496. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.46970/1.27191. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.46339/1.32152. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.47915/1.27762. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.44531/1.18480. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.48809/1.34485. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.45843/1.24622. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.42065/1.25844. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.44210/1.33331. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.42840/1.40711. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.43989/1.29540. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.44835/1.33849. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.44325/1.30975. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.44369/1.27386. Took 0.43 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69907/0.70986. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69200/0.71252. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69296/0.70420. Took 0.49 sec\n",
      "Epoch 3, Loss(train/val) 0.69001/0.69691. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68762/0.68711. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.68192/0.68316. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.68382/0.68192. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.67963/0.67719. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.68478/0.68876. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67803/0.69175. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67250/0.69972. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67461/0.70367. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67454/0.70746. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67136/0.71295. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67274/0.72170. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67248/0.72688. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66486/0.72152. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66072/0.72912. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66513/0.72910. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65670/0.72118. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65887/0.72283. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.65701/0.72508. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65379/0.73083. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65144/0.73326. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64905/0.72817. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64294/0.74054. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63918/0.73148. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63539/0.73970. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.63869/0.74592. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63707/0.74300. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62886/0.76716. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63204/0.76864. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.63131/0.74688. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.62022/0.75080. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62295/0.75158. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62667/0.74932. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61805/0.77112. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60815/0.77420. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60683/0.78244. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.60941/0.80113. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.61032/0.79041. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.60595/0.78931. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.59062/0.79058. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59057/0.79760. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58894/0.81002. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59197/0.82312. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58423/0.84532. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.58883/0.84457. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57495/0.82885. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.59408/0.81956. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57023/0.81693. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.57766/0.81177. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.55023/0.85272. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.56655/0.84301. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.56494/0.85231. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.55099/0.87322. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56287/0.85160. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.56023/0.82821. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.54841/0.83425. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53959/0.84707. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54046/0.84480. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.56260/0.86293. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.54577/0.85618. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.53968/0.84563. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.51297/0.85401. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.52704/0.89148. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51786/0.87990. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.51004/0.87739. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.51576/0.90791. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.50517/0.88874. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.51309/0.90003. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.50924/0.91898. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.51369/0.92435. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.50655/0.93142. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.49875/0.97358. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49720/0.93298. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.51291/0.90920. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.49822/0.91293. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.48914/0.92813. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.48996/0.93179. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.50184/0.94478. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.48211/0.97286. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.47244/0.99268. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.46112/1.01519. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.46636/1.02028. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.47208/1.03088. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.45476/1.03000. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.46477/0.99102. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.47066/0.97700. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.46360/0.99174. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.46879/0.99171. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.44368/0.99298. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.44288/1.07069. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.44638/0.99631. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.44871/1.07772. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.43441/1.04775. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.44811/1.06710. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.44539/1.08875. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.42285/1.04564. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.41752/1.07371. Took 0.45 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.72301/0.69640. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70026/0.68906. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70591/0.68415. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.70928/0.68632. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69874/0.69603. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69952/0.68286. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.70114/0.68485. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69889/0.68423. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.70633/0.69387. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.69114/0.69160. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.69749/0.70357. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.70133/0.69765. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68953/0.70848. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.68874/0.69555. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.69044/0.68869. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.68436/0.69298. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.69247/0.69645. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.68739/0.70306. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.68485/0.70201. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.68149/0.71215. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.68307/0.71292. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.68539/0.71112. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.67485/0.72363. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.67662/0.72129. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.67257/0.71443. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.66840/0.71317. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.66900/0.72590. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.67382/0.72035. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.67308/0.73461. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.67311/0.72593. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.66760/0.72484. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.65721/0.74322. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.65542/0.76388. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.65495/0.75907. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.65460/0.77056. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.64411/0.76830. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.64480/0.77404. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.63600/0.80309. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.64048/0.77758. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.62913/0.80942. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.63081/0.83788. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.63047/0.84410. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.62968/0.85073. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.61806/0.85854. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.61187/0.88285. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.62751/0.84479. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.63032/0.83795. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.63258/0.83541. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.61930/0.86270. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.60744/0.86630. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.59602/0.88189. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.60609/0.85838. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.60099/0.86706. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.59715/0.89282. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.60429/0.90617. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.59739/0.91077. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.58648/0.91180. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.59489/0.91613. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.58823/0.92358. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.58103/0.92109. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.57577/0.91123. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.59982/0.91338. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.58585/0.89317. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.57581/0.86325. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.55600/0.89983. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.58202/0.94060. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.58161/0.87948. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.55876/0.88030. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.56110/0.93732. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.56797/0.88126. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.57728/0.87474. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.56333/0.85526. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.56044/0.87158. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.56070/0.86491. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.55465/0.88827. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.54791/0.87648. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.56824/0.87120. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.54230/0.88356. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.53956/0.86734. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.53505/0.89100. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.53228/0.92233. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.55899/0.94822. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.55046/0.90939. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.53193/0.92389. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.53281/0.92595. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.52431/0.96471. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.53689/0.94293. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.52431/0.93655. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.53296/0.93575. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.51826/0.95855. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.52856/1.02410. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.52820/0.97671. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.50839/0.98986. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.50514/0.98086. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.48842/1.02926. Took 0.42 sec\n",
      "Epoch 95, Loss(train/val) 0.51461/1.10706. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.50328/1.02559. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.49307/1.00449. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.49677/1.05774. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.49322/1.05765. Took 0.44 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.70177/0.69490. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70308/0.72158. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69499/0.71536. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69379/0.73144. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.69040/0.74487. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68780/0.74909. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68949/0.75450. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68940/0.75836. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69163/0.75987. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68969/0.77046. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68817/0.76308. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68461/0.75927. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68911/0.75962. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68527/0.76118. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68033/0.75940. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68328/0.76537. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.68031/0.76873. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67957/0.78294. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.67687/0.79414. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67070/0.81265. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66688/0.82588. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66503/0.84825. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65881/0.87634. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.66648/0.86511. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65973/0.89034. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.65523/0.90057. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65142/0.90355. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.65655/0.89994. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65170/0.93024. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65479/0.91786. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.65646/0.91303. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.65126/0.91833. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.65359/0.92290. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.65121/0.91292. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.64295/0.92577. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.63568/0.92568. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.63608/0.94175. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.64066/0.94011. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62610/0.94626. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.63126/0.98225. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.61744/0.98207. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.62996/0.97489. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.62096/0.98748. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.62089/1.00376. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.61198/0.99933. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.60617/1.03100. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.60809/1.00815. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.59846/1.01238. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.59250/1.02758. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.59899/1.02396. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.60004/1.01680. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.58147/1.02790. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.58521/1.04000. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.58246/1.02962. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.59350/1.04580. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.58598/0.99062. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.57166/1.06454. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.56907/1.02384. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.56252/1.04455. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.56640/1.07198. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.56354/0.99848. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.55021/1.06347. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.55506/1.01839. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.54560/1.08814. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.55205/1.05146. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.56081/1.03518. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.54090/1.05979. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.53751/1.08902. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.53773/1.07496. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.52555/1.07778. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.51934/1.12133. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.52663/1.14056. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.51269/1.15850. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.51935/1.10807. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.52686/1.06236. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.51072/1.12725. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.50008/1.14855. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.51336/1.10336. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.49961/1.12857. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.48290/1.17420. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.49515/1.14952. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.48186/1.17341. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.48990/1.14566. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.46393/1.18634. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.47461/1.18523. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.49996/1.17060. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.48320/1.11580. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.45747/1.20658. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.47333/1.19941. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.47863/1.20105. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.46585/1.24799. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.46485/1.22469. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.45800/1.20794. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.45397/1.26858. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.43978/1.22520. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.45434/1.29621. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.44503/1.27229. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.43902/1.26446. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.44443/1.26928. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.44984/1.21125. Took 0.44 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69547/0.69293. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69344/0.69181. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69194/0.69170. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69150/0.69081. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69017/0.69344. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68871/0.69182. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68926/0.69232. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68833/0.69048. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.68676/0.69187. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68549/0.69291. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68533/0.69529. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.68581/0.69652. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.68267/0.70331. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68083/0.70406. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.68065/0.70510. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67839/0.70945. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67480/0.71419. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67193/0.72133. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66981/0.72076. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66972/0.72175. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66343/0.72195. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.66133/0.72644. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65457/0.74540. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65297/0.73060. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.65037/0.74751. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64664/0.74966. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64706/0.74732. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63600/0.74658. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62913/0.76293. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61668/0.75857. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.61659/0.77744. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62701/0.76041. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61147/0.79015. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60802/0.79912. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59439/0.81237. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59602/0.79635. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.58410/0.85397. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58743/0.85449. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58281/0.85340. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.59403/0.81399. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57371/0.84882. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.57617/0.85939. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56053/0.92772. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.55248/0.90394. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.55363/0.91896. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55429/0.97125. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.53087/0.95441. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.53832/1.00156. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.53503/0.96371. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.52311/0.93915. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.52234/0.95211. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.52163/0.95786. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.51625/1.00007. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.51765/1.01914. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.49880/1.00505. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.50045/1.02626. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.49685/1.05433. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.50220/1.01743. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.47359/1.05617. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.48400/1.10012. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.48350/1.08839. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.46728/1.09240. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.47421/1.05008. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.47353/1.07328. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.44581/1.16166. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.45894/1.16399. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.46807/1.12588. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.44956/1.16972. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.45965/1.19520. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46084/1.19899. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.43515/1.26151. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.42162/1.20551. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.42442/1.23318. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.40845/1.31164. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.42402/1.26769. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.41574/1.24320. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.42241/1.23928. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.42836/1.25105. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.41695/1.21373. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.42161/1.29348. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.38944/1.28688. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.38056/1.38048. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.38641/1.40682. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.41093/1.37431. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.38716/1.36612. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.35202/1.44457. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.37337/1.33976. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.38317/1.36989. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.37698/1.39673. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.35974/1.39893. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36973/1.42974. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.36824/1.38487. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.34045/1.44726. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.34210/1.44732. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.34018/1.43408. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.33710/1.50401. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.34058/1.44853. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.32689/1.48893. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.35409/1.47492. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.33923/1.44766. Took 0.43 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.70861/0.68344. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70508/0.69976. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.70773/0.70026. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69966/0.70481. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69887/0.69667. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69653/0.70157. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.69608/0.70385. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.69350/0.70189. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.69082/0.69543. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.69727/0.69502. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68677/0.69476. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68366/0.69365. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.68697/0.68824. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68519/0.68712. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68004/0.68754. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68500/0.69287. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.68358/0.70651. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.68613/0.69835. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67868/0.71706. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67807/0.72162. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67932/0.72166. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.67633/0.72000. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.67758/0.72062. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66995/0.72632. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.67740/0.74137. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.67238/0.74443. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.66398/0.73577. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.66363/0.76142. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.66613/0.75127. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.66537/0.75159. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64854/0.75458. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.65737/0.76479. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.65302/0.76084. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.65353/0.77471. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.64784/0.80845. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.64506/0.82045. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.65229/0.79946. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.64449/0.82078. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.64491/0.80866. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.64275/0.84833. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.63860/0.82003. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.63846/0.84061. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.63222/0.84242. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.62886/0.82433. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.63975/0.85842. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.63249/0.81183. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.63460/0.83443. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.64209/0.79778. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.62384/0.80880. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.62968/0.84060. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.62722/0.83606. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.62256/0.79911. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.61450/0.82326. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.62251/0.80400. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.62082/0.80088. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.60723/0.83093. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.60920/0.80996. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.60339/0.84249. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.61273/0.82844. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.59887/0.83068. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.60917/0.80913. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.60520/0.82609. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.59795/0.82961. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.60396/0.84214. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.59370/0.84865. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.58766/0.84504. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.58750/0.82243. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.57247/0.85719. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.58448/0.83578. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.57401/0.87738. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.56973/0.87279. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.55432/0.90982. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.56160/0.86361. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.55065/0.88188. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.56118/0.85318. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.55060/0.89802. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.54360/0.92973. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.54528/0.93915. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.54682/0.96823. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.53353/1.00424. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.53722/0.99459. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.54482/0.95289. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.53992/0.98683. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.51530/1.05841. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.52076/1.04740. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.51937/1.02177. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.53112/1.01119. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.51696/1.03637. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.51504/1.10327. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.50339/1.09736. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.50918/1.10494. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.49652/1.08172. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.50801/1.17212. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.48692/1.19387. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.49991/1.10952. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.51262/1.11745. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.52200/1.07093. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.49427/1.10896. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.48774/1.15724. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.46882/1.17706. Took 0.43 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.70837/0.68972. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69964/0.69861. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69905/0.70809. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69282/0.70100. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69205/0.71530. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68810/0.71734. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.69096/0.71179. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68499/0.72692. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68267/0.72431. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68262/0.73612. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68632/0.73902. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67958/0.73852. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67446/0.73206. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66776/0.74738. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67426/0.74120. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66967/0.75385. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67027/0.73907. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66283/0.74675. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65849/0.74752. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65769/0.76276. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65219/0.75082. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64974/0.75629. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64516/0.76272. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63076/0.77689. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63702/0.78661. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63636/0.78669. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62575/0.79052. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.62973/0.79006. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61762/0.77598. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61951/0.80838. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62153/0.81572. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61783/0.80904. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61451/0.81122. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61014/0.82387. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.61928/0.80994. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.61535/0.81241. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59842/0.84052. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.60951/0.84688. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.59430/0.86487. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.59205/0.84197. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58733/0.88365. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.59324/0.86683. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.56610/0.89012. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.58752/0.84476. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57342/0.88560. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.56278/0.88459. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57272/0.88718. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.56897/0.92101. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.56119/0.91040. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.55034/0.91675. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55251/0.91631. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.55908/0.88139. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.54626/0.90328. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.53249/0.94414. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.52246/0.93855. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.50388/0.95912. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.51584/1.00059. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.52223/0.92762. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.49887/0.95874. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.52499/0.98965. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50857/0.97143. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.50857/0.98895. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.49066/0.95976. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.49472/0.98634. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.47918/1.06893. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.47425/0.96594. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.46399/1.01830. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.46617/1.03187. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.44891/1.08608. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.44361/1.06247. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.46679/1.01816. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45548/1.05753. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.44856/1.17608. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.44210/1.16204. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.41982/1.16197. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.45873/1.08289. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43792/1.06748. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.41536/1.06915. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43398/1.03130. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.42670/1.12650. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.45350/1.08841. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.40286/1.15033. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.40232/1.15053. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.42557/1.11466. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.42417/1.05974. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.40895/1.10338. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.41289/1.09221. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.38140/1.13806. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.42050/1.14492. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.42134/1.06373. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.38833/1.12815. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.39591/1.23777. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.42701/1.09891. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.38670/1.11110. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.38650/1.12589. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.38471/1.15741. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.41744/1.15581. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.41617/1.05724. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.37235/1.15154. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34429/1.17251. Took 0.43 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.70983/0.70973. Took 0.65 sec\n",
      "Epoch 1, Loss(train/val) 0.71160/0.68625. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69461/0.69431. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69666/0.70915. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69323/0.69275. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68716/0.70687. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68919/0.70651. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68391/0.70424. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67830/0.70974. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67816/0.70312. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68013/0.70044. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67832/0.71150. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67707/0.71093. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67103/0.71238. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67188/0.70767. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66943/0.70604. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67138/0.70566. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66723/0.72567. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66737/0.72960. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66459/0.71936. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66202/0.71778. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66646/0.71094. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66056/0.72769. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.65860/0.72178. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65005/0.73548. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65404/0.72561. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65247/0.72311. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64899/0.72897. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64495/0.72491. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65573/0.71961. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.64642/0.75151. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.63814/0.71970. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.64238/0.72621. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63252/0.71106. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.63606/0.70073. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.62114/0.70279. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.62037/0.69766. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.61335/0.70117. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60659/0.72656. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.61196/0.72258. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58910/0.75870. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59523/0.76864. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.58038/0.78117. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57374/0.76267. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57698/0.80066. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57030/0.77581. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.57237/0.82883. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56427/0.79033. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55691/0.80327. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.54293/0.87059. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54677/0.81073. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.54960/0.85326. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52410/0.86818. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.51865/0.87724. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52977/0.88483. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.52072/0.89741. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52710/0.88426. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.51502/0.87695. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.49865/0.93187. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50031/0.88390. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.52057/0.89759. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.49977/0.90622. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48792/0.88897. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.48161/0.93457. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.48513/0.93721. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.48684/0.95831. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.49471/0.99965. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48802/0.96922. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.45865/0.99695. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45848/1.03505. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.45859/1.03933. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.46312/0.99527. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46950/1.02882. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.45624/1.01084. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.44962/1.10287. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44520/1.08823. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.43310/1.04135. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.43812/1.05368. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43477/0.99898. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.45474/1.05609. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.41233/1.04582. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.42234/1.02038. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.42357/1.11305. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.43757/1.07062. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.45166/1.09391. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.44678/1.01939. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.40757/1.01043. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.42712/1.01224. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.44476/1.12593. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.43899/1.03844. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.41405/1.05634. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.40163/1.05915. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.40776/1.15159. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.40712/1.15496. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40163/1.08235. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.37443/1.16201. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.40323/1.07557. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.43819/1.11142. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.40014/1.10176. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.40260/1.06062. Took 0.43 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69914/0.68422. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69072/0.68908. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.68812/0.71420. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69319/0.71991. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68725/0.70720. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68297/0.72307. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68668/0.72535. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68277/0.72287. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.68297/0.71735. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67769/0.72212. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67392/0.73251. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67869/0.72464. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67361/0.73570. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67417/0.73722. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67586/0.73686. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66946/0.73785. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67085/0.73889. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66626/0.74422. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65708/0.75425. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65991/0.76304. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65244/0.78725. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65245/0.78427. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64705/0.79388. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63687/0.82464. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.62350/0.83441. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62776/0.84469. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64068/0.87512. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62276/0.87717. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62858/0.84783. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61798/0.88014. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61665/0.85015. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61052/0.85861. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61006/0.86101. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.59913/0.87389. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.59475/0.87647. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.59820/0.85347. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.57550/0.91985. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59031/0.89929. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57949/0.87128. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57926/0.88521. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.56940/0.87645. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57496/0.91244. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.55881/0.88557. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56240/0.92846. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.54616/0.94697. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.54878/0.96256. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.53667/0.95941. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55015/0.93511. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.52536/1.01205. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54459/0.99790. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.50696/1.04048. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54535/0.97013. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52692/1.00340. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.51722/1.09307. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.53102/0.97839. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51066/1.05475. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.50287/1.06975. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.48349/1.05191. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.48983/1.12251. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.47408/1.10704. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.47401/1.14660. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.47408/1.24645. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.47350/1.24424. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.47462/1.18850. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.47793/1.27765. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.47606/1.25058. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.46527/1.17471. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.46357/1.27370. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.46494/1.24848. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.43235/1.28010. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.44362/1.36894. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45461/1.23529. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.41820/1.36868. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.43209/1.36362. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.44557/1.28776. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.41906/1.33171. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.42795/1.27091. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.45429/1.31556. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.40371/1.30292. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.41086/1.38185. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.40702/1.47368. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.43806/1.38498. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.40937/1.28492. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.38479/1.42001. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.37483/1.47656. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.36990/1.60188. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.38198/1.57140. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.37418/1.43248. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.37204/1.51770. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.38484/1.47280. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36889/1.48464. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.35487/1.53197. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.33859/1.53398. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.36264/1.48711. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.35279/1.53242. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.35253/1.46568. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.33011/1.42537. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.36042/1.57003. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.37982/1.62831. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.42579/1.58016. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.72322/0.69842. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.71054/0.69684. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69938/0.68639. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.70424/0.69958. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69572/0.69693. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69582/0.69308. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.69785/0.69546. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.69029/0.69859. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68535/0.67608. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.68426/0.68155. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68520/0.66213. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.67353/0.65230. Took 0.47 sec\n",
      "Epoch 12, Loss(train/val) 0.67341/0.67237. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67568/0.68090. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67321/0.66822. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67159/0.69520. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67065/0.70262. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66904/0.70259. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66011/0.72281. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65572/0.72544. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65724/0.71911. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65262/0.73409. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64976/0.72042. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64909/0.71119. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64187/0.70444. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64714/0.71791. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63958/0.70852. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64097/0.71130. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62961/0.73109. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62161/0.72362. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62826/0.73999. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62099/0.72649. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61883/0.73476. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61779/0.74576. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61219/0.72849. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.60398/0.74979. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.59629/0.76187. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58679/0.76553. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59964/0.77255. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.59252/0.74898. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57349/0.76069. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58114/0.78439. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57262/0.80593. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.57218/0.84833. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.56661/0.80361. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55741/0.85721. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55700/0.84196. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.57310/0.77179. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.54725/0.78004. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.54990/0.78460. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54058/0.84704. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.52735/0.79402. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52928/0.87438. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.51700/0.87404. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.52921/0.80434. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.52946/0.82553. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.50593/0.85003. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.50557/0.85990. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.49866/0.87078. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.50995/0.86377. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.50939/0.84706. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.49297/0.90094. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.47553/0.88849. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.49173/0.88455. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.48155/0.95454. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.48258/0.92710. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.46994/0.97150. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.46850/0.97234. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.47659/0.99791. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.44993/0.95267. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.45904/1.00296. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.46677/0.89691. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.45470/0.94437. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.43720/0.97754. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.43123/1.02259. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.43964/1.00791. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.41423/1.06922. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.43188/1.11859. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.40112/1.14164. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.40192/1.20022. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.41751/1.18645. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.39591/1.22336. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.40096/1.16766. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40136/1.13804. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.38960/1.20598. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.39253/1.20810. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.38394/1.21503. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.38045/1.34195. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.36376/1.28068. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.37120/1.36970. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36240/1.25987. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.35362/1.33489. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.34882/1.30776. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.34911/1.40979. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.34056/1.40298. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.33052/1.28937. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.35644/1.39482. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.35667/1.43181. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.32292/1.40124. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.33209/1.37959. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69806/0.70637. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69345/0.71463. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68721/0.71947. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68277/0.73534. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.67960/0.73962. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67856/0.74787. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.67809/0.75004. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67413/0.75435. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67378/0.75511. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67122/0.76185. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67095/0.76640. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66933/0.77314. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66407/0.77839. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66472/0.77736. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66037/0.77485. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65889/0.78169. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65999/0.76976. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65360/0.77663. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65497/0.76213. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65147/0.75758. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65199/0.75600. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64924/0.75612. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64249/0.77123. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63494/0.77027. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63839/0.77336. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.63599/0.77180. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63191/0.76068. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62108/0.75022. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62409/0.76628. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61520/0.78354. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.61654/0.77254. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.60966/0.78636. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60137/0.79986. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60514/0.77004. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.58568/0.77135. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.58661/0.79152. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.58436/0.78252. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57073/0.80132. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57425/0.77607. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57166/0.80683. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57793/0.78349. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.55988/0.80231. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.53124/0.82262. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.55016/0.81097. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.52959/0.83715. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.52883/0.85016. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.53707/0.83570. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.53259/0.87553. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.52276/0.86347. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.50291/0.86354. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.48907/0.91572. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.48382/0.91033. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.47741/0.91517. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.48328/0.90237. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.49365/0.90532. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.45668/0.90144. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.46474/1.02452. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.45981/0.93741. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.42501/1.03518. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.43791/0.96513. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.44412/0.97619. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.42998/1.01256. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.43070/0.98845. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.42159/1.04102. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.44264/1.02818. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.42103/1.05390. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.42090/1.05305. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.39232/1.06237. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.37736/1.06938. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.41756/1.01470. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.38113/1.12805. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.37056/1.16139. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.37949/1.13914. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.37375/1.08916. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.42133/1.03937. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.41196/1.03325. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.38225/1.04664. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.35796/1.12646. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.35658/1.12109. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.32626/1.18549. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.35398/1.12873. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.32632/1.14289. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.34852/1.22402. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.34437/1.17654. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.32591/1.29058. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.31500/1.29114. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.33176/1.26672. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.32124/1.26952. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.36303/1.23670. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.31224/1.24115. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.30229/1.22904. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.31011/1.21831. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.28368/1.25641. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.31119/1.21556. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.28952/1.37220. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.27237/1.35459. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.24367/1.32757. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.26240/1.39406. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.26652/1.44504. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.27784/1.37676. Took 0.44 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.71237/0.70442. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70580/0.69954. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69196/0.69154. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69726/0.68419. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.67818/0.68569. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68623/0.69951. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68285/0.70522. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67841/0.70453. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67736/0.71399. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67174/0.72054. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67163/0.70175. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66884/0.72280. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67877/0.70827. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.66575/0.72512. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66947/0.71900. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66872/0.72282. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66336/0.72382. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65631/0.72196. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65836/0.73721. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66012/0.73362. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65320/0.74072. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.65251/0.73676. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65304/0.74483. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65163/0.75226. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64358/0.76057. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64043/0.76149. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64929/0.77477. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63747/0.79394. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63167/0.80889. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62483/0.80262. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.62202/0.80447. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62514/0.79150. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61325/0.79728. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.60891/0.81217. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61756/0.81981. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60550/0.82997. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61133/0.83998. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.61345/0.82190. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.60383/0.82895. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.59357/0.84364. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.60169/0.85041. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.59658/0.89051. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.59082/0.88473. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.57896/0.90639. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58567/0.91987. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57526/0.91064. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57140/0.92691. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.56816/0.98889. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.57160/0.98871. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56395/0.98113. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56122/1.04492. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.55170/1.01392. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.54823/1.06910. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.53016/1.13026. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.53968/1.09106. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.53007/1.20626. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.53020/1.17605. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52275/1.07704. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.51419/1.12353. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.50686/1.13720. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.51788/1.16827. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.50247/1.22077. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50627/1.17292. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.49693/1.19149. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.49539/1.24390. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.48400/1.16885. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.46181/1.22819. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.48640/1.20151. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.47535/1.23287. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.48737/1.15054. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.46399/1.17307. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.45474/1.26584. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46591/1.18164. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.45989/1.22973. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.45923/1.24821. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.43876/1.31173. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43179/1.27146. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.42134/1.28955. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.45175/1.26491. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.42336/1.13625. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.43206/1.25090. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.42687/1.25069. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.40943/1.26101. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40272/1.33772. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.42256/1.32427. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.41949/1.35003. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.38945/1.38998. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.37957/1.39994. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.39034/1.37297. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.36074/1.46920. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.39186/1.27394. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.38638/1.28920. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.36257/1.43636. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.37539/1.43464. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.37432/1.39440. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.38589/1.41103. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.41104/1.32746. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.36399/1.42766. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.34546/1.40198. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.33693/1.44532. Took 0.43 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.72197/0.72806. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.71618/0.72472. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70715/0.70261. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69815/0.70523. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69001/0.70135. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.68741/0.71610. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68841/0.72503. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.69680/0.73374. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68490/0.73996. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.69051/0.73158. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68166/0.73032. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68324/0.73281. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68305/0.72629. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68561/0.74915. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67235/0.74773. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68082/0.74581. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67661/0.75610. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67243/0.75487. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67149/0.77839. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66860/0.76540. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67574/0.77878. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.67032/0.77996. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65732/0.78464. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.66143/0.79823. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.66667/0.79596. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66650/0.80081. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.66188/0.78593. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65432/0.77990. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.66285/0.78527. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64442/0.78603. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.65453/0.78582. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.64944/0.81127. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.65485/0.81113. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.63768/0.83316. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.64678/0.84756. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.65347/0.84048. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.64150/0.83798. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.63982/0.85851. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.63069/0.86360. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.63559/0.88197. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.63344/0.86365. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.62299/0.87124. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.63153/0.87530. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61310/0.90514. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.61727/0.91436. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.60242/0.91956. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.60857/0.92295. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.60171/0.92264. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.60108/0.93774. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.59264/0.96737. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.60268/0.90994. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.59842/0.90651. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.59773/0.87935. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.58259/0.91320. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.58608/0.88595. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.58179/0.86416. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.58222/0.92176. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.56979/0.92854. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.57292/0.91879. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.57799/0.91304. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.57316/0.91204. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.56386/0.93811. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.56612/0.90045. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.54831/0.93647. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.56360/0.93782. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.54570/0.97348. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.55310/0.90969. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.55204/0.92835. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.52769/0.93118. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.53795/0.93399. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.54491/0.93030. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.54749/0.98068. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.54802/0.95126. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.51815/0.94348. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.52226/0.96561. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.52214/0.96792. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.51432/1.05594. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.51395/1.07840. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.49857/1.01120. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.50496/1.03153. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.49404/1.07507. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.48799/1.06425. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.47055/1.08706. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.49441/1.08451. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.49095/1.08067. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.48251/1.02643. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.46753/1.10916. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.47591/1.08578. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.47692/1.02700. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.46508/1.12393. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.45258/1.11601. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.45101/1.08383. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.47129/1.03022. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.45686/1.15279. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.43657/1.07134. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.43933/1.14392. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.43322/1.13321. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.43308/1.16258. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.43532/1.17524. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.42619/1.12575. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.70584/0.69657. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69386/0.69906. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69282/0.71935. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68516/0.72136. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.67694/0.71728. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67912/0.71549. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.67913/0.69719. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67110/0.71345. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.66793/0.71954. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.66743/0.71818. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.66728/0.71409. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66639/0.75809. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66269/0.74679. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66195/0.74147. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66931/0.75701. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65562/0.78512. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65677/0.77144. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65797/0.76816. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66209/0.74939. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65690/0.76273. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64792/0.77467. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.65485/0.76470. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64346/0.82098. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.63999/0.81858. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65148/0.84506. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.63811/0.83766. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64355/0.83628. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63118/0.85784. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.63918/0.83085. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.63097/0.88928. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.62919/0.85635. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.62702/0.89526. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62786/0.86252. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61957/0.89145. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62953/0.89568. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.61639/0.92209. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.64267/0.86394. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.63613/0.81172. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.61984/0.84238. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.60958/0.87258. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.61984/0.83694. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60442/0.87321. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60216/0.87231. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59335/0.85793. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60755/0.88339. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.60406/0.87115. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.59481/0.89082. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58854/0.88580. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.57478/0.93850. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57472/0.89875. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.59263/0.85621. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.58535/0.87106. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.57507/0.89752. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55859/0.93914. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.55271/0.94216. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.56385/0.92860. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56094/0.88591. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.56202/0.92585. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.55017/0.87071. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.55075/0.92135. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54095/0.92046. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.54143/0.89894. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.53511/0.94348. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.52563/0.99782. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.51819/0.94882. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.51303/0.97992. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.51094/1.05817. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.49517/1.06939. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49231/1.03053. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.49690/1.06613. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49403/1.01663. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.47673/1.04733. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48030/1.11220. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.47779/1.07208. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.47462/1.07162. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.47925/1.04827. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46535/1.08629. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.44236/1.09904. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.46306/1.16947. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44260/1.05602. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.42142/1.21708. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.42400/1.16746. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.42802/1.12099. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.42919/1.15352. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.41886/1.12193. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.40965/1.22360. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.40249/1.25161. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.41448/1.21105. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.38867/1.25966. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.40583/1.24570. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.39482/1.19354. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.39772/1.17117. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.40511/1.17934. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.37514/1.16777. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.36982/1.16799. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.37508/1.16119. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.35360/1.22835. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.37658/1.18136. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.34576/1.23155. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36498/1.32107. Took 0.43 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.72945/0.70314. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70783/0.70393. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.70929/0.68062. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69864/0.68789. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.70207/0.69343. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69402/0.68739. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68893/0.69018. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.69258/0.68737. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69272/0.70322. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68846/0.70320. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67626/0.73575. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.69070/0.73570. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.68987/0.72905. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.68655/0.73950. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67513/0.74803. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67245/0.73389. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67111/0.74484. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67534/0.79797. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66632/0.75642. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66591/0.78485. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66071/0.81706. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.65989/0.81093. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65223/0.84066. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64676/0.83936. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65663/0.86484. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65625/0.82246. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65246/0.79484. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65255/0.79831. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63728/0.81375. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64295/0.86713. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.64280/0.82969. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.63585/0.86834. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62874/0.86232. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.62339/0.90118. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.63411/0.87111. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.62395/0.91029. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61924/0.87611. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62025/0.78287. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.61376/0.88324. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.61739/0.81041. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.59790/0.91475. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.62042/0.88039. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.60947/0.90883. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60866/0.86483. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.61721/0.89811. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59629/0.93878. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58357/0.94248. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.59347/0.93388. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.57650/0.93356. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.58082/0.97686. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.57441/0.94879. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.58462/0.89472. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.58209/0.95068. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54949/1.00063. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.56900/1.01036. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.56443/1.03992. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.54577/1.00838. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.54063/1.12248. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54430/1.00504. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.54105/1.08571. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.53577/1.00918. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.53692/1.10468. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52568/1.08519. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.53035/1.12295. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.53004/1.08236. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.52283/1.06704. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51652/1.17056. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.48619/1.16907. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.50925/1.22892. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.49386/1.14320. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49503/1.17500. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.46432/1.18728. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.45701/1.19306. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.45378/1.16493. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.45523/1.22475. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44103/1.32478. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.45115/1.06548. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.44112/1.28199. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.44004/1.20015. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44046/1.15017. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.42082/1.30231. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.42078/1.37129. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.43782/1.26317. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.41972/1.41424. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.41237/1.36135. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.41512/1.27788. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.38942/1.26843. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.41058/1.32219. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.41734/1.48045. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.38458/1.41586. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.38518/1.51547. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.36733/1.49494. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.37506/1.44882. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.37057/1.48027. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.36902/1.54802. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.35727/1.48396. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.34789/1.46221. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.37534/1.42492. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.34896/1.45754. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.35487/1.53206. Took 0.44 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.70018/0.69086. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68727/0.69569. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69327/0.70650. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68598/0.72322. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68633/0.73286. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68346/0.73728. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68408/0.73341. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68417/0.73866. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68108/0.73520. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67938/0.73522. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67515/0.75311. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67263/0.74705. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67391/0.75105. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67180/0.75701. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66671/0.78150. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66674/0.78429. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66098/0.79191. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65625/0.79578. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65840/0.79809. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66270/0.79457. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65570/0.79518. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65715/0.78691. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65088/0.79616. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64517/0.79869. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65087/0.79454. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.63503/0.79006. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64057/0.79936. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63189/0.79099. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.61744/0.78620. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.61949/0.80678. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61894/0.80860. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60741/0.82287. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60777/0.82986. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61072/0.77116. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60808/0.80019. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59747/0.80683. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.58865/0.83637. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.58568/0.83233. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59342/0.82320. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57644/0.83602. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58389/0.81029. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.58199/0.82511. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.56227/0.85442. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56794/0.87390. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.56599/0.85978. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55292/0.88310. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.54903/0.85884. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.53437/0.86199. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.54702/0.87488. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.54414/0.88021. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53525/0.88790. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.52439/0.90968. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.51942/0.90242. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.51177/0.94923. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51360/0.88188. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.52162/0.93490. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.50728/0.93609. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.49632/0.95369. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.49279/0.94076. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.48502/0.95486. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.49255/0.90107. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.49202/0.93314. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48596/0.94816. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.46034/0.99404. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.46654/0.95547. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.47270/0.94627. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.48139/0.96318. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.45220/1.01897. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.46599/1.05201. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46469/1.03491. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.43832/1.03870. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.45874/1.08096. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.44366/1.06717. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.46803/1.02730. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.43646/1.04068. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.44153/1.01162. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.42769/1.04641. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.43040/1.04691. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.42073/1.07829. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.43187/1.12768. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.39892/1.16430. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.39431/1.10370. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.42169/1.14357. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40683/1.08653. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.37629/1.19011. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.37298/1.21764. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.39449/1.11289. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.38761/1.13362. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.42657/1.11164. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.40032/1.15752. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.39707/1.10746. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.37139/1.14661. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.37068/1.15732. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.39969/1.18606. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.36951/1.16934. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.37600/1.19422. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.36772/1.17269. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.37142/1.16115. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.35348/1.20868. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.36263/1.17727. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69678/0.69015. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.68621/0.69351. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68755/0.68331. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68150/0.67999. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68033/0.67703. Took 0.48 sec\n",
      "Epoch 5, Loss(train/val) 0.67891/0.67582. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.67603/0.66929. Took 0.47 sec\n",
      "Epoch 7, Loss(train/val) 0.66782/0.68279. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67444/0.70080. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.66622/0.71290. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66992/0.68689. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66140/0.68714. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66681/0.68425. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67569/0.69905. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66067/0.70511. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65256/0.70741. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65741/0.69411. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.64968/0.73041. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65473/0.72725. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65929/0.70781. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64096/0.73051. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64441/0.73461. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65476/0.71078. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.63861/0.74171. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63315/0.73668. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63917/0.72568. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63761/0.73856. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63392/0.74619. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62661/0.74007. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62259/0.78222. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.62016/0.77930. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.62464/0.78389. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61098/0.79648. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.60563/0.82573. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.60314/0.84921. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61004/0.90723. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.59635/0.85493. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60186/0.85087. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58043/0.90670. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59442/0.90596. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.58663/0.93678. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.59105/0.95024. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57686/0.95751. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56707/1.06994. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.57141/0.99582. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.55948/1.07464. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55881/1.02265. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55795/1.07330. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.54721/1.05075. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.53613/1.03259. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53208/1.06176. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.53556/0.99677. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54529/0.96761. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.53024/0.96927. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51405/1.00773. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.50705/0.96023. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.50910/1.10335. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.50105/1.08894. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.51398/1.05411. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50781/1.04692. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49388/1.13850. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.49036/0.94487. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.47474/1.04872. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.46461/1.05833. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.45704/1.08351. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.46824/1.09666. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.47158/1.10653. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.46417/1.06693. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.45460/1.13692. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45072/1.11447. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.43189/1.10158. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.42617/1.11201. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.42142/1.13057. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.43066/1.10319. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.41084/1.12418. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.40561/1.14407. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.40907/1.19262. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.40916/1.17850. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.40269/1.15520. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.40783/1.16911. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.37988/1.17837. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.39794/1.20522. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.38209/1.22747. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.37847/1.22698. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.37881/1.18201. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.36362/1.25610. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.35762/1.28196. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.36433/1.26917. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.34138/1.37873. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.34146/1.20698. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.35139/1.35064. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.32174/1.42243. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.34835/1.24931. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.37508/1.27972. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.34716/1.29616. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.31961/1.45258. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.30135/1.39404. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.32624/1.42493. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.32831/1.38436. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.30557/1.34448. Took 0.45 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.72534/0.74224. Took 0.59 sec\n",
      "Epoch 1, Loss(train/val) 0.71519/0.71691. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69244/0.73617. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69973/0.75109. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.70327/0.72319. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69092/0.70854. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.68761/0.72502. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69245/0.72619. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69956/0.72070. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.69545/0.72889. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.69045/0.71111. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.69060/0.71130. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68938/0.70056. Took 0.49 sec\n",
      "Epoch 13, Loss(train/val) 0.69482/0.71710. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.68956/0.71652. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67889/0.70489. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.68367/0.71677. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67534/0.73663. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.68354/0.71470. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.68400/0.72212. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.67027/0.72761. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.67313/0.70269. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66984/0.73005. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.67028/0.74562. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.67089/0.74707. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66968/0.72902. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.66943/0.75780. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.66349/0.76172. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.66319/0.77683. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.66750/0.78382. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.66048/0.77350. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.65628/0.75790. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.65916/0.77679. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.66525/0.75702. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.65018/0.75756. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.64257/0.78885. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.64811/0.76832. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.64828/0.78638. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.64023/0.81227. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.64796/0.80554. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.63746/0.78509. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.64009/0.79874. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.63805/0.82176. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.63277/0.84026. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.63701/0.82867. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.65127/0.78143. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.64906/0.80985. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.64419/0.80202. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.64210/0.79898. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.64243/0.78925. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.63774/0.78948. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.63957/0.80274. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.63986/0.79916. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.63597/0.79953. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.63277/0.80248. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.63065/0.82092. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.63125/0.78875. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.62692/0.80861. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.62053/0.83394. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.62139/0.82742. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.62282/0.81037. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.61348/0.81266. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.61756/0.82974. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.61442/0.86142. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.61300/0.85921. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.59618/0.86465. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.60634/0.93537. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.59983/0.89001. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.58790/0.91066. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.60050/0.90684. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.59874/0.92544. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.57261/0.90407. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.57211/0.90828. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.56549/0.97031. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.56290/0.95449. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.55982/1.00807. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.55417/1.01215. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.54998/0.94996. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.57733/0.98481. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.55730/0.99913. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.53919/1.03796. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.54087/0.99672. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.55602/1.06503. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.54528/1.01245. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.54529/1.01394. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.52909/1.09795. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.54018/1.04277. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.53775/1.08676. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.52119/1.07549. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.53149/1.08468. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.52177/1.08921. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.52818/1.08621. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.51949/1.16230. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.52000/1.12725. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.52189/1.08885. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.51382/1.12637. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.51185/1.10929. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.51211/1.17100. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.50415/1.15832. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.48253/1.21871. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.71525/0.69975. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69826/0.69548. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69311/0.68823. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68328/0.71313. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68608/0.70082. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67810/0.70375. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.67593/0.72959. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67484/0.75517. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67918/0.76561. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66491/0.78217. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.66414/0.76161. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66074/0.75768. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65605/0.75014. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.66057/0.74154. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66404/0.75487. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66471/0.75236. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.64860/0.75617. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65255/0.75994. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64151/0.75341. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64665/0.76215. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65404/0.78240. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64815/0.75694. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.63877/0.75616. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63632/0.74720. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63214/0.75700. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.63208/0.74679. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63039/0.75516. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62740/0.75535. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64104/0.75414. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62602/0.76621. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62352/0.76146. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.61963/0.76680. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.63046/0.76225. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.62140/0.75623. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.61865/0.75902. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61421/0.74089. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60387/0.75286. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61540/0.76996. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60737/0.76286. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59733/0.77570. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60541/0.76913. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.58727/0.77849. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60111/0.78327. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58676/0.77112. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59709/0.78983. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.58507/0.79374. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.58000/0.78720. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.58974/0.80544. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58421/0.80741. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.56202/0.81723. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.56164/0.80748. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.56397/0.83438. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.56605/0.80144. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.57926/0.80141. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56601/0.79527. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.56707/0.80949. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.56226/0.79098. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.54734/0.78988. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.55428/0.80180. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.55180/0.81147. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.54361/0.82218. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.53372/0.81884. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.54142/0.83806. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.52960/0.87570. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.53549/0.86525. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.54272/0.87607. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.52394/0.90910. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.51981/0.88057. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.54240/0.88509. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.52764/0.87549. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.51838/0.91706. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.50450/0.93942. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.51997/0.91978. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.51194/0.92295. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.49785/0.93512. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.50688/0.94565. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.50549/0.99197. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.51399/0.92449. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.48990/0.95167. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.48182/0.93514. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.49306/0.97626. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.46900/0.99461. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.47630/0.96786. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.46406/0.97594. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.46625/0.98511. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.44957/1.04100. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.43737/1.01254. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.46307/1.06140. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.45908/0.96073. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.43775/1.04065. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.43051/1.03807. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.44755/1.02274. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.43404/1.01508. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.41848/1.00866. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.44723/0.97922. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.43025/1.03256. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.42389/1.06772. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.43274/1.06578. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.45043/1.03402. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.42110/1.00191. Took 0.44 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.71192/0.72164. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70378/0.74749. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.70018/0.72409. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68917/0.72387. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68780/0.73272. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68398/0.74861. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68697/0.74497. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68574/0.75143. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68142/0.75679. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68205/0.75992. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67793/0.76637. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67828/0.76177. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67464/0.75281. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67095/0.75571. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66707/0.77796. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66064/0.79914. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65243/0.79714. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66094/0.81672. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65022/0.83637. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65633/0.81870. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64371/0.81479. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64055/0.81814. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63882/0.80890. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63465/0.84693. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64530/0.82536. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64182/0.82748. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63161/0.84617. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63277/0.85693. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62904/0.84017. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62598/0.83393. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63158/0.82853. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61177/0.86582. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61264/0.89432. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.62375/0.86012. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61300/0.86764. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61063/0.86861. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61315/0.90670. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60612/0.92506. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60087/0.94424. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.59572/0.96871. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57432/0.99581. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59419/0.97854. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57053/0.95701. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59279/0.97637. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.57695/1.00243. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.59207/1.03106. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.57821/0.99873. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.56654/1.01097. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56417/1.04574. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57275/1.04502. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56489/1.02661. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.55873/1.01025. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.55025/1.05499. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56747/1.01524. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54025/1.04979. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.53545/1.06973. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55306/1.03887. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53433/1.06684. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.52429/1.08856. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53739/1.04469. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52897/1.06632. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.52139/1.06343. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.50956/1.05883. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.51588/1.07937. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50702/1.06278. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50935/1.07310. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51679/1.05046. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.49859/1.05334. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52161/1.02527. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.50010/1.06583. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49117/1.04495. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.49331/1.05673. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.50457/1.08009. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.48769/1.08685. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.47819/1.12338. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.47701/1.09378. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.47439/1.08896. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.47756/1.03487. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.48825/1.08867. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46905/1.02914. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.46778/1.07845. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.48281/1.13287. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.49471/1.08303. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.46363/1.10567. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.45888/1.11745. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.46989/1.14035. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.44313/1.14842. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.45004/1.15040. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.44878/1.12585. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.43246/1.18235. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.46179/1.11652. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.42574/1.16851. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.42914/1.22153. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.45005/1.15982. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.42639/1.17498. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.41083/1.22502. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.43509/1.21980. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.42631/1.18579. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.41077/1.20683. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.41954/1.17272. Took 0.44 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.71266/0.69842. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70911/0.68347. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69335/0.69125. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69844/0.68947. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69383/0.68140. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.69139/0.69110. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69179/0.68369. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68725/0.68574. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68468/0.67992. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.68349/0.68239. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68693/0.68006. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.68460/0.67847. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.68148/0.68240. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67885/0.68783. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.68074/0.69159. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68396/0.69536. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67673/0.69771. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67783/0.70037. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.67267/0.71734. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.67017/0.71277. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67207/0.72092. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.67116/0.71477. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.67048/0.72575. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66741/0.71733. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.66713/0.72314. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.66748/0.72518. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.66657/0.71799. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.65762/0.72536. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65993/0.71429. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.65495/0.71247. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.65560/0.71288. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.65242/0.70850. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.65361/0.71605. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.65304/0.72025. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.65375/0.72005. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.64101/0.71333. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.64690/0.72049. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.63646/0.73948. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.64049/0.74550. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.63159/0.75412. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62621/0.76281. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.62471/0.77865. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.61734/0.79033. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.62217/0.81816. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.62772/0.78429. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.61521/0.80792. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.61732/0.80595. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.60622/0.82543. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.60562/0.79913. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.59908/0.85768. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.59201/0.83393. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.60369/0.81607. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.58925/0.81158. Took 0.47 sec\n",
      "Epoch 53, Loss(train/val) 0.60083/0.81645. Took 0.47 sec\n",
      "Epoch 54, Loss(train/val) 0.58899/0.84895. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.58039/0.82089. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.57722/0.81952. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.57059/0.85339. Took 0.47 sec\n",
      "Epoch 58, Loss(train/val) 0.56884/0.85814. Took 0.47 sec\n",
      "Epoch 59, Loss(train/val) 0.56073/0.85632. Took 0.47 sec\n",
      "Epoch 60, Loss(train/val) 0.55752/0.87674. Took 0.47 sec\n",
      "Epoch 61, Loss(train/val) 0.56967/0.80954. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.56638/0.79671. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.55616/0.81730. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.56405/0.80463. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.55169/0.85120. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.53524/0.77725. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.54564/0.80183. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.54711/0.85352. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.55054/0.80595. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.52893/0.79814. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.53796/0.88728. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.54217/0.78438. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.52837/0.80882. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.52528/0.82969. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.51677/0.88227. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.52501/0.89608. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.51695/0.85330. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.51892/0.88379. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.50602/0.93875. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.50434/0.92528. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.50802/0.90507. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.49979/0.86487. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.50037/0.93683. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.49747/0.90548. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.49827/0.94194. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.48567/1.01419. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.47611/0.97634. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.47468/0.93901. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.46551/1.00371. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.47125/0.93547. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.45383/1.06921. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.45707/0.96060. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.45330/0.96069. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.45986/0.99799. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.46836/0.96974. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.45168/0.93648. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.43729/1.05488. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.46988/1.04753. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.42554/1.03885. Took 0.43 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.71183/0.72854. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70354/0.72790. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69968/0.72001. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69261/0.71671. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69938/0.71413. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.68901/0.71654. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68823/0.71924. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68803/0.72264. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68302/0.72972. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68943/0.70857. Took 0.48 sec\n",
      "Epoch 10, Loss(train/val) 0.67915/0.71126. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68057/0.72380. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68371/0.72617. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67828/0.74718. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67653/0.73055. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.68538/0.70715. Took 0.48 sec\n",
      "Epoch 16, Loss(train/val) 0.67404/0.71562. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67240/0.72473. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67362/0.72297. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67118/0.72073. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66257/0.72930. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66069/0.73337. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66675/0.74751. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65944/0.73482. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66194/0.75901. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65671/0.78775. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.66933/0.76855. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.65435/0.78986. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65947/0.77226. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.66073/0.77669. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.65906/0.78162. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.64947/0.80150. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64677/0.79630. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.64768/0.83101. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.64388/0.82569. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.63971/0.83153. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.64014/0.81025. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62987/0.83212. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.63323/0.84005. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.64273/0.84177. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.63610/0.82401. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.62105/0.83375. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.61929/0.86820. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.62446/0.85108. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.61597/0.86990. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.63920/0.80402. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.63617/0.81833. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.60942/0.83377. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.60904/0.84549. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.60814/0.89869. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.62021/0.82893. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.61060/0.85368. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.60528/0.87479. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.60912/0.84974. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.59450/0.87850. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.59959/0.88820. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.59568/0.88617. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.58675/0.88778. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.59124/0.90078. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.58885/0.91326. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.59575/0.88484. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.59139/0.91750. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.58514/0.92242. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.58282/0.93248. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.58818/0.91479. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.58252/0.95377. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.58363/0.93095. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.58093/0.91840. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.57809/0.90254. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.58114/0.90912. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.57463/0.90289. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.58496/0.89869. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.57939/0.93390. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.59193/0.89640. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.58130/0.88130. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.57656/0.88402. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.57886/0.91349. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.55149/0.89456. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.56424/0.90912. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.55063/0.85395. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.55627/0.86815. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.55108/0.88801. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.54748/0.89325. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.53271/0.92480. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.54703/0.93445. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.53337/0.92084. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.52169/0.93251. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.52867/0.95776. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.51982/0.93291. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.51977/0.94280. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.50989/0.94511. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.50094/0.96669. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.50664/0.99030. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.50506/0.96254. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.48872/0.99940. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.49241/1.01285. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.50585/0.93942. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.48640/0.98197. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.50331/1.06910. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.49329/0.96413. Took 0.44 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.70208/0.69998. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70264/0.70497. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69874/0.71278. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69172/0.69653. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.69736/0.71633. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68500/0.71043. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.69466/0.71288. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68677/0.71293. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.69029/0.71798. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68725/0.71652. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.69221/0.71656. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.69123/0.72141. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68399/0.73461. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68424/0.74967. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.68873/0.74161. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68562/0.73203. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67440/0.74820. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67284/0.76553. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67641/0.76648. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67930/0.76816. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67460/0.75234. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66975/0.76789. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.67082/0.75798. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.67254/0.76379. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66601/0.77065. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.66595/0.77477. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65659/0.77069. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.65932/0.77571. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.65648/0.78478. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.65132/0.78883. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.65635/0.77644. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.64890/0.77695. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.66728/0.79949. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.65359/0.78148. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.64145/0.79610. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.63724/0.79918. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.64645/0.80614. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.65023/0.82346. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.64148/0.81013. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.64186/0.84145. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.63706/0.83588. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.63468/0.82211. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.63474/0.82952. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.63136/0.83758. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.63198/0.85751. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.62471/0.84019. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.62608/0.84261. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.61318/0.85975. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.61785/0.85832. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.61546/0.85061. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.60466/0.84382. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.61126/0.86957. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.60855/0.84963. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.59471/0.83146. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.59949/0.84934. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.59965/0.84785. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.58592/0.85683. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.58521/0.82091. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.59647/0.77560. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.58477/0.78205. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.57144/0.79632. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.57461/0.77520. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.56691/0.80014. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.57761/0.81317. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.57307/0.81002. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.55765/0.82133. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.57026/0.82757. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.55029/0.83560. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.55021/0.82221. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.56548/0.87022. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.54584/0.82166. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.54695/0.89015. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.54007/0.90492. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.53080/0.91247. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.53719/0.92490. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.52487/0.93072. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.52199/0.97152. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.52894/0.91489. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.52139/0.93105. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.51993/0.96093. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.53012/0.96990. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.51525/1.02018. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.52666/0.96758. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.50219/0.95740. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.51393/0.94442. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.50817/0.94270. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.49241/1.02755. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.51922/0.95784. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.49747/0.93245. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.50742/0.95737. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.50777/0.93809. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.50740/0.97600. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.49007/0.96503. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.49525/0.99975. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.46804/0.99506. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.50510/0.96286. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.48399/0.95711. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.45905/1.03056. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.47182/0.96479. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.45589/1.00248. Took 0.43 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.73149/0.69427. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.71384/0.75185. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.71206/0.73302. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.70822/0.73163. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69367/0.72963. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69730/0.75019. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68844/0.73731. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68993/0.73900. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69291/0.73993. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68470/0.74850. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68675/0.73947. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68819/0.72588. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67555/0.73113. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67474/0.75154. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.68019/0.73311. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68856/0.75983. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67879/0.77544. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67927/0.77315. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.68192/0.75566. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67064/0.78556. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66745/0.79615. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66365/0.79091. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.67095/0.79611. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.66662/0.81384. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.68061/0.77681. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.67614/0.77963. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.66525/0.79130. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.66125/0.76770. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65821/0.77834. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.66678/0.79640. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.67515/0.77465. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.65684/0.77477. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.65473/0.77735. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.65738/0.78983. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.66136/0.78542. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.65503/0.77174. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.65197/0.78490. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.65934/0.79715. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.66082/0.76306. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.64684/0.77939. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.64427/0.79035. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.64638/0.80541. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.63564/0.80465. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.64375/0.79238. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.63855/0.81498. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.64771/0.78938. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.64531/0.78395. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.63813/0.78921. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.63234/0.80642. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.63944/0.80029. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.62834/0.83814. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.62789/0.80827. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.62175/0.82573. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.61253/0.82792. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.61027/0.85745. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.60921/0.84564. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.61369/0.86013. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.61688/0.86340. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.61023/0.88458. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.59782/0.87765. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.61097/0.88926. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.61348/0.85242. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.59505/0.89583. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.59741/0.92445. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.59157/0.91632. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.58175/0.94387. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.57846/0.93970. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.58226/0.93788. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.57800/0.95769. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.57560/0.94520. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.56574/0.93951. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.57361/0.96214. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.57377/1.00968. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.55726/0.96895. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.56724/0.98341. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.55103/0.99379. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.54544/1.01496. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.55554/0.95130. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.56237/0.97596. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.55371/1.00603. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.53819/1.02190. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.53545/1.02407. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.53435/1.02859. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.52670/1.05103. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.52535/1.08396. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.53331/1.10933. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.51846/1.08797. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.52897/1.03085. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.53388/1.02798. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.50479/1.03092. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.50269/1.10150. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.50323/1.07272. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.52246/1.12343. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.50507/1.03580. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.50050/1.10104. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.49747/1.13723. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.47038/1.16348. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.50602/1.14621. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.49084/1.06881. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.47415/1.10884. Took 0.43 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.70140/0.71241. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68966/0.71189. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68430/0.70879. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68452/0.72389. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.67864/0.73145. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.67263/0.74597. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.66991/0.73243. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.66410/0.75439. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.66235/0.77580. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66312/0.76842. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.66608/0.78955. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67000/0.76959. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65500/0.80615. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67149/0.77652. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66493/0.78528. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65239/0.81802. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65447/0.79511. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65599/0.80186. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.64822/0.80175. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.63602/0.81721. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64424/0.81844. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.63554/0.79381. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63016/0.79080. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.62715/0.79413. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62891/0.80017. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.62386/0.79142. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62090/0.79020. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62366/0.81731. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61627/0.82090. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.61670/0.81581. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60900/0.83260. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61303/0.82662. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.60188/0.82940. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61349/0.81190. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59901/0.83233. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59682/0.83478. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.59525/0.82611. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59056/0.82655. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59686/0.80268. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58488/0.84467. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.57462/0.84826. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.58263/0.84431. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56700/0.86635. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.56618/0.85568. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56255/0.89710. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56172/0.91151. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56921/0.84948. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55964/0.90408. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55573/0.89891. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.55569/0.90075. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.53265/0.91757. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54191/0.84440. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54366/0.85388. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.53788/0.87012. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54973/0.86862. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.54086/0.88287. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.53082/0.86988. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51808/0.84984. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.51887/0.87499. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.51721/0.89802. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51420/0.92335. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.51887/0.90061. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51612/0.88554. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.50616/0.89649. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.50985/0.91687. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.49983/0.91795. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.50776/0.86524. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.51486/0.88749. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.49364/0.91699. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.48007/0.94837. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48523/0.94999. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.45581/0.92674. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.45845/0.95486. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.45722/1.00991. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.45167/0.96956. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44255/0.99689. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.46573/1.05601. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46361/1.01322. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.45240/1.05232. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.43367/1.01449. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.41513/1.02682. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.44535/1.04252. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.41911/1.07603. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.43890/1.01418. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.41799/1.04330. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.38703/1.06826. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.39177/1.12750. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.40201/1.13465. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.37553/1.12374. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.39121/1.20267. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.39755/1.23072. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.39333/1.14219. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.36667/1.15011. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.37748/1.12792. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40955/1.16916. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.36045/1.16719. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.36970/1.21511. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.35190/1.19575. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.33075/1.16876. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.34787/1.17394. Took 0.44 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.71742/0.72799. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70790/0.72160. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70539/0.71072. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.70544/0.71565. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69126/0.73601. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68782/0.71790. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69214/0.73113. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67893/0.72764. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68597/0.73098. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68407/0.71617. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68901/0.72205. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67261/0.72591. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68594/0.71366. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.68870/0.71146. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67302/0.70523. Took 0.47 sec\n",
      "Epoch 15, Loss(train/val) 0.67005/0.71421. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67365/0.72214. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66656/0.71845. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66848/0.72557. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67273/0.71323. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66773/0.74542. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.66586/0.73367. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.67078/0.75258. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.67404/0.73125. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66704/0.70734. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66529/0.72328. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.66605/0.72302. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.66029/0.72540. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65418/0.76001. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64151/0.77212. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.65571/0.78339. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.65819/0.75321. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64173/0.76761. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.64721/0.77526. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.64796/0.79128. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.65676/0.77101. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.64744/0.75895. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.65886/0.73499. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.65418/0.75322. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.64296/0.74182. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.64246/0.74977. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.65867/0.75348. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.63275/0.75755. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.62277/0.79582. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.63004/0.78044. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.63582/0.82321. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.62502/0.82930. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.62183/0.76179. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.61635/0.78334. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.62947/0.78537. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.61157/0.81347. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.61531/0.79986. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.61041/0.78024. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.59765/0.80091. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.60070/0.81078. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.59279/0.81083. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.58820/0.80053. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.59344/0.82522. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.60299/0.86291. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.58973/0.85787. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.57260/0.85847. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.58241/0.87116. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.56186/0.85257. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.57199/0.90153. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.58159/0.89149. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.56762/0.88317. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.56385/0.88421. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.57532/0.93173. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.55979/0.96297. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.56919/0.91674. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.54385/0.93654. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.55672/0.92981. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.56487/0.93960. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.53789/1.02284. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.53314/0.96839. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.53221/1.01515. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.52258/1.03459. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.52025/1.03761. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.52479/1.02081. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.52372/0.98805. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.49348/1.06239. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.52775/1.03974. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.50731/1.06756. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.50693/1.02323. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.48873/1.10342. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.50085/1.08212. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.52219/1.05264. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.48435/1.11901. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.48710/1.08798. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.49705/1.14143. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.48269/1.16877. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.47365/1.12962. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.49100/1.27060. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.48654/1.21414. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.47142/1.22120. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.46535/1.22377. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.47701/1.18351. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.45338/1.21221. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.43985/1.25351. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.46200/1.27691. Took 0.43 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.71544/0.72691. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70538/0.72831. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69976/0.72495. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69632/0.74463. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69074/0.76209. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68051/0.76260. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.67968/0.78108. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68521/0.76580. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67844/0.76409. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.66939/0.78781. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67331/0.78579. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67441/0.79277. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66344/0.78900. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.66721/0.77569. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66575/0.79785. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66808/0.80480. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66595/0.79035. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65558/0.81763. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66267/0.80069. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65440/0.80851. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65309/0.82387. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64073/0.78777. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64485/0.81685. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.63749/0.82330. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64748/0.84289. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63575/0.80507. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.62351/0.81692. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62585/0.81980. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61764/0.81437. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62526/0.79182. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.62588/0.79780. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60735/0.82200. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61334/0.80603. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.61807/0.79011. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.60187/0.82604. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60818/0.82003. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61065/0.82633. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.60637/0.80852. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60701/0.83702. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60712/0.80205. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.59957/0.82841. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58805/0.83941. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57835/0.81743. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57391/0.88072. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.59479/0.83518. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.58390/0.86136. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56478/0.91990. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.57495/0.89545. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56292/0.91297. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.57437/0.91094. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.56254/0.93606. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.56798/0.89134. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54962/0.98341. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55524/0.97022. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.54901/1.10888. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55096/1.03715. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.53576/1.10656. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.53267/1.04031. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.53857/1.07186. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.50473/1.11203. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.50009/1.16885. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.50994/1.17533. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52078/1.17647. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.50786/1.17521. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.49003/1.33722. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.49064/1.11794. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.51300/1.19569. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48512/1.24956. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.48848/1.34212. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.47766/1.31388. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48231/1.51928. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.49230/1.30995. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.45794/1.45448. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.44196/1.42790. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.45165/1.33875. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.46846/1.31671. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.46155/1.32055. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.44829/1.39707. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44885/1.45735. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.44051/1.50315. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.44562/1.50401. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.44141/1.54938. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.40952/1.48903. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.41776/1.51808. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.40943/1.54883. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.43103/1.42691. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.40467/1.68200. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.40087/1.49761. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.40565/1.45766. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.39738/1.60713. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.38750/1.65442. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.38952/1.53879. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.38372/1.42506. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.38759/1.47652. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.38259/1.55792. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.36994/1.54268. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.37413/1.60171. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.37014/1.55897. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.36713/1.68076. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36985/1.60308. Took 0.43 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.70347/0.70322. Took 0.67 sec\n",
      "Epoch 1, Loss(train/val) 0.69322/0.72349. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69615/0.73474. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69507/0.74341. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69419/0.75168. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68836/0.76016. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68069/0.76987. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68550/0.78204. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67528/0.78782. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67853/0.77631. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67618/0.78284. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67762/0.79530. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67102/0.77897. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67258/0.77301. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67613/0.77367. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67003/0.78082. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65866/0.79315. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66325/0.79454. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66626/0.78474. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65821/0.81469. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65263/0.80710. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66255/0.80781. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64918/0.82717. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.64824/0.83815. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64369/0.82625. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64863/0.82271. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64160/0.83131. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63646/0.87551. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64020/0.87104. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63427/0.86990. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64887/0.79771. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64220/0.77843. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63998/0.86454. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63537/0.84652. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62809/0.86451. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.62223/0.87737. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.62541/0.86929. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61641/0.89457. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62537/0.87785. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.61026/0.90182. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60263/0.87690. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60321/0.89729. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.59600/0.89558. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.59898/0.90408. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59181/0.94480. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.58300/0.97503. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.58178/0.97647. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.57598/0.97925. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57991/1.00986. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.57352/0.90596. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57644/0.93511. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.58675/0.97780. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.58254/1.00664. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.57482/1.01623. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.57290/1.05976. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.57613/1.09415. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56251/1.09940. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.57218/1.00068. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.55866/1.05937. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.54513/1.09502. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54768/1.15298. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.53870/1.17273. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54112/1.13240. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.53357/1.27035. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.53418/1.25789. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.53163/1.21491. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.52758/1.28145. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.50921/1.34499. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.51773/1.13614. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.51555/1.20682. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49907/1.37549. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.51649/1.49594. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.49411/1.49753. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.49813/1.24504. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.49299/1.20436. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48113/1.47253. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.48552/1.39614. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.47605/1.50411. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.47497/1.59593. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.45127/1.39532. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.46265/1.53080. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.45609/1.57579. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.45386/1.57300. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.44284/1.55655. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.46097/1.61475. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.46703/1.69188. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43582/1.68235. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.46355/1.58647. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.43963/1.68293. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.42115/1.62421. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.42112/1.61773. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.41967/1.75344. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.42462/1.72253. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.40173/1.72534. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.38931/1.80351. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.40474/1.75620. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.37574/1.77573. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.40135/1.81356. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.38981/1.79294. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.37270/1.80948. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.70054/0.73121. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69148/0.72680. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68951/0.73498. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68497/0.72710. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68721/0.71937. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.68239/0.72317. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67843/0.72112. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67702/0.72801. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67834/0.73414. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67904/0.73441. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67591/0.72982. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67010/0.73452. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67899/0.74079. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67726/0.73328. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67250/0.74772. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67673/0.73668. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67955/0.72605. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.67049/0.71089. Took 0.47 sec\n",
      "Epoch 18, Loss(train/val) 0.67212/0.70806. Took 0.47 sec\n",
      "Epoch 19, Loss(train/val) 0.67137/0.71349. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66969/0.71556. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.66290/0.72624. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66472/0.72741. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.66021/0.74327. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65831/0.73417. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65574/0.74701. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65437/0.74392. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65613/0.74985. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.65398/0.75389. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65620/0.78642. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64932/0.80131. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64040/0.81084. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.65048/0.78552. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64660/0.79683. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.64576/0.81498. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.64437/0.79943. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.64187/0.81073. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.63296/0.82014. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.63433/0.82126. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.63219/0.82488. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62175/0.82565. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61368/0.83960. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.61104/0.85535. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.61639/0.78026. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.61701/0.84552. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59750/0.88322. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59714/0.83145. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.59496/0.84889. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58741/0.85794. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57108/0.84764. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57628/0.82651. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.56807/0.88514. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.55129/0.88397. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.56024/0.90047. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54600/0.95622. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55420/0.91374. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53853/0.93221. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.52200/0.93762. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.52921/1.00542. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.50832/0.98124. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52097/0.97194. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.51127/1.02421. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.51594/0.98871. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.50064/1.06044. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.50212/1.09111. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.49027/0.99536. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48390/1.07079. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48717/1.04779. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.47738/1.10134. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.47261/1.10866. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.45649/1.14731. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45528/1.13374. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46780/1.17475. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46943/1.18668. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.45335/1.13965. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44174/1.13334. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43637/1.15438. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.46862/1.06240. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.42523/1.15925. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.41020/1.16514. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.43599/1.18501. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.40972/1.20622. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.41036/1.27730. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.41659/1.20305. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.40692/1.20045. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.39305/1.18557. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.38270/1.30595. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.38032/1.24007. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.39573/1.23843. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.38367/1.29871. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.35578/1.26679. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.32415/1.36736. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.34746/1.34546. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.35519/1.37658. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.33583/1.43159. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.32988/1.45074. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.30568/1.41209. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.32598/1.51516. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.33457/1.51859. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.29364/1.58032. Took 0.43 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.71489/0.71213. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.71117/0.72080. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.70625/0.72025. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69413/0.72636. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69303/0.72768. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.70211/0.74829. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68914/0.75104. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68476/0.74218. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68441/0.75720. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68135/0.73904. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68375/0.72894. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67761/0.75087. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67853/0.74558. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66791/0.73666. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67724/0.71703. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68488/0.74074. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67768/0.72717. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.68026/0.76112. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67147/0.75747. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67053/0.75262. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66651/0.74836. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.67301/0.75585. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.66731/0.74604. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.66903/0.75241. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66119/0.76898. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.65978/0.78014. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.66008/0.75499. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65925/0.77061. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.66125/0.78632. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.65674/0.78199. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.65490/0.76146. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.65438/0.75978. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.65312/0.76374. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64571/0.76849. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.64250/0.77484. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.64581/0.78769. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.64419/0.80536. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.64426/0.79797. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.64729/0.78626. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.64531/0.79584. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62911/0.85514. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.63127/0.85331. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.63211/0.84767. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61659/0.85397. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.61754/0.86642. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.61117/0.87867. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.61189/0.86825. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.62394/0.84627. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.61772/0.86117. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.60857/0.89342. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.60945/0.85763. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.61082/0.86689. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.60005/0.88082. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.60381/0.88509. Took 0.42 sec\n",
      "Epoch 54, Loss(train/val) 0.58223/0.88837. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.58751/0.90404. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.58745/0.88188. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.57122/0.89714. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.57189/0.88468. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.55202/0.93439. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55013/0.92684. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.55490/0.93129. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54715/0.94994. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.53315/0.96286. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.53425/0.96663. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.52613/0.96346. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51072/0.98526. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.51926/0.97250. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49931/1.00588. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.51020/1.00725. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48890/0.99788. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.50608/1.03507. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.47372/1.03701. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48550/1.08055. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.46455/1.06173. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.47364/1.05794. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.44649/1.07477. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46203/1.10507. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.46924/1.03809. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.44451/1.12387. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.45723/1.07363. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.46162/1.05890. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.44659/1.10986. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.43550/1.11125. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.46085/1.10298. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.44187/1.01368. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.42053/1.13057. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.40620/1.13916. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.40764/1.17649. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.42133/1.10569. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.41130/1.22634. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.37162/1.17240. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.39844/1.18173. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.37703/1.17652. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.37227/1.23453. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.37203/1.21219. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.37228/1.22534. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.37242/1.28892. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.35078/1.31126. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34596/1.29087. Took 0.45 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.70657/0.70826. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70730/0.71918. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69757/0.71237. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69426/0.70438. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69273/0.71472. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68391/0.69740. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.68290/0.69922. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68232/0.71325. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68059/0.71437. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67800/0.72026. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67967/0.71613. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67603/0.72931. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67464/0.72786. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67349/0.70635. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67600/0.71146. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66838/0.71126. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66866/0.71310. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66167/0.72140. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66476/0.72572. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65972/0.73509. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65862/0.71438. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64663/0.73448. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64699/0.75248. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64855/0.76427. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65805/0.77040. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64386/0.78859. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64272/0.80929. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64486/0.81681. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63558/0.84213. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62752/0.86270. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63320/0.82479. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61606/0.85613. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61778/0.83212. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.62352/0.85656. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.61062/0.84594. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.61022/0.83394. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61239/0.85620. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.60077/0.85820. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.59971/0.89048. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60137/0.87771. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59464/0.89426. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57647/0.85614. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.58039/0.88064. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.58465/0.88468. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.57900/0.90656. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.56696/0.92485. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57404/0.91613. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.55483/1.01147. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.56533/0.97437. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54974/0.98164. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.54749/1.01561. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54069/0.97560. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.55473/1.01647. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.54009/1.04420. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.53116/1.07602. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.53354/1.08754. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.52823/1.06859. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51736/1.07954. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.50891/1.02383. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.51922/1.07144. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51464/1.06714. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.50226/1.14171. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51295/1.07874. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.51144/1.13620. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49878/1.11404. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.51728/1.04254. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.49065/1.03282. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.48509/1.08599. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.47630/1.16880. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.47761/1.08890. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.46726/1.10689. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.45939/1.10879. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.45451/1.10549. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.44879/1.10482. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.45461/1.07908. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49199/1.11159. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.47227/1.12480. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.45281/1.13497. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.42802/1.19363. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.42485/1.15052. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.43053/1.20951. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.41942/1.24140. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.40775/1.22760. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.39891/1.29040. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.38927/1.23697. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.39408/1.26581. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.40770/1.28640. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.38761/1.32808. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.37628/1.30988. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.35473/1.40138. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.37767/1.33851. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.37854/1.39681. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.38881/1.33997. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.36909/1.38688. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.35274/1.41458. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.34612/1.47494. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.33910/1.53204. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.35500/1.54895. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.35177/1.48297. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.35643/1.49219. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69313/0.68392. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69921/0.71138. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69558/0.70911. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69534/0.71981. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69420/0.71931. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68836/0.73521. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69137/0.75055. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68443/0.73383. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.69018/0.73911. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68038/0.73556. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68695/0.73873. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.68013/0.74806. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68159/0.73847. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68730/0.72722. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.68218/0.73804. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67750/0.73762. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67873/0.73075. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.67689/0.73411. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67797/0.74971. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.67340/0.75546. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67374/0.71869. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66510/0.72332. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.67239/0.71572. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.67069/0.74580. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.67310/0.72998. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.66767/0.73582. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.67183/0.71734. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.66784/0.73640. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.66440/0.73095. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.65902/0.74322. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.67707/0.73875. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.66331/0.74589. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.66245/0.74266. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.65596/0.75826. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.65628/0.77211. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.66345/0.75583. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.65327/0.78983. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.65268/0.79380. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.65159/0.79454. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.65077/0.80901. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.64653/0.80088. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.64716/0.79989. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.64037/0.84876. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.64234/0.83599. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.64068/0.78256. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.63982/0.78923. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.62936/0.80979. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.64050/0.78713. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.63334/0.77982. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.61742/0.85371. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.62306/0.81689. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.62468/0.82222. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.62809/0.83535. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.60816/0.86147. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.62424/0.82973. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.61194/0.87134. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.60841/0.87893. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.60457/0.89358. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.60928/0.83835. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.62386/0.88771. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.59873/0.89933. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.60063/0.87813. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.59480/0.92079. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.58548/0.88352. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.58143/0.89491. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.57523/0.90040. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.57269/0.89615. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.59084/0.91385. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.56744/0.93544. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.56043/0.93855. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.57232/0.96085. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.55142/0.95207. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.54486/0.98494. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.55476/0.98155. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.54325/0.94509. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.54597/0.97954. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.54630/1.01255. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.55939/0.97237. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.55151/0.99723. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.52686/1.01331. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.52362/1.00116. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.52223/1.05553. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.52726/0.96652. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.51242/1.01714. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.50283/1.04602. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.51985/1.01698. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.50620/0.98171. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.49436/1.04398. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.51616/1.09062. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.50597/1.00739. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.48517/1.06647. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.48898/1.02520. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.47270/1.02264. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.48126/1.09811. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.48671/1.06172. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.45896/1.10907. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.46425/1.06909. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.47211/1.12741. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.46277/1.10519. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.44424/1.20279. Took 0.44 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.71257/0.73623. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70511/0.74293. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.70395/0.74056. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69528/0.73942. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69550/0.74012. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69317/0.73920. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68883/0.74100. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68992/0.74269. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69075/0.75025. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68612/0.75998. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67801/0.75646. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67800/0.76395. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67121/0.77391. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67329/0.77037. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66743/0.76883. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66320/0.76362. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66066/0.76554. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.64951/0.77683. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66295/0.78101. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65341/0.79043. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64370/0.79926. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64437/0.80980. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64370/0.82293. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64260/0.83435. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63467/0.83915. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.63077/0.85042. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62408/0.89524. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.61983/0.89901. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62174/0.89292. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62869/0.87619. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61424/0.88959. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60988/0.91609. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.60712/0.94923. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.59986/0.95161. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.59829/0.94463. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.58296/1.02174. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.57801/1.02006. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.56233/1.07745. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.56750/1.12608. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.55771/1.10982. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.55085/1.09783. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.54486/1.12775. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.54032/1.15839. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.54467/1.09120. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.52789/1.18337. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.52037/1.20067. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.51736/1.23860. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.50401/1.14242. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.51137/1.22517. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.50475/1.21615. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.49454/1.18848. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.49114/1.25012. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.48569/1.29150. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.46502/1.30430. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.45230/1.29720. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.46361/1.35475. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.45840/1.39909. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.47976/1.23525. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.46055/1.36737. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.44934/1.40255. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.43868/1.37861. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.43324/1.40529. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.43003/1.34525. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.42717/1.47235. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.40994/1.59302. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.43856/1.38688. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.42823/1.45169. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.42242/1.40279. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.41501/1.37842. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.42401/1.37007. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.38778/1.49876. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.41993/1.46262. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.37201/1.56463. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.39784/1.62945. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.39863/1.53975. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.37404/1.56895. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.36758/1.61931. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.36291/1.64785. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.36868/1.63149. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.38589/1.60459. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.36100/1.58463. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.36478/1.67950. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.33440/1.76613. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.33932/1.66374. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.37993/1.53767. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.35870/1.62135. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.35942/1.66171. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.32341/1.68093. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.32609/1.75218. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.30014/1.65428. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.32258/1.71226. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.31827/1.69915. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.31528/1.70393. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.32127/1.71326. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.30312/1.82459. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.31915/1.88048. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.30160/1.91973. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.30682/1.72867. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.28273/1.84555. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.31766/1.78742. Took 0.44 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.71536/0.70881. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70871/0.70791. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70385/0.69959. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.70694/0.70518. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69629/0.70543. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.70044/0.68653. Took 0.48 sec\n",
      "Epoch 6, Loss(train/val) 0.69539/0.68992. Took 0.42 sec\n",
      "Epoch 7, Loss(train/val) 0.68765/0.69379. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.69094/0.68957. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68893/0.70688. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68370/0.70056. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68387/0.68790. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67887/0.68687. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67988/0.69905. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.68139/0.69861. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66602/0.69135. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67595/0.69635. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67232/0.71162. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66712/0.70342. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65677/0.70320. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66516/0.71224. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65358/0.72988. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65196/0.72446. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64775/0.73490. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64432/0.72354. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64925/0.74824. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64676/0.74344. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64768/0.75521. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64904/0.78833. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63431/0.79361. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63677/0.78670. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.62477/0.81797. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.63113/0.83511. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.62799/0.85976. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.63214/0.84692. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.61844/0.88974. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62405/0.88559. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.62163/0.85860. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60980/0.87918. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.60700/0.90832. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60397/0.91543. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60780/0.89974. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60858/0.86589. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59998/0.86725. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.59235/0.90311. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.58806/0.89238. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.59180/0.88127. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.57911/0.93379. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57278/0.92742. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.56853/0.94732. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55953/0.93921. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56321/0.96194. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.55618/0.94001. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56762/0.93402. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.54657/0.96299. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.53095/0.96507. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.55056/0.96745. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.55145/0.96491. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.53910/0.99635. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52585/0.98284. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.52269/0.98992. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.52272/0.97146. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.51800/1.01973. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.51016/1.04279. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.51328/1.02505. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50165/0.98921. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.49779/1.01207. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.48698/1.04526. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50673/1.01183. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.47680/1.04929. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.49013/1.07631. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.46022/1.09184. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46415/1.14632. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46446/1.05718. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.47480/1.08786. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.46241/1.08115. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46111/1.10408. Took 0.42 sec\n",
      "Epoch 77, Loss(train/val) 0.47317/1.05109. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.48107/1.04851. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.43970/1.08835. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.43122/1.11844. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.45186/1.08986. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.45752/1.06419. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.46282/1.04542. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.43832/1.07399. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.44792/1.05536. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.46033/1.07089. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.42786/1.09871. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.42694/1.20812. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.46605/1.11879. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.41205/1.17233. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.41817/1.09644. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.40832/1.21285. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.41458/1.13131. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.44634/1.16308. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.42377/1.15540. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.39836/1.14842. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.38111/1.16227. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.40162/1.11810. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36226/1.16153. Took 0.44 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69834/0.69952. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69087/0.70746. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69186/0.70183. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69121/0.70602. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69057/0.70626. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68703/0.71425. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.69197/0.71871. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68832/0.72510. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68009/0.72554. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68060/0.73588. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68482/0.75368. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68066/0.75481. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68126/0.77649. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67785/0.78322. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67917/0.78657. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67135/0.78982. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67508/0.81709. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66973/0.80352. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66628/0.80565. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66832/0.85493. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66357/0.83572. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66401/0.85520. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.66298/0.87498. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.65907/0.86724. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66385/0.91628. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.65666/0.90811. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65721/0.93319. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64574/0.90916. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65025/0.92221. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64422/0.92814. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64079/0.93139. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64409/0.91832. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.64059/0.91094. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64607/0.90483. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.63175/0.92403. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.63217/0.97709. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62237/0.96906. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.62846/0.98376. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.61347/1.01722. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.61929/1.04718. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.62111/0.96735. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60923/0.99885. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.61299/0.93784. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.61918/0.97727. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.61479/0.94045. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.60211/0.96052. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.60367/0.88136. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.57897/0.99769. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.59202/0.90682. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.58825/0.88328. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.57772/0.91681. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.58206/0.94131. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56788/0.90293. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56398/0.97444. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56038/1.01204. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55630/0.98758. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55531/0.99024. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.55307/1.00994. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.53429/1.05521. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.53404/1.05326. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.53856/1.05079. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.52016/1.06747. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.51630/1.03717. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.52185/1.09002. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.52243/1.07471. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50562/1.11657. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51245/1.15463. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.49796/1.16875. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49048/1.10249. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.48356/1.14493. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.45932/1.24245. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.46134/1.17839. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.46540/1.19634. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.46234/1.22515. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.44434/1.26340. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.44843/1.30401. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.45569/1.34416. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.43213/1.35273. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44716/1.40416. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.43003/1.32200. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.42681/1.38487. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.39429/1.48821. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.40073/1.52200. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.42463/1.45985. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.41795/1.46206. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.42122/1.49323. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.41090/1.45058. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.39418/1.47702. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.37880/1.51694. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.40533/1.49598. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36921/1.47099. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.38287/1.50083. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.35928/1.62137. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.37394/1.68334. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.37892/1.66458. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.36912/1.58696. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.36556/1.60244. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.35757/1.69118. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.36496/1.75819. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36292/1.76212. Took 0.44 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.71036/0.71494. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70693/0.69712. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.69908/0.69740. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69110/0.69437. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.69326/0.68887. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.69083/0.68749. Took 0.48 sec\n",
      "Epoch 6, Loss(train/val) 0.68147/0.68170. Took 0.47 sec\n",
      "Epoch 7, Loss(train/val) 0.68706/0.67890. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.68408/0.67951. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68150/0.68119. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67987/0.67665. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.67731/0.67090. Took 0.47 sec\n",
      "Epoch 12, Loss(train/val) 0.67652/0.68101. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67686/0.67857. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66600/0.67299. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66751/0.68437. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66425/0.69144. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66645/0.70789. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66240/0.71750. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65419/0.72592. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65490/0.73086. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64437/0.73999. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64070/0.75321. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64288/0.76673. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63759/0.76716. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63596/0.78558. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62810/0.79829. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.62208/0.78984. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.61902/0.81754. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62014/0.83506. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.61445/0.84568. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.60367/0.85787. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61411/0.86456. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.60929/0.83901. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60281/0.84886. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60226/0.82729. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58796/0.84641. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59591/0.85908. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.58567/0.87688. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.58719/0.86849. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58954/0.85855. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58326/0.84628. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58310/0.84322. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.56516/0.87267. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57337/0.92276. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56947/0.90384. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.56585/0.88177. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56490/0.90249. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.54328/0.92936. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.54782/0.92228. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.54928/0.90770. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53084/0.94160. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.52919/0.99957. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52553/0.96143. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51303/0.98176. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.49932/0.96799. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.50930/0.98107. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51182/0.98085. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.50386/0.98269. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53293/0.98967. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50288/1.01246. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.48191/1.04301. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49012/1.07883. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.47762/1.06578. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.48529/1.05639. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.47581/1.11623. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.46804/1.13345. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.46470/1.17206. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.45481/1.12382. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.44676/1.14641. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.45387/1.14739. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.46125/1.16354. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.44121/1.24522. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.43555/1.27503. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.44440/1.29205. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.42417/1.27668. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.43726/1.30568. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.43456/1.34454. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.42750/1.32260. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44200/1.28739. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.41886/1.36413. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.37967/1.45843. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.40974/1.30896. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.36395/1.39249. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.39004/1.41904. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.40176/1.46920. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.38190/1.38513. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.36844/1.40662. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.33709/1.43184. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.36270/1.39055. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.37198/1.46137. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.35658/1.38478. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.38117/1.40198. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.33356/1.49837. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.34555/1.56741. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.36373/1.46525. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.32192/1.53894. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.34037/1.55431. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.36465/1.47494. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34051/1.37567. Took 0.43 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.74671/0.72940. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.73294/0.71147. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.72262/0.70736. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.72110/0.71663. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.71408/0.72310. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.70891/0.71858. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.70676/0.71233. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.70224/0.71546. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.70185/0.72118. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.69558/0.71852. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.69657/0.71838. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.69552/0.73323. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.69608/0.73430. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.69036/0.73114. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.69152/0.73303. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.68192/0.74856. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.68183/0.74336. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67163/0.74146. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.68422/0.74396. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66892/0.74419. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67154/0.75456. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.67339/0.75852. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66341/0.75451. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.66716/0.75585. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66782/0.74114. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.66197/0.74478. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.66336/0.73253. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.65877/0.73792. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65369/0.74754. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.65949/0.74154. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64674/0.75335. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64707/0.75448. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.65473/0.76182. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64061/0.76347. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.65131/0.75131. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.64665/0.75238. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.64114/0.75418. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.64008/0.76037. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.63903/0.75716. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.63747/0.78106. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62488/0.79233. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.62610/0.78717. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.62605/0.78222. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.62017/0.80742. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.62541/0.82779. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.62096/0.84544. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.60900/0.86943. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.62009/0.86849. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.60885/0.87967. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.60498/0.88755. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.60085/0.88151. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.59632/0.90934. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.59965/0.91999. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.59418/0.96535. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.59978/0.92392. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.58498/0.95768. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.58720/0.98061. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.59205/0.95289. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.58219/1.02312. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.58008/0.99279. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.59065/0.94511. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.57444/0.99130. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.56685/1.00673. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.56697/0.95931. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.56927/0.91814. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.54833/0.99965. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.55028/1.05173. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.54960/1.00810. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.55578/1.04814. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.55161/1.03743. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.56944/0.98139. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.55344/1.01727. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.54505/1.07161. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.55552/1.02833. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.56250/0.92224. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.54064/1.05469. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.52787/1.15003. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.53591/1.09838. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.53067/1.01488. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.53620/1.04021. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.53333/1.04753. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.53240/1.02959. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.52840/1.10280. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.52317/1.08703. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.53064/1.13043. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.51256/1.07912. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.51125/1.06606. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.50765/1.10626. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.48886/1.12246. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.50361/1.08265. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.50634/1.12402. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.50918/1.15173. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.49406/1.13969. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.50462/1.14347. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.49358/1.11086. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.49208/1.15898. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.50756/1.13756. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.49951/1.14327. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.50468/1.11343. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.48004/1.12372. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.72557/0.70831. Took 0.62 sec\n",
      "Epoch 1, Loss(train/val) 0.71546/0.68665. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.71719/0.68801. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69518/0.70089. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69225/0.67796. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.69001/0.69781. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68854/0.68947. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67816/0.69474. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67978/0.71142. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67479/0.70177. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67254/0.70296. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66746/0.71109. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66984/0.71445. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67451/0.71345. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66486/0.73475. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66312/0.72254. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66181/0.73299. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.64286/0.74889. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65250/0.73625. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65335/0.72791. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64903/0.75657. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64288/0.74035. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64072/0.75848. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.62517/0.75579. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.62551/0.74796. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.63223/0.73616. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62623/0.74635. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62075/0.76934. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62625/0.77079. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.61533/0.75414. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60993/0.77205. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.61232/0.77602. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62097/0.76319. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59677/0.78497. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.59871/0.80088. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60397/0.80833. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59958/0.82212. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60006/0.80101. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.60772/0.81431. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59645/0.84018. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.58748/0.85840. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.59248/0.85958. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57970/0.87631. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.58539/0.88512. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58577/0.89362. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.56743/0.93761. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57031/0.90318. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.56443/0.93994. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.55585/0.94460. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54694/0.94485. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.53289/0.98511. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53338/0.98264. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.53356/1.04631. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.53567/0.94942. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.51665/0.97937. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51820/0.93225. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53288/0.96319. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52855/0.97979. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50159/0.96574. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50339/1.02794. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.48605/1.02473. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50377/1.01811. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.49673/1.03987. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.47293/1.02113. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.48151/0.99526. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.46743/1.06937. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.45479/1.09841. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.45742/1.08249. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.47144/1.05153. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.43784/1.12628. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.45148/1.16780. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.44529/1.12529. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.44421/1.08358. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.43794/1.14861. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.42069/1.16283. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.41831/1.24048. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.43559/1.13236. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.42238/1.22482. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.41732/1.32460. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.38866/1.26837. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.39346/1.37770. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.42676/1.13337. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.37090/1.38251. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.37937/1.31959. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.40398/1.30420. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.39204/1.13938. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.36271/1.15387. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.37847/1.23845. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.35859/1.34678. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.38141/1.43052. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.36051/1.31949. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.35281/1.40907. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.35394/1.42609. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.37336/1.28613. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.34073/1.43620. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.33002/1.44630. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.32497/1.42225. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.33186/1.48406. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.37128/1.36217. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.31452/1.60741. Took 0.44 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.71391/0.70427. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.70173/0.70748. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.68606/0.72263. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68985/0.72201. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69501/0.71361. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67798/0.72595. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68472/0.73970. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68210/0.72273. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67602/0.70846. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67501/0.71116. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67533/0.71323. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67139/0.71628. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67489/0.70639. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.66722/0.70892. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66523/0.72447. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66394/0.72714. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65903/0.74510. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65969/0.75522. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66070/0.75736. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65203/0.77141. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64575/0.78435. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64405/0.78754. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63538/0.79773. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64523/0.79146. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63329/0.79872. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.62191/0.80958. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62729/0.80731. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62862/0.79599. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62646/0.80109. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62347/0.80840. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.61815/0.82328. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.60488/0.81029. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61625/0.83315. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60727/0.83351. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.59878/0.85418. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60272/0.83549. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58756/0.82748. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58388/0.85827. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59358/0.84965. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58444/0.83189. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56817/0.83703. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.56235/0.84493. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.55605/0.83846. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.56036/0.86703. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55686/0.86697. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.55542/0.87685. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.56371/0.83581. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56338/0.83969. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.53873/0.81887. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54218/0.87678. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.53272/0.86344. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53017/0.89711. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52055/0.86059. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.52230/0.87779. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.50966/0.83437. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.52554/0.90112. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.51571/0.90413. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.50194/0.87896. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.48529/0.86592. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.49438/0.84604. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.48082/0.93765. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.48028/0.98556. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.47124/0.86110. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.47843/0.94788. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.47431/0.95354. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.48365/0.94545. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.46596/0.92131. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.47101/0.93645. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.44541/0.96451. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.44623/1.02072. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.43664/0.99769. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.44400/1.03990. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.41892/1.08431. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.44064/1.03381. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.42121/1.04908. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.40602/1.06897. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.40389/1.04899. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.40450/1.08979. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.39895/1.15659. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.38577/1.15047. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.40332/1.14364. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.37581/1.12538. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.39956/1.28084. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.39770/1.14736. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.37750/1.21950. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.36052/1.22031. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.35646/1.23087. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.34418/1.32556. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.35808/1.24704. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.37731/1.35671. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.37351/1.21516. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.36598/1.29122. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.34154/1.25448. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.32984/1.27278. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.32980/1.34121. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.33131/1.31936. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.30001/1.41924. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.30181/1.39264. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.32456/1.47146. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.30921/1.48848. Took 0.43 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.72701/0.75148. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.70721/0.72563. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70833/0.71941. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69968/0.71123. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69933/0.71623. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.70089/0.70703. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.69708/0.70856. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68606/0.70465. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.68664/0.71089. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68985/0.71672. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68495/0.70702. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.69001/0.71322. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67972/0.71864. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67801/0.70726. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.68064/0.70110. Took 0.47 sec\n",
      "Epoch 15, Loss(train/val) 0.67767/0.70016. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.67219/0.70193. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65521/0.72602. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66659/0.73349. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66529/0.75113. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.66419/0.74813. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65417/0.75806. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64671/0.77339. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64480/0.80024. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64824/0.80072. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64030/0.81601. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63469/0.82321. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63717/0.84828. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62764/0.86809. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.63016/0.86797. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62822/0.86328. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62510/0.88421. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.61578/0.90479. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61924/0.91853. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61559/0.96376. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.62361/0.95424. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61811/0.93890. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.61062/0.97057. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60444/1.02251. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59822/1.01173. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.60076/1.04920. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60119/1.05995. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59750/1.05084. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.60016/0.99456. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58294/1.01705. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.59170/0.96894. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.58297/1.00106. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57092/1.07178. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57729/1.02770. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.56589/1.04245. Took 0.47 sec\n",
      "Epoch 50, Loss(train/val) 0.56559/1.07456. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.55788/1.06951. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.55437/1.09297. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54645/1.11989. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55932/1.15130. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.54685/1.12688. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.54482/1.16274. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54800/1.18711. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.56180/1.05404. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.56439/1.05761. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54562/1.09075. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.55121/1.06306. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52673/1.10741. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.52675/1.14558. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50125/1.23394. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.51836/1.26249. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.50338/1.21726. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.50973/1.26179. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.51429/1.30184. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.53721/1.32641. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.49773/1.25254. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.48786/1.28372. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.47355/1.33345. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.49041/1.28484. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.48468/1.34829. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.47806/1.29934. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.47401/1.33833. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.48244/1.30077. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.46393/1.33317. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.44758/1.37512. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.45756/1.39817. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.46923/1.46467. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.44911/1.36636. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.42788/1.37886. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.45426/1.35209. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.43520/1.33703. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.44310/1.36379. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.44911/1.40410. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.42663/1.41217. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.42137/1.50946. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.42180/1.42721. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.47692/1.26692. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.47634/1.25834. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.42844/1.30085. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40583/1.38255. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.41458/1.34245. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.42105/1.37659. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.39587/1.41150. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.38645/1.51977. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.38113/1.49501. Took 0.43 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69662/0.69781. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69008/0.69995. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68971/0.70301. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68375/0.69551. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68242/0.70868. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68255/0.71373. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67620/0.71223. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68334/0.72100. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68107/0.72637. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67835/0.72802. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67409/0.71713. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67500/0.72867. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66654/0.73466. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66396/0.73860. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66089/0.75141. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67129/0.74860. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66266/0.75819. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66690/0.76889. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65999/0.78227. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66320/0.81230. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65993/0.81060. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65334/0.83265. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65050/0.83812. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.65825/0.81685. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65604/0.80099. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64594/0.81897. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64643/0.83687. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64446/0.85389. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63962/0.85337. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.63895/0.85875. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63885/0.86698. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63128/0.87255. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62599/0.89880. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.62364/0.86682. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61730/0.90133. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61303/0.90192. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.61189/0.88768. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61435/0.86162. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.60869/0.88751. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59776/0.90638. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.59518/0.90565. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.59378/0.92807. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.59231/0.89852. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57343/0.95878. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.57234/0.95205. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.56511/0.96330. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57278/0.97828. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57615/0.94540. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56009/0.98943. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55456/0.98690. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.54348/0.98820. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55087/0.97684. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.54768/0.96641. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.53606/0.98663. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53427/0.97233. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.53018/1.00609. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.50978/1.01762. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.51128/1.00319. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.51444/0.99342. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.50180/0.98001. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.50539/1.03583. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.49149/0.99480. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.48818/1.05323. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.49515/1.08034. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.49870/1.08801. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.48672/1.05825. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.47015/1.04035. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.45865/1.08721. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.47498/1.10677. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47113/1.12598. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.46496/1.08155. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45422/1.11886. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.43528/1.13976. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.44565/1.13834. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.44211/1.15466. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.43254/1.16964. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43863/1.19735. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.42108/1.16781. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.41724/1.12739. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.41321/1.08686. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.41378/1.19334. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.40795/1.19863. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.40229/1.26759. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40614/1.15178. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.41286/1.22782. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.40925/1.29330. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.40185/1.22787. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.37260/1.21236. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.38734/1.13084. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.38995/1.28608. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.37390/1.17295. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.37688/1.26144. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.38932/1.31726. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.37787/1.30503. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.36176/1.27550. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.36982/1.36646. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.36005/1.30437. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.36034/1.33467. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.32403/1.33590. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34901/1.38916. Took 0.44 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.72955/0.72964. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.71788/0.71829. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.71559/0.70736. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.71484/0.69061. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.71107/0.68968. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.69058/0.71404. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69751/0.72151. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68860/0.74383. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68828/0.73512. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.69217/0.71008. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.69816/0.73437. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.68712/0.72851. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67739/0.73551. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.68391/0.74291. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.68205/0.74509. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67906/0.74295. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67197/0.74050. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67151/0.74227. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66874/0.73985. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66506/0.72608. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.67244/0.74451. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66658/0.74383. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66571/0.74164. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66274/0.73876. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66005/0.74460. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66088/0.74490. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65571/0.75060. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65315/0.75094. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.65478/0.75576. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64362/0.74971. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.63764/0.78267. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.64634/0.77752. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63356/0.78416. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64344/0.80056. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.63841/0.79905. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.64162/0.80590. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62635/0.82366. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62178/0.80450. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.62418/0.83754. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.63287/0.82502. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.61438/0.84410. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61992/0.86226. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60255/0.86530. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61647/0.86318. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60833/0.87775. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.60423/0.93945. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.61038/0.88877. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.60731/0.89276. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.60546/0.90350. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.60020/0.94464. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.58626/0.93135. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.57697/0.92998. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.58029/0.94399. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.58700/0.99529. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.59179/0.99222. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.57191/1.01024. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.58433/1.01781. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.56028/1.05962. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.57270/1.07531. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.57191/1.03499. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.54960/1.06159. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.56492/1.06899. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.55386/1.05078. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.55949/1.06208. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.54323/1.09225. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.55389/1.08270. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.54491/1.08239. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.52870/1.09115. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.54348/1.07752. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.51638/1.10300. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.52723/1.09053. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.52633/1.10120. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.50938/1.15085. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.50147/1.14058. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.52678/1.15356. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.52956/1.13918. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.51394/1.11746. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.50841/1.12671. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.49332/1.11553. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.49634/1.11655. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.48942/1.10649. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.48586/1.12940. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.48717/1.12374. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.46930/1.20327. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.48314/1.18084. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.48194/1.17938. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.45747/1.17155. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.47014/1.20995. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.46557/1.28981. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.47474/1.18069. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.45933/1.26254. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.47389/1.26451. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.45931/1.24122. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.42867/1.27605. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.45638/1.30095. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.46709/1.28384. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.43701/1.30951. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.44170/1.32608. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.44045/1.30939. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.43346/1.35747. Took 0.44 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.70221/0.69830. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69017/0.69393. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69250/0.68994. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.68924/0.68955. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68584/0.68529. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.68429/0.68929. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68125/0.68083. Took 0.47 sec\n",
      "Epoch 7, Loss(train/val) 0.68392/0.68384. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68270/0.68990. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67745/0.68290. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68101/0.68808. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67730/0.69488. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68000/0.69558. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67184/0.70238. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66717/0.70501. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66567/0.71141. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66394/0.71358. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66283/0.71558. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66017/0.72692. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65700/0.72646. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65072/0.73948. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65442/0.75945. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64786/0.77293. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64614/0.77181. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.64652/0.78399. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.63494/0.79343. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63350/0.78159. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63764/0.76857. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62385/0.78335. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62539/0.79984. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.62220/0.81322. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.61404/0.84146. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61337/0.82218. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.59595/0.81092. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60261/0.84191. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.59163/0.86281. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58256/0.88885. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.58086/0.91899. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.56499/0.88947. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.56780/0.87212. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56413/0.88930. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.54812/0.90439. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55136/0.86501. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.54748/0.90679. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.54684/0.92204. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.54603/0.93238. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.53045/0.97896. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.52929/0.92978. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.52211/0.95268. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.51683/1.00478. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.50954/1.01578. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.51470/0.98865. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.49760/1.06519. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.49554/1.03943. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.47350/1.03420. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.46296/1.16519. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.45730/1.12418. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.47738/1.07868. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.46845/1.13692. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.45992/1.18144. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.45633/1.12716. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.43829/1.14731. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.45296/1.10818. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.42841/1.15936. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.43324/1.05152. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.43254/1.05208. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.41606/1.15303. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.41263/1.16211. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.38470/1.17905. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.39576/1.23091. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.40398/1.21562. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.40554/1.20867. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.38820/1.20859. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.39088/1.24324. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.39281/1.25387. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.37594/1.21794. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.37725/1.16693. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.34889/1.25871. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.36945/1.21419. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.34368/1.28251. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.33590/1.24105. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.32977/1.25849. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.32259/1.28716. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.34532/1.29706. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.32004/1.29450. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.32274/1.29524. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.34964/1.26234. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.33243/1.28229. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.28166/1.32614. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.29300/1.31858. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.28666/1.40459. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.28615/1.44163. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.28205/1.53285. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.26722/1.50943. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.28279/1.48945. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.27749/1.52560. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.26664/1.53695. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.28805/1.49022. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.26468/1.50024. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.25431/1.55423. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.71185/0.71002. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69578/0.72506. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.68705/0.73900. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69372/0.72905. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68726/0.75078. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68447/0.74615. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68424/0.75120. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68494/0.74518. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68374/0.77027. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67737/0.78596. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67611/0.82016. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66579/0.83245. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66436/0.86267. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67051/0.84091. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66585/0.87736. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66438/0.85266. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65363/0.86894. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65716/0.90928. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.64772/0.92800. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65546/0.90112. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64438/0.89224. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66273/0.89037. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64853/0.87414. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64829/0.87612. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65304/0.86714. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64733/0.87511. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64661/0.87379. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63879/0.86896. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.64454/0.85405. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63814/0.85819. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63431/0.87728. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62715/0.88619. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63428/0.87371. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.62359/0.89985. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62268/0.91709. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62107/0.91417. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.62150/0.92050. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61129/0.92485. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.60784/0.94019. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.60559/0.92427. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.58935/0.97838. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.59270/0.95181. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58368/1.01470. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.58512/1.02570. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.59347/0.96670. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.58317/0.97701. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.58993/0.98310. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57229/1.03826. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57545/1.04020. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56194/1.05279. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.55227/1.08395. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.55849/1.05084. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54682/1.06337. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.54032/1.12253. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54299/1.14907. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.54079/1.13487. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.52451/1.14605. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.52578/1.15005. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.52550/1.15489. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.52784/1.19379. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.51507/1.25631. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50757/1.29097. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51048/1.33719. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.49034/1.30878. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.52464/1.33038. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.50026/1.36885. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.49217/1.29913. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.49786/1.35739. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.47259/1.33515. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.50012/1.30986. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.50356/1.36808. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.49469/1.38761. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.47430/1.44397. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46388/1.42174. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.47453/1.48296. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.45376/1.42657. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.44943/1.48771. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46439/1.42866. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.46177/1.38976. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46784/1.50028. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.43923/1.51308. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.44463/1.47855. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.43158/1.59116. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.45369/1.45645. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43912/1.48863. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.42709/1.50006. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.42244/1.47827. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.40454/1.54635. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.40425/1.53085. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.41715/1.57702. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.40380/1.57909. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.41459/1.61965. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.40132/1.66824. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.39173/1.65467. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.38826/1.74732. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.38115/1.66704. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.39531/1.62905. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.40637/1.63321. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.37901/1.75940. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.36544/1.77622. Took 0.43 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69819/0.71836. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.68931/0.71957. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.68465/0.71931. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68860/0.71880. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68297/0.72135. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68100/0.72486. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68267/0.72710. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67782/0.73386. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67989/0.74027. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67930/0.73823. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67505/0.74486. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67163/0.75082. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67574/0.74249. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67534/0.74405. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66999/0.75920. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67137/0.76057. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66364/0.76417. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65966/0.77450. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66717/0.76832. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65318/0.77493. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66058/0.78165. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65660/0.78054. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64903/0.78290. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64401/0.78653. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.64288/0.77894. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63809/0.79266. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.63520/0.79848. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62484/0.80871. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62429/0.80214. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61889/0.79690. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.62757/0.81183. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.62551/0.81365. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61729/0.82869. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61643/0.85376. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60756/0.86142. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.61600/0.84614. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60495/0.87414. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60135/0.88401. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59867/0.87219. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59268/0.90352. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58570/0.91449. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59552/0.91379. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.58756/0.93055. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.58658/0.88236. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.58011/0.90338. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57971/0.88804. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.56701/0.87345. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57593/0.83989. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.57988/0.85818. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.55811/0.90914. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54423/0.86923. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55378/0.88487. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54064/0.90410. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.53489/0.89025. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.53817/0.90710. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.53681/0.87422. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.53418/0.88658. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.53398/0.89614. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.51785/0.93116. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.52812/0.92850. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51519/0.93719. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.50376/0.89366. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.51784/0.94246. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.50502/0.90724. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49104/0.92008. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.49062/0.92057. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.49019/0.90601. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.50953/0.90641. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.50889/0.96791. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.48120/0.98467. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.48003/0.98101. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.47982/0.98457. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46792/1.01767. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.46573/0.96201. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.45983/0.96120. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.46066/0.92702. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.46070/0.91265. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.45160/0.99625. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.45417/0.98977. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.47081/0.95419. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.46424/0.92948. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.44078/1.01224. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.43313/1.01868. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.44425/1.03548. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.42894/1.04516. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.43887/1.01518. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43484/1.06603. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.41446/0.93628. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.43144/0.99466. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.45710/0.93363. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.41514/1.01705. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.41647/0.92261. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.40293/1.04811. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.42003/0.99580. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.42685/0.94256. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.38790/0.96535. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.38655/1.04652. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.39642/0.93275. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.37658/1.07582. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.37380/1.11118. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69763/0.69980. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69602/0.69557. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.69057/0.70031. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68764/0.70233. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68862/0.70932. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68450/0.70972. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68370/0.71105. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68636/0.70677. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68365/0.70993. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67783/0.70169. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67672/0.70705. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67174/0.72065. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66458/0.71455. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66807/0.71704. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66073/0.71432. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65872/0.73986. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65055/0.75853. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64695/0.77698. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.63963/0.78388. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64093/0.79090. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63482/0.80353. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.62613/0.81991. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.62653/0.82297. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62851/0.83743. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.62605/0.84798. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62057/0.82298. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.62292/0.82073. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62059/0.81942. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61972/0.84115. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60578/0.84211. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60134/0.85932. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60358/0.86650. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60562/0.88584. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59801/0.86778. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.59513/0.89598. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59293/0.91077. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.57935/0.96654. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58894/0.94412. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57751/0.96552. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.58421/0.96051. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.56958/1.00635. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58010/0.95500. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56203/1.02735. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56319/1.04274. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.55146/1.04318. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55260/1.03918. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55539/1.01544. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.53619/1.08046. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.55102/1.02826. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.53902/1.05522. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.52281/1.04594. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.52972/1.06451. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.51948/1.16654. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52745/1.11501. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.52285/1.13582. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.51788/1.07946. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.51232/1.09055. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.50391/1.14535. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.49783/1.17524. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.49935/1.18522. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.48447/1.20288. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.49775/1.15744. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.49220/1.10480. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.47905/1.15954. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.47443/1.15820. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.46748/1.16570. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.44199/1.20868. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.45193/1.22587. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.45437/1.23386. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.43218/1.24195. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.45547/1.27719. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.45506/1.11829. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.45577/1.17457. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.43666/1.24721. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.42954/1.21227. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.41901/1.28988. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.41375/1.27833. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.42237/1.23465. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.40887/1.28816. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.39275/1.27622. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.41120/1.30669. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.37414/1.25948. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.40754/1.39330. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.36772/1.24746. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.39327/1.38730. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.37532/1.31333. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.36713/1.35599. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.38490/1.34002. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.34915/1.40706. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.37371/1.32078. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.33600/1.36627. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.37018/1.43608. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.34405/1.34927. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.33984/1.40469. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.31889/1.38278. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.31734/1.38715. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.32282/1.44861. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.32138/1.38996. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.30223/1.46003. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.33153/1.53241. Took 0.44 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.70872/0.71116. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69026/0.71511. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69693/0.70605. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69621/0.70147. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69858/0.70004. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.69059/0.70644. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68803/0.70734. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68223/0.70825. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.69356/0.70872. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68103/0.72071. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68785/0.73681. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.68754/0.75738. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67897/0.76624. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67902/0.76613. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67807/0.76857. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67624/0.78110. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.67452/0.79840. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67519/0.77750. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67430/0.78935. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.67646/0.79432. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66574/0.80547. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.66263/0.82473. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65119/0.86719. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65705/0.85235. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65137/0.86348. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.65057/0.87982. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64267/0.88671. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63543/0.89017. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63738/0.91028. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63815/0.89563. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63462/0.89634. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62328/0.94827. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.62772/0.90651. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60312/0.94248. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60636/0.96630. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.61418/0.92957. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60694/0.91641. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.60965/0.91702. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60550/0.93249. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60278/0.93661. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.59760/0.94262. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59885/0.93303. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.59405/0.92339. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.58678/0.94761. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56911/0.98445. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57545/0.99567. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56928/0.99311. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56619/1.03528. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56143/1.08261. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.56073/1.05699. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56027/1.05134. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.56558/1.06134. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54551/1.10880. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55208/1.13663. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52875/1.22025. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.52383/1.22947. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52585/1.25931. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51845/1.26465. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.50937/1.31581. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.50390/1.30792. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.50646/1.32818. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.51713/1.32503. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50727/1.35040. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48516/1.35639. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.49443/1.37336. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.47896/1.35010. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.49103/1.39356. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.49350/1.31064. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50909/1.27776. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.48581/1.34967. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.45725/1.38235. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.45550/1.40466. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.45967/1.38363. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.45074/1.42812. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.46847/1.40787. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.45207/1.36991. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.45051/1.44981. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46287/1.41758. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43322/1.40005. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.42947/1.44507. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.44998/1.44428. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.43754/1.40506. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.43845/1.39967. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.42677/1.46241. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.41462/1.49897. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.41772/1.60717. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.40754/1.62136. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.41068/1.60505. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.43270/1.53940. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.41067/1.59506. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.39057/1.52299. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.42291/1.53923. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.39250/1.67125. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.38145/1.73792. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.37646/1.73542. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.40867/1.65909. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.39335/1.71613. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.41354/1.55960. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.43898/1.49001. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.38881/1.65153. Took 0.44 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70547/0.70324. Took 0.65 sec\n",
      "Epoch 1, Loss(train/val) 0.69136/0.71052. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.68702/0.71977. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69024/0.71727. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68157/0.73191. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68199/0.73463. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68343/0.72569. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68159/0.72938. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68051/0.73623. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67277/0.75337. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.66904/0.75868. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66269/0.79001. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66401/0.78540. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65321/0.77841. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65720/0.78195. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.64350/0.81719. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65070/0.79690. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.63634/0.84962. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.63676/0.84312. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.62181/0.87198. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.62137/0.87913. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.61542/0.92109. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.60549/0.93868. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.60258/0.94937. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.59186/0.96648. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.58888/0.97139. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.58473/0.98468. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.57661/0.99461. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.57832/1.01486. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.57617/0.99421. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.56657/0.99645. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.57019/0.96922. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.55455/0.98434. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.55604/0.97624. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.54953/0.98745. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.54771/0.97541. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.54524/0.97526. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.53845/1.00586. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.53266/1.01270. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.52904/0.98735. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.53620/0.98665. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.53021/1.02602. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.52179/1.01143. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.52041/1.00975. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.51740/1.04111. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.50197/1.06872. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.48946/1.07886. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.49961/1.04703. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.48735/1.10637. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.49817/1.09635. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.48283/1.08610. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.48694/1.14610. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.46781/1.13186. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.46161/1.14364. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.46058/1.13964. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.46357/1.15326. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.44100/1.16250. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.44375/1.17447. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.44174/1.20955. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.44346/1.18010. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.43858/1.25446. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.42803/1.20724. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.42572/1.28592. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.43048/1.23780. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.39816/1.30008. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.41457/1.26892. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.39856/1.29200. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.38135/1.35160. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.42207/1.28695. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.37151/1.36522. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.39818/1.32612. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.36015/1.38931. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.36207/1.48418. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.37393/1.40698. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.35638/1.38773. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.37784/1.42822. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.36268/1.41850. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.35232/1.43394. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.34822/1.49930. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.33804/1.48235. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.34361/1.47519. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.34074/1.44196. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.32865/1.51594. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.32500/1.51473. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.31852/1.58613. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.29823/1.57001. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.30448/1.59106. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.33529/1.62400. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.31331/1.53481. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.30874/1.57509. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.31005/1.56706. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.33699/1.62030. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.32415/1.62925. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.31424/1.65297. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.30619/1.61934. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.31085/1.64403. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.30011/1.59253. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.25876/1.59373. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.28667/1.71545. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.27734/1.72264. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.72380/0.71280. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70914/0.68719. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.69092/0.69501. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68543/0.69448. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69522/0.70556. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69389/0.70072. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68573/0.70188. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67601/0.71148. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67888/0.71199. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.69050/0.70936. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68192/0.71015. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67513/0.70360. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67781/0.70145. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67760/0.71729. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66766/0.71608. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66870/0.70866. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66827/0.69906. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66686/0.71301. Took 0.42 sec\n",
      "Epoch 18, Loss(train/val) 0.66715/0.71670. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66746/0.72244. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65977/0.70825. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.65505/0.70743. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64560/0.70523. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64905/0.69075. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.64509/0.67996. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.63686/0.68415. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63446/0.69553. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63470/0.70276. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61457/0.71641. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61146/0.71958. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62277/0.74468. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60573/0.72000. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.59866/0.73506. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58107/0.74443. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.58623/0.73922. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.58524/0.75334. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.58741/0.75095. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.59035/0.77639. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.56266/0.80773. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.57695/0.79129. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.55090/0.78448. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.56399/0.78083. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.56697/0.79521. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.52771/0.83171. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.54636/0.83169. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.54302/0.85010. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.54341/0.84716. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.53696/0.86898. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55019/0.85504. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.53165/0.82018. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53467/0.84336. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.51955/0.84199. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.52471/0.85491. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.50639/0.85937. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.50178/0.88607. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.49321/0.85521. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.51123/0.86668. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.50188/0.85121. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.51402/0.83506. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.47626/0.86627. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.47973/0.86896. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.49310/0.85510. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.46080/0.85790. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.46334/0.86317. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.45776/0.89112. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.46042/0.89757. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.46111/0.92734. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.45411/0.96697. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.45172/0.92657. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.44088/1.00430. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.43068/1.06384. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.44787/1.02877. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.43453/1.00849. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.44124/1.11881. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.43601/1.07015. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.41896/1.05483. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.40874/1.03821. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.40507/1.08244. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.40651/1.05515. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.40244/1.09418. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.41163/1.14274. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.40836/1.10195. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.40412/1.09501. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.40516/1.13315. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.42000/1.09701. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.40809/1.04134. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.38284/1.06521. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.36999/1.18481. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.35425/1.17781. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.34421/1.20758. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.37081/1.20275. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.35827/1.19232. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.38427/1.14724. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.35129/1.13746. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.34418/1.18790. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.38116/1.17172. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.35044/1.15970. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.42903/1.06004. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.32784/1.11778. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.29344/1.15606. Took 0.44 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.71303/0.72183. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.70598/0.71989. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.70235/0.73989. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69703/0.72968. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69024/0.74182. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69106/0.73658. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68507/0.74246. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68222/0.75579. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67298/0.77710. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67517/0.74751. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66682/0.76282. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66767/0.74778. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.65690/0.76943. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.65881/0.77330. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66183/0.76969. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.65024/0.78826. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65176/0.79669. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.63600/0.81290. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64342/0.83027. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.63273/0.84816. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63584/0.84600. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.62359/0.88439. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.63293/0.89757. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.62822/0.88121. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.61174/0.91982. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.61864/0.95191. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.61347/0.96030. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61236/0.92940. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61263/0.95483. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60908/0.98101. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.59713/0.97009. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.60868/1.01554. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59497/1.01944. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59074/1.01960. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.58374/1.06068. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.58605/1.07628. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.59497/1.04470. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58838/1.09713. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57801/1.07956. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57535/1.07125. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59057/1.03228. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.56804/1.08661. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.55347/1.11890. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.54771/1.15828. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56598/1.13723. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.54875/1.09671. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.53705/1.15305. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.54218/1.16420. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.54929/1.10063. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.53868/1.15137. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53535/1.11762. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.52146/1.15427. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.53531/1.14100. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.50291/1.17446. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.49683/1.24308. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.49463/1.22885. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.49321/1.23058. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.48724/1.21060. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.46707/1.32880. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50004/1.19006. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.47984/1.16284. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.48521/1.15856. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.44973/1.24916. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.45890/1.30076. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.45448/1.30648. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.44221/1.34597. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.44694/1.35094. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.45350/1.30532. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.42754/1.34015. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.41349/1.26962. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.41633/1.30550. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.42783/1.31662. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.42637/1.39362. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.42261/1.42662. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.41520/1.33384. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.42363/1.34724. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.38900/1.51185. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.37751/1.38351. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.39023/1.48602. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.37584/1.38944. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.38449/1.39253. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.38140/1.42144. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.39438/1.50918. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.38952/1.44569. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.35564/1.50427. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.36774/1.46194. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.35861/1.49765. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.34613/1.53758. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.35951/1.51249. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.35968/1.43389. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.34902/1.40742. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.32017/1.44172. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.30486/1.49939. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.34309/1.68197. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.36306/1.58442. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.33866/1.45103. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.32350/1.45263. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.31640/1.60476. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.31231/1.59368. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.28425/1.59043. Took 0.44 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.71145/0.70367. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69953/0.70842. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68929/0.70902. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68514/0.71491. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68329/0.72001. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.67906/0.73248. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.67382/0.72877. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67508/0.73410. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67473/0.74454. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67279/0.76291. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66840/0.75394. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66619/0.77931. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66302/0.78531. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66216/0.81275. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65246/0.80919. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.65189/0.83787. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.64355/0.84850. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64172/0.83036. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.63779/0.85112. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.63942/0.80631. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63278/0.84726. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.63153/0.80651. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.62420/0.83922. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.61996/0.85491. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.61659/0.84798. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.60810/0.85390. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.60229/0.81241. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.60143/0.84504. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.58378/0.86672. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.58157/0.83591. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.58401/0.81282. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.56687/0.90384. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.56113/0.87157. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.55907/0.89680. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.55219/0.91068. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.55847/0.92292. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.55017/0.92854. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.55130/0.89912. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.53406/0.94371. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.53508/0.95675. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.51787/0.96761. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.52542/0.98672. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.52362/1.00351. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.50961/0.96737. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.49029/1.05362. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.50310/1.03467. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.49855/1.16015. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.47778/1.06423. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.49295/1.08608. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.48607/1.05983. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.48104/1.06354. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.45983/1.04141. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.46096/1.04707. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.45136/1.09773. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.43601/1.07923. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.47091/1.08322. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.47897/1.09649. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.42215/1.09841. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.43247/1.19565. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.41577/1.14464. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.42086/1.11057. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.41930/1.12549. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.42381/1.09213. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.40928/1.06036. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.42346/1.17409. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.41382/1.07181. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.41900/1.14825. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.40520/1.09790. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.39783/1.13228. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.36923/1.15507. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.36709/1.24957. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.44500/1.10127. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.41344/1.18112. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.39587/1.18316. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.39871/1.10481. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.37582/1.14911. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.36379/1.14612. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.36182/1.24923. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.33902/1.36532. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.34312/1.31974. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.32745/1.32947. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.33571/1.39767. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.35930/1.43911. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.35383/1.38179. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.38742/1.35071. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.35974/1.30277. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.35726/1.30470. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.32114/1.30592. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.34313/1.34582. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.32972/1.32710. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.30734/1.34433. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.32973/1.32150. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.33452/1.34307. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.28776/1.37627. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.27770/1.55961. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.28968/1.57708. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.30631/1.61640. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.34584/1.33715. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.32501/1.34384. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.31260/1.47776. Took 0.43 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.71984/0.70038. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.71198/0.69180. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.71097/0.68306. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69747/0.66980. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.70275/0.66276. Took 0.48 sec\n",
      "Epoch 5, Loss(train/val) 0.69550/0.65711. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.69265/0.64895. Took 0.47 sec\n",
      "Epoch 7, Loss(train/val) 0.68880/0.63888. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.68355/0.64116. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68869/0.64346. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67837/0.63596. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.68181/0.64181. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67811/0.64971. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66691/0.64341. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66878/0.65004. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66331/0.65044. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66287/0.65432. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65578/0.66629. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65257/0.66726. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.65184/0.66957. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64986/0.68146. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64639/0.70166. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64106/0.69474. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64982/0.69200. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63195/0.67774. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.63685/0.69851. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63561/0.70367. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63123/0.69211. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.63287/0.69443. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62301/0.70594. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61826/0.70115. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.62111/0.71760. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61234/0.72901. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61027/0.73065. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.61230/0.75867. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61674/0.77560. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.60888/0.75429. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59930/0.77052. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.61487/0.80477. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60154/0.77764. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.60856/0.74156. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.59412/0.74734. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58780/0.75070. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58638/0.77550. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.58778/0.76117. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57683/0.77238. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58193/0.74710. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.56543/0.78916. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57220/0.71820. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57479/0.72211. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57146/0.74660. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55537/0.78871. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56121/0.74628. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.54348/0.79354. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55152/0.76224. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.55046/0.82401. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52966/0.80694. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.53642/0.91486. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.52969/0.87953. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.50776/0.92287. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.51831/0.87017. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.52389/0.96601. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.51615/0.95834. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.49175/0.94846. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.49723/0.97642. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.47492/1.05082. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.48714/1.05489. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.52087/0.94602. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.49472/0.94166. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.47809/0.88976. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.48021/1.02386. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.46432/1.09502. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.44425/1.07921. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.46217/1.07242. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.46677/1.05608. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48541/1.02405. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.44130/1.10653. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.44923/1.11469. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.42640/1.10150. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.42431/1.16891. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.44574/1.01092. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.43656/1.04935. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.43451/1.08178. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.42174/1.04616. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.38996/1.15629. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.40171/1.19843. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.40208/1.11489. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.40184/1.14206. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.39383/1.17121. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.37705/1.15918. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.40160/1.20115. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.38151/1.23688. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.39400/1.12699. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.38970/1.19363. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.36188/1.26320. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.36706/1.28718. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.36829/1.19218. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.36202/1.15687. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.33362/1.40596. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.34169/1.31968. Took 0.43 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.70833/0.73571. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69563/0.75505. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.68907/0.74522. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.67929/0.74196. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68142/0.74101. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68110/0.74104. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.67170/0.74908. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67644/0.75876. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.66813/0.76675. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.66185/0.75999. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.66901/0.75646. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66611/0.76246. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.65163/0.76589. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65517/0.77731. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65222/0.77811. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.64662/0.78846. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.65038/0.78410. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.63755/0.77572. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.63994/0.80737. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.64263/0.81746. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63946/0.82407. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.62698/0.83480. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.62608/0.82790. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62809/0.83042. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62592/0.81742. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.61925/0.83772. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61691/0.83954. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.61117/0.84174. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61664/0.83969. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.59736/0.80940. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.60539/0.87087. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60567/0.87872. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60787/0.88349. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59775/0.84919. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59217/0.83247. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60165/0.84709. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.59855/0.81148. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59593/0.81304. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58103/0.82907. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58388/0.82655. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.57204/0.80829. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.57278/0.85072. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56565/0.82611. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.56179/0.83476. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55586/0.84945. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.54901/0.86860. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.55249/0.83235. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55404/0.83110. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.53929/0.83334. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.52970/0.87100. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54112/0.86274. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.51576/0.88094. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52422/0.86488. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.51432/0.85582. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51075/0.94046. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.49223/0.96324. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.49836/1.01489. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.50343/0.98530. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.48827/1.02736. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.48260/1.05195. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.47955/0.97840. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.46355/1.00928. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.45923/1.02464. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.45725/1.10777. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.45379/1.04340. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.46388/1.12019. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.44910/1.13686. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.43219/1.19692. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.45173/1.21985. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.43739/1.24283. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.43365/1.19501. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.41127/1.21798. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.44081/1.14093. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.41021/1.28356. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.41410/1.25036. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.41079/1.30115. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.40471/1.20335. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.43342/1.25273. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.40415/1.30172. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.41241/1.30582. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.39645/1.31087. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.37190/1.40815. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.40895/1.30672. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.38025/1.37239. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.38699/1.27681. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.37748/1.34213. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.37186/1.37499. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.36282/1.42529. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.35141/1.45479. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.35179/1.33630. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.33897/1.44866. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.33303/1.48765. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.33639/1.51042. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.33408/1.43260. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.31504/1.53670. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.30594/1.43545. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.34027/1.51748. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.33358/1.47035. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.32565/1.63579. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.31395/1.62058. Took 0.44 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.71554/0.69451. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69855/0.69914. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69066/0.70317. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69039/0.70311. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68783/0.70184. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68371/0.70371. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68019/0.71360. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67566/0.72866. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68046/0.71885. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67468/0.71650. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68024/0.72812. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66854/0.72681. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67802/0.71930. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66635/0.71501. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67257/0.72553. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66372/0.75121. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66720/0.74559. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65470/0.75740. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66025/0.74139. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65774/0.74641. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65123/0.75267. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64644/0.76312. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63626/0.75723. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.63276/0.75097. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63694/0.74414. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63020/0.74998. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.64019/0.74182. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63178/0.76641. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62221/0.73983. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62324/0.77398. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62133/0.75679. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61819/0.79001. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61403/0.83130. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60272/0.80928. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60734/0.82270. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60851/0.83493. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60773/0.85197. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.59708/0.85922. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.58505/0.89540. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58909/0.90926. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58011/0.96569. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57570/0.99628. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.56683/0.99378. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57833/0.94175. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.56474/0.99038. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55362/1.04439. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56387/1.00422. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56386/1.01462. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.54980/1.00542. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.54883/1.02587. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.56206/1.03753. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53803/1.01432. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.53117/1.00523. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.53281/1.04969. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.50986/1.14272. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.51931/1.08053. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.51970/1.06453. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.50744/1.08325. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50109/1.15761. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.49261/1.14185. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.48534/1.10370. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.48940/1.18253. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49746/1.03499. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48494/1.11894. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.46845/1.15204. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.46421/1.06987. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.47587/1.00375. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.45768/1.04032. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.45142/1.01510. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.44057/0.99926. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.44341/0.99718. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.42351/1.08440. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.42294/1.08206. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.42478/1.01635. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.39911/1.09682. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.41469/1.08295. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.41662/1.04122. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.41466/1.02155. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.39562/0.97423. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.38285/1.05539. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.38788/1.06274. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.38515/1.05523. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.38347/1.13206. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.38299/1.11746. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.36275/1.10501. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.35666/1.08809. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.36405/1.15132. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.34003/1.15901. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.37060/1.20585. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.35602/1.12950. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.36992/1.09413. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.36352/1.09089. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.31911/1.17245. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.31416/1.17872. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.31938/1.21442. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.32215/1.26896. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.31652/1.22632. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.35126/1.27179. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.32177/1.22238. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.30266/1.29181. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69648/0.70684. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69317/0.70945. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69217/0.71088. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68279/0.72180. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68140/0.72558. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68089/0.71793. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67568/0.72759. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67217/0.72653. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.66648/0.73249. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66897/0.75930. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66410/0.75479. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66313/0.75136. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65833/0.76730. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.65557/0.79198. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.65035/0.79293. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.64781/0.79707. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.63850/0.82902. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.63561/0.84803. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.63504/0.85617. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.62815/0.87910. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.62535/0.86726. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.61811/0.87892. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.61866/0.89152. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.60910/0.91008. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.60602/0.91169. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.60140/0.92497. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.59564/0.92179. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.59345/0.92402. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.59198/0.89988. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.58628/0.89895. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.56986/0.95790. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.56631/0.90611. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.57332/0.91625. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.56029/0.91826. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.55782/0.97284. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.55241/0.96110. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.55212/0.96245. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.56492/0.95429. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.53918/0.97146. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.53535/0.98555. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.53117/0.98029. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.53563/0.96443. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.51742/1.06833. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.50624/1.09194. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.51803/1.09751. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.52750/1.10815. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.50968/1.07612. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.50434/1.07620. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.51787/1.07823. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.50674/1.08744. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.49871/1.06918. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.47593/1.16681. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.48578/1.11273. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.48220/1.09297. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.46059/1.17134. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.48425/1.06884. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.45275/1.18662. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.49070/1.01334. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.45724/1.13114. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.45732/1.10076. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.45560/1.10819. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.44421/1.08324. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.42519/1.12833. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.40854/1.18199. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.41776/1.15802. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.41002/1.16502. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.42249/1.14501. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.40306/1.08890. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.38533/1.17614. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.37779/1.27654. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.42449/1.10630. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.38973/1.18672. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.39236/1.21993. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.36996/1.17294. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.36813/1.20803. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.39096/1.19706. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.39171/1.15445. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.36016/1.25013. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.34253/1.25480. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.34989/1.20753. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.33869/1.26484. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.31060/1.26638. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.33171/1.28407. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.32235/1.37244. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.34173/1.20423. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.36646/1.27021. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.31405/1.25185. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.34548/1.31639. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.34008/1.36300. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.31445/1.27988. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.34199/1.22968. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.33349/1.24496. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.29511/1.28685. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.28674/1.30451. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.28641/1.26511. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.31279/1.28961. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.27886/1.27650. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.25549/1.42760. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.27373/1.32790. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.28432/1.26672. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69703/0.68039. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69609/0.70244. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69610/0.70655. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69990/0.71945. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.68650/0.72704. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68901/0.72922. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68374/0.74317. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67591/0.74946. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67996/0.74121. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67916/0.73530. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67562/0.76550. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67110/0.74297. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66836/0.76161. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66430/0.77900. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66738/0.78316. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66189/0.77580. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65449/0.79941. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66192/0.75563. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65138/0.76606. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66184/0.76145. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65098/0.76633. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65592/0.77738. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63656/0.78794. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63781/0.79304. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63924/0.76040. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.63027/0.78811. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63227/0.74893. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62806/0.72920. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62923/0.73080. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63288/0.74984. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63000/0.73808. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.61922/0.74528. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.62494/0.73486. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61544/0.75701. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61326/0.76492. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61783/0.75361. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61474/0.75144. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60007/0.77018. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60333/0.76944. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.59841/0.77641. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58240/0.77137. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.59533/0.79155. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.58092/0.75249. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57447/0.80978. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57738/0.81727. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.58454/0.82308. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57370/0.82328. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.55846/0.84155. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.56702/0.81650. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55690/0.83667. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55220/0.85659. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56678/0.83591. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.55046/0.84009. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54558/0.89261. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.54079/0.85480. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.54607/0.85166. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.53596/0.86572. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.52629/0.87628. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.51759/0.86076. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.51063/0.89788. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49172/0.97108. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50609/0.91593. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.47822/1.00226. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.50378/0.94092. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50620/0.96200. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.49389/0.99426. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.47489/1.05426. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48017/0.96738. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.47081/0.97662. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46097/1.01376. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.45307/1.01588. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.44375/1.05322. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.44017/1.02721. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.45061/1.00521. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.45180/1.10018. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44337/1.07280. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43143/1.14984. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.42097/1.18109. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.41046/1.22316. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.42748/1.24851. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.44677/1.13691. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.38829/1.16286. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.40058/1.24693. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40721/1.19668. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.38452/1.22325. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.38876/1.34597. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.44754/1.11441. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.41618/1.14461. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.39312/1.16241. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.36659/1.21257. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.39833/1.19093. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.36768/1.25282. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.37171/1.22776. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.39222/1.24233. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.36562/1.28575. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.35820/1.29079. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.33031/1.32296. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.34940/1.33757. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.35338/1.27373. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.37308/1.10719. Took 0.44 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.71101/0.70832. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70768/0.72212. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.71598/0.69818. Took 0.49 sec\n",
      "Epoch 3, Loss(train/val) 0.69493/0.70031. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69837/0.70842. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68827/0.70374. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68893/0.70424. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68437/0.69617. Took 0.48 sec\n",
      "Epoch 8, Loss(train/val) 0.67899/0.69033. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.67760/0.67675. Took 0.47 sec\n",
      "Epoch 10, Loss(train/val) 0.67266/0.69600. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67579/0.67546. Took 0.47 sec\n",
      "Epoch 12, Loss(train/val) 0.67394/0.66410. Took 0.47 sec\n",
      "Epoch 13, Loss(train/val) 0.66546/0.68247. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67289/0.69504. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66508/0.69539. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66441/0.70166. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66939/0.69833. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66330/0.69372. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66532/0.70806. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66458/0.69617. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65783/0.71862. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65614/0.72800. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65281/0.71843. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65010/0.72184. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64957/0.73406. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63901/0.73384. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64681/0.74519. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64726/0.73696. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63576/0.74691. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64206/0.72827. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.63150/0.72723. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62740/0.72950. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.62608/0.75494. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.62009/0.75151. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.62677/0.73661. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.61769/0.75033. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60842/0.75293. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61417/0.76454. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60935/0.78957. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.60324/0.79038. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.60245/0.79561. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.58612/0.80744. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.59837/0.84276. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60018/0.84483. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.59936/0.80686. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59727/0.82046. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.58054/0.81599. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.58892/0.84521. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.58921/0.83537. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57433/0.81943. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.58167/0.86359. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.57649/0.87344. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56587/0.88476. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54994/0.92361. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.56614/0.91742. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55546/0.91658. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54233/0.94097. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54154/0.92354. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52661/0.93578. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52574/0.94421. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.52250/1.00955. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.52652/0.99756. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.50058/1.00435. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.50615/1.03905. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50503/1.08847. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.50825/1.08911. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.50942/1.07376. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50054/1.13530. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.51166/1.11920. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.50629/1.11834. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.51761/1.07912. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.50670/1.12770. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.50593/1.09710. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.47455/1.11286. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.47362/1.09500. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.45430/1.15627. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.44362/1.18978. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43133/1.21399. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.43435/1.22401. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.43671/1.27693. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.43004/1.28619. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.42674/1.25784. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.42416/1.32666. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43528/1.26405. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.40469/1.27478. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.40639/1.28297. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.39257/1.32598. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.42823/1.21986. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.39267/1.22530. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.36813/1.30383. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.37538/1.31802. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.40554/1.25397. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.38947/1.29718. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40712/1.25178. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.37862/1.30519. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.36861/1.30177. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.35372/1.29949. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.33721/1.37205. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.35672/1.42444. Took 0.44 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.72825/0.68072. Took 0.64 sec\n",
      "Epoch 1, Loss(train/val) 0.71952/0.70255. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.70894/0.69516. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70190/0.69747. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69408/0.70613. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69668/0.71586. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.69476/0.69317. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68821/0.70878. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.70131/0.70014. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68375/0.69419. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.69378/0.69451. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.69148/0.70112. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.69152/0.71905. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.69002/0.70527. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.68163/0.71892. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67468/0.71658. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.68389/0.72817. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66113/0.72253. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.67699/0.73083. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67605/0.72134. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.67578/0.71112. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66674/0.73054. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66266/0.72303. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.67183/0.73757. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66737/0.72763. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.66784/0.73911. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.66628/0.74959. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.66821/0.76302. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65553/0.75744. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65954/0.77842. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.66133/0.78126. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.65043/0.77215. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.64661/0.78877. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.65629/0.79643. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.63951/0.79698. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.64004/0.78939. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62910/0.81691. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.63227/0.81369. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.63023/0.82808. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.62355/0.85310. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.63694/0.83428. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.63181/0.77547. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.62734/0.82973. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.62436/0.86506. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.61552/0.81666. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.60914/0.84106. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.61071/0.85153. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.61864/0.86059. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.61843/0.85830. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.60972/0.86500. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.59862/0.90031. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.59924/0.88285. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.59247/0.89831. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.58788/0.90069. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.58867/0.92002. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.58060/0.95014. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.58556/0.92253. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.59246/0.93746. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.57517/0.90972. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.57654/0.94404. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.56536/0.93137. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.57400/0.89724. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.57839/0.90942. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.56048/0.95770. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.55967/0.89764. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.57080/0.92121. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.56069/0.90605. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.56126/0.89183. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.56133/0.89539. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.55238/0.95068. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.55270/0.91674. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.55335/0.93099. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.53753/0.96725. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.54877/0.91803. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.53520/0.90262. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.55657/0.97950. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.53380/0.94744. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.52443/0.98480. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.53760/0.88891. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.54360/0.91696. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.51878/0.94640. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.51104/0.98690. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.52597/1.00766. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.51722/0.95932. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.50351/0.96992. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.50561/0.98275. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.50151/0.98002. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.49721/0.99619. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.48985/0.98524. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.48603/1.02277. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.50369/1.03404. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.49382/1.04405. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.47390/1.04817. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.47937/1.04428. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.48847/1.02846. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.49099/1.01528. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.47469/1.02223. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.46402/1.03620. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.47340/0.99369. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.44597/1.07274. Took 0.43 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70295/0.69761. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69466/0.71265. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69568/0.72476. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68990/0.74107. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69189/0.74721. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69040/0.75525. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68037/0.76416. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68099/0.77100. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67445/0.77604. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67759/0.75915. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67526/0.77047. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67187/0.75742. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65904/0.77496. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66840/0.78368. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65847/0.79919. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.65588/0.79846. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65727/0.81857. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65456/0.83066. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65336/0.82282. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64354/0.83119. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64674/0.85488. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64044/0.86249. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63498/0.90567. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63381/0.89448. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62932/0.89693. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62716/0.96757. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62269/0.99036. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61594/1.04361. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61595/1.01613. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.61119/1.07603. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.59588/1.09156. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.59921/1.09718. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60762/1.01064. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58755/1.07339. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.58822/1.08340. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.58315/1.13673. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.57913/1.12976. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.57160/1.10197. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.55700/1.14948. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.55554/1.20122. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56877/1.16620. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.56068/1.10418. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.54988/1.15120. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.54189/1.18505. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.53871/1.15555. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.54599/1.11031. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.54867/1.14962. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.52812/1.23114. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.52458/1.18923. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.50521/1.19457. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.52509/1.20548. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.52161/1.19977. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.50837/1.21101. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.50412/1.27531. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.50303/1.23167. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.48486/1.27663. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.46609/1.31886. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.49472/1.47355. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.46921/1.44278. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.48960/1.41068. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.45584/1.45072. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.46005/1.46487. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.46087/1.42872. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.44053/1.45214. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.42876/1.55182. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.43853/1.56466. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.44368/1.47498. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.43863/1.58127. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.43613/1.65907. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.42023/1.58914. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.41856/1.55831. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.41524/1.54976. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.41270/1.58045. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.39921/1.61339. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.38282/1.74396. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.38757/1.90682. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.36217/1.94377. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.37331/1.90843. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.35845/1.90443. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.35535/1.96030. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.35836/1.79495. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.35446/1.93738. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.39109/1.78779. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.37905/1.70667. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.33402/1.83913. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.34301/1.93954. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.32184/1.76314. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.32092/1.91302. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.31523/2.06102. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.29553/2.22747. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.31261/2.08668. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.31759/1.97256. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.30044/2.04984. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.29591/1.95370. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.28320/2.13009. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.28400/2.14630. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.27551/2.22630. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.28370/2.27257. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.27987/2.08323. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.27109/2.10906. Took 0.44 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.70398/0.69539. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70244/0.72760. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69680/0.72996. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69447/0.73956. Took 0.42 sec\n",
      "Epoch 4, Loss(train/val) 0.68969/0.73744. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68213/0.73047. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68131/0.71667. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68096/0.71804. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67747/0.71662. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67495/0.71909. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67421/0.71348. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67441/0.70967. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67317/0.72246. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66336/0.72603. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66604/0.72699. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.65555/0.74687. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66020/0.74678. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65078/0.74840. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64929/0.75445. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.64837/0.75825. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64993/0.76815. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.63817/0.77554. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.63794/0.76400. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.62829/0.75318. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.62941/0.76012. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62846/0.77030. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62036/0.77582. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62231/0.78718. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61277/0.79802. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.61746/0.78120. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60586/0.80135. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.59340/0.80752. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.59513/0.82673. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58892/0.80944. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.58564/0.83407. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.58766/0.81750. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58607/0.82133. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.58901/0.83931. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57987/0.83898. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.56580/0.82944. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.56432/0.84908. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.55576/0.85821. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55680/0.89449. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.54223/0.87333. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.54012/0.86760. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55490/0.85303. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.53634/0.85449. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.52311/0.89123. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.53364/0.88392. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52899/0.88048. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.53730/0.88390. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.52930/0.90391. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52196/0.93937. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52825/0.96299. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51826/0.93906. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51459/0.92728. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.51067/0.96686. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51016/0.95383. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.50508/0.94785. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.48924/0.99831. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50224/1.02017. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50097/0.99191. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48296/1.04360. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48153/1.02006. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.47712/0.99012. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50525/1.01933. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.48955/1.01199. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.47253/1.01544. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.47562/1.01360. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46150/1.01237. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.44605/1.00154. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.44726/1.01743. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.45559/0.98664. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.45364/1.01099. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.44353/1.01766. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.43026/1.01125. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.42797/1.05162. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.43069/1.07461. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.41805/1.12697. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.42618/1.06864. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.41341/1.02795. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.41115/1.04846. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.41933/1.05056. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40297/1.03037. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.40231/1.05960. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.38833/1.03538. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.38114/1.08846. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.36768/1.08352. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.41178/1.12358. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.39309/1.07740. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.39793/1.08598. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.34855/1.17031. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.37414/1.06395. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.34449/1.11914. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.35584/1.09712. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.35754/1.13117. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.33826/1.16562. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.34987/1.17874. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.33662/1.15483. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36423/1.15618. Took 0.44 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.70742/0.73092. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70169/0.75157. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69341/0.75717. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68050/0.78801. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69569/0.74554. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68544/0.75587. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68251/0.75804. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67979/0.74426. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67610/0.75426. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67731/0.77580. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67885/0.77373. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67388/0.78522. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66714/0.80205. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67172/0.80438. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66695/0.78221. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66514/0.80314. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66976/0.81135. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66172/0.79893. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66143/0.78096. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67045/0.77322. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66225/0.75821. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.65732/0.78002. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65520/0.77580. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65726/0.78407. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.64779/0.80693. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65312/0.80930. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63583/0.81797. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63828/0.81024. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63723/0.83086. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.63251/0.80770. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63033/0.85005. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62179/0.82057. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62352/0.81870. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.61759/0.85902. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.60874/0.84491. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60603/0.85887. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.63124/0.85119. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59919/0.86378. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60141/0.84099. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60357/0.89311. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.60462/0.86232. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59715/0.87965. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.59046/0.84517. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.59261/0.84355. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.58004/0.80498. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.58515/0.82646. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58627/0.84730. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57400/0.88244. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.58428/0.85192. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57107/0.82602. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56407/0.84438. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.57332/0.88580. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.56286/0.86740. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55970/0.85336. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.56058/0.86051. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.54420/0.87665. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.56387/0.89105. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.54764/0.90952. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.53511/0.94773. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.53103/0.93193. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54270/0.93196. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.53278/0.92121. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.53110/0.92342. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.52470/0.93737. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.52462/0.92084. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.51621/0.97905. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.52556/0.95737. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.52892/0.96424. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52578/0.92037. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.51201/0.97192. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.51068/0.99096. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.50484/0.93383. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.51153/0.91951. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.51367/0.87242. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.51223/0.92016. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.47967/0.92929. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.50274/0.91478. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.49258/0.91121. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.47951/0.95727. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.49804/0.92830. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.51193/0.93675. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.49195/0.92943. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.46889/0.95344. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.48320/0.94178. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.47630/0.91730. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.45830/0.91060. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.46470/0.97825. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.49064/0.80116. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.49631/0.83372. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.46314/0.91787. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.43758/0.96678. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.43446/0.99018. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.43556/1.00384. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.43188/1.09836. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.46972/1.01817. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.41236/1.00385. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.42074/1.04088. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.43312/0.98629. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.44300/0.97480. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.42577/1.03962. Took 0.43 sec\n",
      "ACC: 0.6354166666666666\n",
      "Epoch 0, Loss(train/val) 0.70518/0.69540. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.71070/0.69344. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70443/0.69673. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70531/0.68999. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69475/0.69674. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69282/0.69917. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68994/0.69924. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.70275/0.69970. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69226/0.70376. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68593/0.70698. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68258/0.70336. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.69271/0.70636. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68441/0.70256. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.69350/0.69774. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.68421/0.69467. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.68139/0.69929. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.68330/0.69930. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67362/0.70374. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67800/0.70462. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.68556/0.70587. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.67052/0.71060. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.67306/0.71070. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66765/0.69718. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66101/0.72107. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.66932/0.70779. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66055/0.71237. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65418/0.71710. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65845/0.72006. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65984/0.71143. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.64370/0.72202. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.64714/0.73365. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.64611/0.73705. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.63790/0.75105. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.64380/0.76737. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.63805/0.76983. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.64171/0.76364. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.63179/0.77506. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62341/0.78053. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62175/0.79438. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.62053/0.79434. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61577/0.79004. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.62967/0.79067. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.62733/0.78665. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.61305/0.81385. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60979/0.78255. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.60669/0.81531. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.60661/0.77261. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.59684/0.81625. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59874/0.80066. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.60911/0.80471. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.59083/0.81476. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.59476/0.81218. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.58969/0.81227. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.57904/0.83878. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.58027/0.86545. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.57408/0.83820. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56237/0.84506. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.57580/0.82637. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.56276/0.84447. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.56444/0.84001. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.57079/0.84788. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.54485/0.84903. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.55971/0.85104. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.54964/0.88917. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.53956/0.85870. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.54081/0.87755. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.52728/0.87792. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.54022/0.87489. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52254/0.89899. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.51763/0.90831. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.52366/0.87777. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.51650/0.86071. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.50853/0.88453. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48050/0.93221. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.47970/0.93972. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.48320/0.87597. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.47797/0.89490. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.47926/0.89243. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.46198/0.92458. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.46902/0.97248. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.46283/0.97255. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.45723/0.96915. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.46276/0.86874. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.44411/0.89646. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.46433/0.89619. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.44153/0.93201. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.44040/0.95461. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.46393/0.87876. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.44447/0.97221. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.45321/0.98550. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.45233/0.89604. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.43671/0.90574. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.44404/0.86207. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.42740/1.01990. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40946/0.95575. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.40597/0.96925. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.43552/0.90375. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.40180/0.97589. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.40218/0.94851. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.38934/1.03415. Took 0.44 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.70135/0.68239. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69052/0.70216. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69302/0.69451. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69322/0.69790. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69189/0.70148. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68657/0.69365. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68581/0.69194. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.69032/0.69960. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67968/0.69730. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67533/0.70641. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68676/0.69913. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68208/0.69595. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67682/0.69873. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67523/0.71983. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67438/0.71668. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66907/0.73652. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67391/0.72499. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66491/0.70921. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67055/0.71299. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65916/0.74614. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66208/0.72288. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65945/0.73348. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64736/0.75797. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65043/0.76058. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.64713/0.74754. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64620/0.76898. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63538/0.78077. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64116/0.77480. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63490/0.78729. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63475/0.80411. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62752/0.81244. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62555/0.80539. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.62288/0.79624. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.62134/0.81428. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.61720/0.81332. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.61610/0.85536. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61539/0.83146. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61150/0.84206. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60038/0.86909. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.60455/0.86964. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.58365/0.91017. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.59563/0.87240. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59120/0.93551. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57546/0.94272. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56617/0.96063. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.56852/0.99618. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55903/0.97358. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.55931/0.97943. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.56427/0.96751. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.55353/0.96905. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53160/0.99819. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53967/1.02482. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52918/0.99332. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.53105/1.01066. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.53981/1.01692. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51038/1.06553. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.50822/1.06912. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.50278/1.12504. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50930/1.07039. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50999/1.13149. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.49960/1.07182. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.48914/1.15297. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49976/1.14076. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48327/1.11845. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49176/1.19079. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.47311/1.17797. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.47965/1.16457. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.47003/1.14623. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.45272/1.25756. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47311/1.21051. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.45453/1.24649. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.47937/1.25526. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.49330/1.26867. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46055/1.20248. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.44731/1.26566. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.46380/1.26144. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46413/1.22891. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.43008/1.32549. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43703/1.33472. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.42718/1.40232. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.40901/1.37437. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.41741/1.34461. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.41803/1.40588. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.41127/1.38610. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.42188/1.31501. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.40858/1.37029. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.46092/1.23895. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.43443/1.32031. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.39567/1.36448. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.39646/1.36956. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.39034/1.46010. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.40380/1.44743. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.38902/1.44705. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.36007/1.43228. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.37709/1.45894. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.37273/1.51697. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.37840/1.45406. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.35898/1.47616. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.37250/1.37150. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36083/1.49732. Took 0.44 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70621/0.69285. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69421/0.70038. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69218/0.70428. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68926/0.71120. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68087/0.71144. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.67941/0.72334. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68208/0.72952. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67602/0.73413. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67003/0.74148. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67541/0.75186. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67137/0.75241. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67299/0.74481. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66355/0.74886. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66399/0.74604. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66777/0.74814. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66348/0.74845. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65413/0.74403. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64835/0.76196. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.64820/0.74666. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64854/0.73430. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63423/0.73843. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63373/0.73864. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63251/0.77634. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64229/0.74141. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63475/0.74231. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62450/0.73850. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.61133/0.73470. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62438/0.79086. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60686/0.76655. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.59456/0.78161. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.59169/0.78281. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.58850/0.79007. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59324/0.79999. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.58231/0.77450. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.58267/0.79528. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58588/0.80690. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.56839/0.82756. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.56201/0.84719. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.57811/0.84304. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.56196/0.87695. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.55228/0.90035. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.54859/0.87173. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57653/0.83959. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.54257/0.86345. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.54224/0.86135. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.53306/0.89358. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.54614/0.89212. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.52870/0.86486. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.52196/0.88752. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52976/0.87713. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53971/0.89526. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.51876/0.90631. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.50336/0.91374. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.49137/0.89923. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.48786/0.92883. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.53210/0.88249. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52187/0.95149. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.48607/0.96564. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.47939/0.94194. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.47539/0.97811. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.48934/0.96080. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.46777/0.99569. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.44908/0.94337. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.47438/0.94997. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.48393/0.95985. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.47119/0.98455. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.46779/0.95866. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.42981/0.98586. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.42477/1.02606. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.42959/1.06013. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.42402/1.05946. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.42736/1.00769. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.42377/1.04191. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.43216/1.03243. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.40991/0.99825. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.39602/1.07680. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.39370/1.02356. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.41042/1.12078. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.37807/1.07493. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.38012/1.09862. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.38439/1.03866. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.37819/1.08916. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.35442/1.14068. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.37170/1.10028. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.37111/1.17329. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.35789/1.12297. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.33399/1.10449. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.34660/1.18225. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.35268/1.12584. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.36022/1.21176. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.34058/1.18373. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.34263/1.19647. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.32884/1.20433. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.34273/1.22508. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.33224/1.20545. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.32275/1.27182. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.30723/1.23916. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.32853/1.26875. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.29697/1.23244. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.29507/1.26164. Took 0.43 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.74045/0.74291. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.73166/0.72517. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.72762/0.75142. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.71905/0.74155. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.70702/0.75506. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69956/0.72851. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.70766/0.75202. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.70177/0.74448. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69914/0.72829. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.69710/0.75082. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.69660/0.76386. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.70319/0.74978. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.69033/0.74458. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.69608/0.74761. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.68488/0.73501. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.69070/0.74955. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67547/0.75168. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.68077/0.74203. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.68492/0.76298. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.68140/0.74354. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.68066/0.76782. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66980/0.77273. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.67011/0.79104. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.66916/0.79034. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66751/0.77792. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.66068/0.81090. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.66251/0.82378. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.67505/0.81687. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.66855/0.83983. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.66597/0.81044. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.65668/0.82002. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.67195/0.80668. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.65447/0.83113. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.66226/0.80734. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.66019/0.80879. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.64894/0.81558. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.65455/0.80373. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.65726/0.78770. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.65047/0.80648. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.64691/0.80465. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.64752/0.82170. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.64658/0.82319. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.65627/0.80251. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.63785/0.81728. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.64574/0.79991. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.63387/0.78892. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.63540/0.82505. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.63029/0.81662. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.63561/0.80454. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.63815/0.81528. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.62924/0.83130. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.61910/0.84299. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.62801/0.85779. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.62422/0.83586. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.61938/0.83405. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.62625/0.87325. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.61075/0.89783. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.60146/0.90470. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.61318/0.94206. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.60711/0.90830. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.60643/0.92766. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.60473/0.93445. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.60371/0.94689. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.60904/0.96926. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.59723/1.01490. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.60892/1.00850. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.59632/1.01306. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.60083/0.96515. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.60096/1.01020. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.59456/0.94366. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.58620/0.95565. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.58482/0.94206. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.57688/1.00255. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.57759/0.98602. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.56663/0.97508. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.57641/0.98245. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.56091/1.01525. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.57116/1.00930. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.54613/1.03243. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.57812/1.01615. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.55856/1.06351. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.55061/1.06530. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.57272/0.98370. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.55470/0.99752. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.55107/1.02511. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.53596/1.01046. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.55282/0.96462. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.52367/0.98502. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.53218/0.97862. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.53329/1.00986. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.51426/1.01234. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.50701/1.05580. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.51575/1.04164. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.53077/1.07550. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.51095/1.08197. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.53298/1.06774. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.49724/1.09341. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.52545/1.05813. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.50038/1.06625. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.49872/1.09283. Took 0.44 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69439/0.67688. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.68903/0.67252. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68554/0.67014. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68421/0.66720. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68186/0.67888. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.67940/0.68442. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.67709/0.69024. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67362/0.69482. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.66624/0.70456. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67063/0.69327. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66342/0.69927. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.65936/0.69905. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66085/0.68906. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.65859/0.69502. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65451/0.67921. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65480/0.67450. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65628/0.65645. Took 0.47 sec\n",
      "Epoch 17, Loss(train/val) 0.65573/0.67063. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64856/0.66724. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64856/0.64880. Took 0.47 sec\n",
      "Epoch 20, Loss(train/val) 0.64725/0.66413. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63531/0.64130. Took 0.47 sec\n",
      "Epoch 22, Loss(train/val) 0.63871/0.64601. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63469/0.63297. Took 0.51 sec\n",
      "Epoch 24, Loss(train/val) 0.62214/0.66031. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62525/0.66770. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.62152/0.69905. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61314/0.71175. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60683/0.71653. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62056/0.69500. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61029/0.74012. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.59526/0.72611. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59835/0.74598. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58597/0.74667. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.58366/0.74085. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.57825/0.77431. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.57767/0.76070. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58142/0.75663. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.56244/0.76764. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.56280/0.75240. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.55852/0.71647. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.54892/0.76836. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.53999/0.78282. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.54535/0.81993. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.52800/0.81105. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.53277/0.82442. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.51040/0.80664. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.51460/0.87845. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.51972/0.84233. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.50220/0.84230. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.50112/0.82214. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.51096/0.83395. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.48920/0.82029. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.48226/0.87560. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.47680/0.86838. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.47105/0.88708. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.44883/0.87757. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.45797/0.88903. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.47010/0.93816. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.45117/0.91988. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.44797/0.89921. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.44155/0.95266. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.42991/0.97041. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.42615/0.98025. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.44161/0.97422. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.43924/0.90149. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.42893/0.88159. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.41473/0.92343. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.40942/0.93514. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.41453/1.00653. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.42591/0.96116. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.38661/0.95508. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.40121/1.04802. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.41336/1.00332. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.38175/0.99115. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.38144/1.01012. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.38506/0.99794. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.38014/1.03443. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.36411/1.07118. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.36501/1.07725. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.36551/0.99923. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.34769/1.04352. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.35534/0.97192. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.34579/0.99977. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.35210/0.89989. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.33720/1.01507. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.32558/1.08104. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.33324/1.09320. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.31930/1.09510. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.30797/1.03596. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.31908/1.04928. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.31508/1.03518. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.29647/1.09734. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.27991/1.17420. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.29539/1.02409. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.30916/1.05243. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.29606/1.14425. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.31259/1.13103. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.29037/1.15535. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.27648/1.14742. Took 0.43 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.70162/0.72501. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69368/0.71855. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.69241/0.71777. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.68945/0.71864. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68429/0.72214. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68558/0.71949. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68295/0.72464. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68257/0.74617. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67875/0.75313. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68182/0.75196. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68472/0.74168. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68320/0.75305. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67972/0.75078. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67650/0.75449. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67437/0.75494. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67813/0.76305. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67306/0.76479. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67400/0.76259. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66628/0.77386. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67407/0.77214. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66947/0.78098. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66691/0.79315. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66136/0.80135. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65474/0.80972. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65635/0.81415. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.65460/0.82494. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64945/0.84767. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.65390/0.84559. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.64264/0.85658. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.64595/0.85441. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63680/0.86275. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63740/0.85589. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63065/0.88804. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.63537/0.87193. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.62922/0.88657. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61728/0.89134. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61760/0.88829. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61298/0.91075. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.62157/0.91572. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59487/0.93488. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61748/0.95463. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.60026/0.94353. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59845/0.93688. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.60408/0.92563. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59047/0.97736. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.59135/0.97418. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.59351/1.00363. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.57733/0.97573. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56926/1.02426. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57215/1.03458. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.58912/1.00900. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.59052/0.90356. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.57520/0.98409. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.56226/0.99398. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55294/1.03168. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.56185/0.93568. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.54888/0.99940. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.56299/1.00590. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54053/0.98788. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54667/0.94705. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54928/0.94661. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.53152/0.96406. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52427/1.05567. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.51565/1.08973. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50774/1.07672. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.51439/1.06257. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.52259/0.95979. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.48620/1.00983. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.51817/1.10977. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47768/1.10249. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48904/1.13903. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48305/1.09614. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48626/1.08702. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.47984/1.05708. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.45304/1.10502. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.45904/1.14500. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.46611/1.17925. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.44906/1.17896. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.46139/1.09179. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.43357/1.10766. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.42180/1.22432. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.44536/1.20018. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.43357/1.15764. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.41649/1.18106. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.42767/1.18668. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.40118/1.20379. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.40323/1.28924. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.39983/1.25403. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.39664/1.29094. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.37536/1.28328. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.37729/1.24495. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.39764/1.21883. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.37628/1.28960. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.36456/1.44088. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.37437/1.30539. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.35998/1.39176. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.34780/1.33749. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.36619/1.38642. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.34902/1.38254. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.34292/1.31832. Took 0.44 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.70357/0.69692. Took 0.63 sec\n",
      "Epoch 1, Loss(train/val) 0.70175/0.69808. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69244/0.70636. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69536/0.68869. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69078/0.71176. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69054/0.72086. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68873/0.72127. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68904/0.71903. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68401/0.72333. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68112/0.72283. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68020/0.72577. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67405/0.72649. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67692/0.72656. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.66790/0.72626. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67075/0.72138. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66367/0.72253. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66332/0.73040. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66307/0.72461. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65496/0.72895. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65835/0.73124. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65163/0.73630. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64867/0.75996. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64879/0.77297. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64307/0.77752. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64513/0.78520. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64384/0.78217. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64127/0.80810. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64062/0.79775. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62700/0.82045. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62595/0.82202. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62610/0.83451. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.62277/0.83644. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62501/0.83141. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61142/0.83143. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.60680/0.85439. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59895/0.84951. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60357/0.83981. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58701/0.87313. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58689/0.85016. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.58778/0.87273. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.58182/0.88442. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.56135/0.88650. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56670/0.90141. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.55603/0.89025. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55769/0.88882. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.55642/0.89569. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55640/0.90308. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.54640/0.91913. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.52768/0.92835. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.51853/0.90390. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53148/0.92021. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.51707/0.91802. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.51564/0.92893. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.51960/0.93396. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.50890/0.93175. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.49984/0.91768. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.48941/0.93320. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.50148/0.92529. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.48846/0.91795. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.46965/0.92150. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.45759/0.95911. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.45888/0.98217. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.46832/0.97059. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.45615/0.96154. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.44088/0.96345. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.47953/0.97369. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.45610/1.04211. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.42126/1.12638. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.42394/1.09246. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.40498/1.12674. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.41341/1.10722. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.42299/1.01850. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.39398/1.03650. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.39612/1.12915. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.39382/1.04901. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.38393/1.08501. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.36172/1.04556. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.37085/1.15169. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.38563/1.07381. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.36572/1.07933. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.36757/1.13559. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.33948/1.25981. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.32927/1.16059. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.35878/1.12489. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.32857/1.23756. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.33415/1.20937. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.35768/1.09192. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.34365/1.16313. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.33546/1.16744. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.32400/1.32050. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.28290/1.34018. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.29165/1.28140. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.27966/1.37199. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.27586/1.29334. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.29813/1.33129. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.32117/1.34922. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.30192/1.36187. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.29959/1.44722. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.27514/1.35910. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.27830/1.41468. Took 0.44 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.71019/0.74052. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70011/0.73501. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69759/0.73417. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69345/0.73612. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69219/0.73374. Took 0.49 sec\n",
      "Epoch 5, Loss(train/val) 0.68772/0.72430. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.68631/0.73051. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68265/0.71673. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.68229/0.71404. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.68485/0.71975. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68188/0.71136. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.67739/0.71230. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67456/0.70695. Took 0.48 sec\n",
      "Epoch 13, Loss(train/val) 0.67873/0.71113. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67508/0.71229. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66938/0.71054. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67156/0.72619. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66302/0.71453. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65867/0.71575. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66266/0.71962. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65568/0.73166. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64603/0.73820. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65128/0.76286. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65575/0.72161. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64321/0.72102. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64115/0.74212. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65010/0.72956. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63365/0.74771. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62981/0.75113. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.63335/0.77408. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.62494/0.78432. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.62146/0.80429. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62215/0.83661. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61402/0.81257. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.60300/0.82949. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60375/0.82471. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59148/0.86680. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59224/0.85059. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.58884/0.84839. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58269/0.84760. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59837/0.85660. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58409/0.85455. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56979/0.85245. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.55786/0.87633. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.55718/0.87806. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55550/0.88378. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.54710/0.91804. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.54097/0.90753. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.53635/0.92342. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52515/0.93309. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.51817/0.95536. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.51674/0.96538. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.51445/0.99081. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52636/1.00114. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.50287/0.98328. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.48881/1.04832. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.51325/1.02299. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.50138/1.02084. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.48229/1.03541. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.47704/1.03831. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.48798/1.07173. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.47772/1.08553. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.47238/1.12171. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.46328/1.17520. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.45392/1.13928. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.45031/1.11786. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.43794/1.17494. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.45277/1.24880. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.43633/1.20213. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.44731/1.17106. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.40364/1.19255. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.42713/1.29056. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.41576/1.22284. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.41219/1.21311. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.40853/1.25714. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.39972/1.24928. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.41988/1.20925. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.42570/1.23709. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.40305/1.25641. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.38970/1.29675. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.38103/1.25882. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.37464/1.31452. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.41092/1.21998. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.36275/1.25675. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.34506/1.29297. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.33872/1.36984. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.33460/1.40260. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.35919/1.33383. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.33242/1.41071. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.33477/1.37267. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.34808/1.37217. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.32576/1.39880. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.34160/1.36006. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.31618/1.48655. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.34636/1.56957. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.31892/1.42944. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.36263/1.49124. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.32787/1.50314. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.32428/1.48343. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.31202/1.53029. Took 0.44 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69904/0.67543. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69096/0.68170. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69168/0.68450. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68852/0.69098. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68675/0.69333. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68494/0.69179. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68545/0.68734. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68445/0.68601. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67630/0.69075. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67376/0.68739. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67094/0.67349. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.67280/0.67634. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67103/0.68644. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.66973/0.69609. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66330/0.69701. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.65964/0.72109. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.65797/0.68934. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65691/0.67508. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65611/0.68600. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65981/0.68732. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65180/0.67323. Took 0.48 sec\n",
      "Epoch 21, Loss(train/val) 0.65097/0.68263. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64535/0.68796. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64952/0.69761. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.64709/0.69785. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64369/0.68895. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63403/0.70574. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63956/0.67279. Took 0.49 sec\n",
      "Epoch 28, Loss(train/val) 0.63192/0.71161. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62871/0.70253. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62264/0.71878. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.62175/0.70639. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60367/0.73022. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59907/0.75050. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60728/0.75900. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59893/0.74408. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58693/0.78792. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59826/0.73756. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.57822/0.81390. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58537/0.78525. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58148/0.81146. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.55970/0.83635. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56535/0.81906. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.55346/0.81868. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.55059/0.81820. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.54755/0.84054. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.53185/0.82846. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.51861/0.84847. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.52393/0.85199. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.51640/0.83627. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.51076/0.84061. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.50876/0.81093. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.49450/0.85226. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.49390/0.83548. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.48914/0.85148. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.47306/0.85100. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.45367/0.87640. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.46812/0.81513. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.46912/0.86314. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.45687/0.88353. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.45767/0.91275. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.45377/0.87294. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.43803/0.92005. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.42741/0.90916. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.42615/0.92077. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.41789/0.93304. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.42453/0.90729. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.42536/0.93348. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.38926/0.95193. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.39631/0.94375. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.39582/0.94308. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.37613/0.97748. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.38436/1.00881. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.35916/0.99799. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.37423/1.00076. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.35572/1.05653. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.35659/0.95900. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.34289/0.95157. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.33688/1.09156. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.34191/1.05873. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.34103/1.09849. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.33797/1.00179. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.31742/1.07219. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.31409/1.08543. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.32255/1.07031. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.31405/1.06737. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.30479/1.09676. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.28573/1.09190. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.28486/1.10440. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.27520/1.12594. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.28408/1.12334. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.25250/1.12195. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.31741/1.15098. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.27658/1.12248. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.26740/1.13208. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.25568/1.12719. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.25373/1.12013. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.26328/1.14386. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.22464/1.19305. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.24956/1.14415. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.70145/0.67122. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69862/0.68509. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69481/0.68503. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69066/0.68845. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69518/0.67639. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68916/0.67921. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68938/0.67568. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.69124/0.67449. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68936/0.66856. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.68201/0.66982. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68597/0.67787. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.68064/0.67987. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67957/0.69977. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68125/0.67980. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66881/0.70448. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67094/0.71106. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66695/0.71600. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66776/0.74230. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65633/0.75691. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.64896/0.77199. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65353/0.76847. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64426/0.78083. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64020/0.79362. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64461/0.81233. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63446/0.78092. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63462/0.79560. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63641/0.79639. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62761/0.81019. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61704/0.81480. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.61886/0.82415. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61026/0.83985. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.61071/0.85896. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60770/0.82903. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59863/0.82708. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59198/0.83992. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.59562/0.81763. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.59452/0.86299. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.58186/0.86584. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.58856/0.86263. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.56185/0.86674. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56110/0.90125. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.55223/0.90521. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.56728/0.85884. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.54168/0.90045. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.54429/0.90558. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.53141/0.89885. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.53054/0.94879. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.52103/0.91759. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.50724/0.97423. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52913/0.95093. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.51377/0.93712. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.50986/0.93940. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.49461/0.98561. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.49784/0.99268. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.49460/0.99823. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.48230/1.00639. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.49243/0.99468. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.46396/1.02309. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.47487/0.98766. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.45888/1.00437. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.47168/1.01898. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.47126/1.02102. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.45163/1.00911. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.44723/1.04092. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.44487/1.01532. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.44979/1.05742. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.45975/1.03696. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.44492/1.06569. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.43980/1.08871. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.42635/1.12636. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.44404/1.04814. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.43120/1.10551. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.44454/1.10133. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.41661/1.09402. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.40539/1.07842. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.40323/1.13878. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.40842/1.10354. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.39783/1.10453. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.38043/1.15152. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.38754/1.14890. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.37193/1.22725. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.36523/1.20807. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.36342/1.18653. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.36891/1.22733. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.37127/1.22951. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.37876/1.17903. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.35920/1.18206. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.35388/1.20866. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.33851/1.18431. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.30439/1.26389. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.30483/1.31845. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.32936/1.22690. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.32081/1.27805. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.31128/1.20176. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.35029/1.22359. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.35041/1.21032. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.34002/1.20854. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.30358/1.30668. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.29880/1.33014. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.32956/1.26322. Took 0.43 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.72423/0.69761. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70708/0.72187. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69905/0.73208. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69967/0.72091. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69804/0.71388. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69729/0.73575. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69185/0.73075. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.70007/0.73603. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69718/0.72885. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.69057/0.72876. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68920/0.72523. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.68814/0.73110. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68491/0.75101. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68471/0.73358. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.69236/0.74282. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.68097/0.75162. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.68529/0.74186. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.68095/0.73230. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.68326/0.73905. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.68366/0.74346. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.68932/0.75014. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.68129/0.75547. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.66747/0.75978. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.67904/0.75386. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66600/0.74704. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.66613/0.75580. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.66394/0.75587. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.67332/0.74532. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.66800/0.74385. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65887/0.76560. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.66808/0.77136. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.65234/0.77232. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.65582/0.76823. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.65295/0.78055. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.65967/0.78248. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.63968/0.78859. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.63934/0.81291. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.65917/0.79830. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.64106/0.79732. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.63213/0.78015. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.63366/0.80312. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.63636/0.78429. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.64826/0.80352. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.63667/0.79681. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.62776/0.81123. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.62158/0.83065. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.62132/0.83572. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.62218/0.83638. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.62168/0.80185. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.60980/0.82779. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.61957/0.78422. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.61885/0.80678. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.61120/0.83832. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.62271/0.81774. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.59070/0.83803. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.59628/0.84341. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.61166/0.83931. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.59351/0.84915. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.59731/0.84577. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.58546/0.85873. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.57816/0.87537. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.57609/0.87319. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.55953/0.87771. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.57239/0.91042. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.55810/0.91872. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.54642/0.93265. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.55962/0.91782. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.55605/0.89932. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.52448/0.93766. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.52973/0.97477. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.53877/0.93701. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.53490/0.96446. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.53653/0.94172. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.51649/0.95647. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.53184/0.95035. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.52191/0.96298. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.50718/0.97320. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.50724/0.96825. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.51515/0.99619. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.49835/1.01793. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.49359/1.03608. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.49092/0.98214. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.50955/0.99785. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.53873/1.00661. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.52756/0.98337. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.49789/1.02250. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.48901/1.02511. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.47709/1.06759. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.50114/1.01363. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.49636/0.92938. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.47292/1.00498. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.47074/1.06034. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.45826/1.02714. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.50161/1.00908. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.46015/1.00456. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.45733/1.03278. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.43196/1.10145. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.45827/0.98364. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.44298/1.05750. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.44043/1.06938. Took 0.44 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.71635/0.72453. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.71092/0.71598. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.70187/0.72314. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69526/0.73042. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69234/0.72248. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69201/0.75348. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68425/0.73028. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68737/0.72635. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68142/0.75058. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68339/0.74455. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67544/0.74503. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67450/0.73242. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67328/0.73218. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67157/0.73264. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66377/0.72857. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66433/0.70982. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.66813/0.70131. Took 0.47 sec\n",
      "Epoch 17, Loss(train/val) 0.65595/0.71905. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65277/0.72788. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.64564/0.72997. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64220/0.72726. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64796/0.72159. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64056/0.71065. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64724/0.70759. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63348/0.71365. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.63439/0.70401. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.62551/0.71081. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62363/0.72237. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61969/0.71408. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61542/0.73243. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61163/0.74705. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60711/0.73886. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60538/0.74662. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.61632/0.75830. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60564/0.75533. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59399/0.78148. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59286/0.77367. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59725/0.81901. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.58285/0.82101. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57916/0.84140. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57663/0.83809. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57095/0.83873. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.57311/0.86170. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.57517/0.83045. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.55524/0.85986. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56679/0.85429. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.54742/0.94093. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.55660/0.91183. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.53976/0.88017. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55540/0.92154. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55400/0.91601. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53515/0.89924. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.51927/0.94913. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.53569/0.98632. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52978/0.97945. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.52547/0.96456. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.53012/0.95236. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53240/0.94364. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50733/0.97104. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50398/0.93331. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51189/0.91957. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.49985/1.00669. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51968/0.93129. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48990/1.01975. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.48421/0.98054. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.48569/1.03209. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48214/0.97987. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.46812/0.95236. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.47203/0.96922. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47428/0.98182. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.46536/0.99157. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45280/1.00704. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.44739/0.98851. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.45512/0.96448. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.43723/1.03635. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44151/0.98058. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.45238/0.95645. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.43378/1.03936. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.45316/1.00216. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.43370/1.07522. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.40941/1.08214. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.43452/1.05409. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.44210/0.99693. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.41798/1.02948. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.40729/1.09941. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.42299/0.98878. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.39529/1.07650. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.40153/1.07836. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.39390/1.10747. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.36941/1.11382. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.37964/1.17131. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.38135/1.15957. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.34670/1.15646. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.36825/1.16521. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.35649/1.22172. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.36571/1.28168. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.38936/1.14030. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.35260/1.19633. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.36134/1.16371. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.38443/1.18305. Took 0.44 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.71041/0.70257. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70402/0.71807. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69250/0.69891. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69211/0.70878. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68912/0.70712. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68867/0.70797. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68455/0.70587. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68425/0.70250. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68046/0.70926. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67858/0.71240. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68244/0.71231. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67725/0.71215. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68064/0.72472. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67344/0.72470. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67144/0.72734. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67166/0.73694. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66944/0.74134. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67291/0.73956. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66512/0.73341. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66968/0.74607. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65926/0.73693. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66101/0.73453. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65682/0.74915. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.65903/0.74443. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65061/0.75673. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65351/0.76296. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.64812/0.76960. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64703/0.76848. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.63696/0.77614. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63810/0.78186. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64052/0.78252. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.63975/0.78277. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63084/0.78212. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.63592/0.79855. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62086/0.80714. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62337/0.81018. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.61320/0.82390. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.61350/0.84931. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60991/0.85295. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61460/0.88546. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59666/0.91101. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59886/0.91381. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.59710/0.97462. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59140/0.92215. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59173/0.88690. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.58078/0.93348. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57966/0.95589. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.58165/0.93938. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.57647/0.91738. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.58520/0.92708. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56911/0.96505. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.57024/0.98161. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56472/0.98172. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.55721/1.02321. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55303/1.02109. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55675/0.98051. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56154/0.99512. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54567/1.02605. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.53698/1.04823. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54989/1.04323. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.53779/1.02501. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.53050/1.04931. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52120/1.03566. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51717/1.06406. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.52217/1.06689. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.50746/1.08485. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.50462/1.08565. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.50994/1.09800. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50738/1.12671. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.50289/1.13500. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.50876/1.13275. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.48259/1.15401. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48094/1.17158. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.49350/1.18240. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.48377/1.17766. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.46956/1.20241. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.47354/1.19370. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.47548/1.22256. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.46000/1.20238. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46001/1.21789. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.47751/1.15162. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.45191/1.17761. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.45189/1.16603. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.45153/1.19229. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.44641/1.17116. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.45079/1.19538. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.45040/1.16785. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.42967/1.19458. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.42151/1.23975. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.41580/1.21465. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.41728/1.21523. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.41595/1.25928. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.41450/1.19879. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.41724/1.22414. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40278/1.19252. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.39149/1.17338. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.39096/1.23911. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.37907/1.21429. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.38480/1.24180. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.39585/1.20151. Took 0.44 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.68637/0.69304. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.68346/0.68651. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68248/0.68827. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68265/0.68932. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68135/0.68640. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.67976/0.68897. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67774/0.68931. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67758/0.69079. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67479/0.70074. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67527/0.69216. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67439/0.68739. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67282/0.69392. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66833/0.67895. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.66523/0.67715. Took 0.47 sec\n",
      "Epoch 14, Loss(train/val) 0.66126/0.67728. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66425/0.69930. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66601/0.69630. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65168/0.71399. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64889/0.73130. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65207/0.72720. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65291/0.71934. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65116/0.72962. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64211/0.74249. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62890/0.75749. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63415/0.76489. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63164/0.72642. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63291/0.73980. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62991/0.75108. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.63320/0.75586. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63209/0.73394. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.62954/0.75192. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62614/0.75014. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61579/0.75170. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62921/0.72141. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60204/0.74616. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.61171/0.75644. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60587/0.73379. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60318/0.75296. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60779/0.75342. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.59724/0.75164. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.59696/0.74315. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58275/0.75257. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58850/0.75329. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.57896/0.76099. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59165/0.75090. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.57555/0.73773. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59057/0.72501. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57521/0.74065. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.57577/0.75205. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.56772/0.74907. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55278/0.77914. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55904/0.78578. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.56394/0.77380. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55168/0.76854. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54546/0.79340. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.53056/0.81857. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52929/0.79642. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.52104/0.78792. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.52858/0.77852. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.50820/0.81638. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51084/0.83150. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.51067/0.79430. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49574/0.82380. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.51378/0.84914. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50516/0.77890. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.48637/0.86117. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48405/0.87794. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.47671/0.96440. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.47899/0.93059. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.45787/0.93935. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.46765/0.89943. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.45269/0.95657. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.44794/0.95690. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.45201/0.91171. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.44540/0.94886. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.42748/0.90720. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.42100/0.93530. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.44467/0.95484. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.43131/0.98058. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44595/0.91636. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.41434/0.94304. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.38401/0.99433. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.40635/0.98806. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.41517/0.90800. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.39766/0.92569. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.42391/1.00319. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.41469/0.99517. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.40734/1.10818. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.38783/1.01814. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.38923/0.98784. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.40312/1.05794. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.37938/1.02962. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.39133/1.00185. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.39835/1.07700. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.39978/1.08476. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.39629/1.02095. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.36559/1.01458. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.35645/1.10952. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.35547/1.04764. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.39324/1.10065. Took 0.44 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.71157/0.72381. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.71121/0.73705. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.70735/0.72729. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70538/0.74187. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.70590/0.75095. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.70277/0.75358. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69462/0.76471. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69708/0.76851. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68888/0.77013. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.69268/0.78163. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68429/0.77738. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68326/0.76930. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68179/0.77018. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67363/0.77188. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67005/0.79195. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66823/0.79033. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67926/0.79208. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66874/0.79697. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67326/0.79723. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65731/0.79090. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.66869/0.79724. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66055/0.80732. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66313/0.81141. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.65736/0.82205. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65423/0.81946. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.66017/0.81716. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64828/0.83933. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64365/0.85228. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64089/0.86836. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64120/0.89104. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64071/0.90028. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.62662/0.90051. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.63215/0.89962. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63315/0.92075. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.63248/0.94017. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62569/0.91999. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61581/0.96837. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.62223/0.97338. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62268/0.96298. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61595/0.95001. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.62127/0.97295. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.62291/0.98011. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60995/1.01382. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.58741/1.03613. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.60073/1.04895. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59690/1.04968. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59882/1.07307. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.59649/1.07992. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.58856/1.12463. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.59038/1.14146. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.58188/1.10807. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.58272/1.13516. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.57360/1.13955. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56597/1.20522. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.57168/1.21294. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.56314/1.18125. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.54081/1.24216. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.56674/1.15901. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54757/1.23572. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53822/1.26186. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.53345/1.26591. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.52214/1.31116. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53334/1.25101. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.52932/1.24650. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.51702/1.21636. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50886/1.18343. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.50319/1.23684. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.50401/1.29386. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.49378/1.24708. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.51638/1.32570. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49511/1.27653. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48022/1.37505. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48349/1.24680. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48539/1.25511. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.47824/1.26226. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.46885/1.26580. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46759/1.30124. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46425/1.38457. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.48154/1.29639. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.45623/1.29315. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.45855/1.31270. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.45016/1.30927. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.44589/1.39436. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.44524/1.39487. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.42097/1.34289. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.46817/1.34614. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.42373/1.26723. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.41448/1.38745. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.41321/1.32802. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.41533/1.27611. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.43873/1.28494. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.41319/1.32427. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.40480/1.30774. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.40615/1.33852. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40804/1.30528. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.39490/1.27486. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.41175/1.28511. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.40081/1.26050. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.38014/1.26464. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.37390/1.26391. Took 0.44 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69477/0.70027. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69400/0.71827. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68927/0.71037. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69100/0.72514. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68554/0.72238. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68841/0.72524. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68206/0.72218. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68832/0.72353. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.66826/0.73301. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67261/0.73472. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.66619/0.76294. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67052/0.75850. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66959/0.76201. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67206/0.77017. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66267/0.77996. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66697/0.77530. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66048/0.78481. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66292/0.78327. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65928/0.77265. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65894/0.75986. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65809/0.78925. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66155/0.79700. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65457/0.80754. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.65733/0.79903. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65135/0.79989. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64411/0.82496. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65375/0.79794. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64522/0.81513. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64902/0.78828. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.65128/0.81572. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64362/0.81141. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.63743/0.79838. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.63933/0.82238. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.62904/0.82390. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.62854/0.82851. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.63215/0.80424. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.63531/0.79254. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.63596/0.78928. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.62232/0.83722. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61766/0.82419. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62094/0.82345. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.62647/0.86653. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.61522/0.87471. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61301/0.82549. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60350/0.88543. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.62246/0.88709. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.60454/0.86224. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.59524/0.89175. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.61159/0.90447. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.61289/0.90901. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.60451/0.90865. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.60298/0.87061. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.60309/0.90821. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.60045/0.92947. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.59852/0.90368. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.60237/0.87884. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.59945/0.91235. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.58500/0.91314. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.59105/0.89667. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.59407/0.92032. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.58378/0.94701. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.58092/0.91501. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.57785/0.92193. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.57611/0.93503. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.58606/0.94688. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.58290/0.92981. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.58308/0.98038. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.56894/0.92542. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.56254/0.90952. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.56888/0.95391. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.56681/0.96769. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.54545/0.95900. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.55200/0.90750. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.57513/0.92176. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.56485/0.94595. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.54305/0.94684. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.54975/0.99516. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.54528/0.94988. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.54151/0.92147. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.54083/0.98082. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.55170/0.94879. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.53198/0.94164. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.54479/0.92366. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.52971/0.97739. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.53136/0.96603. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.53367/0.93097. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.53316/0.99307. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.52307/0.96243. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.52275/0.94587. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.50866/0.98911. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.50950/0.98974. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.51292/0.98632. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.51411/1.00643. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.50921/0.96329. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.49641/1.02658. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.49140/0.98983. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.49806/0.94923. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.49093/0.96136. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.47434/0.98080. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.48788/0.99433. Took 0.44 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.70598/0.71133. Took 0.63 sec\n",
      "Epoch 1, Loss(train/val) 0.70406/0.69764. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70374/0.70228. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69737/0.70126. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69724/0.70517. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69107/0.68940. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.69616/0.69143. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.69289/0.69877. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68378/0.69518. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68762/0.71128. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.69194/0.71816. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68896/0.73488. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68392/0.73421. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68053/0.75584. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67669/0.75828. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67034/0.75188. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66824/0.76105. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67062/0.79308. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65366/0.82087. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66753/0.79828. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65991/0.80679. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65654/0.80668. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64779/0.81580. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64873/0.82226. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65169/0.81093. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64958/0.81205. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63454/0.83526. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63600/0.85525. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.63123/0.86418. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62916/0.86324. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.62257/0.86140. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62402/0.86468. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62875/0.88636. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61745/0.91501. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62870/0.88249. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61918/0.89161. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.61404/0.91678. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.60871/0.95151. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.59801/0.96156. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.60862/0.93723. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60619/0.93904. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.59393/0.95565. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59474/0.95508. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.58592/0.93336. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58729/0.95044. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.58210/0.93828. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.56993/0.96259. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56987/0.99902. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.57794/0.97943. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56385/0.97495. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56238/0.95906. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56167/1.01327. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.56123/0.99206. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54276/1.01820. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54907/0.98315. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55090/1.03884. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.53757/1.05382. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53710/1.06836. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.52403/1.05837. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53203/1.05066. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.52485/1.09102. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.52223/1.08838. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51386/1.07461. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.51099/1.14120. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.50400/1.14191. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.51291/1.16313. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.49833/1.20634. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.50049/1.15862. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.48592/1.20942. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.48283/1.21273. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.48225/1.13710. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.48878/1.27445. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.49111/1.21711. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.45969/1.30346. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.47249/1.24863. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.46284/1.22852. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.47506/1.23688. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46746/1.27846. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.46570/1.27028. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.47206/1.30929. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.45107/1.30194. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.43716/1.43564. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.44970/1.45585. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.43211/1.42835. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.43855/1.44324. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.44972/1.52379. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.42931/1.45167. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.41346/1.50941. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.43659/1.56272. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.41880/1.55646. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.40125/1.53451. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.42472/1.59024. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.40305/1.60904. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.43318/1.46855. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.42456/1.45892. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.40072/1.52780. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.38461/1.61840. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.40117/1.62683. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.39182/1.62602. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.37785/1.68091. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.70280/0.70783. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69761/0.70532. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69641/0.70016. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68746/0.69772. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.68230/0.69508. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.68803/0.69368. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.68260/0.69968. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68127/0.69240. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.68219/0.72039. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68313/0.71138. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68219/0.72944. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68285/0.72149. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67853/0.73295. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67742/0.73367. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67905/0.75415. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66653/0.76447. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67011/0.76171. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66633/0.76459. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66836/0.76251. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66078/0.78926. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66466/0.76153. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66426/0.76055. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65408/0.76982. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65740/0.76470. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65389/0.76271. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65617/0.75328. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64806/0.76871. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64251/0.76173. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64289/0.75762. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.64716/0.76162. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.63464/0.78354. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.65058/0.77861. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64397/0.77327. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63873/0.76547. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.63917/0.76794. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.63303/0.79232. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62799/0.78631. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.63164/0.80407. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62399/0.79491. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.62023/0.79724. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.62051/0.80088. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.62577/0.82682. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.61662/0.81958. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.62695/0.81503. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.60647/0.81019. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.62121/0.86386. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.61763/0.88964. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.60574/0.89720. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.60338/0.89638. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.60318/0.96958. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.59970/0.94424. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.59539/0.95698. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.59421/0.94827. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.58376/0.95829. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.59097/0.91298. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.58107/0.95057. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.56451/0.95867. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.58236/0.99090. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.57665/0.93237. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.57580/0.92265. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.57460/0.92985. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.55977/0.98473. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.55893/1.02417. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.55201/1.02634. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.55418/1.14326. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.58048/0.96982. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.55221/0.98017. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.53577/1.05843. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.53449/1.09277. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.53689/1.10437. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.53323/1.08539. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.52385/1.07720. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.52692/1.05706. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.51731/1.11958. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.50561/1.14837. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.50176/1.12316. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.51451/1.12635. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.49532/1.24804. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.51843/1.12255. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.50583/1.17037. Took 0.42 sec\n",
      "Epoch 80, Loss(train/val) 0.51066/1.11047. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.51511/1.14383. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.48666/1.16450. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.49091/1.16946. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.50143/1.17181. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.48523/1.14902. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.49486/1.18588. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.47965/1.11902. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.46748/1.27064. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.44256/1.20056. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.46583/1.26194. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.46450/1.33513. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.47439/1.22472. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.47330/1.28943. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.45033/1.23642. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.43223/1.29637. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.44975/1.26953. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.45474/1.32583. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.46057/1.29242. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.43466/1.23468. Took 0.43 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69919/0.70433. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69134/0.70625. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68899/0.71029. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68627/0.72088. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68711/0.71527. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68299/0.72332. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68493/0.72033. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.67941/0.72903. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68311/0.73078. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68149/0.73094. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68128/0.73690. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67369/0.73505. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67767/0.72268. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67779/0.74549. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67613/0.74506. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67568/0.74717. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66907/0.74270. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66422/0.75544. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66664/0.75816. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66895/0.75494. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66230/0.76058. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65517/0.77326. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65327/0.78193. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65685/0.77469. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65298/0.76836. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65432/0.78589. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64394/0.79405. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63560/0.79617. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63737/0.81136. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.63518/0.83709. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62549/0.85398. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63451/0.82613. Took 0.42 sec\n",
      "Epoch 32, Loss(train/val) 0.62480/0.82576. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.62416/0.86870. Took 0.42 sec\n",
      "Epoch 34, Loss(train/val) 0.62487/0.83872. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62037/0.86787. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61863/0.85240. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61465/0.82507. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.60165/0.85311. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.60461/0.84780. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.59575/0.85307. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.59817/0.85155. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58802/0.82733. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.59333/0.84348. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60085/0.81243. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.58190/0.82004. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.57363/0.87315. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57630/0.84517. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.57017/0.88681. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.59405/0.81472. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57267/0.88827. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56440/0.92147. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.57449/0.79826. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.57475/0.83147. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.58060/0.82331. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.58290/0.82219. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56635/0.84930. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.55099/0.82329. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.54786/0.79425. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.54599/0.82718. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54009/0.89811. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.52989/0.86590. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.54313/0.83304. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.53315/0.84120. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.52527/0.88277. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.51903/0.84014. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.51157/0.88739. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.50820/0.87089. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.50911/0.82975. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.50972/0.90245. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.49643/0.90512. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.51063/0.91491. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.50450/0.89976. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.49447/0.86957. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.49246/0.88096. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.48671/0.90575. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.51032/0.88141. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46284/0.91004. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.47838/0.92694. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46704/0.92893. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.46387/0.96335. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.45353/0.93638. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.46430/0.92654. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.47366/0.95509. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.47013/0.94864. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.45971/0.98229. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.46518/0.92641. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.44796/0.88566. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.43799/0.92399. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.42876/0.93757. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.44044/0.92729. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.43957/0.93642. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.45078/0.96063. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.42954/0.99925. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.42124/0.97822. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.42106/1.02185. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.42859/1.03421. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.41483/0.94063. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.42478/1.05139. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.40565/0.99321. Took 0.43 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.70435/0.72803. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69477/0.71889. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69374/0.72655. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68953/0.72189. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69406/0.71193. Took 0.48 sec\n",
      "Epoch 5, Loss(train/val) 0.69407/0.71474. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69280/0.71223. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68540/0.71825. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68992/0.71230. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68515/0.71243. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68417/0.71434. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67379/0.71160. Took 0.48 sec\n",
      "Epoch 12, Loss(train/val) 0.67702/0.71391. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.68004/0.71198. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67388/0.71479. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67503/0.72071. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.67306/0.71304. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65713/0.72809. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65710/0.72802. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65888/0.73707. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65177/0.74090. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.63724/0.74313. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63976/0.73057. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63704/0.74363. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63327/0.76177. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62618/0.75907. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.62860/0.76770. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63066/0.74830. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62379/0.73276. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61496/0.77242. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60688/0.77459. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.61113/0.80697. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.60748/0.77968. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60351/0.79962. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.59673/0.80196. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.58395/0.82708. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.58793/0.82757. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.58616/0.78419. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.57877/0.81927. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.57214/0.79828. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.55855/0.83923. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57014/0.84513. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.56817/0.82164. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.55584/0.86779. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55650/0.85501. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55205/0.86027. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.55614/0.84707. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.55328/0.83553. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.53051/0.87550. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.51989/0.89462. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.54535/0.88458. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.52524/0.92972. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.52129/0.90209. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.52148/0.93425. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52174/0.94406. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.50226/0.95376. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.50487/1.01086. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.52905/0.92852. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50635/0.89234. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52550/0.91379. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50040/0.94059. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50096/0.97299. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49615/0.99424. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48664/0.97823. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49088/0.99921. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.48852/0.97578. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.50132/0.94994. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.48475/1.01619. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.47967/0.99316. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.46800/1.06718. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48896/0.99273. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.49675/0.98269. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.47107/1.03839. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46451/1.06488. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.47706/1.03284. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48072/1.03391. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.46009/1.03931. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.45572/1.08457. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.44936/1.05628. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.45402/1.06053. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.46971/1.03573. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.45040/1.08986. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.45240/1.06641. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.45005/1.04657. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43189/1.09927. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.44636/1.07779. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.44948/1.09262. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43381/1.10912. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.42902/1.11341. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.42284/1.09454. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.43378/1.12163. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.41831/1.09910. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.42347/1.06751. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.40621/1.15577. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.41584/1.13235. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.40143/1.11013. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.41167/1.15435. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.39792/1.18605. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.39894/1.13485. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.40926/1.11452. Took 0.44 sec\n",
      "ACC: 0.6145833333333334\n",
      "Epoch 0, Loss(train/val) 0.72090/0.72597. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.70673/0.72764. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.68799/0.75040. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69933/0.76903. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69929/0.77232. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.70725/0.72930. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69266/0.73901. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.69517/0.74339. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.69141/0.73823. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.69271/0.74129. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68554/0.72559. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.69036/0.72080. Took 0.48 sec\n",
      "Epoch 12, Loss(train/val) 0.69410/0.72631. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.68720/0.73254. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68564/0.73264. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67896/0.73044. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67623/0.72338. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67380/0.74260. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67924/0.73761. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67180/0.74596. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.67052/0.73819. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.66544/0.74594. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66975/0.75763. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66846/0.76488. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.66328/0.76822. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65937/0.77024. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65651/0.78448. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65835/0.77626. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64825/0.79060. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.65374/0.79746. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.65336/0.80201. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.65535/0.79886. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64521/0.78812. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64530/0.81153. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.64318/0.81843. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.64057/0.84047. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.63868/0.86307. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.63146/0.85548. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62770/0.86544. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.64013/0.84893. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.63812/0.87631. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.62421/0.88303. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.62197/0.88278. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61969/0.92244. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.62175/0.86555. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.61246/0.87147. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.61191/0.89484. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.60133/0.91939. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.62428/0.88410. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.59714/0.89344. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.59671/0.90274. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.60104/0.92486. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.60481/0.91794. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.59004/0.92634. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.59625/0.94977. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.58597/0.96055. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.58948/0.93068. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.57953/0.93357. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.58001/0.90434. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.56752/0.92418. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.56320/0.94237. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.56848/0.94005. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.56559/0.95147. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.56274/0.93392. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.56209/0.96168. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.54479/1.00520. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.55341/0.98915. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.54090/1.00485. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.53353/1.01090. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.54529/1.05254. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.52429/1.02673. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.52564/1.03081. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.52287/1.13358. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.50374/1.12867. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.50781/1.06438. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.51846/1.12071. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.50843/1.22381. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.50870/1.17524. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.56238/1.09201. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.51712/1.11352. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.51329/1.07611. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.49351/1.16690. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.48482/1.17406. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.50386/1.18825. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.48954/1.19690. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.49153/1.14638. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.47879/1.15181. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.47494/1.17350. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.47143/1.21655. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.47565/1.10909. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.47283/1.18398. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.50158/1.14595. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.47058/1.11892. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.44665/1.15699. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.43651/1.21430. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.45455/1.16574. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.45744/1.18753. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.43415/1.24530. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.44375/1.21996. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.44522/1.29267. Took 0.44 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.70750/0.68151. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.70820/0.66551. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69973/0.66967. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70463/0.66658. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69369/0.67662. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69047/0.68756. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69716/0.68317. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68862/0.67513. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.69369/0.66917. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.69067/0.66991. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68977/0.68137. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.68376/0.67647. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68105/0.68425. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.68620/0.67910. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.67722/0.67845. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67217/0.69501. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67841/0.69591. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66955/0.70331. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.67231/0.71030. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66497/0.71327. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65610/0.71962. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.66338/0.71679. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66561/0.72140. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66647/0.71628. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65481/0.72610. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.66217/0.72167. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65473/0.71542. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64740/0.72410. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.65833/0.74351. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65065/0.75453. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.65326/0.73828. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64362/0.74971. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.65278/0.74460. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63938/0.73603. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.63025/0.75838. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.63738/0.74855. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62623/0.77477. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.63261/0.77579. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62015/0.77170. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.62898/0.77996. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61403/0.77531. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61437/0.77955. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.61288/0.78399. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59176/0.79897. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.60562/0.79906. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.60965/0.80260. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.60725/0.81954. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.59367/0.83098. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.58487/0.84563. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.58357/0.85117. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.58275/0.84820. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.58226/0.87491. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.58224/0.84590. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.57780/0.85522. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.56836/0.83667. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.56347/0.88461. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55474/0.90344. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.56375/0.90741. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.56097/0.93250. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.55544/0.93842. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.55048/0.95069. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.55398/0.89583. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54576/0.92854. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.53227/0.93738. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.54557/0.90889. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.54308/0.90261. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.53570/0.95698. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.53701/0.91677. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.53684/0.89518. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.54795/0.89464. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.53308/0.92161. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.53327/0.94719. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.52554/0.96725. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.51555/0.98619. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.51068/0.96351. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49884/1.00188. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.51035/1.03978. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.50741/1.08147. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.51085/1.05939. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.50670/1.05708. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.51133/1.07881. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.48954/1.12822. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.48226/1.15158. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.48120/1.16230. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.47567/1.19112. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.48940/1.18750. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.47087/1.14144. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.48185/1.17743. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.46252/1.19640. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.46529/1.25219. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.46224/1.21375. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.47107/1.19170. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.44509/1.27484. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.43575/1.30020. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.43736/1.35984. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.48037/1.16693. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.43316/1.28748. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.44478/1.26320. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.43128/1.34631. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.47165/1.21124. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.70162/0.72251. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69996/0.71946. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69844/0.70252. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69319/0.69729. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68374/0.70864. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68484/0.71377. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69099/0.72643. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68483/0.72316. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68784/0.71824. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68331/0.71398. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67911/0.73097. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68380/0.72379. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.68392/0.72083. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68296/0.72211. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68373/0.72481. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.68146/0.71480. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.68106/0.71460. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67013/0.70680. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67818/0.73002. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67024/0.72212. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67504/0.72358. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.67325/0.72541. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66707/0.72879. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66082/0.73451. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.66221/0.74376. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66906/0.75306. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.66031/0.77131. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.66024/0.78159. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.65510/0.76886. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.64795/0.79567. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.65315/0.80195. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.65832/0.79580. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64951/0.80349. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64138/0.82363. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.64796/0.81096. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.64934/0.83672. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.64412/0.80578. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.63607/0.81067. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.63625/0.79783. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.63325/0.82388. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.63194/0.82235. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.63690/0.83368. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.62750/0.82346. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.62467/0.83930. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.62654/0.84948. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.61930/0.86350. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.62207/0.88386. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.61608/0.87928. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.61762/0.86163. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.60628/0.87713. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.61197/0.86445. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.60847/0.86803. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.60077/0.88561. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.60300/0.90816. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.59575/0.88540. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.59722/0.89703. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.59963/0.88461. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.58263/0.89513. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.57780/0.92764. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.58022/0.94657. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.57030/0.96357. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.57351/0.97521. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.56477/0.96747. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.55224/1.03563. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.54539/1.01857. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.54448/1.03134. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.55054/1.05491. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.53604/1.11467. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52398/1.06936. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.51611/1.07870. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.51493/1.09381. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.51782/1.11330. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.50665/1.10669. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.51800/1.13216. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.50410/1.10318. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48701/1.15934. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.48110/1.16648. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.49525/1.16481. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.48432/1.16661. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.48572/1.14134. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.46775/1.14536. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.47154/1.16478. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.46827/1.21120. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.46720/1.16926. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.45683/1.14438. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.44331/1.20310. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43305/1.18960. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43728/1.16413. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.43680/1.14718. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.43289/1.27781. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.41621/1.23478. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.41523/1.24577. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.42280/1.20914. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.41639/1.23269. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.41155/1.29570. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.42207/1.23465. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.40842/1.26381. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.41615/1.25923. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.41739/1.25523. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.38410/1.31382. Took 0.43 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69998/0.71229. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69197/0.73501. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69058/0.72644. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68965/0.72982. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69312/0.72896. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68355/0.73314. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68547/0.73446. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68229/0.73531. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68194/0.74045. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68090/0.73179. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68173/0.74228. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67992/0.72364. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67583/0.73060. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67108/0.73424. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67070/0.72543. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67136/0.73455. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67085/0.71011. Took 0.48 sec\n",
      "Epoch 17, Loss(train/val) 0.66235/0.74038. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66704/0.72915. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66330/0.75096. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66415/0.74010. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.66035/0.75975. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65508/0.76329. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.65603/0.78764. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.65345/0.78379. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65244/0.78070. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65059/0.78578. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.65362/0.78907. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64886/0.77608. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.65084/0.79328. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.64902/0.79298. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.65044/0.77212. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.65166/0.77778. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.65230/0.77946. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.64056/0.77219. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.64062/0.75750. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.64044/0.76698. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.63307/0.77482. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62905/0.78587. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.62496/0.79218. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.62232/0.79024. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.62696/0.79073. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.61543/0.78739. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.61526/0.80164. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60970/0.81210. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59612/0.83053. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59887/0.82403. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.59790/0.86071. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59482/0.84191. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.58644/0.87363. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.57999/0.87064. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.57139/0.89035. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.57140/0.90915. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.57075/0.86318. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56447/0.88256. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.56922/0.85246. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55303/0.85429. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.55018/0.84324. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.54020/0.87689. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53295/0.89966. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.53959/0.94109. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.53211/0.90713. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53465/0.86736. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.53815/0.88812. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.52478/0.91738. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.51434/0.90297. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.50439/0.94571. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.49395/0.98314. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49623/0.95449. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.49527/0.98594. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48082/0.98698. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.47587/0.96306. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.46860/0.99093. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.45992/0.95112. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.47316/1.00941. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.44085/0.98742. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.45532/0.98384. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.45440/1.01684. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.48994/1.01636. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.47201/0.92970. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.43635/0.98899. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.42472/1.02037. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.41375/1.04400. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.43895/1.07283. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43158/1.11027. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.42013/1.14273. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.41361/1.10281. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.44256/1.02745. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.39945/1.01625. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.39958/1.00305. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.39765/1.10180. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.39274/1.08450. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.39631/1.10887. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.40001/1.15310. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.38263/1.04742. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.42139/0.96922. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.40329/1.00714. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.37653/1.09047. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.35667/1.06917. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.35863/1.09889. Took 0.43 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.70722/0.71751. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69407/0.70423. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69566/0.71840. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68721/0.73064. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68456/0.73924. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68490/0.75122. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67671/0.74372. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68269/0.76056. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67784/0.76235. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67604/0.78084. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67325/0.78706. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66528/0.82537. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66611/0.83963. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67105/0.85496. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66460/0.85506. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66939/0.86703. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65531/0.88381. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65368/0.90282. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65197/0.91144. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.64647/0.91905. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64119/0.89630. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63383/0.88053. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.63519/0.90534. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64094/0.85887. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.62992/0.89803. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63539/0.93365. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.62423/0.88583. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61373/0.86980. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62103/0.87109. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61659/0.83356. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60287/0.88238. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.60903/0.82715. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60159/0.85760. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59044/0.87550. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59203/0.85662. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58431/0.92825. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58993/0.93665. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.58886/0.93279. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.57514/0.93789. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.57666/0.96373. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.55639/0.94721. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.56009/0.95913. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56830/0.94481. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.55129/0.97061. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.56381/1.00106. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.54889/0.99776. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.53831/1.08238. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.53273/1.08287. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.54279/1.12093. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52637/1.09648. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54964/1.12622. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53395/1.11859. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.53057/1.11279. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.52786/1.19486. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.52203/1.18804. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51144/1.20644. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52049/1.17190. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51362/1.20450. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.50511/1.23176. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.50589/1.31795. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52499/1.19238. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.49013/1.33451. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53739/1.20269. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.52120/1.20998. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.50503/1.23325. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.49161/1.17807. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.54029/1.08022. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48886/1.16318. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.52718/1.04201. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.48944/1.12044. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.46988/1.25571. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.51062/1.05154. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.49241/1.09861. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.47193/1.14233. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.47028/1.18904. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.45942/1.15108. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.46791/1.16509. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46287/1.16676. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.47199/1.11845. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.47268/1.16211. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.44690/1.24051. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.44566/1.22069. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.46089/1.20082. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.49178/1.19086. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.51404/1.12970. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.46165/1.19770. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.42938/1.22524. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.42616/1.28370. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.42573/1.29908. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.41401/1.27804. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.41167/1.31251. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.41087/1.35341. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.41660/1.39649. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.39086/1.41203. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.40337/1.41434. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.41321/1.39482. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.36963/1.45778. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.39995/1.43269. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.38616/1.47536. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.36491/1.50081. Took 0.45 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.70479/0.73632. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70373/0.74218. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69813/0.72710. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69530/0.72252. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.70199/0.71761. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.69356/0.72855. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68924/0.72313. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68939/0.73281. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68475/0.73487. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68215/0.73176. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68274/0.73319. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68685/0.72855. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.68599/0.74085. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67454/0.73028. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67680/0.74837. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67254/0.73123. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.67534/0.74337. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67016/0.74722. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67355/0.75652. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67386/0.76369. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67015/0.76621. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.67592/0.77431. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66519/0.76114. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66454/0.79164. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.66345/0.77521. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65780/0.79422. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65918/0.77416. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64720/0.77925. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.64295/0.80532. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64593/0.81876. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63937/0.83947. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.65062/0.81900. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.64172/0.82791. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.63904/0.83170. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.63681/0.80934. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.63307/0.83914. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62805/0.84615. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62617/0.83316. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.61568/0.85108. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61763/0.83370. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61807/0.82672. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61660/0.83763. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.60952/0.81386. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60854/0.84077. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60415/0.83265. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.59194/0.87196. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.60147/0.90069. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.60134/0.82233. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.59255/0.88381. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.59028/0.94943. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.59243/0.89677. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.60239/0.88924. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.57137/0.90175. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.56970/0.92092. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.59110/0.89709. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.57361/0.90130. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55791/0.98030. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.58293/0.87126. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.55964/0.90868. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.55946/0.92640. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55741/0.95116. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.54661/0.95857. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54576/0.95585. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.52983/0.95595. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.53991/1.07170. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.54128/1.01127. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.53203/1.02124. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.52384/1.01291. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.53098/1.04882. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.52338/0.99045. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.50713/1.00812. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.50810/0.95009. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.50225/0.97788. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.49976/1.00729. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.51711/0.98158. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49529/1.03189. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.49139/1.00475. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.47134/1.00306. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.46398/0.99934. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.46461/1.02756. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.45347/0.97574. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.45714/0.95181. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.46223/0.96665. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.45262/0.98443. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.45643/0.99806. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.44872/0.96630. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.44925/0.93889. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.44290/1.05215. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.43190/1.13179. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.43652/1.01485. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.41217/1.00717. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.40387/0.95518. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.41156/1.07798. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.41294/1.01375. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.42199/1.05298. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.39116/1.09381. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.40505/1.03491. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.39307/1.02836. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.38226/1.10518. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.40135/1.10974. Took 0.44 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.70684/0.71719. Took 0.62 sec\n",
      "Epoch 1, Loss(train/val) 0.69205/0.72710. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69103/0.71487. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68187/0.72478. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.67334/0.73183. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67391/0.72357. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.66502/0.71920. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.66542/0.70851. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.65660/0.71741. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.65945/0.71032. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.64794/0.71902. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.65227/0.73918. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65062/0.73993. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.64229/0.74651. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.63772/0.75943. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.62854/0.77820. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.62539/0.78751. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.63624/0.76956. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.61877/0.75973. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.61147/0.74901. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.60469/0.78846. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.60952/0.76068. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.61235/0.77362. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.60299/0.78367. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.58120/0.80005. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.59210/0.77885. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.58270/0.79544. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.57896/0.82879. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.56956/0.83357. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.55934/0.86672. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.56534/0.89546. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.53993/0.92424. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.53689/0.94156. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.53906/0.93780. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.52869/0.93405. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.53516/0.93145. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.52690/0.91880. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.52560/0.93616. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.52346/0.93667. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.51411/0.93129. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.50563/0.95406. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.50860/0.94412. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.49561/0.93474. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.49097/0.94750. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.48881/0.94623. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.49267/0.97571. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.47814/0.98111. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.46807/0.99441. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.46876/0.98763. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.45454/0.99822. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.44205/1.06410. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.45187/1.04261. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.44308/1.05432. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.45289/1.08259. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.46847/1.02047. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.43188/1.02377. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.42643/1.07705. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.41969/1.13402. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.41526/1.14512. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.39897/1.15280. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.40890/1.12930. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.40135/1.10292. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.39006/1.24005. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.39477/1.17847. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.39061/1.20568. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.36802/1.20116. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.35877/1.17767. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.36734/1.23221. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.36413/1.26798. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.36415/1.30453. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.34430/1.39160. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.37088/1.28851. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.36799/1.32666. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.36035/1.34593. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.33381/1.41956. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.32926/1.37965. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.35163/1.32319. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.33462/1.29498. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.33236/1.31734. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.30461/1.36225. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.31245/1.35398. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.30354/1.40064. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.31082/1.35780. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.30803/1.44919. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.32155/1.36901. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.32735/1.47294. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.28347/1.49097. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.27656/1.43407. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.31163/1.29969. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.29001/1.45065. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.26700/1.51515. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.28030/1.45632. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.26539/1.57967. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.29518/1.45716. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.25677/1.68693. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.24423/1.70476. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.24599/1.67322. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.24814/1.70614. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.25767/1.83069. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.22750/1.84394. Took 0.45 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.72922/0.73490. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.72562/0.73098. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70357/0.71992. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69498/0.72484. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.70371/0.72931. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68799/0.71819. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.69064/0.74057. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68233/0.73002. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67735/0.72753. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67461/0.73264. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67658/0.73364. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66937/0.71423. Took 0.47 sec\n",
      "Epoch 12, Loss(train/val) 0.65905/0.71296. Took 0.49 sec\n",
      "Epoch 13, Loss(train/val) 0.66146/0.70307. Took 0.47 sec\n",
      "Epoch 14, Loss(train/val) 0.66539/0.71218. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.65107/0.72304. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66033/0.71946. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65361/0.73808. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65050/0.74875. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64750/0.74106. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64891/0.72386. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64285/0.72858. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.63032/0.73606. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.63567/0.74940. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.62396/0.74388. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.61904/0.74079. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61882/0.75811. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.60942/0.77440. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60829/0.75916. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60071/0.76330. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.60291/0.79432. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.59627/0.78693. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59483/0.78844. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59046/0.79843. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.58305/0.80048. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.58818/0.79082. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.58446/0.81855. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57784/0.83012. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.55256/0.86532. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.55998/0.87112. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56294/0.89611. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.56375/0.81500. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.53571/0.87550. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.54392/0.84090. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.53615/0.84554. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.54584/0.82963. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.54257/0.87568. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.52815/0.83618. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.51806/0.90287. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.51587/0.90283. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.51958/0.90379. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.51842/0.88164. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.53332/0.86369. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.50848/0.92235. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.50809/0.90748. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.49438/0.91328. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.48604/0.96452. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.49601/0.93893. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.49776/0.90957. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.48594/0.89952. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.48122/0.93574. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.47219/0.97399. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.46051/0.92689. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.45772/1.00545. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.46215/1.05381. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.46375/1.04773. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.47082/1.02815. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.45072/0.99938. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.46199/1.04309. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45014/1.06842. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.45373/1.05436. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.42968/1.08214. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.44851/1.05475. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.42868/1.11236. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.42132/1.10244. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.45434/0.99663. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43165/1.09872. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.41026/1.21101. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.38301/1.16847. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.41186/1.16152. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.40133/1.20291. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.39628/1.21889. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.37535/1.18935. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.38903/1.24110. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.37303/1.24429. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.38142/1.28478. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.40698/1.17782. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.37730/1.24955. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.36151/1.26923. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.35545/1.27491. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.35266/1.25128. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.34797/1.31065. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.33560/1.29832. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.32238/1.45128. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.32893/1.38987. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.33300/1.39745. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.33662/1.37791. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.35427/1.33115. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.34447/1.25184. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36538/1.23523. Took 0.43 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.74040/0.71831. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.73686/0.70940. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.72995/0.70698. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.71107/0.70425. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.69737/0.70369. Took 0.48 sec\n",
      "Epoch 5, Loss(train/val) 0.69654/0.71570. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.70000/0.71531. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67938/0.71293. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.69867/0.75237. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68965/0.74546. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67857/0.75752. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68788/0.75072. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67752/0.74214. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67949/0.73990. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.68399/0.75116. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67175/0.76032. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67960/0.74821. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67915/0.75708. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66815/0.76824. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67050/0.76754. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66939/0.76627. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66623/0.76271. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66624/0.77255. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66467/0.75340. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65518/0.76278. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66848/0.76284. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65095/0.75690. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65801/0.76359. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65282/0.77016. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65445/0.74635. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.64981/0.76484. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.64785/0.76387. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64928/0.75746. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63857/0.76500. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.64425/0.76302. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.63541/0.76509. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.63111/0.76824. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62498/0.80062. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62608/0.79018. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.63442/0.81548. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.61945/0.78776. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61543/0.82046. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.61672/0.79765. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60681/0.81445. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60657/0.83885. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59426/0.88289. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59546/0.82885. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58035/0.89588. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.59957/0.84245. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.58132/0.86301. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.58690/0.86324. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.58321/0.84447. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.57850/0.85294. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.58246/0.85733. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56561/0.86741. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.57037/0.85509. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55828/0.84940. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.55549/0.83077. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.55192/0.86662. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.55405/0.87793. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55342/0.87365. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.54871/0.85720. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54215/0.88955. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.54586/0.90407. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.54251/0.89262. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.53680/0.87507. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.52906/0.88890. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.51922/0.90942. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.54546/0.90885. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.52722/0.89993. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.52443/0.91809. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.50814/0.95029. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.51292/0.97128. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.49993/0.90956. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.49469/0.90565. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.49011/0.96110. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.50646/0.95257. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.49788/0.93971. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.50473/0.91630. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.49233/0.96454. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.49746/0.92962. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.47170/0.92009. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.46847/0.96329. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.47498/1.02087. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.46901/0.99528. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.46467/1.03252. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.47733/0.95720. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.44902/1.00119. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.44822/1.09842. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.45398/1.10035. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.43538/1.09535. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.43943/1.09144. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.43779/1.04159. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.44254/1.10125. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.44303/1.08615. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.41659/1.04877. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.41619/1.09351. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.42547/1.15441. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.41788/1.12487. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.40325/1.11967. Took 0.44 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.70123/0.68986. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69118/0.70923. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68772/0.72550. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68865/0.72842. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68369/0.72854. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68471/0.73397. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67797/0.74951. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67818/0.74989. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67614/0.74502. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67156/0.76026. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67444/0.75272. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66900/0.75766. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66373/0.76116. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66848/0.76612. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66496/0.77353. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66334/0.77039. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65998/0.77303. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65688/0.78916. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65227/0.79800. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65404/0.81367. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65279/0.80890. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63968/0.83003. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64118/0.84114. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.63766/0.84258. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.62752/0.84804. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63278/0.84329. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62764/0.83208. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62452/0.82717. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61458/0.85656. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60659/0.86722. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.60723/0.87683. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60876/0.89021. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59154/0.89917. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58551/0.89363. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.58353/0.90898. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.58465/0.91806. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.57747/0.92523. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.57038/0.96146. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57463/0.95207. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.56815/0.93391. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56738/0.96697. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.56430/0.96981. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55009/0.98204. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.54514/0.98649. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.55175/1.00155. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55320/0.97479. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.52926/1.04181. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.52547/1.02429. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.53065/1.01427. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52520/1.07157. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.51236/1.04537. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.52953/1.09025. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.50307/1.11593. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.51410/1.07646. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51475/1.13563. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.50670/1.16193. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.49892/1.16574. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.48528/1.15050. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.48164/1.16184. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.49394/1.25899. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.46559/1.19479. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.47715/1.13558. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.46333/1.27594. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.46524/1.15349. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.44254/1.23664. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.44940/1.34105. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.44110/1.27598. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.45907/1.25188. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.44543/1.30495. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.43302/1.25839. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.42857/1.24072. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.41915/1.37287. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.41800/1.36714. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.41945/1.38885. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.40877/1.34197. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.40624/1.32436. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.40389/1.30170. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.41987/1.31517. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.40915/1.29826. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.38004/1.49753. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.39123/1.51261. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.37857/1.44908. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.39845/1.40428. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.41391/1.38984. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.38713/1.48355. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.38857/1.43148. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.35748/1.55705. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.35433/1.59708. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.34456/1.57146. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.34645/1.58502. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.33955/1.56040. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.35153/1.55317. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.33060/1.67062. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.32590/1.66029. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.33546/1.64460. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.31723/1.59910. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.33855/1.63306. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.31212/1.63523. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.31019/1.68439. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.29645/1.74945. Took 0.44 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.71106/0.69499. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70021/0.69226. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69809/0.69505. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69066/0.69265. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68653/0.70201. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67882/0.69857. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.67875/0.70158. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67811/0.69989. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67443/0.71368. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.66672/0.72433. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66918/0.74305. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66126/0.77207. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65891/0.79885. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.65875/0.82138. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66184/0.80427. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66241/0.81633. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65381/0.79475. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64891/0.81193. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64449/0.80933. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.64618/0.82531. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64928/0.83895. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63708/0.85813. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63301/0.87833. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63479/0.86378. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63349/0.86712. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62870/0.87849. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62161/0.90602. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62414/0.89427. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61923/0.91067. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.61509/0.92620. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61553/0.89773. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61185/0.90770. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.60336/0.92806. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59156/0.95440. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.57750/0.95174. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.57743/0.95309. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.57001/1.00239. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.57637/1.01255. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.56159/1.05231. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.55909/1.04212. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.57372/1.03100. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.55381/1.05361. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.54574/1.05809. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.53397/1.06778. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.52756/1.11155. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.52741/1.08220. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.51681/1.16986. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.50806/1.17317. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.51105/1.16460. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.51188/1.19763. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.50145/1.28901. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.48845/1.21743. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.48722/1.24576. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.49451/1.26546. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.48510/1.24434. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.47424/1.17256. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.46475/1.17397. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.46859/1.30089. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.46121/1.28717. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.46190/1.26635. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.45256/1.28019. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.44503/1.19671. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.43084/1.20707. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.42624/1.27563. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.43391/1.28995. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.40963/1.30738. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.41294/1.43236. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.41227/1.26735. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.40850/1.33811. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.40829/1.26743. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.39577/1.21927. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.39694/1.23990. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.38979/1.29793. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.37066/1.29067. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.38122/1.42878. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.36174/1.35641. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.36701/1.38953. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.34890/1.34563. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.32108/1.47978. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.33578/1.45441. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.32644/1.46292. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.31567/1.49856. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.32579/1.50825. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.35135/1.55696. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.31613/1.62451. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.34352/1.54584. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.30580/1.67492. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.29284/1.77443. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.29625/1.62960. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.30447/1.65945. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.31209/1.84848. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.30127/1.76897. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.26774/1.82402. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.24799/1.82750. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.29153/1.87321. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.28500/1.86900. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.27123/1.82536. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.26738/1.89881. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.23655/1.94634. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.22539/2.08481. Took 0.43 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.71981/0.71127. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70810/0.72195. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.70408/0.71684. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69895/0.71508. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68880/0.71457. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68190/0.70254. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.67937/0.69957. Took 0.47 sec\n",
      "Epoch 7, Loss(train/val) 0.67678/0.68583. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.67418/0.68287. Took 0.48 sec\n",
      "Epoch 9, Loss(train/val) 0.67344/0.69768. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.66820/0.70231. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67147/0.69218. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67018/0.69455. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66168/0.70209. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65189/0.69541. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.64922/0.69049. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.64513/0.70310. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64585/0.71008. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.63712/0.69683. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64790/0.70227. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64165/0.70148. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63755/0.69242. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.62626/0.70521. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.62228/0.68718. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.62100/0.69635. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62339/0.71007. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.61550/0.70503. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61216/0.69825. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61130/0.71326. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.60611/0.72156. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.59692/0.71174. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60015/0.70908. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.60047/0.70659. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.59844/0.71015. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59008/0.71565. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58554/0.71840. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59221/0.72167. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.57771/0.71423. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57371/0.71608. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57549/0.70659. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57117/0.73028. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.56387/0.73962. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.55719/0.75433. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.55828/0.78588. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.55886/0.74439. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55145/0.76187. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.54499/0.78592. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55454/0.74608. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.54372/0.79773. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54563/0.76858. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53383/0.80041. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.52238/0.79873. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.51906/0.83950. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.51508/0.82565. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.50677/0.84465. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.50290/0.86005. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.49401/0.89709. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.47894/0.88121. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.49858/0.90210. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.48954/0.89347. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.48620/0.87742. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.47652/0.89655. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.48318/0.91631. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48188/0.90228. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.45831/0.90476. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.45978/0.90204. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.44618/0.94518. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.44960/0.94255. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.45125/0.99225. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45632/0.93998. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.42474/0.97458. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.42127/1.04385. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.42781/1.02566. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.41577/1.08619. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.41293/1.01373. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.41811/1.05075. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.42077/1.05952. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.40093/1.06669. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.41636/1.12479. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.39194/1.15361. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.39630/1.16306. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.38260/1.18306. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.38037/1.17012. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.37040/1.23290. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.35592/1.26425. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.38280/1.32765. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.36803/1.31476. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.38684/1.22624. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.36083/1.21772. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.35019/1.22407. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.31762/1.36343. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.36961/1.31757. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.36134/1.26318. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.37644/1.23733. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.34512/1.16587. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.33233/1.33038. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.32741/1.27533. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.32166/1.45427. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.32804/1.40055. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.33543/1.42842. Took 0.44 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69472/0.68621. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69039/0.67715. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68768/0.68998. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68548/0.71327. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68066/0.71329. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67274/0.72217. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67321/0.72347. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.66877/0.73947. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.66856/0.74629. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66606/0.75796. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.66591/0.75591. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66153/0.75820. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66493/0.77752. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65632/0.78136. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65594/0.77645. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.64779/0.77567. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.64680/0.80094. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64607/0.80049. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.64326/0.80511. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.63728/0.78587. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.63869/0.78231. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63796/0.80807. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.62419/0.82305. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.61765/0.82696. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.61385/0.83860. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62313/0.83751. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.60338/0.85330. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.59800/0.84912. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60293/0.84396. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.58893/0.86825. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.58783/0.88927. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.57813/0.88168. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.57285/0.92315. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.58074/0.88396. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.56851/0.90421. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.54787/0.89199. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.55375/0.91594. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.55700/0.92508. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.55786/0.89692. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.55207/0.87008. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.54482/0.91466. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.54980/0.94148. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.52930/0.94627. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.53066/0.95204. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.52184/0.96604. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.51555/0.92751. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.51431/0.93151. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.49956/1.01491. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.49850/1.00941. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.48964/0.99668. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.49995/0.99732. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.50675/1.01198. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.49177/1.00312. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.47426/1.01976. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.46683/1.02342. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.46926/1.00208. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.48389/0.97276. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.44875/1.02398. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.43904/1.13123. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.44230/0.99186. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.44632/1.06073. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.43214/1.08194. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.44709/1.02951. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.42442/1.11941. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.40472/1.19283. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.41916/1.14063. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.40208/1.17057. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.40594/1.16145. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.40685/1.10773. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.40033/1.13679. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.39051/1.28301. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.38202/1.12554. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.38209/1.20301. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.36770/1.17812. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.36387/1.16589. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.37764/1.19053. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.33752/1.29879. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.35896/1.26937. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.36239/1.18579. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.33122/1.14760. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.33776/1.27894. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.34350/1.31012. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.34072/1.22865. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.32853/1.30390. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.32312/1.34513. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.31206/1.29607. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.32619/1.36200. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.29386/1.38773. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.30082/1.35324. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.29626/1.33888. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.33515/1.33569. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.29542/1.36570. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.31285/1.31969. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.31115/1.28846. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.28053/1.40739. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.30348/1.39020. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.27701/1.40754. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.28263/1.35188. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.26357/1.44542. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.27084/1.48697. Took 0.45 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.72391/0.74930. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.73862/0.70574. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.70159/0.71188. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70371/0.69452. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.70669/0.72506. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69572/0.70433. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.69947/0.69008. Took 0.47 sec\n",
      "Epoch 7, Loss(train/val) 0.68994/0.70275. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68019/0.70711. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67803/0.71383. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.69701/0.72757. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67953/0.73281. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.68378/0.72718. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.68874/0.72709. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67981/0.74450. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.68224/0.74386. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.68419/0.73837. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.68166/0.72550. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67976/0.75036. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.68561/0.73291. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67529/0.73184. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.67406/0.71261. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.67060/0.72529. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66039/0.72607. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66563/0.71363. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.65861/0.73750. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.65506/0.75486. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65355/0.74852. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65024/0.76030. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65550/0.76547. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.64766/0.75309. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.63941/0.74863. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64386/0.73751. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63337/0.75032. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.62730/0.75650. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.64410/0.76243. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.63163/0.79192. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.63481/0.77374. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62375/0.80085. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.61906/0.80082. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62903/0.79527. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60060/0.81923. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60840/0.85009. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60057/0.87657. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.60225/0.89762. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.60882/0.89075. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.59482/0.90929. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58441/0.90945. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.58687/0.93548. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.59014/0.94376. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.58679/0.98320. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.57796/0.95116. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56963/0.97772. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.57475/0.95368. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.55629/1.01767. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.57900/0.92344. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55760/0.96102. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.54861/1.01322. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.54541/1.01622. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54571/1.05571. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54885/1.05002. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.56142/1.00431. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54448/1.03001. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.53199/1.13094. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.53646/1.09228. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.52942/1.08322. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.52707/1.08994. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.54126/1.05257. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50711/1.12842. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.51929/1.10929. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.51263/1.15658. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.50423/1.14334. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.51392/1.15327. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.50416/1.12467. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.49604/1.09775. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.50121/1.17432. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.49876/1.12699. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.50859/1.10795. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.50607/1.08773. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.49422/1.19715. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.47889/1.21526. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.47451/1.31155. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.46254/1.20546. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.47836/1.26295. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.46985/1.31006. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.45196/1.49343. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.46140/1.35818. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.46275/1.27762. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.46316/1.28098. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.45806/1.30591. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.43882/1.41459. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.46842/1.39235. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.43810/1.42310. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.43995/1.44404. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40389/1.42175. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.42236/1.51188. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.43429/1.52174. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.46555/1.39191. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.41271/1.45365. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.44945/1.34022. Took 0.44 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.70055/0.69046. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69567/0.70066. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69454/0.69740. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69239/0.69643. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69012/0.70676. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69049/0.70134. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68104/0.70169. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68446/0.69586. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68074/0.70933. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67689/0.70745. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67621/0.69854. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67649/0.69899. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67410/0.71857. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68013/0.72823. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67459/0.73156. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67211/0.73608. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67355/0.72970. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66354/0.72119. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.67094/0.72456. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66131/0.72908. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65938/0.74511. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.66102/0.74034. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65485/0.75762. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.65224/0.77214. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.64500/0.77453. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65369/0.76865. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63929/0.77385. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64195/0.79314. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64570/0.78877. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.64899/0.79198. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63892/0.79040. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63218/0.81643. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63867/0.82129. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.63767/0.80077. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.63179/0.79517. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62885/0.82315. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62278/0.84905. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62202/0.87350. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62924/0.82581. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.61237/0.85960. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.61737/0.88106. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61317/0.86405. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.59804/0.91524. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61725/0.87249. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.59908/0.95708. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.59048/0.92742. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.60229/0.92651. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.58720/0.91088. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57605/0.93576. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.58407/0.95463. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56185/0.96961. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.58986/0.96368. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.56755/0.90038. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56046/0.91677. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.59650/0.96986. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.59716/0.95588. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.55805/0.99326. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.56793/0.97194. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.56101/1.05138. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.57208/1.00710. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.55462/1.04726. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.54296/1.11147. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54623/1.12020. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.55514/1.05653. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.52613/1.09625. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.54428/1.00180. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.53218/1.02353. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.53451/0.96665. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52095/1.07880. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.52360/1.03981. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.50511/1.11551. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.50405/1.17424. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.50148/1.12837. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.50444/1.12272. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.46551/1.24988. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.50033/1.16835. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.48215/1.21744. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.49205/1.17177. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.45589/1.14006. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.48528/1.12884. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.47463/1.18319. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.46474/1.19832. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.47257/1.19328. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.47886/1.23468. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.44373/1.31581. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.44943/1.20307. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.44880/1.26539. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43435/1.23841. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.43492/1.25478. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.43434/1.26628. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.42395/1.32620. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.39368/1.33248. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.40265/1.38005. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.42678/1.28441. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.42601/1.30446. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.43894/1.32695. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.45234/1.22078. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.38429/1.36979. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.38790/1.43162. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.38760/1.40112. Took 0.45 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.70201/0.68761. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69658/0.70748. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69535/0.69353. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70136/0.69661. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69590/0.70170. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69300/0.70299. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68965/0.69720. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68311/0.69289. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68755/0.68898. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67716/0.70060. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68809/0.69858. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.68228/0.69668. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67855/0.69806. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.68006/0.69518. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67541/0.70818. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67174/0.71599. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66588/0.72419. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67531/0.74160. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66453/0.73869. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67021/0.73830. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66567/0.75770. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66515/0.75571. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.67210/0.78672. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66380/0.80071. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66624/0.77415. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66476/0.78698. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.66264/0.80771. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65617/0.78979. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64578/0.81724. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.66207/0.78725. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64816/0.81242. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.65072/0.82574. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.64025/0.83992. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64433/0.83791. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.64730/0.86217. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.63295/0.87685. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.63285/0.87585. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62776/0.85964. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.63586/0.85608. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.62883/0.84547. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.62000/0.90265. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61492/0.85236. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.62256/0.85653. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.60272/0.87299. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.62020/0.88073. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.61097/0.86869. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.61147/0.86547. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.61106/0.85555. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.60686/0.89243. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.59795/0.86991. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.60430/0.89066. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.59489/0.88300. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.60699/0.86260. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.60400/0.83150. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.59601/0.89864. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.59529/0.86472. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.60320/0.84352. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.58496/0.88677. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.58070/0.90101. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.57080/0.89363. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.58549/0.88113. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.57037/0.91784. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.56886/0.89285. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.57095/0.93074. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.55354/0.93412. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.55568/0.95873. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.54842/0.99931. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.56908/0.90205. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.55551/0.99826. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.54882/0.93646. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.54580/1.00472. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.53802/0.98590. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.55138/1.02557. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.53074/1.01211. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.52378/1.01573. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.53668/1.03804. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.51958/1.04574. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.52015/1.05983. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.52561/1.09585. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.51061/1.12784. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.49641/1.12171. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.49595/1.17292. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.48954/1.18001. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.49952/1.21602. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.49806/1.20539. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.49751/1.15981. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.50011/1.13304. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.49231/1.17151. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.49797/1.15552. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.46895/1.19289. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.50605/1.21786. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.47219/1.14671. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.45070/1.21346. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.45094/1.26771. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.44963/1.25564. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.46525/1.23220. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.48128/1.19173. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.45050/1.20638. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.44747/1.26405. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.41029/1.30002. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.70381/0.68928. Took 0.63 sec\n",
      "Epoch 1, Loss(train/val) 0.70626/0.66602. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.70003/0.67584. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69659/0.68230. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69259/0.68122. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69683/0.68410. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68757/0.68980. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67917/0.69185. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.69323/0.70346. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67939/0.69894. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67644/0.71658. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67551/0.72324. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67256/0.74126. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67206/0.73993. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66622/0.75151. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67044/0.76428. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66373/0.76370. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65977/0.76990. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65603/0.79105. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66128/0.76909. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64498/0.78638. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64284/0.79026. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64674/0.82035. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63834/0.81600. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63389/0.86271. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63056/0.86848. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62153/0.86029. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62009/0.87468. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61845/0.85173. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60536/0.85759. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60357/0.89830. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.61051/0.90303. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.59749/0.92710. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.60153/0.93368. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.58496/0.95758. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58441/0.97777. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58996/1.00107. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.58272/0.99018. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57977/1.02861. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57428/1.05893. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.56928/1.12737. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57168/1.12471. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56670/1.19827. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56894/1.06064. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58019/1.08162. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56007/1.14667. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55023/1.21970. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55369/1.14625. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55174/1.23908. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54148/1.16496. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.53707/1.22080. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.53308/1.23380. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54913/1.21042. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.52613/1.30199. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51510/1.31267. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51018/1.36262. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.52084/1.22234. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52720/1.28585. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.49182/1.41240. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.48772/1.41108. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.50025/1.28917. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.49797/1.34942. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48406/1.40763. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.46925/1.37344. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.46489/1.41547. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.45960/1.54693. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.47084/1.41945. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.44710/1.53683. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.45942/1.37199. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46121/1.50676. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.44827/1.53595. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.43031/1.57332. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.44284/1.52847. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.43891/1.54335. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.44659/1.48706. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.41345/1.54398. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.43535/1.56122. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.43029/1.47515. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.38515/1.63077. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.40301/1.66186. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.42157/1.58616. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.38791/1.72040. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.39948/1.76784. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.42732/1.56430. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.41893/1.53500. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.37092/1.66656. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.39590/1.67423. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.39949/1.66563. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.37346/1.69052. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.37011/1.68662. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.34935/1.88496. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.39410/1.82314. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.35419/1.86342. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.38167/1.76403. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.35026/1.77787. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.32928/1.85970. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.33904/1.93399. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.36540/1.88466. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.35749/1.64270. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.35800/1.69228. Took 0.43 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.70407/0.75210. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69041/0.76010. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.68828/0.77761. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68609/0.77052. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.67539/0.75562. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67819/0.76061. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.67034/0.75716. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.66590/0.76612. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67316/0.75637. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.66845/0.74225. Took 0.47 sec\n",
      "Epoch 10, Loss(train/val) 0.66679/0.74245. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66520/0.74553. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66385/0.73389. Took 0.47 sec\n",
      "Epoch 13, Loss(train/val) 0.66240/0.74430. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66129/0.73222. Took 0.49 sec\n",
      "Epoch 15, Loss(train/val) 0.65390/0.72644. Took 0.47 sec\n",
      "Epoch 16, Loss(train/val) 0.64938/0.71725. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.65087/0.71770. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64618/0.72904. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.63332/0.74982. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64744/0.73054. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63918/0.74531. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63703/0.75509. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.62911/0.73668. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.63295/0.72584. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.62577/0.72601. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62107/0.76326. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.62360/0.74765. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61362/0.77114. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.61548/0.73002. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61254/0.77110. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.61656/0.80566. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.60926/0.79482. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.58595/0.84101. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.58237/0.84534. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58278/0.82788. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58509/0.84122. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58134/0.84932. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.57342/0.83625. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.56985/0.83021. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.55822/0.85294. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.55505/0.85589. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.54668/0.86680. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.53736/0.88850. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.53483/0.90430. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.52944/0.91369. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.53137/0.87660. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.51441/0.95013. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.50672/0.95693. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.50063/0.94950. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.51438/0.89149. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.49847/0.96645. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.48765/0.99278. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.46737/0.99300. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.49100/1.04187. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.47730/0.95492. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.49862/0.90612. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.47302/0.95689. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.47393/0.95707. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.47442/0.94717. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.48458/0.94646. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.44790/1.00847. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.46329/1.01950. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.46358/1.00441. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.43654/0.99276. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.45050/0.94602. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.43358/0.98197. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.46162/1.06831. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.42125/1.09081. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.42877/1.05949. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.42794/1.06063. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.42177/1.09405. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.41309/1.13156. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.40868/1.12682. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.41250/1.06451. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.41345/1.16489. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.39914/1.25272. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.38601/1.18966. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.37788/1.16993. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.37369/1.24748. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.37229/1.25502. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.35903/1.25396. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.36556/1.34637. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.35861/1.24750. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.37603/1.27088. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.36058/1.28633. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.37945/1.28983. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.35436/1.37600. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.34244/1.40063. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.34499/1.32632. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.34255/1.51036. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.35332/1.41709. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.33315/1.34532. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.34603/1.35770. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.33313/1.43588. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.33292/1.39349. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.30541/1.47552. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.30755/1.51163. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.32865/1.50230. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.31231/1.37104. Took 0.44 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69649/0.68738. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68934/0.67520. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68564/0.68547. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68498/0.68499. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68009/0.70443. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68184/0.69536. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.67665/0.70102. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67200/0.71979. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67337/0.73758. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67060/0.73442. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.66623/0.74302. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66384/0.73881. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65656/0.74719. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65545/0.77625. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.64628/0.76612. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.64424/0.78054. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.63123/0.82481. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.62831/0.83800. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.62412/0.83954. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.61801/0.85636. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.61513/0.86585. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.60822/0.85558. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.60510/0.85726. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.59985/0.87511. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.58630/0.85734. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.58258/0.89770. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.57637/0.88624. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.56949/0.87269. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.57406/0.88746. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.57558/0.91657. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.56162/0.87313. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.55512/0.89707. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.56028/0.89626. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.53782/0.92738. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.54210/0.91889. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.54017/0.89531. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.53734/0.88012. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.53797/0.92052. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.54387/0.89458. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.52088/0.96222. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.50546/0.92832. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.50483/0.91741. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.50894/0.98903. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.49479/0.91585. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.49031/0.96105. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.50249/0.92528. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.49565/0.99565. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.49173/1.01932. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.48022/1.03717. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.48981/1.03987. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.47406/0.99164. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.46238/1.04689. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.45258/0.93022. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.43955/0.94502. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.44513/1.00200. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.45148/0.96468. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.43610/0.97333. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.44126/0.95414. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.41672/1.00061. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.40939/1.00351. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.41535/1.01278. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.41427/0.99879. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.40626/0.96402. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.40931/0.95987. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.40021/0.96872. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.37575/0.97972. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.36444/1.02036. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.38352/1.03617. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.37118/0.94160. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.35406/1.08133. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.36574/1.00501. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.36060/1.03681. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.35292/1.06940. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.34200/1.05603. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.34708/1.14405. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.33888/1.12291. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.35660/1.13908. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.32786/1.06649. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.31853/1.11180. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.32775/1.15704. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.32970/1.09231. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.30027/1.14612. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.29763/1.20811. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.30488/1.17755. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.32349/1.14750. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.32857/1.13473. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.31236/1.09600. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.28455/1.21488. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.29815/1.13068. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.28328/1.16938. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.27318/1.24190. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.29976/1.13180. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.26558/1.27394. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.27254/1.19678. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.29031/1.16081. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.26562/1.19726. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.25392/1.27341. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.24324/1.32296. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.24566/1.28242. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.23824/1.37501. Took 0.45 sec\n",
      "ACC: 0.375\n",
      "Epoch 0, Loss(train/val) 0.69630/0.71439. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68732/0.71537. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.68072/0.73292. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68043/0.73835. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.67533/0.73684. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.67266/0.74142. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67562/0.74422. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67007/0.76316. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.66943/0.75578. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.66554/0.77109. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66599/0.76794. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66136/0.79441. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.65567/0.79224. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.65233/0.80511. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.64868/0.82507. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.64361/0.82443. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.63867/0.84633. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.62918/0.84816. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.63159/0.87732. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.62663/0.86594. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.61335/0.90426. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.60947/0.91371. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.60928/0.94335. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.60722/0.96313. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.59903/0.99181. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.59369/0.96211. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.57974/0.97237. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.57907/1.00243. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.57149/0.96649. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.56848/0.96656. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.56715/0.98156. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.55427/0.99548. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.55852/0.98345. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.55378/1.00412. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.55922/1.03740. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.53794/1.04523. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.53580/1.00539. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.52763/1.01241. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.51646/1.01414. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.50716/1.02445. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.50680/1.00424. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.50982/1.01407. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.49019/0.99841. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.48728/1.05767. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.48283/1.04716. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.46789/1.10509. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.47930/1.05721. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.46107/1.08317. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.47164/1.13007. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.47413/1.03453. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.44886/1.10895. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.46477/1.01499. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.45142/1.05463. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.41962/1.15578. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.42271/1.12148. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.41996/1.13376. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.40163/1.19654. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.40558/1.10843. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.40955/1.08133. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.40826/1.14065. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.40270/1.20117. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.38204/1.16043. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.37067/1.10954. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.36502/1.15425. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.34435/1.23888. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.35351/1.18204. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.35375/1.22651. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.34797/1.23569. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.34729/1.27130. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.35021/1.30894. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.33025/1.30356. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.32895/1.34669. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.32413/1.25211. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.31820/1.31535. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.30569/1.26690. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.30197/1.34030. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.30302/1.34195. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.30424/1.23362. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.28433/1.34097. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.28295/1.36559. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.29726/1.31933. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.27280/1.41980. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.25744/1.38090. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.30465/1.27423. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.29833/1.51981. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.28027/1.36836. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.23754/1.36399. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.27559/1.28541. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.24704/1.38801. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.23262/1.32413. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.21907/1.48106. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.21726/1.49015. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.21701/1.57221. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.21050/1.48666. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.24829/1.35654. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.24447/1.68163. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.24127/1.71105. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.19977/1.70450. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.18577/1.78808. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.17055/1.79099. Took 0.45 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.72307/0.69752. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70203/0.71318. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69325/0.71834. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70977/0.71116. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69552/0.71731. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68717/0.72912. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.69317/0.74755. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68252/0.76057. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67922/0.78480. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68499/0.77211. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68562/0.77241. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67379/0.76701. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66923/0.76676. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67081/0.79558. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66424/0.79894. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66796/0.80539. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65860/0.80020. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65762/0.82129. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65401/0.82903. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65328/0.82796. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64818/0.82187. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64116/0.82242. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63963/0.84065. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63471/0.83545. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63107/0.84835. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.62858/0.85563. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.62089/0.85103. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62463/0.84501. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.61148/0.85676. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.61679/0.86326. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61540/0.87210. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60907/0.88349. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59625/0.87235. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60167/0.91420. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.61149/0.88922. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60947/0.89728. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59890/0.91196. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58564/0.88780. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60028/0.90471. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59915/0.87474. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59297/0.89439. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59749/0.88851. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.58483/0.87760. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.58610/0.90451. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57525/0.91171. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.57762/0.89935. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.58034/0.92629. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58604/0.90229. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57239/0.89321. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57414/0.90580. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56812/0.94476. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56895/0.96474. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56275/0.94008. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54125/0.93734. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55530/0.95697. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.54468/0.95295. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.54751/0.99617. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53063/1.03340. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54112/1.02418. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.51796/1.06146. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.53194/0.99932. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.51992/1.01463. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.52178/1.01708. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.52775/1.00648. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.52771/1.03084. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.51802/1.01694. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51228/0.99586. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.50885/1.03484. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50813/1.00626. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.50897/1.05351. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.50180/1.02744. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.49475/1.05153. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.49329/1.03401. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.49987/1.02388. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.50245/1.04967. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48207/1.03099. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46655/1.09791. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.50323/1.00689. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.50229/1.04124. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.47974/1.06423. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.46452/1.10562. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.48115/1.12015. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.44575/1.13410. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.46842/1.08589. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.48185/1.10665. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.46594/1.07571. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.45373/1.12042. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.44922/1.12911. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.44236/1.15952. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.43267/1.19337. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.44962/1.18942. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.42797/1.14696. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.41747/1.21249. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.42423/1.20874. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.41041/1.26838. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.40333/1.34328. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.42466/1.20642. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.42163/1.26427. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.39436/1.23627. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.40588/1.29281. Took 0.44 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69587/0.71531. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69505/0.71913. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68824/0.72574. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68368/0.72474. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68392/0.71504. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.67885/0.73368. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.67279/0.72008. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67609/0.71571. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67055/0.72032. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66842/0.71575. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.66552/0.72510. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66542/0.74114. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65504/0.73578. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.65455/0.75504. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.64176/0.75466. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.64377/0.77848. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.64481/0.77096. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.64845/0.78952. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.63837/0.78015. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.63341/0.77822. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63480/0.78752. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63044/0.79850. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.62405/0.80620. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62023/0.81388. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62254/0.82612. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.61622/0.81776. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.61430/0.84660. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.60310/0.85979. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61080/0.85812. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.59929/0.86925. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.60242/0.87621. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.59041/0.88906. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59802/0.93214. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58279/0.94173. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.58993/0.93157. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.57903/0.95800. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.56569/0.96131. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.57669/0.96976. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57549/0.94639. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.56485/0.98277. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56762/0.95032. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.56028/0.97209. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.55644/0.97524. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56786/0.91394. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.55298/0.95287. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.52568/0.97904. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.54034/0.95750. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.52862/0.94482. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.52546/1.00731. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.51720/1.02200. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.50764/1.04067. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.49081/1.04169. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.50175/1.03083. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.50675/1.08144. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.51380/1.10813. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51295/1.07711. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.49037/1.07519. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.47690/1.10152. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.47108/1.09068. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.47183/1.13965. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.46862/1.08993. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.46347/1.11420. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.45250/1.16908. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.45291/1.16453. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.44205/1.14545. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.42749/1.14179. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.44392/1.15592. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.43388/1.12694. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.42327/1.15013. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.41829/1.16413. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.40111/1.20617. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.39307/1.24410. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.39098/1.24433. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.39508/1.24660. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.38086/1.26074. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.38921/1.27101. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.38695/1.29958. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.37810/1.26413. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.36632/1.35098. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.34007/1.43294. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.34764/1.33192. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.35771/1.43566. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.35291/1.41594. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.35090/1.36628. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.34321/1.34987. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.34078/1.38204. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.33700/1.41429. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.33575/1.36653. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.29873/1.38625. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.32070/1.37978. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.29682/1.45840. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.30151/1.47844. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.28381/1.43730. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.28782/1.47887. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.28815/1.49880. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.28200/1.49497. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.27091/1.50513. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.24401/1.62897. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.25600/1.57626. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.26655/1.55094. Took 0.44 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.70246/0.68443. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69981/0.68020. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69864/0.69297. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68741/0.68700. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69602/0.69637. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68998/0.68762. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.69275/0.68114. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.69740/0.68772. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68404/0.67544. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.68502/0.67951. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68233/0.67796. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68446/0.67705. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68085/0.67098. Took 0.47 sec\n",
      "Epoch 13, Loss(train/val) 0.68586/0.67012. Took 0.47 sec\n",
      "Epoch 14, Loss(train/val) 0.67727/0.66838. Took 0.47 sec\n",
      "Epoch 15, Loss(train/val) 0.67400/0.66984. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66993/0.68222. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66948/0.69068. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67016/0.68720. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66923/0.69504. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65983/0.69836. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65332/0.70554. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64739/0.70628. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65548/0.70943. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63895/0.72230. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64283/0.70905. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64003/0.71509. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63104/0.73383. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62719/0.76520. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62861/0.79438. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.60771/0.81480. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.61987/0.81597. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61579/0.79647. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61148/0.80662. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60856/0.82100. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.59720/0.82426. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.60486/0.80873. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.59885/0.83121. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58648/0.82880. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57721/0.86140. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.58226/0.85614. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58280/0.82686. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.57179/0.86424. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.56871/0.87726. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.56116/0.86286. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.56458/0.88965. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.54910/0.90926. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55113/0.93352. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.55510/0.93971. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.54232/0.95924. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54882/0.96484. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.54563/0.96767. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.52684/0.99294. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52378/0.97749. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.52006/0.98762. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51270/1.04105. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.51720/0.97034. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.52554/0.93666. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50834/0.96332. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.48731/1.00560. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.47852/1.00309. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.48438/1.03004. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.47345/1.02330. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.47530/1.03891. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.46741/1.04756. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.45256/1.05131. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.45757/1.04924. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.43468/1.05130. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.43733/1.08884. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.43525/1.12379. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.42874/1.13656. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.39774/1.16593. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.43429/1.14636. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.41492/1.18853. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.40167/1.23531. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.41229/1.12060. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.39041/1.21159. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.40984/1.22837. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.39810/1.18023. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.37870/1.19987. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.37649/1.17228. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.37156/1.20831. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.39397/1.18109. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.37372/1.24858. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.35609/1.20904. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.35985/1.25781. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.38202/1.26393. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.35551/1.28172. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.33459/1.26475. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.34601/1.29655. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.32578/1.33405. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.33108/1.32142. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.33663/1.30134. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.33603/1.39665. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.32623/1.34988. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.30556/1.35058. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.29312/1.31915. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.31737/1.31429. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.30288/1.44784. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.30153/1.41621. Took 0.44 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.72877/0.73918. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.71038/0.73457. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.71452/0.72871. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69765/0.71039. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.70281/0.70890. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.69281/0.73567. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68962/0.73605. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69168/0.73464. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68143/0.73346. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67512/0.72302. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68278/0.70860. Took 0.48 sec\n",
      "Epoch 11, Loss(train/val) 0.67046/0.73840. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65837/0.73686. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66542/0.73412. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65740/0.74543. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65928/0.75390. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66108/0.74418. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66368/0.76306. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65578/0.73765. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65221/0.77949. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.64735/0.75979. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64678/0.79568. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64212/0.79888. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64255/0.80118. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.64929/0.80697. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64175/0.83098. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63224/0.84208. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63184/0.84345. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.63221/0.83388. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61504/0.85300. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62594/0.89535. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61887/0.89225. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61261/0.90403. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.61959/0.90806. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.61349/0.89172. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61199/0.89633. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61212/0.90441. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60363/0.89675. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60285/0.90196. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.60120/0.89549. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.59660/0.91518. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.59796/0.92425. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59671/0.94717. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59344/0.92562. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.58665/0.93781. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.58852/0.93637. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.58443/0.98040. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56758/0.95588. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57115/0.96313. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56284/0.99724. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56759/1.06585. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54751/1.05894. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56701/1.04604. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55634/1.05530. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.55051/1.11588. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.55179/1.13485. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.54791/1.10045. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53810/1.11949. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54419/1.08472. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54090/1.08761. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.52416/1.06848. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.51640/1.13733. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.53245/1.11646. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.53281/1.16695. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.51658/1.17675. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.52630/1.16953. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.50415/1.14990. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.50876/1.17644. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.50609/1.16387. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.49441/1.22996. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.49919/1.18914. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.48181/1.19094. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.49212/1.20668. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48236/1.22060. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.47453/1.26434. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.46714/1.33938. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.48262/1.25897. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.45707/1.35302. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.46821/1.37491. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46390/1.29660. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.46885/1.40701. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.46689/1.31983. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.45348/1.34681. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.43004/1.35404. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.45698/1.32794. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.45774/1.42393. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.45064/1.34373. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.45193/1.35738. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.45231/1.32149. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.43652/1.38579. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.41707/1.31043. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.46114/1.35720. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.42632/1.35118. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.42673/1.40859. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.39895/1.44213. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.42443/1.37573. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.41735/1.37275. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.40828/1.37388. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.41787/1.42908. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.39271/1.38262. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69740/0.69915. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69175/0.69969. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68876/0.69676. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.68587/0.69470. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.68397/0.69935. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68378/0.70041. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68220/0.70550. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67719/0.70763. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67564/0.70755. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67106/0.70639. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.66886/0.70958. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66688/0.71403. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66506/0.72687. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66521/0.72572. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66430/0.72145. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.65759/0.72991. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65333/0.72781. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64934/0.73813. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.64460/0.76567. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64331/0.76451. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63509/0.77712. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.62542/0.79008. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.62139/0.80464. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62078/0.81137. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62354/0.81251. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.61709/0.81345. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.60950/0.81663. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.60208/0.83075. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60561/0.84015. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.60378/0.83971. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.60197/0.84439. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.59107/0.84372. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.60317/0.83819. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58748/0.84846. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.58977/0.84595. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.58446/0.88557. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.57639/0.87275. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.56851/0.87156. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.56118/0.85313. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.56055/0.85289. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.54756/0.91160. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.55115/0.89022. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.53807/0.94418. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.54938/0.94908. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.53604/0.95838. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.53278/0.99624. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.51870/1.02261. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.51236/1.00612. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.51502/1.04331. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.50038/1.04263. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.50451/1.05574. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.49458/1.06747. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.49976/1.06512. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.47974/1.11019. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.48247/1.16036. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.48901/1.17289. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.46973/1.13289. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.48201/1.14771. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.47456/1.10898. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.47057/1.10345. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.45257/1.12411. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.44525/1.19693. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.45097/1.21446. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.43539/1.19988. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.43188/1.26159. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.44327/1.23005. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.42182/1.23274. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.44547/1.25348. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.40494/1.32017. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.41256/1.24075. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.40281/1.30146. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.40485/1.27428. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.42630/1.28844. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.40228/1.35251. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.39840/1.27372. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.39501/1.33524. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.38860/1.33122. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.38433/1.43565. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.40269/1.35123. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.37134/1.32917. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.37724/1.33951. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.36322/1.38854. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.36434/1.41017. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.39185/1.35569. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.35732/1.46345. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.36463/1.44135. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.35232/1.53089. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.34752/1.55754. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.32663/1.49500. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.33008/1.51215. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.32974/1.60172. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.32468/1.53316. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.29499/1.59958. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.31333/1.61757. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.30738/1.68570. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.30900/1.51191. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.34792/1.57800. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.32780/1.59024. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.30431/1.69597. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.31737/1.52923. Took 0.44 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.70078/0.67724. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69266/0.67384. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69054/0.67542. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68666/0.67478. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68272/0.67619. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.67996/0.67835. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67784/0.68276. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68312/0.68449. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67378/0.67893. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.66885/0.67166. Took 0.48 sec\n",
      "Epoch 10, Loss(train/val) 0.67078/0.68581. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66391/0.69947. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67330/0.69282. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66139/0.69712. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66815/0.69194. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66200/0.68937. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.65855/0.68663. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65877/0.67764. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66516/0.68251. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65263/0.69167. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65059/0.69688. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64845/0.70701. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64672/0.69866. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64512/0.73352. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.64202/0.73629. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63351/0.71762. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63843/0.73429. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63578/0.74882. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62978/0.76899. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62347/0.77374. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62932/0.76596. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.61711/0.78028. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.60895/0.80905. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60763/0.81043. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60375/0.80208. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60707/0.81197. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.59891/0.81342. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60544/0.82061. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60157/0.77069. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59731/0.82709. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58995/0.83160. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.59335/0.82918. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.58400/0.84290. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59035/0.85837. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.57951/0.84813. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57372/0.85314. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.56169/0.90631. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57781/0.88207. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57853/0.90185. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.57694/0.85620. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56316/0.86937. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54918/0.87174. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.55453/0.83401. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55009/0.88351. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.55893/0.91228. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.53299/0.92462. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.55263/0.87891. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54687/0.88368. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.55050/0.88710. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53762/0.86358. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.53659/0.86392. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.53114/0.90036. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.52178/0.87896. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.52452/0.86872. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.52946/0.83979. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.51091/0.87654. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.51541/0.82285. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.51394/0.85589. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.51073/0.92978. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.48965/0.89395. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.50082/0.86605. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.50648/0.81363. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48997/0.86748. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.47921/0.88674. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.48697/0.90903. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49404/0.88076. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.49691/0.85936. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.48244/0.85923. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.46072/0.90996. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46925/0.80081. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.46666/0.80648. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.48011/0.80721. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.47155/0.90439. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.44937/0.84028. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.45018/0.87460. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.43514/0.86264. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.45245/0.91118. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.44232/0.79985. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.45776/0.82872. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.44210/0.84205. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.43081/0.85862. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.42439/0.90683. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.45006/0.82975. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.43322/0.89636. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.41221/0.90730. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.40190/0.89284. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.40416/0.88142. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.41455/0.87413. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.42423/0.88678. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.39881/0.86746. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.71100/0.70881. Took 0.62 sec\n",
      "Epoch 1, Loss(train/val) 0.70915/0.71267. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.71224/0.71596. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70417/0.71957. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.70436/0.70809. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.70024/0.70628. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.69068/0.69908. Took 0.48 sec\n",
      "Epoch 7, Loss(train/val) 0.70244/0.69224. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.69668/0.69528. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68911/0.69252. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.69465/0.68325. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.69411/0.69691. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68745/0.70343. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.68408/0.70856. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.68798/0.71242. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68084/0.69618. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.68592/0.71164. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.68077/0.71655. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.69035/0.71862. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.67592/0.72631. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.68431/0.71582. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66592/0.73109. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.67415/0.72622. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66914/0.72061. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.67342/0.72362. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.67193/0.72825. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.67201/0.69693. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.67055/0.69492. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.66582/0.69329. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.66174/0.70387. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.65551/0.70712. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64681/0.70081. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.65604/0.69260. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.65030/0.72175. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.64184/0.73929. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.64904/0.72330. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.63760/0.74129. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.63778/0.72002. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.63055/0.74602. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.63083/0.74444. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.63029/0.76468. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.62621/0.75145. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.62639/0.78741. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61707/0.81401. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.61135/0.78812. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.60255/0.83264. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.61622/0.80424. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.60629/0.82025. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59892/0.83626. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.60522/0.85483. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.59195/0.81757. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.59919/0.80768. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.59866/0.81822. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.59086/0.83232. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.58498/0.81421. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.57542/0.77382. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.57974/0.78792. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.57480/0.81589. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.58189/0.84324. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.56480/0.83188. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.56505/0.83465. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.56491/0.80721. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.55497/0.83155. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.54685/0.83653. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.55757/0.85347. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.53445/0.95838. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.54614/0.89480. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.55044/0.89863. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.51968/0.88020. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.52798/0.90540. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.53651/0.92340. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.51109/0.98413. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.51033/0.95702. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.51223/0.93455. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.49813/0.95031. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.51849/1.02341. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.49764/0.97216. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.48290/0.98865. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.50541/1.00178. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.51489/0.97839. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.49851/1.04030. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.48538/1.05023. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.46148/1.02450. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.47505/1.00965. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.48745/1.08653. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.46970/1.06695. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.46397/1.11806. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.46694/1.12620. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.45489/1.24444. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.43406/1.23977. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.43804/1.25663. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.43166/1.25127. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.44478/1.28209. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.43883/1.19297. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.44779/1.11571. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.44971/1.15819. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.44118/1.19489. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.41784/1.18579. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.40894/1.20856. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.40058/1.30155. Took 0.43 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70046/0.70390. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69774/0.70277. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.69415/0.69569. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69248/0.69496. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69281/0.69717. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68773/0.70949. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68847/0.71892. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68766/0.72010. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68287/0.71257. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68184/0.71791. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67980/0.72021. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67894/0.71751. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67768/0.72133. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67248/0.71262. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66948/0.70247. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66776/0.71870. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66457/0.73559. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65701/0.72026. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65791/0.71692. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65381/0.71548. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66219/0.71416. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66109/0.69549. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65690/0.70093. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65497/0.68641. Took 0.48 sec\n",
      "Epoch 24, Loss(train/val) 0.64421/0.70572. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64895/0.73078. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64274/0.73873. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63467/0.76055. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62669/0.75630. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63480/0.75000. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.62114/0.76630. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61192/0.77018. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61165/0.79520. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.60595/0.79489. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.59653/0.83232. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60479/0.81648. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59411/0.82009. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59163/0.82083. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57499/0.82578. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.57009/0.84431. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.57030/0.81958. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.56415/0.87647. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57247/0.81448. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.58074/0.83250. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56233/0.86930. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.55109/0.86012. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.54275/0.85780. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.54142/0.87648. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.52300/0.87723. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52353/0.92002. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.52367/0.89491. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.52292/0.91495. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.51627/0.89694. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52167/0.89627. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.50477/0.88335. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.51154/0.93032. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.51110/0.88810. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.48484/0.95045. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.49062/0.89501. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.48695/0.89893. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49309/0.90896. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.48267/0.92954. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.46566/0.96200. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.46593/0.91097. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.46336/0.90011. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.45000/0.89335. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.43500/0.94167. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.45946/0.98775. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.42703/1.06581. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.43307/0.99506. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.43954/1.02993. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.42534/1.02648. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.42317/0.95209. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.41538/1.01388. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.45957/0.94062. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44103/0.88433. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.41015/1.01713. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.39788/0.97764. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.40876/1.03665. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.38769/1.03886. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.37678/1.06799. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.39293/1.05141. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.36303/1.10276. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.36326/1.15111. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.36733/1.10979. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.35696/1.19050. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.37964/1.22631. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.37992/1.17632. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.34296/1.17086. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.35965/1.18295. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.33600/1.19101. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.32963/1.11234. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.32890/1.22840. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.30660/1.10031. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.32963/1.18144. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.32558/1.20127. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.31880/1.22887. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.30657/1.17873. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.30581/1.14448. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.29925/1.31517. Took 0.45 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.70044/0.68410. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69551/0.69132. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69546/0.69077. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68897/0.69949. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68636/0.70769. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68956/0.70194. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68091/0.71290. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68363/0.71047. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68326/0.70251. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68595/0.70227. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67933/0.69593. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68028/0.70185. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67459/0.70539. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67271/0.70541. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67459/0.69651. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67720/0.70607. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67456/0.70411. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.67740/0.69322. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66678/0.69277. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66691/0.69286. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.67451/0.71700. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66269/0.70087. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66588/0.71959. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66011/0.73522. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65565/0.75699. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65599/0.73367. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64806/0.72986. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65160/0.74158. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64238/0.74321. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.64094/0.76472. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63133/0.77336. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64494/0.77066. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63753/0.78281. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63579/0.79917. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62267/0.81364. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61905/0.82081. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.61773/0.83589. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.63896/0.77004. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61130/0.85246. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.62743/0.78895. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61906/0.78980. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60956/0.80455. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60050/0.85334. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60154/0.88808. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59688/0.86532. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59015/0.89786. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59050/0.87754. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.58556/0.91762. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.57971/0.91235. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57195/0.93689. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56758/0.95827. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55169/0.96593. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56600/0.96529. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54620/0.97480. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.55180/0.98558. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.54086/0.98913. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56020/0.96310. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.53953/0.93618. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.53238/0.99481. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.53179/0.96523. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.53152/1.00206. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.52881/0.96468. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52097/0.97707. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.52188/1.01389. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.51336/1.00237. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.52451/1.01401. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.50251/0.99533. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48791/0.99659. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49434/1.04721. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.49662/1.00967. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48353/1.02434. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48894/1.06221. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48627/1.05015. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48234/1.06655. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.46458/1.08286. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.46267/1.08305. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.45504/1.10247. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.47361/1.06996. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44540/1.07204. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.45218/1.07801. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.48187/1.15190. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.45303/1.20664. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.46564/1.07284. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.42448/1.12032. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.42005/1.17277. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.39383/1.14664. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.41703/1.10488. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.42838/1.13570. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.40797/1.13265. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.42753/1.15367. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.43850/1.16938. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.39091/1.22069. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.39084/1.21012. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.38086/1.20782. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.37903/1.27308. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.40967/1.11198. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.37640/1.18790. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.38131/1.15343. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.37760/1.24798. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.35799/1.23359. Took 0.44 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.71907/0.74087. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.71723/0.72406. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.71120/0.75371. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70710/0.73383. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.70416/0.74072. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.70030/0.73756. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69288/0.75059. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69498/0.76234. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68486/0.75979. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68480/0.77416. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68564/0.77940. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.69437/0.77653. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68588/0.76625. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67817/0.77579. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67966/0.79007. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67404/0.78859. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.68436/0.77732. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67059/0.78698. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67038/0.79996. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67855/0.80532. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.68144/0.80973. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.67991/0.80383. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.67330/0.79741. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.66764/0.81562. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66899/0.83746. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.67041/0.83948. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.66590/0.83871. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.66812/0.85431. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.67368/0.83321. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.66993/0.85267. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.65067/0.86589. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.66286/0.85647. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.66104/0.85231. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.65258/0.87839. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.64851/0.86402. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.65016/0.87289. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.64995/0.86477. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.64115/0.89575. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.63940/0.88850. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.63629/0.89020. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.64010/0.89368. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.63015/0.93053. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.63315/0.93527. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.62495/0.98253. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.62811/0.95608. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.62770/0.94300. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.61629/0.93143. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.61218/0.95042. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.60848/0.96482. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.61160/0.97329. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.60193/0.95184. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.60213/0.93016. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.60122/0.95891. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.58841/0.97303. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.58243/0.98052. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.58127/1.01295. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.58782/0.96675. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.57087/0.94989. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.59051/0.95421. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.56337/0.99897. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.56132/1.00519. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.57010/0.96684. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.57592/0.89323. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.57091/0.96714. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.55742/0.96149. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.55701/1.03496. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.54478/0.97140. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.53250/1.00258. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.54821/1.03673. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.54267/1.00375. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.53656/1.00121. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.53807/0.98651. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.54296/0.98725. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.53998/1.01585. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.52585/1.00927. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.54986/0.94745. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.52620/0.97215. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.51682/1.06849. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.52356/0.96839. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.52283/0.95324. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.51723/1.01274. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.53419/1.12739. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.56233/0.95955. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.54187/0.99487. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.52252/1.00590. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.48790/1.05508. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.49434/1.07414. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.47007/1.10088. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.48792/1.00397. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.49672/0.95812. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.50119/1.03861. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.49443/1.04710. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.47634/1.05060. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.48511/1.02593. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.47542/0.96678. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.47418/0.97483. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.47079/1.03869. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.47002/0.94000. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.48971/0.96668. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.48005/0.93116. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.72650/0.73141. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.72567/0.70309. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.71910/0.70716. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.72111/0.69581. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.70857/0.70108. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.71034/0.70049. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.71304/0.69951. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.70760/0.69844. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.70668/0.70778. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.69633/0.69912. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.70353/0.69423. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.70074/0.71079. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.69958/0.69562. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.69158/0.70470. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.69100/0.70072. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.69087/0.70220. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.68831/0.71346. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.68909/0.72344. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.68915/0.73015. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.68530/0.72440. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.68100/0.74194. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.67271/0.73078. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.68528/0.73154. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.67966/0.73843. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.67656/0.74114. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66960/0.73850. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.67280/0.73634. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.66258/0.75670. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.66510/0.75942. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.65656/0.77062. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.66720/0.77988. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.65216/0.79045. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.65851/0.76943. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.64539/0.80910. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.65318/0.81404. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.64362/0.81861. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.65389/0.80865. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.64642/0.84079. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.65163/0.82061. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.63946/0.81636. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.64347/0.84816. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.64861/0.77716. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.62825/0.79300. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.64620/0.78716. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.62347/0.82355. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.63018/0.82102. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.61704/0.81759. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.60890/0.82807. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.61449/0.85073. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.63243/0.81337. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.62797/0.78969. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.61562/0.80685. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.60837/0.83667. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.59868/0.84661. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.59633/0.82212. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.58513/0.90381. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.58571/0.88019. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.60370/0.87099. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.60410/0.84838. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.59626/0.89239. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.58725/0.89128. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.56539/0.90816. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.57155/0.95694. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.56302/0.94543. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.55480/0.94543. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.56872/0.95901. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.56601/0.91337. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.56979/0.98714. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.55595/0.95910. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.57004/0.97683. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.53783/0.99686. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.54990/0.99953. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.54712/0.96598. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.53286/1.01494. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.52837/1.02198. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.53538/1.05911. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.53033/1.04960. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.52063/1.02915. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.54535/1.00348. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.52466/1.02381. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.51636/1.00480. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.50584/0.99899. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.51613/1.00941. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.51218/1.03234. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.50548/1.05208. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.51851/1.01268. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.53200/0.97883. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.54072/0.96828. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.50776/1.00714. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.49011/1.03465. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.48638/1.10104. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.49301/1.06853. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.48565/1.12390. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.48287/1.08138. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.45953/1.11732. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.48743/1.00581. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.46682/1.10454. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.48115/1.06199. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.44361/1.17171. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.45417/1.07304. Took 0.46 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.70597/0.68290. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69965/0.69906. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69639/0.69908. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69339/0.70025. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69545/0.68081. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.69280/0.69221. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68502/0.69432. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68961/0.69677. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68210/0.69638. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68154/0.69731. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68014/0.69289. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67659/0.68431. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67634/0.69048. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67252/0.71971. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67037/0.72716. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66334/0.71810. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66915/0.72512. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65964/0.73609. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66892/0.75382. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65994/0.77962. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64996/0.80571. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65413/0.82784. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65029/0.81250. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64392/0.85267. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.63684/0.87316. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64292/0.88509. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62732/0.93165. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63753/0.92666. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63463/0.93064. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62353/0.96278. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.61484/0.98208. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61415/0.98963. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61760/0.99892. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.61715/1.01076. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.61183/1.00104. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.60286/1.00810. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.59647/1.02844. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.59376/1.04783. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.58473/1.07464. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57774/1.04337. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57432/1.05236. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.57053/1.09625. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56496/1.12771. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.56686/1.07230. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.56549/1.08281. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.55774/1.07100. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.54932/1.07488. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.54622/1.07082. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.55288/1.09210. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.53263/1.09701. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55112/1.07258. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53931/1.14256. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.52647/1.07298. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.54098/1.06584. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.53387/1.07395. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.52690/1.12559. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.52273/1.09769. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.51568/1.16657. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50734/1.14585. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50520/1.17104. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.50456/1.13639. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50181/1.14962. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49721/1.19711. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.47838/1.27226. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.50212/1.20076. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.48374/1.22200. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.46910/1.24904. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.47095/1.23928. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.47237/1.21656. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.47565/1.22327. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.45310/1.25133. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45626/1.25351. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.44927/1.31748. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.45062/1.33212. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.43390/1.31338. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44068/1.41880. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.44215/1.41992. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.44088/1.29721. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44075/1.34137. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.41364/1.41307. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.42402/1.37369. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.40697/1.46789. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.39632/1.40270. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.41589/1.39951. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.43263/1.40858. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.42045/1.47690. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.41412/1.49090. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.39582/1.48431. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.39558/1.48431. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.39197/1.40028. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.42681/1.34813. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.39232/1.47917. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.37162/1.44999. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.36258/1.45914. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.37023/1.57115. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.37239/1.65731. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.37022/1.43921. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.35252/1.60356. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.36791/1.57780. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36364/1.70317. Took 0.43 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.70065/0.72404. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69721/0.73845. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.68734/0.75843. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.67847/0.75422. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68709/0.74634. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67914/0.76852. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.67641/0.75633. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68313/0.75066. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67861/0.73965. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67872/0.74174. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67730/0.75650. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66821/0.74465. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66666/0.72799. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66173/0.72608. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67419/0.71625. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.65985/0.71966. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65972/0.72929. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65565/0.73795. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66527/0.75844. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66233/0.76247. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66547/0.77698. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66006/0.80315. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64681/0.79254. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64667/0.78913. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64764/0.77542. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63974/0.81631. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63681/0.80479. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64554/0.80198. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64045/0.80603. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.63320/0.81319. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.62777/0.83420. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.62266/0.85020. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62447/0.84441. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62419/0.84754. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.61441/0.85708. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61478/0.85515. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.61633/0.86328. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61123/0.87267. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61425/0.86050. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60843/0.86933. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59571/0.89659. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58930/0.90431. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.57822/0.88864. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.59202/0.92060. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59733/0.86518. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.58902/0.88839. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.58868/0.90823. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.58400/0.89523. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.57694/0.92472. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56421/0.90009. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56260/0.94754. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56111/0.96130. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.58080/0.88672. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55927/0.92933. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.55218/0.90761. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.55944/0.91977. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.55120/0.96043. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53692/0.95991. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.55667/0.94601. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54519/0.94236. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54823/0.93101. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.55467/0.93047. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53955/0.96851. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.52292/0.99421. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.53979/0.96098. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.52756/0.98935. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.54055/0.98882. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.52493/0.99524. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52996/1.00778. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.52364/1.04236. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.50097/1.05513. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.50517/1.04551. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.51189/1.07670. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.49418/1.04191. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.49733/1.13995. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48859/1.14309. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.51125/1.08981. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.48648/1.11753. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.49409/1.09931. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46469/1.15004. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.48176/1.11471. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.49941/1.13206. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.47221/1.12821. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.47437/1.19799. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.46880/1.18255. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.46996/1.20717. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.46046/1.19564. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.45896/1.17978. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.43402/1.26911. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.44502/1.24814. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.45975/1.20122. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.44256/1.24020. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.44976/1.20598. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.41646/1.24959. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.41743/1.25229. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.42498/1.28507. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.42633/1.29532. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.40571/1.33458. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.41075/1.30429. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.42788/1.35793. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69858/0.70941. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69513/0.70221. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68768/0.72118. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68755/0.71865. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68723/0.71715. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68955/0.71835. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68742/0.71996. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68184/0.73312. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68507/0.72002. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67711/0.72650. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67714/0.74077. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67900/0.72870. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67611/0.73824. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67210/0.74006. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68011/0.74360. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67466/0.75211. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66803/0.75011. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66636/0.75186. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66336/0.75435. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66009/0.76097. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66292/0.76048. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65225/0.78412. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65605/0.77736. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64822/0.79389. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65915/0.76784. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65014/0.77208. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65667/0.77276. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63985/0.77566. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64102/0.78357. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.63770/0.79879. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.63110/0.81722. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64381/0.81150. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63751/0.82996. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.63094/0.82509. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.62367/0.83006. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.61556/0.85830. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62489/0.86325. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62395/0.89272. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62369/0.78121. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61320/0.82088. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.60478/0.83800. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60902/0.87367. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60126/0.91636. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60057/0.84479. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.58168/0.93409. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59951/0.88979. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.57594/0.97665. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58549/0.86327. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.57924/0.91966. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.57999/0.93355. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56109/0.95343. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.57747/0.92486. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56083/0.96824. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56027/0.94426. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.53968/0.97769. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.56156/0.95457. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.56184/0.93889. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.55972/0.95851. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54692/1.05921. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.56683/0.93013. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.55782/0.94097. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.55847/0.94256. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54527/0.98381. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.54101/0.97354. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.52749/0.94946. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.51763/1.06004. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.55121/0.92290. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.54235/0.94838. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52605/0.98809. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.52959/0.99661. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.52458/1.02849. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.52613/0.92218. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.49880/0.98935. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.50191/0.95396. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.50202/0.98599. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.51265/0.91950. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.49189/0.99107. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.48400/0.94456. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.49564/1.00718. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.50103/1.00254. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.49205/0.99786. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.47678/1.01221. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.47615/0.95365. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.48081/1.00710. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.46186/1.00939. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.47277/1.02749. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.45953/1.03444. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.45585/1.08766. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.48797/0.98789. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.46738/1.11500. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.44514/1.16983. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.42952/1.16509. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.41908/1.17436. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.43500/1.20795. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.42958/1.23992. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.45325/1.10999. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.41664/1.26976. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.40031/1.24221. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.40285/1.35060. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.44978/1.14495. Took 0.43 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.70269/0.69539. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69213/0.70157. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68294/0.70776. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68266/0.72254. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.67436/0.72673. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.67667/0.72379. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67443/0.73241. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67126/0.73421. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.66881/0.73681. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66651/0.73489. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66602/0.74669. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66670/0.75284. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66283/0.78221. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66238/0.79431. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65591/0.78341. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.65184/0.79649. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65097/0.79741. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.64847/0.79953. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64258/0.80667. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.63614/0.81182. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.62415/0.83602. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.62773/0.83274. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.62346/0.84397. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62415/0.83227. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62120/0.83654. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.60892/0.83918. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.60900/0.82326. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.60839/0.85411. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.60757/0.82620. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60332/0.83362. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.59667/0.80600. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.60528/0.81246. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59340/0.82733. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58807/0.83721. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.58468/0.81841. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.58935/0.79721. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.58280/0.82328. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.57597/0.84481. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.57802/0.83486. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58432/0.83297. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58042/0.83090. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.56832/0.84651. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56342/0.84973. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56740/0.80825. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55768/0.82274. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55744/0.78568. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.54463/0.82136. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.53093/0.85305. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.54896/0.85621. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.55180/0.86195. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.52613/0.81957. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.53714/0.83913. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.51901/0.86033. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.53549/0.85855. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53379/0.81702. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.51150/0.76751. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.52326/0.79814. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.53143/0.80623. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.50540/0.76104. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.51755/0.76455. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.51521/0.76160. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50553/0.79139. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.49886/0.75252. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.50538/0.75770. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.49523/0.73503. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.47656/0.78510. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48531/0.79629. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.49118/0.81733. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.48174/0.81585. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.48916/0.78513. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.47315/0.78382. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.46877/0.78565. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.43372/0.77905. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.45270/0.79305. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.46534/0.85834. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44498/0.79406. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.44382/0.83958. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.45648/0.81250. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.44172/0.80590. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.42423/0.77244. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.42691/0.76233. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.41875/0.78522. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.39924/0.79160. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.40129/0.80245. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.41095/0.84430. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.37930/0.86378. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.40983/0.82335. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.44066/0.82622. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.40866/0.90386. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.39145/0.86026. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.38938/0.89967. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.37806/0.88505. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.38423/0.87358. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.36421/0.93628. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.35239/0.87216. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.35492/0.95075. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.34322/0.94995. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.36384/0.96124. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.35743/0.92481. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.32598/0.93774. Took 0.44 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.70328/0.71264. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.68773/0.71687. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.68210/0.75280. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.67979/0.75816. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68164/0.76362. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67702/0.75689. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.67179/0.77509. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67487/0.78706. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67737/0.77999. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66669/0.78812. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67116/0.81146. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66330/0.80671. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.65981/0.80458. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65886/0.81774. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65169/0.81483. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65637/0.81596. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.64920/0.83045. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.64121/0.83792. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64625/0.84867. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.63210/0.83884. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64004/0.82304. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63971/0.81255. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63779/0.80022. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63688/0.80590. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.62884/0.82077. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62715/0.81158. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61555/0.81696. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.62072/0.82545. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62029/0.84734. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.60980/0.82993. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61393/0.83555. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60552/0.87340. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60790/0.85775. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.60008/0.85599. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.59159/0.85787. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58628/0.87947. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.58323/0.87025. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.59009/0.88148. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57771/0.86790. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57951/0.85875. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.57334/0.85482. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58147/0.85043. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56128/0.87260. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56548/0.87179. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.55350/0.88753. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.54829/0.92427. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55013/0.90121. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.54330/0.93005. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.53649/0.93147. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.53103/0.95444. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.54984/0.99815. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.52134/0.96169. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52049/0.99198. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.50785/1.00033. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.50589/1.00675. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.50455/1.05428. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.49916/1.07765. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.50203/1.09761. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50245/1.08489. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.49346/1.07671. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49464/1.08574. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.47622/1.09266. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.49611/1.07208. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48703/1.08779. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49471/1.08576. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50952/1.05803. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.48836/1.08426. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.48888/1.07320. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.48361/1.04742. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47860/1.06170. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.46516/1.04470. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.44617/1.06643. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48607/1.01396. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46860/1.06144. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.44083/1.08189. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.44453/1.06933. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.44102/1.09465. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.44578/1.08855. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43871/1.14917. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.42504/1.15397. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.41408/1.16347. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.44022/1.10091. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.42339/1.15560. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.44671/1.17251. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.40184/1.17057. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.43209/1.06574. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.42017/1.19966. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.40971/1.17566. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.39536/1.18387. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.39600/1.20191. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.38950/1.17167. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.41741/1.16237. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.39330/1.20507. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.38399/1.28731. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.38008/1.21800. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.35471/1.35378. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.37370/1.33183. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.36282/1.37717. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.38911/1.23697. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36544/1.28133. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.71726/0.71547. Took 0.58 sec\n",
      "Epoch 1, Loss(train/val) 0.69802/0.71677. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.70314/0.71131. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69562/0.73155. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69678/0.73656. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68995/0.74989. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.69095/0.76726. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68276/0.77072. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68988/0.78634. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68699/0.77221. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67941/0.77496. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68006/0.75901. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67462/0.75166. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68033/0.76343. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67717/0.78949. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67167/0.77232. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67904/0.77878. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67325/0.79286. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67941/0.79544. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66344/0.82380. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65488/0.81291. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.65654/0.82718. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66665/0.84467. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65765/0.86009. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65927/0.85073. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65651/0.84286. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64733/0.84993. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64403/0.85263. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64245/0.86474. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65052/0.84793. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64167/0.85246. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64341/0.86619. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63806/0.85391. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.63205/0.86470. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.63682/0.88511. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.63537/0.85592. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.62884/0.87675. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.63060/0.85131. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.62847/0.86554. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.62623/0.85676. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.62976/0.87553. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.61164/0.86632. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.61631/0.85081. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60579/0.86859. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.59688/0.87710. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59896/0.86763. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.60929/0.85199. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.60421/0.87084. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.60555/0.86800. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.59451/0.90555. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.60185/0.89117. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.58417/0.91629. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.58990/0.92472. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.57350/0.97841. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56917/0.91595. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.57240/0.90224. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56822/0.95252. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.56874/0.97247. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.56561/0.93080. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54818/1.01284. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54649/0.99574. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.54202/0.96423. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.55093/1.00602. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.53171/0.99515. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.52943/0.99556. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.53568/0.98680. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.52916/1.02887. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.51440/1.07060. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52309/1.00508. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.50356/1.02226. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.51561/1.04862. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.53542/1.06742. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.50430/1.06243. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.49883/1.04114. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.50082/1.05080. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.49971/0.98709. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.50855/1.00478. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.51042/1.03682. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.47843/1.06213. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.48962/1.03052. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.47772/1.03212. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.49009/1.07608. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.46296/1.05656. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.49369/1.03724. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.47552/1.05911. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.47406/1.05727. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.47082/1.07220. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.47263/1.05283. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.47556/1.08345. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.45098/1.11130. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.45285/1.12346. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.44486/1.12526. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.42880/1.17595. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.43954/1.24104. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.44576/1.14820. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.41486/1.25332. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.43291/1.29119. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.40784/1.29944. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.41570/1.27868. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.42787/1.29881. Took 0.43 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69866/0.69699. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69258/0.70061. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69169/0.70796. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69018/0.70670. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68264/0.70826. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68506/0.72163. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67791/0.72756. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67992/0.71767. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67745/0.71390. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67507/0.72383. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67204/0.72243. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66871/0.73396. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67271/0.73962. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66545/0.75314. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66511/0.76927. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66765/0.77251. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66608/0.76921. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65798/0.75457. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65896/0.76451. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64591/0.76751. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64685/0.76459. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63598/0.77426. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64772/0.75592. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63788/0.76542. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64367/0.74263. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.62772/0.76916. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63201/0.74163. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.61670/0.76002. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62187/0.75327. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.61311/0.77596. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62020/0.79058. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60211/0.77428. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60482/0.75531. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61126/0.76418. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.59809/0.76361. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59700/0.76447. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.58804/0.77743. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.59040/0.76429. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58806/0.77977. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58661/0.78694. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58350/0.76811. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.57674/0.76073. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56695/0.77375. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57412/0.79184. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56965/0.79430. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.56037/0.80293. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.54733/0.79581. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.55918/0.82974. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.53537/0.81201. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56104/0.82139. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53525/0.83399. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.53933/0.86102. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54511/0.83652. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52900/0.84626. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53631/0.84830. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.52166/0.86420. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.54537/0.82652. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51559/0.82197. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.52689/0.86489. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50728/0.83637. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.49828/0.83756. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50865/0.85909. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49214/0.81354. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48395/0.80059. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.49070/0.78350. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.47452/0.79595. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.49723/0.80403. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.47037/0.80834. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.47941/0.86494. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.47121/0.79811. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.44450/0.79447. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45326/0.87985. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.45772/0.95874. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.44102/0.86727. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.45269/0.83023. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.42925/0.84853. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.45249/0.94129. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.42350/0.88152. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.41134/0.92925. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.41046/0.98428. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.42680/0.87823. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.40492/0.91979. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.41017/0.90956. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.49508/0.84497. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43272/0.88830. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.41399/0.85526. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.39699/0.87186. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.40141/0.93682. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.40221/0.95890. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.38889/0.93670. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.39897/0.93921. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.38880/0.91113. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.39011/1.04421. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.36355/0.96782. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.45069/0.95939. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.42676/0.93674. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.38297/0.95291. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.36191/0.97104. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.34578/0.97712. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36086/1.04914. Took 0.44 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69468/0.70804. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69068/0.70528. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.68964/0.69887. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69344/0.70546. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69062/0.70423. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68697/0.70812. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68690/0.70790. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68353/0.70941. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68191/0.71526. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68088/0.71828. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68140/0.72517. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67403/0.75005. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67529/0.75176. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66958/0.74739. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66464/0.75212. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66344/0.76252. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66307/0.76945. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65719/0.78260. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65979/0.79790. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65283/0.79118. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64585/0.78525. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64970/0.77559. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64118/0.79631. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63850/0.83083. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63915/0.81622. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63451/0.81344. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.62496/0.83613. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62750/0.84296. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61793/0.82095. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62394/0.83540. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62037/0.82916. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.60694/0.86171. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61283/0.87065. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60630/0.89428. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60536/0.92633. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59309/0.90936. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.58568/0.90393. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57615/0.95030. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57871/0.95623. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57287/0.97206. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.57406/0.93159. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57176/0.96745. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.56050/0.99156. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.54523/1.02365. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.54971/0.99739. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.53773/1.02222. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.52320/1.02563. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.52865/1.05723. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.51428/1.07974. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.51186/1.08993. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.51451/1.03938. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.50375/1.05123. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.50082/1.05486. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.50729/1.04614. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.49423/1.10348. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.49475/1.05889. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.46538/1.07595. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.47877/1.10334. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.48353/1.05198. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.47121/1.07840. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.47981/1.13404. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.46138/1.09227. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.45665/1.07227. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.45596/1.07135. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.45484/1.06157. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.44704/1.16996. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.43003/1.08362. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.42442/1.09839. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.43079/1.10545. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.41903/1.06595. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.40922/1.09599. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.41181/1.10911. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.42010/1.11014. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.42778/1.07052. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.40851/1.12023. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.39329/1.14371. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.38219/1.18274. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.37269/1.16395. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.39079/1.23191. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.36580/1.31197. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.36851/1.18510. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.37604/1.18380. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.38348/1.22386. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.34504/1.25566. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.37282/1.17565. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.37077/1.25222. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.35440/1.29991. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.38353/1.28731. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.32491/1.36035. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.33168/1.36603. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.35761/1.32846. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.33168/1.39826. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.36120/1.31200. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.31897/1.30288. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.29767/1.34327. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.30920/1.38975. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.33102/1.30889. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.28652/1.37184. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.29357/1.35935. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.30776/1.40659. Took 0.43 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.70818/0.69203. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70651/0.70833. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.70084/0.71008. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69503/0.70248. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69638/0.70370. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69974/0.70841. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69027/0.71197. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69015/0.71162. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69202/0.70633. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68671/0.70574. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68726/0.71039. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68300/0.71117. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68747/0.71018. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68357/0.71821. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67513/0.72057. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.68086/0.73191. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.67556/0.73167. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67404/0.73766. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.67116/0.73607. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66946/0.74975. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.67613/0.74947. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66591/0.76456. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.66597/0.77202. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66475/0.77783. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66103/0.78110. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.66452/0.78225. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65296/0.78616. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.65658/0.79078. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65016/0.79387. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.64957/0.79101. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.65196/0.79628. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64561/0.81253. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63898/0.82754. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63811/0.82483. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.64246/0.80841. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.63073/0.81640. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.62666/0.82205. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62619/0.83991. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.61973/0.85653. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.62174/0.85056. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62161/0.84043. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61533/0.84109. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.60821/0.87141. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60841/0.86771. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.59872/0.87988. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.58729/0.88840. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59516/0.90145. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58180/0.91444. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57950/0.93934. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.58301/0.90199. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.59169/0.92955. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.56854/0.93899. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56656/0.94065. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56719/0.91949. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56768/0.92987. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.57374/0.94766. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55916/0.93200. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.55720/0.96427. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.56234/0.94372. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.55553/0.94332. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55450/0.95679. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.53942/0.94110. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.53209/0.99830. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.54600/1.00206. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.52920/0.99319. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.52778/1.03591. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.51853/1.03177. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.53847/1.03175. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.51292/0.99310. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.51760/1.04847. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.50103/1.06737. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.49748/1.05237. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.51496/1.04141. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.50977/1.03053. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.48444/1.08121. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48217/1.09609. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.50027/1.06365. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.49112/1.11453. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.47788/1.11886. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46611/1.12089. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.48366/1.04028. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.45507/1.09431. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.47055/1.16482. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.46391/1.12756. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.45741/1.14727. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.44582/1.21982. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.43599/1.23551. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.45117/1.16900. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.42594/1.12467. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.43203/1.15402. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.45169/1.20502. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.44563/1.15846. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.44590/1.15088. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.43617/1.15746. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.41939/1.15368. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.40610/1.27932. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.45368/1.23679. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.41914/1.21169. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.42358/1.23456. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.37760/1.24496. Took 0.44 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.72000/0.67428. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70360/0.67986. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69424/0.67673. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69188/0.67339. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.69527/0.68061. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68539/0.69153. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68595/0.70431. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68439/0.72263. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68541/0.73479. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68184/0.73961. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67491/0.73459. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66957/0.73389. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66867/0.72993. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66964/0.73971. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66523/0.75073. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66192/0.74638. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66281/0.73311. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65337/0.71942. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.64994/0.73578. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.64275/0.74859. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65197/0.74213. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65020/0.75148. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64156/0.77376. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.63241/0.81497. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63561/0.81469. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.62694/0.83511. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61430/0.85443. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61553/0.86385. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60963/0.85718. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.61214/0.87789. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.60297/0.87366. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.60294/0.88469. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60430/0.86610. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60076/0.86518. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.60546/0.86069. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60488/0.84118. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.57910/0.87386. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.58486/0.87099. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.58680/0.87334. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58951/0.86922. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57167/0.87007. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.57252/0.86765. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57642/0.85135. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.56999/0.83138. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55742/0.82187. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.55645/0.85293. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.54347/0.83813. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55212/0.84241. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.53248/0.83448. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.52716/0.88409. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.52935/0.87461. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.52263/0.95516. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.53562/0.91156. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.50835/0.89001. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.50929/0.89536. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.52906/0.95025. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.53185/0.88967. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.53732/0.86857. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.50810/0.95122. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.51303/0.90320. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51063/0.82501. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.48511/0.90223. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49162/0.94214. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48259/0.93098. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.48580/0.90480. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.47286/1.02728. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.47363/1.02813. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.47147/1.02928. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.46500/0.98789. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46680/0.94822. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.46548/1.01261. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.44297/1.07015. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46329/0.96260. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.43288/1.05057. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.44372/1.02070. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44194/1.03232. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.43756/1.03149. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.42733/0.98024. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.41140/1.00425. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.41375/1.08061. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.42847/1.07736. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.41816/1.00654. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.39240/1.17009. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.40070/1.08270. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.39214/1.16615. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.39782/1.18008. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.39791/1.28920. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.38197/1.16377. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.40548/1.12213. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.38172/1.12576. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.37600/1.18169. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.36890/1.13453. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.36627/1.16692. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.38263/1.21712. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.35291/1.14615. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.36398/1.17457. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.36302/1.16044. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.32554/1.16356. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.33348/1.28962. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34284/1.17157. Took 0.46 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.72146/0.70680. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.71589/0.70372. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70925/0.71418. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70042/0.71531. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69625/0.72624. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69774/0.72244. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69050/0.73119. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68991/0.73201. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68828/0.73698. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.69105/0.74196. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68241/0.74766. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67524/0.76839. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67064/0.76760. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68006/0.75908. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66619/0.78043. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66894/0.78901. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66893/0.80471. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67286/0.76938. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65972/0.77572. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66584/0.80145. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65676/0.82651. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66103/0.80703. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64946/0.80939. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.66209/0.82181. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65396/0.81686. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65387/0.81878. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64940/0.80249. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64664/0.81438. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64605/0.80427. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64552/0.82812. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.64087/0.83183. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.64310/0.81762. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64487/0.82973. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63983/0.83079. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.63888/0.83286. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.63122/0.82840. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.62654/0.85981. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.62363/0.82164. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62349/0.84521. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.62054/0.82273. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62047/0.84842. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61551/0.82726. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.61050/0.83388. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60668/0.83411. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.59677/0.84949. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.59985/0.84874. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58756/0.85806. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.60431/0.87768. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.59509/0.84206. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56938/0.88698. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57897/0.90850. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.57486/0.91042. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.57216/0.90213. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56438/0.92814. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.57394/0.91471. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55343/0.96838. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.54086/0.99953. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.54331/0.99829. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.53711/1.06081. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54455/1.00526. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.53610/1.02781. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.53853/1.05900. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52942/1.02628. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.53391/1.04479. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.53312/1.06668. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.51778/1.12082. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.53169/1.09170. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.51976/1.11706. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52103/1.09940. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.52638/1.08524. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.50803/1.10867. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.52140/1.09693. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.49836/1.15835. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.49777/1.12346. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.48673/1.17078. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48241/1.21061. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.49351/1.18673. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.48551/1.21046. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.47766/1.23652. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.48497/1.16851. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.47240/1.21164. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.50014/1.12528. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.47819/1.15897. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.46790/1.18740. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.47964/1.10016. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.45543/1.24836. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.48451/1.16217. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.46298/1.17901. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.46033/1.22800. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.45252/1.22669. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.44115/1.33038. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.46199/1.27080. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.43714/1.26085. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.43036/1.31006. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.43782/1.29824. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.44062/1.27380. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.41031/1.35789. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.41867/1.36006. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.40954/1.33313. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.43928/1.34215. Took 0.43 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.71505/0.70757. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69787/0.70731. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.71005/0.70904. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70348/0.69706. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69287/0.70283. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69552/0.71818. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69331/0.72648. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69303/0.72164. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68963/0.73562. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68975/0.73118. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.69029/0.73717. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68912/0.72818. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68648/0.73564. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67923/0.74900. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68278/0.75355. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68323/0.74474. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67718/0.74063. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67350/0.75227. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66971/0.75606. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66677/0.77777. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66416/0.77569. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66394/0.77746. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65805/0.77910. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.66028/0.78635. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65844/0.78801. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65358/0.80157. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64543/0.82140. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65338/0.82713. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63583/0.84414. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63265/0.83110. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.63248/0.87845. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.62259/0.86926. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61851/0.88725. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.62019/0.89921. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.62408/0.89886. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60706/0.90831. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61274/0.92799. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59539/0.97249. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58177/0.95929. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60366/0.94599. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.59979/0.95413. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.58950/0.96904. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58126/0.93325. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57944/0.97052. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.59108/0.97815. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57497/0.98959. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56932/0.98580. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.56037/0.98748. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.55508/1.00720. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56390/0.98476. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56265/1.02151. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56087/1.01578. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54492/1.01095. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.53552/0.99424. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.52372/1.00770. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.53370/1.00455. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52875/1.02948. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.52358/1.06542. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.52933/1.02277. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52600/1.04154. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.50759/0.99124. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.51018/1.09784. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51145/1.01603. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.49864/1.02245. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.49211/1.02070. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.51103/1.07953. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.47839/1.10322. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.46677/1.10390. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.48185/1.09662. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.48839/1.07109. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48994/1.03599. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.47727/1.11779. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.45223/1.15448. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.45270/1.17727. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.47319/1.14228. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48264/1.11890. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.47202/1.07452. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.47939/1.07250. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.46191/1.07573. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44285/1.14458. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.43488/1.19282. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.41503/1.16747. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.41457/1.23997. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.41518/1.18486. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43490/1.21020. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.42168/1.22921. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.40915/1.20335. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.40170/1.23938. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.37773/1.30976. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.39446/1.26732. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.38432/1.35385. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.41100/1.31941. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.42162/1.24237. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.40570/1.26005. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.37516/1.38471. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.37301/1.41224. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.37930/1.35096. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.38788/1.33536. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.38321/1.32424. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.34653/1.42518. Took 0.44 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.71206/0.73852. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70572/0.73048. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.70081/0.72725. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.70251/0.73893. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69579/0.74726. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69028/0.75533. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68798/0.75952. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68594/0.76651. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68039/0.76258. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68334/0.76077. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67931/0.74435. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67777/0.77159. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.68305/0.76908. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68449/0.76244. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67879/0.76538. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66799/0.77779. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67132/0.79238. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66868/0.79184. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65674/0.82464. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66040/0.82843. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65237/0.84587. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.66213/0.85967. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64579/0.89185. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64610/0.89423. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64716/0.86430. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64220/0.86857. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64249/0.86137. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63449/0.84206. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62422/0.89269. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62281/0.89754. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63026/0.92595. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60800/0.96823. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.62347/0.96906. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60528/0.95346. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.60368/1.01006. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61179/0.96938. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60198/0.97061. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59967/1.01692. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.58177/1.03117. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.58792/1.01357. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.58139/1.03002. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58280/1.07111. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.58725/1.05287. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.58239/1.04946. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.57578/1.07919. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57021/1.04302. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56857/1.12413. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55564/1.13160. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56388/1.11590. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55213/1.14076. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.55466/1.13561. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54021/1.18240. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54392/1.14532. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.53514/1.14843. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.52503/1.18444. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.53114/1.18585. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.53907/1.15990. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.52183/1.22495. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.52420/1.16287. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50880/1.16804. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50878/1.19287. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50319/1.21611. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48926/1.21826. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.50503/1.12956. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49505/1.16207. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.48590/1.20124. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.48173/1.23983. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.49402/1.22128. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.46489/1.22913. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.47313/1.19953. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.45976/1.23180. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.46957/1.27382. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.47376/1.29747. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.47097/1.26033. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.45148/1.23924. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.44087/1.33364. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43802/1.37229. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.45040/1.36515. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.44862/1.38401. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.41772/1.34651. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.43497/1.41155. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.41679/1.39838. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.41467/1.52150. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40191/1.47779. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.44209/1.30421. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.42590/1.52571. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.41072/1.49485. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.37090/1.55357. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.37551/1.64871. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.37447/1.72041. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.39464/1.59050. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.38406/1.54598. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.36660/1.57431. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.36105/1.61030. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.37582/1.74750. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.40402/1.61054. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.37607/1.67190. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.34705/1.65488. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.32803/1.85701. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34816/1.70653. Took 0.45 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.70251/0.69626. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69602/0.69081. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.68957/0.69373. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68909/0.69553. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69121/0.70162. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68630/0.70256. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68485/0.70652. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67764/0.71261. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67748/0.71769. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67710/0.71191. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67281/0.72213. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67091/0.72828. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66549/0.72783. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66467/0.71973. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65953/0.70514. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.65423/0.71089. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65760/0.70959. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.64711/0.72021. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64800/0.72143. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64192/0.71200. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64444/0.71709. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.62947/0.72474. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63054/0.71700. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62880/0.74833. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62864/0.73772. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.61496/0.75658. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62114/0.75259. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.62091/0.74643. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61368/0.80135. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60084/0.77923. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60445/0.77914. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.59301/0.79693. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.58551/0.81288. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.58364/0.82587. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.58581/0.82366. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.56973/0.83200. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.57493/0.84450. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.56141/0.85333. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.56091/0.86512. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.56000/0.86388. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.54743/0.84217. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.54326/0.87982. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.55154/0.84372. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.53008/0.88246. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.53477/0.91188. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.52316/0.92737. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.51744/0.92184. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.50666/0.96516. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.50883/0.90464. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.50707/0.96453. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.51065/0.93719. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.49893/1.00533. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.48568/1.01506. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.48120/1.03653. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.48208/1.06507. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.46702/1.04333. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.45764/1.05266. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.44979/1.05078. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.43113/1.10132. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.43937/1.06941. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.44203/1.14440. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.43916/1.14856. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.41905/1.20090. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.42274/1.07590. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.40741/1.19493. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.40522/1.17185. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.39380/1.16776. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.38939/1.23571. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.39353/1.18271. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.38453/1.20685. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.38468/1.22354. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.37392/1.22755. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.36775/1.26158. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.37919/1.26974. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.38451/1.21236. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.36743/1.27088. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.33036/1.26581. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.33798/1.30749. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.34068/1.31956. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.36823/1.28353. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.33750/1.30802. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.31288/1.36019. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.34283/1.31574. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.30906/1.41782. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.32088/1.34164. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.30968/1.29555. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.31999/1.23906. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.30522/1.43989. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.30515/1.37860. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.28119/1.35759. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.28641/1.29945. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.26883/1.25353. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.26387/1.36732. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.25878/1.40570. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.27253/1.39802. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.26833/1.48592. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.28952/1.43047. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.27588/1.42363. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.25645/1.40757. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.23588/1.36746. Took 0.43 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.72421/0.71496. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.71082/0.70827. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70239/0.74489. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.70544/0.73642. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.70360/0.75356. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.70075/0.74379. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69782/0.74538. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69556/0.75106. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.69076/0.76420. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68918/0.76955. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68912/0.78537. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67659/0.78992. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68067/0.80489. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.66964/0.82125. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67456/0.84188. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66764/0.86767. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66604/0.86707. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66027/0.87276. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66197/0.87966. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65165/0.91279. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64907/0.91585. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64348/0.92823. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64859/0.89502. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64916/0.89167. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64274/0.88845. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64330/0.92541. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63943/0.91563. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64095/0.92539. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63285/0.94687. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63206/0.96088. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62632/0.99933. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63004/0.99217. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.62338/0.98355. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61659/1.04427. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61553/1.01664. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61915/1.03455. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61641/1.03171. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60742/1.05791. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60866/1.03186. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60135/1.06169. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59930/1.05485. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.60097/1.06532. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.60198/1.06933. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59875/1.06386. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.59854/1.10717. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.58486/1.11132. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.58409/1.10944. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58746/1.10433. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59731/1.07609. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57611/1.12425. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.58748/1.11737. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.57123/1.13359. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.56982/1.14945. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.57366/1.14859. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56508/1.17764. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.56856/1.12390. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56006/1.18021. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.55803/1.13347. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.55395/1.17621. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54406/1.19438. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.55323/1.11606. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.53902/1.20200. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54974/1.21561. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.54318/1.28224. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.54314/1.13854. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.53827/1.23228. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.52452/1.18702. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.52138/1.21220. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.50282/1.26462. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.51598/1.28611. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.50896/1.25200. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.52128/1.28937. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.51908/1.17097. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.51799/1.27557. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.50276/1.24850. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.49945/1.24207. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.50031/1.27411. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.51638/1.12214. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.48604/1.23415. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.50279/1.20892. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.49433/1.29247. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.49840/1.28294. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.49011/1.27981. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.49372/1.22283. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.48763/1.23073. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.48174/1.23371. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.46715/1.30581. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.47666/1.27209. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.46699/1.27490. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.46626/1.28289. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.43964/1.27249. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.46093/1.32161. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.48227/1.28288. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.47658/1.17929. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.47480/1.25288. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.44804/1.29798. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.44518/1.25508. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.44883/1.32863. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.42828/1.24700. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.45586/1.26211. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.71016/0.73617. Took 0.63 sec\n",
      "Epoch 1, Loss(train/val) 0.71088/0.71017. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.70230/0.75360. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70665/0.75679. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68930/0.74508. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69039/0.74857. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69125/0.75177. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68637/0.72735. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68327/0.75883. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68094/0.75704. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68317/0.75084. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68216/0.76163. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.68619/0.76687. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67538/0.79000. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67799/0.76645. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67265/0.79022. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.67254/0.77464. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66571/0.78847. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66353/0.80042. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66107/0.79902. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65509/0.79497. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66483/0.79289. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66201/0.79384. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65858/0.82087. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64977/0.85579. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64494/0.82064. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64173/0.88441. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64028/0.83460. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.64152/0.85611. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63577/0.84796. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63144/0.84503. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.64299/0.84359. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.62493/0.88687. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.62547/0.86956. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.61974/0.88316. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62007/0.89250. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62416/0.90259. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.62073/0.91725. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61705/0.92275. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60226/0.96596. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.61826/0.92204. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.59996/0.92174. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60928/0.89871. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58317/0.94615. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58828/0.91437. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.58620/0.91493. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.59251/0.91550. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.57991/0.90023. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.57622/0.90219. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.56129/0.94179. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57311/0.93399. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55436/0.97451. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56673/0.93318. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54733/0.95225. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54668/0.92248. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.55118/0.95425. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.54069/0.95064. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.51690/0.96841. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.51921/0.97617. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52374/0.99989. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51566/1.00492. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.51127/1.00772. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.50398/1.00192. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.50004/1.05658. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.51218/1.05985. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.49725/1.01448. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.48970/1.07264. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48506/1.10712. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.46944/1.07711. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46627/1.01073. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49023/1.05472. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.46012/1.11626. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46992/1.15932. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.45642/1.10143. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.44316/1.10042. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.43902/1.14891. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.43233/1.11732. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.44346/1.09292. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44435/1.19695. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.40419/1.20941. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.43175/1.17768. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.42254/1.20827. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.40348/1.17465. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.39023/1.20084. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.39505/1.18504. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.36155/1.32195. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.37400/1.29210. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.39556/1.24788. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.39422/1.19476. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.35789/1.28121. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.34981/1.30737. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.35308/1.24685. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.33905/1.32256. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.34997/1.26326. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.33943/1.35280. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.32980/1.33411. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.31430/1.37219. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.34731/1.40165. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.32028/1.47126. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.34112/1.39232. Took 0.43 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.70620/0.71385. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.71084/0.71003. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69412/0.72797. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69483/0.74259. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69585/0.72883. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69759/0.72906. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68822/0.72906. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68704/0.74692. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68795/0.74713. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68042/0.75684. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68351/0.75075. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67851/0.76350. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.69331/0.76100. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67263/0.76147. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67801/0.76394. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67211/0.76377. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67505/0.76251. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67581/0.76335. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.67685/0.76109. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66884/0.77145. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.67217/0.76902. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.66813/0.77622. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65973/0.77932. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.66156/0.77257. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65514/0.78345. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64906/0.80321. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64217/0.81001. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64044/0.81588. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64410/0.82101. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.63534/0.84356. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.63804/0.85880. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.62756/0.86557. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63354/0.87394. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63127/0.88339. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62225/0.89378. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62911/0.86510. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.61676/0.89163. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.61567/0.88964. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60947/0.91094. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60683/0.92803. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60288/0.93812. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.60823/0.94147. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.59580/0.93943. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58974/0.92792. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.58055/1.02144. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.57918/0.97653. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.58288/0.97293. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.57723/1.00093. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.57375/0.95426. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.58219/0.97540. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.57013/0.96947. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56357/0.97031. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.56138/0.97994. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.56505/0.98806. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54522/0.98536. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55225/0.95813. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55693/1.00056. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.55591/1.01306. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.53890/0.97400. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.53816/0.99747. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54328/0.99058. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.53456/0.99663. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.53180/1.01919. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.51790/1.03456. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.54875/0.99727. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.52032/1.03193. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.52898/1.03221. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.52855/1.00399. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.50005/1.03874. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.48637/1.03802. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.50363/1.04740. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.50858/0.94426. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.51331/0.97172. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.53319/1.01507. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.50022/0.94485. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.49001/1.00803. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.47843/1.04535. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.48808/1.02320. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.47356/1.10049. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.46418/1.06322. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.47985/1.07939. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.48626/1.02031. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.46673/1.05732. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.45197/1.10039. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.46040/1.07683. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.47295/1.11937. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.46835/1.07394. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.45036/1.09916. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.44650/1.10918. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.43530/1.08604. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.44309/1.09445. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.43422/1.06361. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.45102/1.09142. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.45563/1.16989. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.45058/1.16902. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.45873/1.10379. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.42766/1.23246. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.45128/1.19314. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.42375/1.18296. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.42614/1.18124. Took 0.44 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.70143/0.74308. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69641/0.74962. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.70107/0.74851. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69371/0.74873. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68675/0.74581. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69213/0.75103. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68764/0.75334. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69058/0.75209. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68687/0.75101. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68770/0.74303. Took 0.47 sec\n",
      "Epoch 10, Loss(train/val) 0.67866/0.75207. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68284/0.76559. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.68150/0.76182. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68111/0.76748. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68299/0.76253. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68396/0.76757. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67945/0.78169. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67926/0.78512. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.67435/0.79477. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.67611/0.79719. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.67502/0.79745. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.67194/0.79665. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.67115/0.80199. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66588/0.80685. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66469/0.81172. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66029/0.81929. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65418/0.81305. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65989/0.79925. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.65324/0.80612. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65362/0.80480. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64375/0.79267. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64179/0.80208. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.63701/0.80990. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.63452/0.79796. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.63325/0.80134. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.63238/0.81440. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.63402/0.81875. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.63210/0.81379. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62355/0.83439. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.62702/0.81762. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.62639/0.82123. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.62142/0.83074. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.61075/0.83571. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61805/0.86036. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.61192/0.80724. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.62604/0.78928. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.60925/0.77280. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.60143/0.78090. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59591/0.76746. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.59960/0.76244. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.59046/0.77544. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.59962/0.79107. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.59621/0.78063. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.58617/0.83083. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.58038/0.83968. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.58248/0.87480. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.57641/0.88925. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.55948/0.92757. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.57114/0.93358. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.57044/0.91755. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.56002/1.00385. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.55232/1.01691. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.56940/0.99972. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.56464/1.04356. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.55565/1.00482. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.54572/1.10495. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.55158/1.16414. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.58198/1.01495. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.54351/1.02802. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.54717/1.05275. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.53389/1.16048. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.53576/1.13114. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.51488/1.15009. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.51875/1.19191. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.52891/1.22754. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.51065/1.26114. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.51260/1.03443. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.51599/1.23292. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.50671/1.06214. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.47441/1.16493. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.50430/1.27598. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.49061/1.26593. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.49541/1.18105. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.46538/1.18272. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.46959/1.07617. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.49325/1.23191. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.51653/1.06743. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.48862/1.18493. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.50826/1.29978. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.51849/1.16874. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.47784/1.22949. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.47621/1.24953. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.46895/1.27237. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.47312/1.31293. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.47735/1.38267. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.47003/1.36762. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.45662/1.37948. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.46678/1.26681. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.45187/1.27033. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.44525/1.32042. Took 0.45 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.71602/0.68018. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70429/0.68950. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.70771/0.69130. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.70200/0.69063. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69390/0.68288. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69934/0.68040. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69596/0.69905. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68872/0.70849. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.69517/0.70709. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68991/0.70413. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.69043/0.70681. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68193/0.71979. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67856/0.72918. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67783/0.71170. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67685/0.72489. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66740/0.72701. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67322/0.71840. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66859/0.73509. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66157/0.73731. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66411/0.74831. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.66174/0.75540. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66976/0.72712. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65496/0.75372. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.65588/0.74765. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.65870/0.73359. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64918/0.74468. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65069/0.75048. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64979/0.73627. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64767/0.75869. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.63880/0.74931. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63922/0.79113. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.63698/0.77023. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63008/0.80688. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63137/0.81303. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.62032/0.84878. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.63275/0.81261. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61352/0.88241. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.61212/0.86092. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61295/0.86798. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61205/0.86314. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61204/0.83801. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59789/0.87435. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60235/0.87958. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.59325/0.91154. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.57173/0.95401. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.58613/0.93263. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58649/0.90059. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.58680/0.92411. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.57914/0.96796. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56922/0.98948. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55613/1.02040. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56653/1.01057. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54947/1.03339. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56077/1.03682. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56331/1.01860. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.55660/1.02353. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55452/0.99601. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53297/1.05289. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54813/1.02365. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53506/0.98072. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.53580/0.96203. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.53055/0.98106. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51850/0.95197. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.51980/0.98513. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50216/1.09872. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50726/0.98448. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51040/1.04624. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.51086/0.90524. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.53791/0.84278. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.52772/0.88885. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.49412/0.90429. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.49854/0.98412. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.49109/0.97509. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.51779/0.89305. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.48877/0.96535. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.46871/1.09482. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.45663/1.10223. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.45826/1.14997. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.47144/1.03746. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.45610/1.10832. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.44182/1.10433. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.44514/1.04862. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.44386/1.06101. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.45253/1.06639. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.41095/1.15390. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.41602/1.11697. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.42205/1.15620. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.42267/1.15549. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.41098/1.22534. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.40388/1.15702. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.44264/1.13915. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.48047/1.19976. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.45851/1.10493. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.42491/1.22027. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.41318/1.23592. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.41112/1.19501. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.39076/1.39431. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.38879/1.33270. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.37947/1.28075. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36608/1.34169. Took 0.45 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69823/0.69213. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69132/0.69901. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.68838/0.69672. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68787/0.70381. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68453/0.69406. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67589/0.70964. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.67440/0.71626. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.66949/0.72301. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67325/0.73687. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66929/0.74166. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.66899/0.75236. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66403/0.75864. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66828/0.74680. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66179/0.76332. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66320/0.76589. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.65976/0.77497. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65420/0.78971. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65597/0.79693. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.65222/0.81133. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65280/0.81632. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64304/0.82370. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63840/0.81599. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63214/0.82910. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62766/0.84288. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.62436/0.84153. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.61842/0.84741. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62125/0.84308. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.60935/0.83638. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62106/0.84315. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.60542/0.86560. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.60330/0.84351. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.59538/0.84820. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59209/0.85043. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.58970/0.84008. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.57009/0.90305. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58636/0.89265. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.57803/0.90914. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.57283/0.96947. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.56321/0.91437. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57714/0.90336. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.54805/0.98851. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.55200/0.94130. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.53840/1.02259. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.54724/1.01964. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55057/0.92368. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.53763/0.96330. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.53432/0.96488. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.53593/0.94835. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.53914/0.96743. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52209/0.94832. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.50980/0.99241. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53008/0.97032. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.51298/0.96711. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.51367/0.99468. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51681/0.96670. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.50782/0.95973. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.49931/0.97211. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.50665/0.95107. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.48995/0.92770. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.48464/0.89781. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.48008/0.91459. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.47364/0.92997. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.47383/0.94603. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.48564/0.91448. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.46705/0.98605. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.49203/0.94119. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.44655/1.01586. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.44704/1.00028. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.45393/1.00621. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.43754/0.99646. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.46065/0.97152. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.42893/1.02796. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.42786/1.04866. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.43678/1.00499. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.42211/1.01846. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.42083/1.02418. Took 0.47 sec\n",
      "Epoch 76, Loss(train/val) 0.41718/1.07681. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.38302/1.07687. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.39114/1.10462. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.39665/1.10302. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.38828/1.10221. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.38862/1.09929. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.38614/1.09340. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.37697/1.08957. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.37314/1.12045. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.38518/1.17544. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.36468/1.17097. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.34899/1.16876. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.36526/1.16173. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.36688/1.19413. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.35324/1.17403. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.33768/1.21679. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.35458/1.20252. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.34411/1.15312. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.33015/1.22543. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.32513/1.21996. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.34444/1.21568. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.34881/1.25201. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.31058/1.31481. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.29800/1.32375. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70697/0.72156. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69143/0.71951. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69356/0.73114. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68031/0.73468. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68251/0.72764. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67496/0.74689. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67635/0.72911. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67190/0.73468. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.66950/0.74106. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66645/0.76118. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66173/0.77225. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.65710/0.78771. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.65535/0.79244. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.65680/0.78989. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65760/0.79100. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.64703/0.77550. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.64978/0.78484. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64972/0.78942. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64261/0.80291. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.63390/0.81340. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63091/0.81441. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63502/0.84578. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.63058/0.83350. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.62664/0.82958. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62216/0.86591. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.61987/0.84859. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.60579/0.86958. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.60736/0.88300. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60859/0.86412. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60047/0.88079. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.59714/0.87938. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60200/0.88260. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59035/0.88148. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.58654/0.92437. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.58639/0.94450. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58367/0.95103. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.58202/0.93611. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57536/0.92904. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.57004/1.00396. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.56804/0.98894. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56712/0.95694. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.55538/0.97293. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.55603/0.99038. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.54676/0.97949. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.54136/0.98522. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.54404/0.99487. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.53265/1.07935. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.54218/1.03134. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.53199/1.03294. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.53284/1.02492. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54128/0.98956. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.51726/0.98491. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.55976/0.93530. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55388/0.93373. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.54199/0.96497. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.53764/0.96841. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52628/0.96391. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.51994/0.99224. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.51032/1.00917. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50961/1.03623. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49857/1.07453. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.51379/1.06682. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51367/1.04743. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.49318/1.05610. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.49221/1.07100. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.47291/1.12800. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.47564/1.12450. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.45325/1.21759. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.45011/1.22295. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.45941/1.23772. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49367/1.15040. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.44999/1.14235. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.43311/1.14861. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.45307/1.26097. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.45822/1.20525. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44323/1.14520. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.42047/1.18611. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.44313/1.24096. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43568/1.31929. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.42532/1.33338. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.42526/1.32205. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.39380/1.33134. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.40062/1.36890. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.41308/1.34415. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.39651/1.38101. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.38945/1.39949. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.39983/1.35765. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.39242/1.36266. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.37806/1.38712. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.35721/1.49303. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.35768/1.49360. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.36767/1.51484. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.38659/1.57536. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.36476/1.47967. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.35014/1.53684. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.34859/1.55937. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.34419/1.62260. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.32210/1.59931. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.34051/1.63404. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.32903/1.68529. Took 0.43 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69545/0.69092. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.68643/0.69803. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.68461/0.68891. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68477/0.69149. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68170/0.70459. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67229/0.71298. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.67614/0.70923. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.66981/0.70674. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67612/0.69739. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67232/0.72530. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.66861/0.73581. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.66966/0.74422. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66732/0.74400. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66446/0.74429. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66057/0.78047. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66411/0.76240. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65216/0.76899. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65262/0.77611. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65645/0.78609. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65659/0.77382. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64937/0.77873. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64289/0.76365. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.63977/0.77893. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65006/0.78648. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63590/0.80350. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63959/0.81643. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63881/0.78096. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63143/0.79894. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63010/0.79000. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62149/0.81022. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62413/0.82115. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61773/0.82186. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.61273/0.82755. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61675/0.80131. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60550/0.82332. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61002/0.82304. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.59515/0.87855. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.59079/0.84115. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58580/0.84501. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.59555/0.84930. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57356/0.82217. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57567/0.85527. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.57369/0.80691. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.55910/0.84331. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.55752/0.87178. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55830/0.85932. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.55591/0.92649. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.57509/0.87666. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.56709/0.84412. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.54571/0.89590. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54447/0.92022. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.54429/0.87542. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.54563/0.88557. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54321/0.88531. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54152/0.88606. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.50794/0.89646. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.53688/0.87722. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.54157/0.87519. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.53066/0.83415. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.52584/0.91028. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.51155/0.89748. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.50278/0.97052. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51614/0.93279. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.50770/0.93605. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49182/0.97364. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.48803/0.96785. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48927/0.97249. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.48281/0.96204. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.48187/0.99292. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.47644/1.03348. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.47460/1.00370. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.50398/0.96813. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.49231/0.95489. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.48744/0.89123. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.44730/0.95483. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.45715/0.95115. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.47720/0.97558. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46902/1.03688. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.45310/0.97115. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.45799/1.01264. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.45358/0.98581. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.43521/1.11227. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.45403/1.11677. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.46327/1.10631. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.42865/1.06349. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.42405/1.09648. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.42286/1.08444. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.43699/1.19598. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.43056/1.12760. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.39619/1.15922. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.43164/1.16633. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.39430/1.11360. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.38824/1.17558. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.37923/1.16455. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.39402/1.21812. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.36350/1.18597. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.39692/1.13205. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.38075/1.17656. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.39324/1.16010. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34304/1.22821. Took 0.45 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.72082/0.71316. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70839/0.69981. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70215/0.70622. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.70049/0.70757. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.70011/0.69566. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.68996/0.71276. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.69593/0.69690. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68829/0.70526. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68146/0.70582. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68085/0.70101. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.67971/0.69615. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68252/0.69699. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68005/0.70064. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67534/0.69945. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67544/0.70413. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.68055/0.69967. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67021/0.70220. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66559/0.70843. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67589/0.69891. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66972/0.70031. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.66376/0.69490. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.66440/0.70774. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65623/0.71057. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.65896/0.70268. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64731/0.72000. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64583/0.70166. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.64171/0.70554. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64543/0.70543. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.64835/0.69699. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63572/0.70594. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.64174/0.70157. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63343/0.68457. Took 0.49 sec\n",
      "Epoch 32, Loss(train/val) 0.63330/0.68219. Took 0.47 sec\n",
      "Epoch 33, Loss(train/val) 0.62486/0.67290. Took 0.48 sec\n",
      "Epoch 34, Loss(train/val) 0.62620/0.67337. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.62420/0.68316. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61986/0.67228. Took 0.48 sec\n",
      "Epoch 37, Loss(train/val) 0.62881/0.68083. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60845/0.68873. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.61710/0.69217. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60936/0.71072. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61326/0.72937. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.59437/0.70944. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.59554/0.73107. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.59257/0.73248. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.59330/0.71418. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58849/0.72051. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.60420/0.71264. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.58358/0.73793. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.59086/0.75122. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.56198/0.74595. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.57662/0.74982. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.56888/0.77132. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.56898/0.77717. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.55609/0.79338. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.56062/0.76274. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56207/0.81121. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54189/0.86097. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54904/0.83238. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.54619/0.83796. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54063/0.86574. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.54397/0.87907. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53079/0.91780. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.54718/0.97325. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.52829/0.97153. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.51909/0.93296. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.50892/0.99336. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.53878/1.04746. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.50415/0.98816. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.51408/1.06105. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49290/1.04111. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.49558/1.07540. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.50314/1.08658. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.47859/1.07152. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.48409/1.04959. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.47342/1.07774. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.44867/1.11243. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.46273/1.14273. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.47205/1.15209. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.46271/1.15989. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.49852/1.11759. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.48513/1.12166. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.46411/1.17389. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.44337/1.24328. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.44626/1.29467. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.43677/1.18298. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.43298/1.30911. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.42894/1.26145. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.42981/1.33192. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.42731/1.23042. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.42080/1.35205. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.42042/1.30003. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.40763/1.35876. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.38119/1.38385. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.41335/1.35682. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.36742/1.38744. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.38229/1.38994. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.39747/1.31711. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.39748/1.28303. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.40866/1.27908. Took 0.45 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.71127/0.71803. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70300/0.71955. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69971/0.71149. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69691/0.71240. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69737/0.70707. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.69524/0.71324. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.69011/0.70745. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.69102/0.71119. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68880/0.72227. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68807/0.71315. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68040/0.71321. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68254/0.70897. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67795/0.70832. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67128/0.71369. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66915/0.72626. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66829/0.73355. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66841/0.71941. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66302/0.73933. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65425/0.75717. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65524/0.74946. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65419/0.75101. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.65165/0.74525. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65342/0.76768. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64136/0.78433. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64856/0.78199. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64023/0.79890. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63721/0.81124. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63413/0.81203. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63452/0.82218. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.61813/0.84796. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.62698/0.83571. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.62079/0.83512. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60696/0.86100. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60047/0.84671. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.59494/0.86962. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60225/0.89103. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58907/0.87928. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.58232/0.87017. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.57594/0.90254. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.56052/0.90053. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.56560/0.96161. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.55788/0.91609. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.54962/0.92609. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.55280/0.91918. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.54302/1.01304. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.55614/0.90143. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.53157/0.99333. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.53005/0.95266. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.52659/0.93119. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.51985/0.93041. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.52337/0.95467. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.51260/0.90420. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52356/0.90676. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.50974/0.97078. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.49594/0.99191. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.50184/0.99367. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53284/0.89499. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.50018/0.98711. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.49269/1.02699. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.48866/1.03991. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.49124/0.96909. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.47550/0.99192. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48456/0.96596. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.50130/0.99233. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.46522/0.98958. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.44671/1.01453. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.46270/1.06871. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.43854/1.02792. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.44694/0.99569. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.43955/1.01350. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.42442/1.10462. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.43217/1.08034. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.41971/1.06976. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.43243/1.03909. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.43662/1.09583. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.41055/1.15294. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.42129/1.04403. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.41860/1.07734. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.40938/1.13942. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.39830/1.14543. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.40041/1.17992. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.36554/1.19650. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.38126/1.21681. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.37703/1.29793. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.37684/1.18421. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.36413/1.25084. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.36585/1.27054. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.34258/1.25910. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.35240/1.27603. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.36523/1.36532. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.33986/1.37157. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.32416/1.31128. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.33730/1.36247. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.33000/1.41512. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.32127/1.43737. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.28837/1.47370. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.35736/1.29487. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.31920/1.38737. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.30400/1.56704. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.30817/1.46931. Took 0.43 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.74651/0.76052. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.73627/0.75951. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.72625/0.77714. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.73404/0.78821. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.72400/0.75199. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.72187/0.76841. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.71249/0.76386. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.71293/0.74923. Took 0.48 sec\n",
      "Epoch 8, Loss(train/val) 0.71341/0.75485. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.70364/0.76668. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.69857/0.75815. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.69112/0.76689. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.69292/0.76411. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.68754/0.79288. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.69203/0.76643. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67725/0.79087. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.68086/0.79924. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.67709/0.81801. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67084/0.82508. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.67202/0.84536. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.67525/0.84724. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.67082/0.82175. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.67741/0.85915. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.68129/0.87581. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.67056/0.85499. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.65568/0.86748. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64930/0.89385. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.65869/0.88290. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.65812/0.92793. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.64731/0.91582. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.64942/0.95274. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64699/0.96488. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.65261/0.97291. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64944/1.00460. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.63938/1.03785. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.63745/1.02458. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62682/1.05944. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.63133/1.07855. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62403/1.07895. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.62149/1.08643. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62784/1.10766. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.62134/1.08310. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.61338/1.12386. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61026/1.11078. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.60622/1.07961. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.60838/1.15087. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.60390/1.14244. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.60093/1.20596. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59534/1.16325. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.61092/1.17686. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.58429/1.19621. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.59862/1.19585. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.59351/1.17788. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.58017/1.13801. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.59022/1.17136. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.57242/1.27395. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56338/1.31705. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.58466/1.20900. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.57077/1.18277. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.56725/1.18445. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55577/1.24408. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.54160/1.31670. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54719/1.32420. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.57161/1.27853. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.56553/1.19202. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.54584/1.29186. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.54039/1.28142. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.53802/1.26676. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.53764/1.34893. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.53435/1.38229. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.53282/1.33133. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.52389/1.37274. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.52781/1.42259. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.52770/1.43200. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.53386/1.37746. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.53291/1.29673. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.52126/1.42298. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.51717/1.48161. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.52053/1.46454. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.52643/1.45183. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.50433/1.45258. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.50950/1.49574. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.51268/1.46764. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.50122/1.50761. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.49592/1.47924. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.48821/1.53775. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.49186/1.56708. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.47456/1.58652. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.50743/1.57352. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.48554/1.61975. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.47711/1.72723. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.47704/1.51189. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.47633/1.58985. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.45579/1.70625. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.47126/1.70957. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.45914/1.55433. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.48449/1.65803. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.46337/1.68503. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.45181/1.69623. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.46657/1.73203. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70284/0.72273. Took 0.58 sec\n",
      "Epoch 1, Loss(train/val) 0.69595/0.70464. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69438/0.70247. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69204/0.71708. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69642/0.72233. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68226/0.76144. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68178/0.77837. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68182/0.76986. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67342/0.76326. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68359/0.75740. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67763/0.74325. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67483/0.73590. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67504/0.75433. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67498/0.75444. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66563/0.74311. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67530/0.74501. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67015/0.73427. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66737/0.72945. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66290/0.73518. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66265/0.73306. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65872/0.75511. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64956/0.73316. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64848/0.75486. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65701/0.73436. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63880/0.77039. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64799/0.78850. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65041/0.79714. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64322/0.85903. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.63808/0.88032. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62884/0.89842. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.62313/0.87540. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63439/0.98509. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.62291/0.96780. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.61728/0.93875. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.62094/0.98521. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.61767/0.98575. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.61614/1.05517. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.61705/0.93367. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.61453/0.96684. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59990/1.05328. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60806/1.05735. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.61336/0.91504. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.59706/1.03279. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.60220/0.98722. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59314/1.08391. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.59698/0.95762. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.59457/1.02305. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.59261/0.95506. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.58391/1.00335. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.57609/0.98635. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57698/1.01507. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56323/1.03836. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.57383/1.06838. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.56820/0.96781. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.54893/1.10274. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55810/1.07594. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.54463/1.19099. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.55820/1.02370. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.53796/1.06602. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54897/1.11169. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.54119/1.07161. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.54425/1.15184. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50825/1.22850. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.52882/1.16508. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.51931/1.24466. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.53432/1.11651. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.51900/1.23606. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.51455/1.21693. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.51336/1.24245. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.50104/1.21143. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49154/1.25994. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.49590/1.22661. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.49993/1.21551. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.48047/1.28398. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.49598/1.34170. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.49376/1.38794. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.48603/1.23903. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46478/1.32154. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.46249/1.42791. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46262/1.36700. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.46926/1.41816. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.45618/1.47849. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.45712/1.41803. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.45522/1.48277. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.44614/1.58029. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.43998/1.58182. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.44896/1.49368. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.43495/1.46042. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.44896/1.50518. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.43094/1.43292. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.41842/1.50755. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.41866/1.55320. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.42686/1.46913. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.42773/1.60741. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.40162/1.63949. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.39868/1.66782. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.39355/1.68724. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.39676/1.67840. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.42590/1.81359. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.41037/1.68459. Took 0.43 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70716/0.68735. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69590/0.71632. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69873/0.71275. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69249/0.71479. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68431/0.70903. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68305/0.72193. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68839/0.72992. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68471/0.73846. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68278/0.73231. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67415/0.71720. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67304/0.73366. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67308/0.73566. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66827/0.72584. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65875/0.73863. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67555/0.72566. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65612/0.72929. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65542/0.73654. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66102/0.74268. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65342/0.73125. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64956/0.73625. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64901/0.71981. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64661/0.72655. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64401/0.72902. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.63479/0.73580. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63457/0.72566. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.63263/0.74390. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62845/0.74313. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62364/0.77013. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62189/0.80336. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.61287/0.80397. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61147/0.78470. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61181/0.80694. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.59757/0.81122. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61187/0.84009. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.60274/0.82998. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.59521/0.84423. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59067/0.88596. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.58411/0.89302. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59319/0.91847. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.58639/0.88655. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58537/0.89964. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57355/0.89585. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58261/0.88280. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58208/0.82132. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58212/0.81564. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57907/0.81052. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.57561/0.81746. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.57524/0.84979. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56463/0.84571. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.56833/0.81895. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.56021/0.83169. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54974/0.82118. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.55844/0.84371. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.54468/0.84453. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53539/0.84396. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.53735/0.86258. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53419/0.87335. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53901/0.86254. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.53472/0.86444. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.51539/0.87763. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52711/0.88116. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50791/0.89291. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.50175/0.90541. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.50596/0.88879. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.49631/0.91439. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.49465/0.89913. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.47358/0.90826. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.51255/0.88984. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.49543/0.92526. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46792/0.92899. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.47074/0.91755. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.45778/0.96786. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46488/0.96908. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46104/0.98055. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.45447/0.96354. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44999/0.95057. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.44724/0.95063. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.43996/0.96078. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.41530/1.00095. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.43023/0.97927. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.43546/0.98901. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.41942/0.98051. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.42982/0.99270. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.42164/0.97722. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.39298/1.00607. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.40674/1.05179. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.39310/1.05016. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.40815/1.02230. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.38823/1.06714. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.37847/1.06889. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.40428/1.03385. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.37040/1.08636. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.36657/1.10847. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.38049/1.08801. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.35927/1.12050. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.33467/1.14694. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.38224/1.14430. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.35126/1.14246. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.34271/1.19356. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34256/1.21178. Took 0.43 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.71394/0.71253. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.70003/0.71779. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68928/0.71654. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69291/0.72125. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68665/0.71512. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68110/0.70537. Took 0.48 sec\n",
      "Epoch 6, Loss(train/val) 0.68449/0.72782. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68348/0.73529. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67938/0.73731. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67376/0.73771. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67663/0.75162. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67315/0.75383. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66830/0.75642. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66969/0.75566. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66731/0.76107. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66645/0.76222. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66810/0.78211. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66032/0.78420. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65978/0.80335. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65904/0.79067. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65745/0.79331. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64958/0.81522. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64513/0.80709. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63354/0.83252. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63418/0.80886. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63878/0.80889. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63205/0.81184. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61423/0.85221. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61998/0.86087. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.60779/0.87576. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60420/0.87515. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60918/0.85446. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60247/0.88919. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59289/0.88974. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.58837/0.90661. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60203/0.83798. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.57664/0.92236. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57435/0.89468. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58392/0.89095. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.57135/0.91708. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57873/0.93786. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.56745/0.96157. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57556/0.95229. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56342/0.97116. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.56184/0.95202. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.55850/0.98427. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.55756/0.97797. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.54507/0.99364. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.54484/1.06112. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54153/1.02325. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53772/1.07122. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53919/1.06277. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.52677/1.05463. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52349/1.07432. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.51856/1.09577. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.51485/1.12857. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.49937/1.13992. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.49575/1.15411. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.49755/1.12880. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.48551/1.17431. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.48789/1.23954. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.49625/1.15882. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.48042/1.16984. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.46807/1.17594. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.47154/1.19512. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.45424/1.21543. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.45124/1.20265. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.47679/1.22570. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.43056/1.30679. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.44812/1.30134. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.44810/1.24852. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45546/1.29856. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.43693/1.31111. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.44768/1.20039. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.43178/1.30719. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.42889/1.36772. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.43137/1.36485. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.42323/1.35193. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.40822/1.44573. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.41582/1.45483. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.40960/1.49376. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.40940/1.48134. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.37688/1.47219. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.39821/1.50741. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.39379/1.52762. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.39752/1.49625. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.41366/1.41848. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.37868/1.42595. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.36679/1.50548. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.36850/1.50176. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.37369/1.50968. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.36809/1.54325. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.35079/1.49615. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.34558/1.54831. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.37485/1.54862. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.34312/1.62889. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.37133/1.50772. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.35540/1.58212. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.34410/1.54300. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34370/1.64369. Took 0.44 sec\n",
      "ACC: 0.625\n",
      "Epoch 0, Loss(train/val) 0.70892/0.69930. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69653/0.69372. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.69281/0.69675. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68829/0.69414. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69387/0.70608. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68937/0.69836. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68572/0.70261. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68284/0.68761. Took 0.48 sec\n",
      "Epoch 8, Loss(train/val) 0.69020/0.68787. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68264/0.69391. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68545/0.68231. Took 0.49 sec\n",
      "Epoch 11, Loss(train/val) 0.68497/0.68208. Took 0.47 sec\n",
      "Epoch 12, Loss(train/val) 0.67613/0.69319. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68423/0.71001. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67419/0.71701. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66875/0.71723. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.67011/0.74652. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66896/0.72505. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66811/0.73495. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66033/0.73473. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66299/0.71909. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65728/0.72221. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65617/0.71551. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.65467/0.71230. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64638/0.71830. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64658/0.71822. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63861/0.71796. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63661/0.71358. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63172/0.72262. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63170/0.70799. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.63456/0.71440. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.62787/0.72058. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.63440/0.71645. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.61679/0.71608. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61094/0.71462. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.60445/0.72105. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60001/0.74208. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.60336/0.74946. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59322/0.74596. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.59689/0.73173. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59361/0.74770. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58333/0.75780. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58743/0.78807. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57694/0.78281. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56015/0.78840. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57100/0.76753. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56446/0.78883. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.56733/0.79152. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.54954/0.82495. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55970/0.79199. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54463/0.80996. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53668/0.83089. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54049/0.82787. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.53319/0.84001. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54070/0.82249. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.53738/0.85211. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.52608/0.84486. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.52492/0.83436. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.51868/0.82406. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50202/0.83400. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.51242/0.86519. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50135/0.87007. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53156/0.86017. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.50307/0.80986. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.48603/0.84825. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.49937/0.89177. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48432/0.86225. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.49822/0.88358. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.50875/0.90314. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47393/0.92670. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48160/0.93723. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.49827/0.94661. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.49604/0.94775. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46834/0.96993. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.46220/0.98181. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.47271/0.91957. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.47040/0.93609. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.44913/0.99335. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.46087/0.96254. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44828/0.98064. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.44413/0.95424. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.46199/0.95683. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.45890/0.98597. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.43842/1.03160. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.43012/1.06449. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.41664/1.03107. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.42287/1.02270. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.42783/1.10093. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.43619/1.01937. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.44251/1.02063. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.40632/1.06868. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.42028/1.09040. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.42605/1.07005. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.39193/1.13545. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.41004/1.06056. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.42292/1.00691. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.38677/1.16789. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.39279/1.12639. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.36750/1.10368. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.41881/1.05056. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.71218/0.69155. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.70235/0.68130. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69683/0.68130. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68825/0.68563. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.67918/0.69075. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67946/0.69634. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67647/0.71031. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67298/0.71279. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67217/0.71932. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66602/0.71450. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.66125/0.72179. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67009/0.71153. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66444/0.71373. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66013/0.70897. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.65561/0.70501. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.65301/0.70316. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65028/0.70592. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65353/0.69817. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64294/0.70720. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64380/0.71998. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64664/0.72222. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64946/0.72883. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.63978/0.73987. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63451/0.74398. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.62648/0.76195. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63966/0.75924. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62894/0.79669. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62442/0.79164. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63090/0.80327. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61735/0.81244. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61878/0.82011. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62448/0.80367. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62106/0.83434. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61739/0.80188. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.61486/0.80932. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60300/0.82737. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.59313/0.82888. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60367/0.86012. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61773/0.81865. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60042/0.85048. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.60314/0.86305. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.59364/0.87646. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58522/0.86836. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.59439/0.87466. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58102/0.85809. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.58581/0.87960. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58644/0.88400. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.58055/0.89922. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57131/0.87577. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.57163/0.91020. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.56932/0.92144. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.56166/0.92667. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54817/0.96174. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56898/0.95514. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56367/0.97118. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.54385/0.99967. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.54569/1.03056. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53053/1.04184. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.53859/1.03755. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53026/1.04135. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.51881/1.01683. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.51440/1.04482. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51930/1.07671. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51981/1.06577. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50881/1.10725. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.49410/1.06766. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.50453/1.07649. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.49454/1.08682. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.48634/1.08882. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.48263/1.15414. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.50921/1.09265. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.51559/1.05370. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48191/1.05780. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.48424/1.15811. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.46865/1.10493. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.47382/1.13362. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46753/1.15508. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.48703/1.16568. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.50226/0.99403. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.47722/1.07050. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.47178/1.09729. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.46947/1.07796. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.46458/1.07435. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.44603/1.08397. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43744/1.07286. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.43168/1.12275. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.45200/1.07475. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43693/1.11713. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.43271/1.16068. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.42209/1.18522. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.42731/1.15753. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.45377/1.19610. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.41528/1.15498. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.43786/1.15176. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.42838/1.15949. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.44048/1.16453. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.45991/1.11615. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.40403/1.20383. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.41650/1.18399. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.40640/1.18746. Took 0.43 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.70188/0.69899. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69388/0.71098. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69236/0.72043. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68816/0.73851. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69299/0.74668. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68857/0.74396. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68959/0.74405. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68230/0.74434. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68312/0.74308. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67680/0.73940. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68029/0.75009. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67005/0.74965. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67466/0.75952. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67833/0.76626. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68106/0.76585. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.67925/0.77187. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.68268/0.77753. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67775/0.76223. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67765/0.76169. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67213/0.75656. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66925/0.76584. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66305/0.77129. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66475/0.75448. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66650/0.76239. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65155/0.78679. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65624/0.78680. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.66185/0.79734. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64969/0.79479. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.65082/0.78197. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64651/0.80267. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.65152/0.83085. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.65320/0.81214. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64720/0.85448. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.65019/0.84871. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.64841/0.87668. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.64686/0.88385. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.63409/0.87421. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.64379/0.86057. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.64145/0.87391. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.63454/0.87548. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.63100/0.88214. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.62255/0.92535. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.61843/0.88821. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61652/0.90152. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.61657/0.89399. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.62021/0.89887. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.62068/0.90815. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.61794/0.86576. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.60602/0.90324. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.60055/0.87992. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.59936/0.90703. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.59933/0.87218. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.60646/0.89228. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.59384/0.89567. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.57610/0.90575. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.58194/0.92089. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.57055/0.93091. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.57056/0.95489. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.56083/0.96017. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54929/0.96516. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55554/0.94529. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.55034/1.00662. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.54483/0.99000. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.54859/0.97808. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.55222/1.01076. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.53215/0.98339. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.53969/1.00933. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.51805/1.03856. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.52618/1.05499. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.52476/1.04265. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.50655/1.07787. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.51264/1.05478. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.49792/1.11532. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.50295/1.11765. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.49593/1.07001. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48691/1.11312. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.48330/1.15838. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.50091/1.14250. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.49521/1.14733. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.47654/1.17429. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.46870/1.17069. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.46831/1.22204. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.45819/1.24916. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.48054/1.16159. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.46293/1.20192. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.45617/1.25027. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.45527/1.19136. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.44325/1.20835. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.44340/1.26099. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.43352/1.27755. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.45116/1.26530. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.43283/1.24910. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.41871/1.32831. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.42595/1.33256. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.43735/1.29222. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.43238/1.40730. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.41828/1.37978. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.43433/1.29102. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.40367/1.31759. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.38196/1.36460. Took 0.44 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.70958/0.68284. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69958/0.68417. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.68910/0.69026. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69589/0.70056. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68999/0.70178. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.68604/0.70407. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68032/0.70792. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67609/0.70941. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67752/0.70004. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67345/0.69610. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66796/0.69470. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66701/0.69274. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66843/0.68484. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66982/0.68454. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66047/0.69472. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66328/0.69202. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.65791/0.70214. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65944/0.69142. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65738/0.70757. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65069/0.72398. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65535/0.75727. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63680/0.75680. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64155/0.77949. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64041/0.78310. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63625/0.78951. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65165/0.78998. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64656/0.80954. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63395/0.79898. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63322/0.83547. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63053/0.84307. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61557/0.85255. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61905/0.85081. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62500/0.86401. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.60932/0.87110. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.61579/0.87247. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61218/0.86863. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60182/0.88232. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60217/0.86549. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.59570/0.89075. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.60253/0.89292. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59139/0.92563. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.57972/0.94298. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59301/0.96189. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.59173/0.92004. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57304/0.97171. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56533/0.99921. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.57084/0.98276. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.55568/1.04414. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55561/1.05485. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55746/0.98885. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56053/1.06299. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55235/1.08506. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.58039/1.02126. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.57135/1.00692. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.55568/1.02782. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.54388/1.04944. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.54221/1.06990. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54384/1.04304. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.53367/1.04967. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.52748/1.07991. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52046/1.07931. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.51769/1.12187. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.51433/1.12559. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51527/1.12912. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.50716/1.10173. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.49693/1.15795. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.49679/1.17302. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.49182/1.09921. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.48137/1.19949. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.49327/1.28731. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.47951/1.20491. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.46236/1.27313. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.45786/1.28924. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.43902/1.42213. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.45958/1.29695. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44397/1.44559. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.42831/1.41153. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.45231/1.44019. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.45729/1.44260. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.43424/1.48838. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.41164/1.46596. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.43166/1.53884. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.50815/1.16021. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.45089/1.38826. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.42189/1.49087. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.40577/1.50028. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.38721/1.58696. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.38288/1.63948. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.38319/1.68276. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.40898/1.44380. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.42663/1.59344. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.36828/1.73586. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.38573/1.60890. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.35752/1.63962. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.35640/1.72343. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.35967/1.86428. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.34652/1.73931. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.36756/1.72853. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.36310/1.59650. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.33395/1.90013. Took 0.45 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.70910/0.70529. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69602/0.70599. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69322/0.70198. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.68904/0.71224. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68334/0.72210. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68167/0.73998. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68367/0.72810. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68195/0.72539. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67730/0.73373. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67651/0.74890. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67967/0.72597. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66684/0.71665. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67252/0.72807. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67277/0.72142. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66999/0.71171. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66241/0.73291. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67087/0.73416. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66159/0.74088. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65778/0.74079. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66181/0.74709. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65874/0.74895. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65788/0.76132. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65468/0.75813. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65673/0.77073. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64849/0.78410. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.63972/0.81379. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63761/0.83907. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63205/0.84175. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62499/0.85050. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62236/0.89688. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62306/0.83983. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62476/0.87773. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61341/0.88979. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60879/0.92269. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.59873/0.93492. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60917/0.94117. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.60263/0.92840. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59380/0.90086. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59360/0.90979. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.59721/0.92148. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59165/0.91230. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57647/0.96576. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.57218/0.95614. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57599/0.93204. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.56093/0.92095. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.57126/0.92505. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57097/0.95078. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.54513/0.92507. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55571/0.95959. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.55449/0.94422. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.54685/0.93941. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53340/0.97358. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.52907/0.97796. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.51920/0.99607. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.51785/0.99166. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51529/0.99297. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.50701/1.02412. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.49767/0.99998. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50226/1.03123. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.48504/1.01550. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.50455/1.03419. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50940/0.99570. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49057/1.02826. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.47317/1.04693. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.46457/1.04714. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.47578/1.02379. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.45743/1.05321. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.45063/1.03750. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.45944/1.06263. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.41858/1.07832. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.44871/1.11084. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.44756/1.05610. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.41581/1.13897. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.42883/1.05213. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.42492/1.09435. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44531/1.19764. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.44306/1.07630. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.41072/1.16817. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.40484/1.05292. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.39810/1.11157. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.38816/1.17906. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.38539/1.20137. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.35651/1.20974. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.37835/1.13243. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.37044/1.16840. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.34713/1.18096. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.33648/1.23193. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.33340/1.10266. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.35901/1.20490. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.30623/1.14591. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.31243/1.25715. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.35301/1.30336. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.32575/1.29532. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.31452/1.29733. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.29165/1.41031. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.30115/1.35442. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.30998/1.33259. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.29345/1.33533. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.28071/1.40278. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.27468/1.36383. Took 0.44 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69800/0.70119. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69295/0.70024. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68735/0.72124. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68770/0.71497. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68412/0.72396. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68229/0.72007. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67773/0.73187. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.67787/0.72571. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67703/0.74229. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67527/0.74055. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67066/0.75233. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67266/0.74308. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66904/0.75070. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67330/0.76030. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66262/0.79684. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66059/0.82075. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65455/0.83980. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64457/0.84636. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64109/0.86855. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.63361/0.86986. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.63121/0.93681. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.62550/0.93964. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.62696/0.94465. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62924/0.92021. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.61369/0.91041. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.61330/0.94459. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.60809/0.93131. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.60092/0.99826. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.59250/0.99829. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.58400/0.98166. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.58000/0.96708. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.56696/1.02682. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.57741/1.04143. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.56634/1.04169. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.55247/1.03479. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.55565/1.04614. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.56579/1.05605. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.55546/1.03886. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.55468/1.05066. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.54275/1.05448. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.55634/1.06862. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.53828/1.10523. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.52647/1.11716. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.53047/1.10038. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.53091/1.07230. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.52471/1.12822. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.52009/1.14611. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.50736/1.14134. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.51049/1.16078. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.50862/1.17198. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.48960/1.17001. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.50134/1.22327. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.48639/1.24591. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.47993/1.23251. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.46920/1.27038. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.48490/1.21549. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.46637/1.27197. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.45839/1.33047. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.43973/1.34168. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.45320/1.32593. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.42301/1.35616. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.45189/1.44489. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.41696/1.42896. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.41476/1.40891. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.42707/1.47280. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.41675/1.48228. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.41751/1.48370. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.41756/1.53120. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.39946/1.47158. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.38740/1.51825. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.39503/1.50344. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.38589/1.57952. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.35402/1.61825. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.36701/1.67620. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.35230/1.58001. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.36349/1.62400. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.36412/1.58892. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.35493/1.60719. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.36359/1.63670. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.35299/1.65100. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.33065/1.63733. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.35555/1.68864. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.33334/1.63681. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.31783/1.71911. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.31079/1.67486. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.30392/1.71262. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.28612/1.66314. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.31063/1.74557. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.28112/1.75741. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.27636/1.76515. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.31230/1.72578. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.29140/1.75962. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.28738/1.69513. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.29760/1.81045. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.27043/1.87893. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.27028/1.92646. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.25290/1.95521. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.26167/1.99711. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.26006/2.03642. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.25628/1.96619. Took 0.45 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69582/0.68671. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69281/0.67969. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68787/0.68504. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68862/0.68144. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68616/0.68230. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68270/0.68781. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68313/0.69299. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68331/0.69298. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68494/0.69568. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67862/0.69967. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68347/0.69779. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67342/0.70196. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67494/0.69898. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67686/0.69272. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67311/0.68541. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66815/0.68825. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66854/0.69467. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66570/0.69792. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66460/0.69932. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65987/0.70316. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65724/0.70320. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65070/0.70885. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64948/0.70675. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64842/0.72597. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64321/0.72625. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64531/0.74329. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63827/0.73308. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63791/0.73987. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63642/0.72997. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62373/0.72522. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.62167/0.72927. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62067/0.77575. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.60781/0.80933. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.61302/0.81664. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60986/0.81924. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59761/0.82902. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.59714/0.88082. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.58486/0.85853. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58887/0.88249. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.57202/0.88584. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.58293/0.94333. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.57281/0.94659. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55468/0.93538. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.57097/0.94583. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56022/0.99434. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55907/0.93338. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.54947/0.97098. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.54785/0.98745. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.53818/0.98226. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54889/0.96752. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.53878/1.02209. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53680/0.99469. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.52878/1.00987. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.52816/0.96264. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51762/1.01708. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.51096/1.05295. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.50426/1.05843. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.50513/1.06208. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.50120/1.07073. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.49270/1.03901. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.49422/1.02823. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.48685/1.07134. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.49526/1.03462. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.47882/1.07930. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.47868/1.08542. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.47616/1.06650. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.47695/1.09144. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.47951/1.06429. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.46345/1.07924. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.45827/1.09815. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.46007/1.13206. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.43860/1.11750. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.43448/1.16753. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.45747/1.14875. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.43521/1.16362. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.45712/1.12401. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.42005/1.15709. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.42510/1.18196. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.43691/1.22389. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.43271/1.24225. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.42029/1.23008. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.40096/1.27357. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.41196/1.34957. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.40989/1.26190. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43833/1.21071. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.43391/1.24401. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.39910/1.24727. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.40865/1.22720. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.38313/1.28068. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.37735/1.32934. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.38134/1.31800. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.37264/1.33265. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.36554/1.37376. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.36806/1.39813. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.36309/1.40051. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.34958/1.51119. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.38298/1.42492. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.36278/1.42920. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.34714/1.45038. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.33334/1.42890. Took 0.44 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.70288/0.68344. Took 0.64 sec\n",
      "Epoch 1, Loss(train/val) 0.69349/0.68005. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69197/0.68325. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68898/0.67889. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69080/0.69655. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68866/0.70571. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68524/0.68818. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67811/0.70893. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67995/0.73130. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67681/0.74637. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67440/0.75109. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67720/0.76966. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67299/0.78376. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67248/0.77109. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67131/0.75117. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66669/0.76856. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66876/0.79295. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67102/0.80089. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66021/0.79980. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66244/0.79698. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65354/0.81491. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64614/0.84885. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64949/0.81975. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64221/0.83127. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.63918/0.82065. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.63929/0.82864. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63244/0.85588. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62762/0.87190. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62688/0.85698. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62025/0.85888. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61565/0.87495. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60767/0.89114. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60268/0.89717. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60816/0.88169. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61369/0.87579. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.60015/0.90262. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60109/0.88436. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57302/0.93344. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.58165/0.96173. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.57735/0.97451. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58279/0.96696. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.56570/0.98693. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58456/0.97830. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57001/1.01209. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.55085/1.06600. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.55717/1.04292. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.55484/1.07193. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.54590/1.08430. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55097/1.07183. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.53981/1.03751. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.53884/1.07963. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.55902/1.08630. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54235/1.07171. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.54442/1.04216. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53712/1.07156. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.53406/1.05493. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53449/1.10467. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52276/1.12423. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.52177/1.12793. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.49047/1.14707. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.53024/1.12885. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.51162/1.13062. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48415/1.15503. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.49380/1.15283. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50270/1.17162. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.48419/1.17185. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48097/1.17355. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.47189/1.15535. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.48953/1.11540. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.48343/1.16554. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.46275/1.21209. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.46538/1.24763. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46116/1.21712. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.45822/1.23560. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.43845/1.25048. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.45003/1.23276. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.45133/1.25451. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.44521/1.20676. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43843/1.23103. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.42213/1.24631. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.43654/1.16899. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.41315/1.23847. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.43075/1.34390. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40716/1.32414. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.41445/1.33734. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.40361/1.31965. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.39619/1.34500. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.40915/1.31762. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.39955/1.36037. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.40351/1.34506. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.38262/1.34198. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.37475/1.39726. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.38947/1.35139. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.39503/1.27880. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.36278/1.34284. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.37053/1.45899. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.39392/1.41029. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.38881/1.46520. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.38556/1.35319. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.35151/1.40682. Took 0.44 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69702/0.70120. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.68966/0.69215. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69076/0.69386. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68973/0.70480. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68625/0.70353. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68684/0.71654. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.68816/0.71039. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68459/0.72048. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67992/0.72570. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67929/0.72137. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67462/0.74099. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68095/0.74779. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67593/0.76874. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67196/0.76624. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67194/0.73998. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66804/0.76104. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66529/0.79742. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66914/0.74699. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66498/0.79220. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65739/0.77855. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65212/0.82562. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65514/0.81351. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64879/0.86022. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64250/0.85891. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64632/0.80400. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.63759/0.81377. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63217/0.86260. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62858/0.85301. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62829/0.85155. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62004/0.88156. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.61087/0.85452. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61877/0.85905. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.62295/0.82620. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60507/0.87178. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59516/0.92448. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.59629/0.90494. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.58732/0.88551. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.59399/0.88613. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58484/0.96363. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57615/0.96888. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.58981/0.88477. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.56904/0.95223. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55641/0.96984. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56117/0.95628. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55800/0.95670. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.55425/0.98689. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55476/1.01067. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.54230/0.98533. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.51348/1.01588. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.51922/1.02792. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.52651/1.02577. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.52564/1.04823. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.50012/1.09959. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.50509/1.11937. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.49151/1.12856. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.47633/1.19909. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.49421/1.18691. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.47313/1.32543. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.48147/1.15284. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.48275/1.27049. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.47223/1.36135. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.45955/1.28905. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.46645/1.27476. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.45224/1.30902. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.43813/1.34749. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.42726/1.35705. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.41853/1.46235. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.42182/1.32708. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.41089/1.37251. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.42085/1.45341. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.41645/1.41253. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.39045/1.44152. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.40219/1.60269. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.40149/1.54106. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.36946/1.54903. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.40114/1.49815. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.38206/1.60527. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.38000/1.57213. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.37903/1.49049. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.36219/1.71729. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.36118/1.51892. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.36351/1.51335. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.34073/1.58230. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.35531/1.59859. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.31972/1.58017. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.30821/1.70495. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.33200/1.75348. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.38154/1.52518. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.36919/1.56160. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.32941/1.59018. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.30178/1.60096. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.30410/1.65236. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.29401/1.69703. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.29438/1.71663. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.29244/1.83771. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.28542/1.92819. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.29078/1.73807. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.30705/1.80599. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.26058/1.88473. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.27976/1.90874. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.70083/0.69618. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69696/0.69871. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69208/0.70435. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69096/0.70700. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.69172/0.71738. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68627/0.71980. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68672/0.72656. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68278/0.73383. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67775/0.74349. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67680/0.75396. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67284/0.75975. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66952/0.77951. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66517/0.79511. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66265/0.81827. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.65443/0.81802. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.65109/0.82431. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65114/0.81889. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.63862/0.82326. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.64019/0.81575. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64006/0.85174. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.63922/0.86635. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63620/0.88292. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63066/0.88227. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63075/0.87083. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62765/0.88617. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62628/0.87443. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61670/0.89070. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.60777/0.87688. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60073/0.92574. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.60073/0.91680. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.58447/0.96022. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.59841/0.91568. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.58070/0.96189. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58033/0.95032. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.56712/0.97152. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.56383/0.95651. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.56789/0.95662. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.56139/0.97381. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.55693/0.99536. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.53945/1.00318. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.54878/0.96071. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.55670/0.94667. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.53914/0.92617. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.53545/0.94732. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.52565/0.92124. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.53305/0.88755. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.50820/0.98423. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.50946/0.98590. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.51214/0.98761. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.50731/0.95639. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.50365/0.94233. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.51844/0.95380. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.48536/0.96167. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.47140/0.99000. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.46453/1.00092. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.46702/1.01860. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.47722/1.05203. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.45138/1.02266. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.44508/1.05692. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.43286/1.10730. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.43410/1.10279. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.42689/1.13024. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.40937/1.13224. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.41981/1.13813. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.41652/1.09011. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.39763/1.14379. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.40019/1.14698. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.39198/1.22495. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.39465/1.23426. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.37597/1.14501. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.37274/1.18777. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.38819/1.19927. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.35291/1.21595. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.36092/1.36062. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.36425/1.28747. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.35325/1.29353. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.35288/1.30189. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.34400/1.37752. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.32173/1.20900. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.30553/1.30575. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.29749/1.46605. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.33381/1.41618. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.30535/1.44790. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.32471/1.31283. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.29673/1.25422. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.31092/1.35510. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.28725/1.46937. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.27264/1.45244. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.28204/1.42305. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.25933/1.46618. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.26923/1.45156. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.26214/1.37167. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.25821/1.34099. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.29886/1.39252. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.29216/1.38325. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.30273/1.43461. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.32695/1.38495. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.26100/1.39764. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.24476/1.54050. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.24532/1.43844. Took 0.44 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.70562/0.70086. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70901/0.71203. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69408/0.71307. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69847/0.72499. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69707/0.74206. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69183/0.73241. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68868/0.73737. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69620/0.73072. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69495/0.75337. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68354/0.73887. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68990/0.73981. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.69283/0.75599. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.69103/0.75221. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68600/0.74029. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68330/0.72885. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68751/0.73219. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.68221/0.73733. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.68817/0.75128. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67737/0.75512. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.68027/0.75571. Took 0.47 sec\n",
      "Epoch 20, Loss(train/val) 0.68224/0.74267. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.67421/0.77407. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.67140/0.75516. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.67698/0.75460. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66921/0.75590. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.67289/0.77388. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.66298/0.77958. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.66447/0.77162. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65828/0.79649. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.66813/0.78233. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.65376/0.78056. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64418/0.77224. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.65034/0.80357. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.64886/0.79014. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.64454/0.81614. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.63631/0.83477. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62680/0.87368. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.62337/0.85396. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61945/0.82583. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.61692/0.94468. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.60947/0.90818. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60600/0.96052. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.59060/0.95896. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59589/0.89732. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59629/0.90384. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.58093/0.96338. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58367/0.98147. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57774/1.01287. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.58083/1.02980. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.58100/1.00615. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55379/1.06210. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56756/1.09457. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54625/1.08935. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.55083/1.10672. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54577/1.02799. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55395/1.13535. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53030/1.05606. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53174/1.12926. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.53104/1.09137. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52708/1.11311. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52616/1.20824. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.53621/1.03239. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.52393/1.10266. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51425/1.11805. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50450/1.14886. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.51938/1.05984. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.51788/1.15675. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.51003/1.08485. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49083/1.16344. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.48269/1.25566. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.47907/1.22345. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.50182/1.17981. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.50182/1.21183. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46427/1.09694. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.46654/1.15187. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.47056/1.18119. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.48840/1.22255. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.46386/1.23856. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44472/1.35182. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.43112/1.23809. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.44897/1.36616. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.46379/1.31489. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.44212/1.29137. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.45148/1.39696. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.45544/1.28141. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.47373/1.41453. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.44084/1.39209. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.44221/1.48570. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.42280/1.37916. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.43335/1.42096. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.41884/1.54816. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.43697/1.49732. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.43131/1.44712. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.41406/1.46597. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.40272/1.54428. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.40478/1.52797. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.38145/1.51554. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.39426/1.51197. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.38371/1.62611. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.38876/1.64585. Took 0.44 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.70214/0.68067. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70502/0.68544. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69316/0.68710. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70063/0.67698. Took 0.49 sec\n",
      "Epoch 4, Loss(train/val) 0.69024/0.69356. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68306/0.68559. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68174/0.71056. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67659/0.70805. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67913/0.70360. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67323/0.70528. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67158/0.71453. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67431/0.68995. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67036/0.70502. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66247/0.71076. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66419/0.69452. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66785/0.71581. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65771/0.70297. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65690/0.71542. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65644/0.68622. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64574/0.68697. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.63693/0.69861. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63479/0.70977. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63540/0.70559. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.62414/0.71058. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.62267/0.71425. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.62915/0.71715. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61770/0.74095. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.60929/0.74169. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61639/0.72957. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.61182/0.74599. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60515/0.72885. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60392/0.74094. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.59414/0.72842. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.59134/0.71946. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.59732/0.76465. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58508/0.73991. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.57579/0.76538. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57068/0.75964. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.56511/0.76124. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.55395/0.75566. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56483/0.78455. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.54619/0.76935. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.54516/0.76869. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.53495/0.76735. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.53337/0.76572. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.52302/0.78491. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.52618/0.81340. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.50979/0.82660. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.50866/0.83844. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.50455/0.86993. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.51571/0.84917. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.49234/0.79698. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.49355/0.82751. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.49009/0.82596. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.47744/0.90263. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.46507/0.85505. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.45700/0.89533. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.46420/0.86596. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.45628/0.90369. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.46474/0.84963. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.44117/0.90014. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.42762/0.89259. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.45694/0.88409. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.44696/0.88848. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.43995/0.87438. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.43016/0.89195. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.40365/0.91256. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.40294/0.91512. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.41543/0.89660. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.38776/0.96025. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.38864/0.95583. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.38505/0.95398. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.37460/0.97290. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.38526/0.92744. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.36457/0.95605. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.35497/0.96111. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.37085/0.98439. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.34864/1.02763. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.33553/1.07664. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.34676/1.00332. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.32843/0.98582. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.32894/1.05477. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.31616/1.02451. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.31554/1.04597. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.33984/1.10668. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.30196/1.04768. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.29375/1.04526. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.30118/1.05155. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.30872/1.04624. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.28314/1.05638. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.28558/1.07587. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.26921/1.13067. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.25931/1.16333. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.28370/1.16295. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.27987/1.16196. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.24133/1.22158. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.26004/1.21822. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.24413/1.27839. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.25613/1.30710. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.24671/1.26146. Took 0.44 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.72834/0.69971. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.71387/0.67494. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.71198/0.67912. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70436/0.68374. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.70970/0.67135. Took 0.49 sec\n",
      "Epoch 5, Loss(train/val) 0.69570/0.69352. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68746/0.67410. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.69374/0.67283. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.69259/0.67077. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.69085/0.67992. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68819/0.68247. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.68294/0.69520. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67951/0.71888. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.68116/0.72477. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67852/0.74664. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.68116/0.76225. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67363/0.76971. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65986/0.74953. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66979/0.78084. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66782/0.80219. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.67409/0.76617. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65657/0.81017. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65562/0.83848. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65006/0.83901. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65108/0.84165. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63639/0.91559. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63841/0.92441. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63470/0.90930. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62681/0.98103. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.63715/1.02026. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.61968/0.97568. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.61336/1.02741. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62347/0.99869. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60526/0.98351. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.62040/0.96723. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60698/1.03463. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60939/1.08166. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59586/1.02627. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60234/1.08678. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.60240/1.01879. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59001/1.11918. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.57812/1.10201. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58647/1.06144. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.59468/1.07361. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.57653/1.15655. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.57820/1.11625. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57099/1.14937. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57820/1.08757. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.55867/1.15121. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56648/1.14805. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.55988/1.21534. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.55025/1.22769. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54531/1.17349. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54891/1.18509. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54910/1.21345. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.56435/1.08061. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.54587/1.24639. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54741/1.17679. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.55246/1.13327. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52731/1.28920. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.53307/1.31158. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.52520/1.15009. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.51843/1.32989. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.50386/1.31196. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.52698/1.26189. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.51277/1.28478. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.51340/1.29554. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.50368/1.26965. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.49898/1.29033. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.50316/1.36057. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49202/1.30107. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48439/1.38514. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.49108/1.35404. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.48780/1.29021. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.49084/1.32885. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48169/1.36732. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.46906/1.43180. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46267/1.51586. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.45689/1.41351. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.46101/1.48432. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.46828/1.56419. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.43623/1.56799. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.45008/1.48668. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.50987/1.41856. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.48852/1.42660. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.44000/1.52879. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.42877/1.58317. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.44595/1.59105. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.42103/1.64072. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.44102/1.66243. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.43085/1.58710. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.39662/1.68548. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.42373/1.52935. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.40032/1.63044. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.41335/1.68773. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.41209/1.69569. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.40955/1.66870. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.41454/1.62189. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.38163/1.69221. Took 0.42 sec\n",
      "Epoch 99, Loss(train/val) 0.38402/1.76266. Took 0.44 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.70583/0.68930. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69377/0.69551. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69862/0.69746. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69571/0.68865. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.68716/0.69837. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69162/0.70059. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68602/0.69057. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68396/0.69717. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68852/0.68494. Took 0.48 sec\n",
      "Epoch 9, Loss(train/val) 0.68184/0.69499. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68152/0.69230. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67730/0.69594. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67718/0.71546. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67074/0.73003. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68088/0.72509. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66893/0.71163. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.67599/0.71594. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67391/0.72354. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67484/0.73628. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.67005/0.73431. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66293/0.73397. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66515/0.74169. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65432/0.74009. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.66544/0.73966. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64757/0.76447. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64907/0.77462. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.63745/0.81210. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63416/0.83713. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63661/0.85441. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.63422/0.85977. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62576/0.83758. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.62647/0.87224. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62071/0.88648. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60803/0.89514. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61932/0.92681. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.61050/0.89427. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.60126/0.88921. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61403/0.87795. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59047/0.87822. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.58553/0.94950. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59189/0.93249. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.59504/0.88262. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58365/0.90334. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.58837/0.91170. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.59054/0.89864. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57038/0.92194. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57775/0.89871. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.56798/0.89386. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.56558/0.92821. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.56405/0.92024. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.56216/0.93788. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55012/0.97340. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.55212/0.97607. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.54889/0.95583. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.53490/0.99473. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.53264/0.97678. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.52354/1.02729. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54429/1.03858. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.52031/1.02354. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50861/1.08414. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50352/1.08014. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.50703/1.06707. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50435/1.11675. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.49752/1.07760. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.48909/1.11378. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.48842/1.13907. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.47284/1.15825. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.47170/1.17970. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.47096/1.18524. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47184/1.18695. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.45465/1.17111. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45818/1.22224. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.45623/1.19446. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.43516/1.28428. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.45227/1.16027. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.46798/1.22992. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.43652/1.27082. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.44679/1.23582. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44452/1.17388. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.42909/1.31424. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.42197/1.31192. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.40348/1.34870. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.41425/1.35656. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.42092/1.22901. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.41314/1.36408. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.40831/1.37034. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.40496/1.36734. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.38820/1.48056. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.38559/1.42396. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.39830/1.45326. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.36302/1.50842. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.37054/1.56289. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.36614/1.49273. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.34122/1.57875. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.34526/1.52777. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.35872/1.55463. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.37023/1.53169. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.36197/1.57146. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.32267/1.61650. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.33819/1.62182. Took 0.45 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70234/0.73291. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68708/0.71902. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68907/0.72245. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68109/0.71276. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.68464/0.72159. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.67904/0.71487. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67406/0.71621. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67514/0.70754. Took 0.49 sec\n",
      "Epoch 8, Loss(train/val) 0.67064/0.70260. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.66498/0.69748. Took 0.47 sec\n",
      "Epoch 10, Loss(train/val) 0.66956/0.71643. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66941/0.73182. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66091/0.74123. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66229/0.72788. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66199/0.73808. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66202/0.73776. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66530/0.71897. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65375/0.72841. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65409/0.74017. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64934/0.72725. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63827/0.74636. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64055/0.76189. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64176/0.75774. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63600/0.75600. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.63137/0.76028. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.62409/0.76908. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62063/0.79594. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61049/0.81116. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61549/0.79892. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.61236/0.81758. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60263/0.79315. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61155/0.79994. Took 0.47 sec\n",
      "Epoch 32, Loss(train/val) 0.59479/0.85265. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.60151/0.84644. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.58632/0.81842. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59225/0.84443. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.58145/0.89945. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.58482/0.88553. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58167/0.84175. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58708/0.84052. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.56641/0.85978. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.56513/0.88437. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58382/0.87523. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58101/0.87107. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56714/0.86263. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.57035/0.85738. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55514/0.87149. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.54326/0.90542. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.54429/0.89224. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54455/0.91111. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.53675/0.92575. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53367/0.98723. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52907/0.93523. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52590/0.90444. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.52012/0.95357. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.51808/0.92906. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.50493/0.95811. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.50948/0.92060. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.48850/0.95172. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.49559/0.94981. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.51067/0.93182. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.47908/0.96192. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.47041/0.95490. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.47302/0.93332. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.46835/0.97236. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.45358/1.00778. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.46767/0.96925. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.45083/0.97823. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.44199/0.97079. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45292/1.04405. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.44824/1.04804. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.42439/1.04828. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.40961/1.08448. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.40729/1.15226. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.40707/1.04952. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.41205/0.98879. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.38741/0.99874. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.39796/1.10519. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.39205/1.03124. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.39277/1.04071. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.37810/1.09390. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.37660/1.07853. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.35700/1.08063. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.35876/1.14168. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.37088/1.04573. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.36147/1.10652. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.36713/1.12206. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.32091/1.18635. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.32111/1.23133. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.33434/1.20704. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.35569/1.22728. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.35215/1.23655. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.35578/1.29049. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.32011/1.35845. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.30938/1.26240. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.32527/1.29833. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.30263/1.34496. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.29927/1.36669. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.30050/1.28452. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.31996/1.34044. Took 0.44 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69115/0.68047. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68301/0.67964. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68208/0.67922. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.68391/0.70018. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.67251/0.70060. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67447/0.71929. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67003/0.71396. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67147/0.70510. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.67046/0.71235. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.66753/0.73399. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66575/0.73303. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.65957/0.74545. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.65188/0.75825. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.64901/0.76731. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65081/0.77926. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.64689/0.75604. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65302/0.74715. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64531/0.75956. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.63981/0.78040. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.63871/0.78355. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63714/0.78875. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.63361/0.79396. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.62775/0.78004. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.62975/0.79167. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.61951/0.78948. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.61763/0.82378. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.60617/0.83748. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.60283/0.82746. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60549/0.82947. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.59566/0.81445. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60152/0.81632. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.58983/0.83506. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.57923/0.81131. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.57657/0.82150. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.57384/0.84307. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.57338/0.82128. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.57362/0.86169. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.56414/0.87826. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.56226/0.88568. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.55264/0.90444. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.55498/0.90747. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.54535/0.90948. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.54512/0.86413. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.55089/0.89892. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.53145/0.92839. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.53032/0.91748. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.52614/0.88368. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.52598/0.88248. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.51259/0.86873. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.51364/0.88415. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.50741/0.89744. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.50457/0.89991. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.50635/0.88049. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.50825/0.89466. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.49909/0.89968. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.48432/0.95138. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.47764/0.95610. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.46692/0.94303. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.47334/0.91462. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.45272/0.96108. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.46554/0.95308. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.46525/0.93849. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.44781/0.93913. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.46700/0.93560. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.45362/0.90568. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.45459/0.87211. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.44247/0.91751. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.43738/0.96747. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.44624/0.87184. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.43026/0.95479. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.41191/0.92221. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.41327/0.94164. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.41218/0.92326. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.39776/0.92345. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.40494/0.94805. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.41250/0.93226. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.39834/0.96900. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.38917/0.98352. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.41424/0.93870. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.38053/0.99229. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.36797/0.99839. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.38967/1.02913. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.35116/1.05794. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.37794/1.07056. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.37153/0.96267. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.37720/1.05094. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.35947/1.02426. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.33839/1.02935. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.33178/1.03426. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.35646/1.16917. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.32667/1.14625. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.32052/1.12133. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.32776/1.24053. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.31557/1.22963. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.33098/1.12975. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.31237/1.13193. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.33448/1.11481. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.30205/1.13258. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.28094/1.23706. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.30501/1.25729. Took 0.44 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.72942/0.74461. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.71156/0.80996. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.70730/0.81185. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69893/0.81950. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69566/0.84777. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69358/0.86063. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69444/0.85849. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.69081/0.87552. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68625/0.89658. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68611/0.91865. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.69135/0.92150. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67269/0.92421. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67651/0.95082. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67301/0.96662. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67495/0.96003. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66699/0.98863. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65755/1.00832. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65468/1.00338. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66255/1.04149. Took 0.42 sec\n",
      "Epoch 19, Loss(train/val) 0.65934/1.05411. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64601/1.04519. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64512/1.06207. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64031/1.07334. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.63959/1.09699. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63940/1.12627. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.62794/1.13369. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61870/1.15834. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.62425/1.16215. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.61792/1.14335. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.60671/1.14930. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60923/1.18208. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61199/1.16559. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59318/1.16902. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59453/1.18059. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59095/1.16496. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58293/1.19702. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.58683/1.22568. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57151/1.24487. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.56431/1.28557. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.57005/1.26441. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.55479/1.26707. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.55295/1.31202. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55940/1.26881. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.54096/1.28182. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.53979/1.31252. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.52684/1.31472. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.53677/1.34909. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.52796/1.33162. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55367/1.25386. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.52264/1.29800. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.51184/1.27513. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.49664/1.36571. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.49158/1.38197. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.49224/1.39227. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52027/1.29973. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.48564/1.31826. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.46057/1.31907. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.47959/1.34435. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.45224/1.38171. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.45941/1.35988. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.45404/1.30695. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.44984/1.33321. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.44291/1.35903. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.43052/1.41860. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.41948/1.36893. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.43036/1.40852. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.42915/1.35454. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.41974/1.43765. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.41857/1.38298. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.40580/1.40111. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.37485/1.47962. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.37817/1.53569. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.39042/1.50617. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.35728/1.56973. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.37353/1.59862. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.37001/1.56386. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.35867/1.58339. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.35029/1.62775. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.34084/1.61950. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.35609/1.55771. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.34654/1.68316. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.34927/1.66053. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.33319/1.64004. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.31829/1.61126. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.30977/1.68140. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.32313/1.62982. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.30517/1.65778. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.28335/1.70873. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.28346/1.76715. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.29547/1.76063. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.31486/1.68760. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.29457/1.83350. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.29205/1.89745. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.33457/1.80201. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.29885/1.70274. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.27950/1.81754. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.26754/1.92605. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.25609/1.89816. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.24784/1.77539. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.25202/1.78388. Took 0.44 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.70238/0.74638. Took 0.63 sec\n",
      "Epoch 1, Loss(train/val) 0.69663/0.73464. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69849/0.74071. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69448/0.73451. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.69215/0.75462. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68608/0.73824. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68696/0.73163. Took 0.47 sec\n",
      "Epoch 7, Loss(train/val) 0.67823/0.72667. Took 0.48 sec\n",
      "Epoch 8, Loss(train/val) 0.68606/0.71016. Took 0.49 sec\n",
      "Epoch 9, Loss(train/val) 0.67843/0.70377. Took 0.47 sec\n",
      "Epoch 10, Loss(train/val) 0.67430/0.71815. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67319/0.71005. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67876/0.70325. Took 0.47 sec\n",
      "Epoch 13, Loss(train/val) 0.67636/0.72507. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66661/0.76151. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67070/0.76484. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66676/0.74605. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66158/0.76443. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65950/0.79026. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65638/0.80535. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65339/0.79553. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64950/0.81226. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64001/0.78975. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64458/0.77718. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63942/0.75986. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63672/0.77579. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62905/0.79861. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62660/0.78826. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62082/0.80186. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.61320/0.78691. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.59656/0.81870. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.61079/0.81214. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59900/0.80586. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59751/0.76930. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.59938/0.75954. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58942/0.79804. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58987/0.76746. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.59117/0.78682. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.58539/0.80178. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59115/0.82566. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.58328/0.82941. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58037/0.84647. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55769/0.92386. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56767/0.86769. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.54951/0.92164. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55165/0.93035. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.56282/0.88956. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56482/0.94224. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.54248/0.91683. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.53779/1.03228. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53965/0.95155. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55521/1.04228. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.55246/1.05877. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.52934/1.03629. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.54639/1.00652. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.53412/1.05373. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.50694/1.09737. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.51144/0.97188. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.51430/1.06592. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.51620/1.00302. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50396/1.03053. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.49074/1.01605. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48510/1.04144. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.47953/1.10527. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.48635/1.01582. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50618/1.06998. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.46745/1.11977. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.46313/1.12216. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.45702/1.16304. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45652/1.11925. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.45985/1.07945. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.45993/1.13899. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.45852/1.11059. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.43997/1.16987. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.44322/1.04699. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.46036/1.09320. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43200/1.14095. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.41749/1.26193. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.40445/1.21159. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.40280/1.30043. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.40624/1.24950. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.40407/1.21020. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.42616/1.22164. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.39990/1.16565. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.39011/1.24238. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.39008/1.23933. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.38324/1.30390. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.38426/1.36221. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.38724/1.29642. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.37049/1.25270. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.37571/1.35903. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.38474/1.27109. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.39483/1.28860. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.36550/1.30710. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.36074/1.24434. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.37508/1.30874. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.34821/1.39647. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.34944/1.25888. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.33821/1.32779. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.32259/1.35508. Took 0.43 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70397/0.73073. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69806/0.70784. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.68942/0.70217. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.68735/0.70990. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68546/0.73146. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68358/0.73612. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68302/0.74392. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67645/0.75225. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67860/0.75751. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67225/0.76373. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67954/0.75441. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67473/0.75931. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67421/0.74982. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67042/0.73290. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66604/0.72781. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66397/0.72641. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66847/0.73229. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66446/0.73159. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66365/0.72597. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66315/0.73476. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65921/0.74114. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66190/0.73877. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65991/0.73283. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65606/0.72539. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65138/0.72406. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64806/0.74325. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64343/0.74722. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63660/0.74427. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63205/0.76025. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63715/0.76063. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62859/0.78367. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.62357/0.80972. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.62320/0.82251. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61792/0.83798. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61908/0.87353. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.60843/0.88153. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.61400/0.90797. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60183/0.93030. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58489/0.95347. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59835/0.97182. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59486/0.96097. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.58812/0.95237. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.58414/0.92671. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57763/0.95995. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.56259/1.01346. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.56875/0.96187. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.57118/0.93276. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57285/0.98588. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.54460/1.00527. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54078/1.00654. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.55939/1.00682. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54446/1.08086. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54933/1.04636. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52431/1.10413. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53409/1.10096. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.52370/1.09352. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.52616/1.12551. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.51454/1.08995. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.51737/1.08443. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.49773/1.13262. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49131/1.14099. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.49056/1.17614. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.49756/1.16250. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.52195/1.05914. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.51503/1.10102. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50059/1.07333. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48303/1.12430. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48607/1.17586. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.47140/1.12800. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.46569/1.16065. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.45837/1.21967. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.44680/1.20655. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.45478/1.24545. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.44719/1.22145. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.43786/1.20933. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.43705/1.22638. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.42114/1.32020. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.44953/1.31046. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43804/1.22484. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46216/1.10140. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.43987/1.14839. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.42509/1.21358. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.41921/1.22155. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.43199/1.24521. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.40320/1.26551. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.39212/1.36853. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.38522/1.30383. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.38425/1.34981. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.42562/1.25752. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.39820/1.33552. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.37405/1.35622. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.39987/1.29009. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.36737/1.35998. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.37765/1.41260. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.36109/1.37905. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.35502/1.38834. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.38451/1.29810. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.34167/1.47510. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.35546/1.42549. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.35406/1.36205. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.71747/0.74247. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70980/0.70197. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70302/0.70463. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69786/0.72029. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.70732/0.69143. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.69897/0.69200. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69109/0.69459. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.69109/0.68973. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.69058/0.68393. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.69392/0.69988. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.69379/0.68596. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68200/0.68698. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68449/0.68754. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68693/0.67266. Took 0.48 sec\n",
      "Epoch 14, Loss(train/val) 0.68730/0.68028. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68378/0.67915. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.67685/0.67960. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67810/0.68080. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.67309/0.67644. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67296/0.67816. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.66703/0.67017. Took 0.48 sec\n",
      "Epoch 21, Loss(train/val) 0.66904/0.67415. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66710/0.67681. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.66484/0.68294. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66349/0.67410. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65976/0.67911. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.66061/0.64618. Took 0.47 sec\n",
      "Epoch 27, Loss(train/val) 0.66264/0.66768. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65436/0.67109. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65102/0.68972. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.65422/0.67765. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.65177/0.67401. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64871/0.69508. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.64870/0.68345. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.64567/0.68997. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.64589/0.69447. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.63919/0.69402. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.63602/0.69926. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.63761/0.70012. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.63257/0.70140. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62200/0.72888. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.63060/0.70781. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.62639/0.70199. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61990/0.72906. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.62267/0.71744. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.61470/0.71277. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.60882/0.73197. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.60117/0.73282. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.61075/0.73660. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.59529/0.75325. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.59376/0.76352. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.59695/0.75097. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.58413/0.76829. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.58411/0.78207. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.57747/0.78563. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.57518/0.78849. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.56880/0.79778. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54960/0.79789. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.55642/0.83417. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.55401/0.82997. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.55439/0.82850. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.53634/0.82656. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54598/0.87813. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.53944/0.81536. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.54424/0.83760. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.53005/0.91282. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51657/0.88819. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.52072/0.90238. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50737/0.89914. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.50886/0.85363. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.51643/0.93474. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.50686/0.94000. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.49881/0.86929. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.48494/1.00102. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.47804/0.96237. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49619/1.00901. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.47505/0.96547. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.47020/1.05327. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.47895/1.00280. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.47304/0.97442. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.46507/1.03693. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.44750/1.02464. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.43723/1.02963. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.43257/1.03194. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.43978/1.04155. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.43389/1.12083. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.41527/1.21889. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43309/1.15006. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.42291/1.17003. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.43743/1.11227. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.42568/1.12426. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.40007/1.16065. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.40201/1.14293. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.41606/1.15118. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.38662/1.16873. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.39556/1.21898. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.39881/1.21959. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.38643/1.25123. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.38553/1.22766. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.37826/1.21532. Took 0.44 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.70663/0.71816. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69988/0.72006. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69436/0.72420. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69370/0.73165. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68841/0.73459. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69201/0.74014. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68420/0.75227. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68434/0.74590. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67661/0.74901. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67384/0.75871. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67961/0.76306. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67105/0.78001. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66919/0.77943. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66229/0.80922. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66130/0.81115. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65960/0.80404. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65392/0.83696. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65230/0.86383. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64540/0.85620. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64927/0.86329. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64285/0.87640. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63536/0.87184. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.63699/0.87626. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62931/0.88853. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62565/0.87134. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63168/0.91008. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.61673/0.90472. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.61534/0.95285. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.61210/0.95680. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.61248/0.93396. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.60075/0.93584. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.59161/0.94963. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.59708/0.91830. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.58718/0.93046. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59543/0.90885. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58806/0.91593. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.56444/0.89689. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.57519/0.91808. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.56816/0.92554. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.55585/0.96722. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.56616/0.95309. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.55129/0.99034. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57565/0.93748. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.54354/0.96462. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.53413/0.95521. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.54155/0.97882. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.52849/1.01499. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.52491/0.98448. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.53034/0.98975. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52788/1.01026. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.52527/1.02690. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.51905/1.00969. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.50964/1.03145. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.49188/1.07456. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.49744/1.02926. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.50081/1.04487. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.49164/1.05453. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.50730/1.08623. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.48913/1.05693. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.47647/1.10357. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.48367/1.03804. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.46158/1.09022. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.46320/1.05144. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.44878/1.16403. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.44236/1.14415. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.43843/1.09225. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.43640/1.09181. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.43665/1.06643. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.42347/1.17133. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.44290/1.10246. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.42214/1.07134. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.39563/1.11288. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.40604/1.12336. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.39344/1.17777. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.40582/1.16931. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.40179/1.10313. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.39295/1.11723. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.38658/1.21455. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.36425/1.18916. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.37006/1.22138. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.36640/1.22830. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.38531/1.22668. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.35430/1.25315. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.34361/1.22294. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.35784/1.25532. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.32117/1.30271. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.33196/1.38226. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.37474/1.25417. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.32806/1.20674. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.31358/1.30160. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.31211/1.35831. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.29907/1.35331. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.28549/1.28752. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.27729/1.33195. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.28637/1.39193. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.29173/1.41572. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.29314/1.42152. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.29141/1.42015. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.27399/1.43124. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.29051/1.43813. Took 0.43 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69936/0.73011. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69770/0.73518. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69419/0.74169. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69159/0.73905. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69088/0.74014. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68911/0.73149. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68217/0.72837. Took 0.48 sec\n",
      "Epoch 7, Loss(train/val) 0.68172/0.72081. Took 0.49 sec\n",
      "Epoch 8, Loss(train/val) 0.68457/0.71938. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.68206/0.71718. Took 0.48 sec\n",
      "Epoch 10, Loss(train/val) 0.68233/0.71367. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.68085/0.72711. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67808/0.71831. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67354/0.74619. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67552/0.73393. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67269/0.73729. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67324/0.74071. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66717/0.73438. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66538/0.74088. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66606/0.74681. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66004/0.76511. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66285/0.77675. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65559/0.76190. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.65777/0.76184. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.65355/0.74673. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65176/0.76632. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64749/0.78272. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64079/0.79641. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.64835/0.76920. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64062/0.76357. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63413/0.76379. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62755/0.78244. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62164/0.77878. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61104/0.81891. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62019/0.79169. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.62899/0.82323. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.61260/0.80012. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.60697/0.79847. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.60588/0.77756. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.60117/0.78909. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59755/0.77124. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59121/0.78126. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58194/0.81097. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59532/0.82935. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.58401/0.78510. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56211/0.79222. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56009/0.80992. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56352/0.81579. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56912/0.85154. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56399/0.83566. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.55770/0.80800. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55590/0.85012. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.54348/0.78517. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52879/0.89365. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.54137/0.83924. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.54605/0.85771. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.53414/0.90506. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54477/0.87372. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.52805/0.86377. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50540/0.87339. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.52916/0.90180. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.50020/0.89582. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51862/0.90544. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.51598/0.86555. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.50882/0.91627. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.49131/0.89568. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.50858/0.96101. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.51026/0.93022. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.46809/0.93177. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47894/0.91264. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.46395/0.98952. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.46434/0.91643. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.45983/0.99316. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48424/0.92695. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.46436/0.99483. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.46052/0.99141. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.45537/0.92871. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.44829/0.96286. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.43102/1.00101. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.44212/1.02644. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.43646/1.08559. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.44446/1.05080. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.45118/1.05243. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.43044/1.05994. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.44165/0.99532. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.43833/1.07231. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.41980/1.09783. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.40567/1.11661. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.41535/1.10903. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.40406/1.10062. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.40533/1.12522. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.39886/1.17034. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.40174/1.15129. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.38568/1.22170. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.36841/1.21455. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.37898/1.22507. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.35609/1.20107. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.37489/1.17472. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.41697/1.03131. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.39743/1.19156. Took 0.44 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70560/0.70159. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69516/0.69138. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.68493/0.70107. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68764/0.71641. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68471/0.71645. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67795/0.71807. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68689/0.72943. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67970/0.72571. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67850/0.72769. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67108/0.74859. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67150/0.74957. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67335/0.80390. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67417/0.79202. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66380/0.82229. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66469/0.82093. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66376/0.82625. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67472/0.80548. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66076/0.83407. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65678/0.84335. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65232/0.85278. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65930/0.84873. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.65205/0.82863. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65049/0.84487. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.65016/0.84558. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.64495/0.84450. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64032/0.88014. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63557/0.85394. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63700/0.82767. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.64343/0.86492. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62762/0.86689. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63021/0.86908. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.62005/0.87461. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.62901/0.86167. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61818/0.87426. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.60822/0.89438. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60989/0.89191. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.60584/0.87736. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59742/0.87225. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.60508/0.87669. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59365/0.88878. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58411/0.88041. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59645/0.90657. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.58787/0.88338. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59083/0.88241. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57134/0.91265. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.57480/0.90720. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56790/0.87276. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57022/0.85068. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55873/0.86351. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55479/0.88497. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.55909/0.89734. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55285/0.87910. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.53874/0.89735. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.53399/0.90636. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53304/0.91795. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.52033/0.88341. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.51206/0.92812. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51937/0.90531. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.50116/0.92110. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.48162/0.88209. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.48993/0.92420. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.48246/0.91702. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48835/0.89772. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.48524/0.96690. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.48881/0.92569. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.46364/0.93196. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.46990/0.88492. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.47302/0.94541. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.44905/0.93764. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45684/0.89719. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.45892/0.91647. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45342/0.94246. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.42438/0.97229. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.43465/0.96357. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.41668/1.02078. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.41829/1.04626. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.42369/1.04350. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.39427/1.05957. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.40128/1.18132. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.39511/1.17437. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.39200/1.15382. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.40472/1.07360. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.38675/1.12867. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.36996/1.21830. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.37979/1.27817. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.36035/1.22984. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.38766/1.20299. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.36078/1.25836. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.36187/1.30523. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.36088/1.30164. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.37888/1.23383. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.35292/1.26107. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.33744/1.36377. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.35311/1.35369. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.31744/1.35094. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.31725/1.35942. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.29814/1.35171. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.33397/1.31687. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.31333/1.39865. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.35470/1.33824. Took 0.44 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.70980/0.73245. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69748/0.72610. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69485/0.72152. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68672/0.72043. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68381/0.71973. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.67989/0.72301. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68043/0.72400. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67718/0.71936. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.67151/0.72529. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66690/0.72382. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66417/0.72398. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66489/0.71624. Took 0.47 sec\n",
      "Epoch 12, Loss(train/val) 0.66100/0.73613. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65638/0.74124. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.65536/0.74987. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65638/0.75677. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.64844/0.75432. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65220/0.75548. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.64367/0.76721. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64237/0.75710. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64249/0.75427. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63262/0.75442. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.62865/0.75558. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.62474/0.74312. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.61532/0.76586. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.61793/0.76459. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.61614/0.79279. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.60530/0.80084. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.58990/0.81907. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.58504/0.82025. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.59231/0.82165. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.57422/0.84313. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.56495/0.83387. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.56588/0.86567. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.56100/0.88658. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.55303/0.90005. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.55240/0.88126. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.55328/0.91223. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.52846/0.93322. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.52311/0.91471. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.52437/0.91916. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.52626/0.92083. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.51543/0.93989. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.51202/0.91958. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.48676/0.98776. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.48421/1.00431. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.47955/0.98166. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.47283/0.97496. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.46611/0.99627. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.46744/1.02153. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.46112/1.01405. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.42493/1.03113. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.43674/0.98980. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.43970/1.04728. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.42102/1.03580. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.42751/1.03419. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.42752/1.01802. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.40467/1.01885. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.39174/1.10573. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.36987/1.08527. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.38283/1.08427. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.35784/1.15057. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.36885/1.13670. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.36966/1.11107. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.32621/1.15904. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.33489/1.11596. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.33031/1.13299. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.32206/1.19141. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.31751/1.24455. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.31271/1.18699. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.29092/1.30019. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.29527/1.23476. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.29344/1.26940. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.30309/1.25660. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.28438/1.27507. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.27530/1.31037. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.24851/1.30257. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.26699/1.39235. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.27351/1.26487. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.23537/1.32425. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.24055/1.54065. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.23801/1.40014. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.21632/1.41502. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.22287/1.41562. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.24419/1.56702. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.22738/1.48456. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.22812/1.45051. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.22067/1.56360. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.22332/1.47554. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.20486/1.49675. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.19712/1.61194. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.20058/1.51064. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.18583/1.55903. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.18716/1.53320. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.20991/1.56181. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.17639/1.49727. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.16423/1.61131. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.16311/1.69073. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.17100/1.71619. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.16838/1.74491. Took 0.44 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.70358/0.68195. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69672/0.67029. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69592/0.67374. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68547/0.67406. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69084/0.67393. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68180/0.67444. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67921/0.67370. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67924/0.66687. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.68195/0.67692. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67754/0.67262. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67466/0.67542. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67555/0.68144. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67685/0.66722. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67433/0.67389. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66779/0.67520. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66680/0.69556. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66683/0.68726. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.67081/0.69640. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66288/0.70100. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66078/0.71280. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65206/0.73011. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65331/0.70975. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64980/0.70504. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.63996/0.70141. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.64325/0.71174. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64169/0.70344. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63459/0.71240. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62545/0.73132. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62837/0.72320. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.61896/0.72432. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61549/0.74604. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.61944/0.72916. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60810/0.73004. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61402/0.74539. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60471/0.73137. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60398/0.74894. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60052/0.76073. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59756/0.75497. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.59179/0.78150. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59353/0.75728. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59200/0.77269. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57864/0.75926. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56980/0.79087. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57665/0.78153. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56465/0.78685. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57222/0.78695. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59585/0.72265. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57316/0.74510. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56271/0.76710. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55554/0.77479. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.55604/0.78878. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.54813/0.75725. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52931/0.74976. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.53194/0.76152. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.53852/0.76722. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.51680/0.73502. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.51897/0.76525. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.50301/0.80141. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.50562/0.76823. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.48670/0.77388. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49708/0.76215. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.50385/0.71303. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.49279/0.76860. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48041/0.79262. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.48377/0.78016. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.46283/0.78220. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.48926/0.78667. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.48200/0.78709. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.45796/0.78351. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.48686/0.78827. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.47662/0.82968. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.46319/0.81305. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.45367/0.83659. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.44163/0.86948. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.44473/0.84603. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44540/0.87198. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43769/0.86630. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.43236/0.82433. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.40999/0.90984. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.42313/0.92011. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.45588/0.90695. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.43081/0.92552. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.41424/0.94054. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.41980/0.91047. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.42375/0.96858. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.39554/0.87455. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.41102/0.84357. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.37764/0.92472. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.37515/0.93680. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.37432/0.96054. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36912/0.97444. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.38143/0.90016. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.37896/0.90245. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.35391/0.93368. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.36250/0.97119. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.37574/0.96771. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.37994/0.98221. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.36525/0.89107. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.37101/0.93091. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36725/0.94239. Took 0.44 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.70193/0.69051. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69543/0.69209. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.68953/0.68965. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68855/0.69848. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.67972/0.68698. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.67809/0.68259. Took 0.48 sec\n",
      "Epoch 6, Loss(train/val) 0.67652/0.68011. Took 0.47 sec\n",
      "Epoch 7, Loss(train/val) 0.67818/0.68879. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67000/0.70175. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.66403/0.70771. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66351/0.71159. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66012/0.71567. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.65960/0.70287. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.65079/0.70603. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.64553/0.70351. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.64034/0.70251. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.63990/0.71024. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.63226/0.71321. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.62319/0.71418. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.62171/0.72182. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.61587/0.72049. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.61638/0.71330. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.61293/0.73417. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.59982/0.72179. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.59474/0.72164. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.58968/0.76079. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.58376/0.75061. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.57827/0.74445. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.57664/0.79316. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.56251/0.79437. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.55437/0.81717. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.54896/0.81181. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.55199/0.79506. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.54489/0.85850. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.52781/0.84264. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.52577/0.84002. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.51990/0.86848. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.51695/0.87985. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.49685/0.90350. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.50666/0.88514. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.50396/0.89205. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.49273/0.89979. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.48532/0.93528. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.47130/0.97370. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.49076/0.95969. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.50039/0.99329. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.46094/0.92472. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.47068/0.96043. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.46219/1.01817. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.46041/0.97477. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.44087/1.07562. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.44068/1.04849. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.43868/1.09588. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.43860/1.01156. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.41363/1.07231. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.40651/1.13684. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.40168/1.08163. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.40262/1.10613. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.40328/1.19080. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.38912/1.22510. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.37559/1.18730. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.39500/1.22858. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.40259/1.21454. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.38250/1.20346. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.36846/1.23142. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.38527/1.29549. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.39799/1.26152. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.36448/1.21863. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.35554/1.25836. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.34811/1.28839. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.32623/1.28495. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.33861/1.32627. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.34192/1.38872. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.32785/1.55323. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.31074/1.37759. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.31065/1.50474. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.29581/1.43314. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.29426/1.56789. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.29188/1.62677. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.28664/1.61448. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.30129/1.65320. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.28223/1.65049. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.27440/1.61461. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.30149/1.52584. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.29197/1.46462. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.24907/1.56980. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.27219/1.62199. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.27377/1.78728. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.27253/1.78276. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.26362/1.53174. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.24614/1.76156. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.24866/1.68114. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.25198/1.61852. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.23249/1.76780. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.23184/1.75223. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.23295/1.66283. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.23210/1.92121. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.21903/1.86574. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.22362/1.87484. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.19133/1.96600. Took 0.43 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69911/0.71645. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.68868/0.72107. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68137/0.72930. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.67956/0.74252. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68165/0.74083. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.67848/0.75776. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67629/0.74795. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67379/0.76237. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67604/0.77149. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67348/0.77063. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.66757/0.79018. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66929/0.78711. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66173/0.79882. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65759/0.78713. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65711/0.79370. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.64900/0.80408. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.65330/0.81717. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64689/0.82378. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64172/0.82478. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64032/0.84847. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63747/0.83956. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63582/0.85690. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63702/0.83291. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.62854/0.86587. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62956/0.84795. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62244/0.86481. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62467/0.89310. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.62008/0.87596. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60783/0.87775. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60984/0.89118. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60627/0.90716. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.60243/0.92381. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61441/0.87455. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60373/0.89573. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59833/0.92930. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.60610/0.90366. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59974/0.90708. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58444/0.94614. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.58956/0.96474. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58534/0.98184. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57703/1.00292. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59193/0.95464. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.57388/0.94941. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.56427/1.04761. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56408/1.00546. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.56266/1.03184. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.55342/1.07070. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56464/1.00291. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.55685/1.03232. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56222/1.07413. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.55302/1.04295. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.54567/1.07394. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.54395/1.15579. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54719/1.02770. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53853/1.09760. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.54506/1.02762. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.53519/1.08362. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52829/1.07667. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.53732/1.07909. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.52107/1.13761. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52691/1.12879. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.51388/1.19500. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52016/1.16043. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.52470/1.10538. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.51762/1.10669. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.51471/1.19587. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.52209/1.14433. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.50486/1.21289. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52026/1.18176. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.50364/1.24611. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.47607/1.21541. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.50556/1.24408. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.49575/1.21976. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48979/1.21218. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.49945/1.19398. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.48494/1.22390. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.47905/1.18941. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.47595/1.19908. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.47370/1.23359. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.44124/1.32122. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.46174/1.27720. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.44326/1.31037. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.44712/1.30392. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.45058/1.31110. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.46068/1.27979. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.44280/1.27374. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43852/1.31081. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.44186/1.22777. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.45386/1.22265. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.43695/1.29081. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.43667/1.27660. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.41236/1.27642. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.43133/1.28364. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.45203/1.24466. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.43585/1.28536. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.47101/1.25247. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.44978/1.30745. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.44638/1.24669. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.45851/1.22489. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.41790/1.28432. Took 0.46 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69666/0.70048. Took 0.63 sec\n",
      "Epoch 1, Loss(train/val) 0.69485/0.69826. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.68869/0.70240. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68376/0.70360. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68661/0.70160. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68205/0.70218. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68003/0.68878. Took 0.48 sec\n",
      "Epoch 7, Loss(train/val) 0.67432/0.67951. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.67466/0.68496. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67209/0.69190. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67004/0.70494. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66885/0.74617. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67025/0.70349. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67112/0.75377. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66749/0.77602. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66578/0.71574. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66310/0.76791. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65717/0.75226. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65022/0.78418. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64375/0.83716. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64434/0.82295. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64255/0.79326. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63937/0.83164. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63165/0.87424. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63088/0.91188. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62808/0.86740. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63294/0.81821. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62558/0.86176. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61588/0.83703. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61927/0.84865. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.60131/0.87141. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60572/0.92674. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61029/0.78767. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60559/0.80000. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59452/0.83480. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58695/0.85368. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59388/0.89116. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59314/0.81477. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59580/0.79378. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.57924/0.83521. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.57472/0.88195. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59533/0.89903. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57809/0.84350. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.57454/0.89500. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55735/0.88767. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56667/0.95547. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55006/0.93802. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.55145/0.93056. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55057/0.89180. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.54959/0.90838. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.53075/0.90010. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.52754/0.96461. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.53470/0.95176. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.53793/0.95784. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51538/0.97589. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.52759/0.89333. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.51058/1.00725. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.50997/1.04344. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50145/1.06930. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.49738/1.10021. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.49471/1.11594. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.48801/1.08507. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48339/1.08669. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.47035/1.16433. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49250/1.14388. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.47435/1.11809. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.45524/1.17033. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.47160/1.12096. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.45355/1.28839. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47461/1.05534. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.45862/1.13876. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.43168/1.07888. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.42578/1.24948. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.43229/1.21942. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.44542/1.21737. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.43933/1.16846. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43158/1.21114. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.40370/1.20044. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.42880/1.26110. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.41546/1.18855. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.39610/1.33626. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.40674/1.33533. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.39066/1.25344. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.40009/1.28905. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.41334/1.20524. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.40719/1.26322. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43075/1.22134. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.42507/1.17727. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.40272/1.28133. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.37251/1.28629. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.36203/1.33315. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.38300/1.34378. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.38167/1.23206. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.36609/1.28105. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.37073/1.38732. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.36590/1.41521. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.37432/1.42552. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.36630/1.37652. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.35919/1.37647. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.34907/1.45641. Took 0.44 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.71846/0.71319. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.71076/0.71129. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.71085/0.70380. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69781/0.71777. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69670/0.71532. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69612/0.72682. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.70044/0.72111. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69865/0.71075. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69144/0.70840. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.69232/0.72551. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.69383/0.74048. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.69155/0.74272. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.68351/0.73936. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.68352/0.75360. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68430/0.75756. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67823/0.75257. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.68329/0.76511. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67450/0.75395. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67047/0.76086. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67065/0.76484. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67209/0.79913. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66773/0.80043. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66610/0.78784. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66129/0.78346. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.66166/0.75708. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65707/0.76814. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65797/0.77864. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.65251/0.77620. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65585/0.76880. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64098/0.77373. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64236/0.77607. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.64281/0.78390. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63929/0.79444. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.63452/0.83260. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.63845/0.79559. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.63121/0.83924. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.63561/0.83570. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62110/0.85180. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61571/0.86930. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61562/0.82616. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.60600/0.85439. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.59868/0.90397. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59746/0.90160. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59787/0.93700. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.59616/0.93200. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.59622/0.97133. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.61742/0.88411. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.59182/0.89519. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58047/0.94679. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.57960/0.93890. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57312/0.99583. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.56563/0.99597. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56659/0.97288. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.57241/0.93931. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55510/0.93220. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.54468/0.95059. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.54682/0.98310. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54039/0.97237. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.52973/0.98760. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53729/1.02072. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55147/1.01744. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.52950/0.97861. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52616/1.09684. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51340/0.98712. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.51110/1.07645. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.52533/1.03623. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51494/0.98395. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.52433/0.96740. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49964/0.93213. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.48866/1.03353. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.49896/1.00275. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48711/1.02875. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.49426/0.95816. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.47974/0.98835. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.48675/0.99827. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.45602/1.03998. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.47832/1.05717. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46570/1.01939. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.45270/1.04787. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44311/1.04382. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.46000/1.03305. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.46739/0.98986. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.45152/0.87107. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.44812/0.98477. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43255/1.00491. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.42359/1.07501. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.44040/0.99175. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.42494/1.04428. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.41545/1.06089. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.40599/1.03553. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.40196/1.05716. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.43051/1.03963. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.41952/0.99854. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.39884/1.01168. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.38675/0.98419. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.39870/0.99390. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.38853/1.03802. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.38326/0.98900. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.38063/1.11977. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.38262/1.04714. Took 0.44 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.73084/0.69616. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.70697/0.70802. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.70701/0.71502. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70604/0.69458. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.70413/0.74014. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69516/0.71937. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68869/0.70537. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67719/0.70209. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67728/0.70740. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67795/0.69626. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.68485/0.71971. Took 0.48 sec\n",
      "Epoch 11, Loss(train/val) 0.68123/0.71724. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67532/0.72494. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67949/0.72240. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66872/0.71292. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.67293/0.70797. Took 0.47 sec\n",
      "Epoch 16, Loss(train/val) 0.67472/0.69550. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66362/0.71578. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66891/0.71898. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66037/0.73150. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66632/0.73970. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65264/0.72498. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66059/0.73492. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65644/0.73696. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66188/0.74773. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65692/0.74148. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65167/0.73933. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64013/0.75565. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63746/0.75068. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63234/0.78183. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62941/0.76768. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61730/0.78849. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.62249/0.79017. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61652/0.80497. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61540/0.82512. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61001/0.83177. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.61532/0.82821. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60532/0.83152. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.59808/0.84828. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.60228/0.83466. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.58435/0.86456. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58164/0.85243. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58584/0.85379. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.58518/0.91489. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58021/0.89504. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.58410/0.87603. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.57663/0.88450. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.57858/0.89406. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56318/0.89654. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56138/0.88704. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.56460/0.89580. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56240/0.88911. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.55274/0.89895. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.54115/0.91923. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54902/0.89164. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.52975/0.92754. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53857/0.93420. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54093/0.94725. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.53495/0.95579. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52390/0.95805. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.51797/0.97627. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50768/0.99636. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52895/0.94768. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51992/0.91094. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.52692/0.94811. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.50097/0.95460. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.51740/1.01920. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.49093/0.96362. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.49470/1.00262. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.49500/1.02843. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.47336/1.01974. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.48410/1.01613. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48174/0.98861. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46232/1.07397. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.45546/1.06530. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.43568/1.08556. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46330/1.07659. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.45473/1.09664. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.46750/1.06490. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44432/1.11074. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.42494/1.17727. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.44584/1.28119. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.41909/1.15849. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.41935/1.36005. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.40627/1.25945. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.42672/1.25171. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.38189/1.33204. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.42708/1.39807. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.43453/1.24041. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.40470/1.35817. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.39203/1.35491. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.39236/1.44364. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.40118/1.34336. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.37178/1.46549. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.35574/1.44820. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.36035/1.44940. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.36612/1.52139. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.37644/1.47185. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.34905/1.44452. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34644/1.52793. Took 0.44 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.74841/0.70036. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.73599/0.73501. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.70923/0.72423. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.71730/0.71959. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.70228/0.71587. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69828/0.71752. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69400/0.71628. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69768/0.73317. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.68711/0.72196. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68567/0.72801. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68252/0.73917. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.68488/0.72280. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67562/0.76306. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.68490/0.75108. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67858/0.73479. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67042/0.73752. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67595/0.75547. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.68005/0.75477. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.67174/0.76562. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66977/0.76507. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67219/0.74714. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.67213/0.75081. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66068/0.79858. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.65803/0.79690. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.66288/0.80577. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65594/0.80368. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65194/0.80805. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64763/0.84936. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65628/0.84665. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.64358/0.83163. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.64596/0.85360. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.64085/0.85806. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.63171/0.92948. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.64154/0.88132. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62289/0.90400. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.64798/0.87460. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.61822/0.84606. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61563/0.92404. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.61540/0.98529. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.62497/1.00753. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60845/0.96578. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.60733/0.99273. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.59903/1.03337. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.61891/0.97399. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59231/1.02192. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.60626/0.96581. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.59043/0.97264. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.58403/1.05518. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.57911/1.03271. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57502/1.14660. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56860/1.12771. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.56554/1.17616. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.56531/1.16318. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55604/1.22638. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56846/1.16952. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55276/1.20185. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55086/1.20118. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.55569/1.19518. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.54327/1.22598. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53311/1.21254. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.53802/1.18397. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.55155/1.19469. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52939/1.22765. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.54300/1.32601. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.54103/1.19105. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.53473/1.25317. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.51738/1.23150. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.51781/1.32680. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52182/1.33163. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.50994/1.39706. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.48732/1.45906. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.49694/1.37298. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48492/1.58499. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.50569/1.39325. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.49660/1.46030. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49143/1.37409. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.50106/1.46725. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.48291/1.41698. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.49980/1.47622. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.47286/1.42066. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.49547/1.47967. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.47612/1.40583. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.45137/1.40051. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.47416/1.44463. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.47496/1.48761. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.44656/1.51518. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.44766/1.61180. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.45102/1.62113. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.43236/1.54162. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.43464/1.55846. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.45899/1.49768. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.43600/1.55118. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.42316/1.58209. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.41215/1.56867. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.42142/1.67933. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.41644/1.62500. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.42434/1.65320. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.41897/1.65642. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.40344/1.51565. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.39475/1.84836. Took 0.43 sec\n",
      "ACC: 0.375\n",
      "Epoch 0, Loss(train/val) 0.72263/0.69420. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.70958/0.68780. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69686/0.70871. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69284/0.70141. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69583/0.71465. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68998/0.72894. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68885/0.72259. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68512/0.71856. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68563/0.71055. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67475/0.70735. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67975/0.71574. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.67181/0.73745. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67059/0.73864. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66915/0.74059. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67379/0.76089. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66565/0.74973. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66066/0.76575. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65570/0.77183. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65172/0.78192. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65225/0.78330. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64386/0.78212. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64110/0.78268. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63928/0.79429. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63680/0.79901. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.63058/0.79993. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63882/0.80628. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61992/0.82571. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62375/0.83443. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63773/0.82725. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61665/0.86123. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62542/0.83680. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.61852/0.84699. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61337/0.88082. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60952/0.88869. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60425/0.91788. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59014/0.91298. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.59174/0.93537. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60332/0.93201. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58957/0.96902. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.58959/1.00625. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59383/0.94473. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57342/0.99932. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57620/1.04179. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.59167/1.00247. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.59121/0.97557. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56705/1.00137. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.56923/1.02351. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58267/0.99033. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55931/1.01251. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55527/0.99022. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56417/1.05360. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54733/1.09396. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.55762/1.03028. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.54192/1.05854. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54371/1.06932. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.52698/1.12322. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53017/1.09765. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.52717/1.09331. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.52370/1.05503. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52687/1.11118. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.51844/1.07459. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.51747/1.11039. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50881/1.08540. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.51339/1.12251. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49792/1.20559. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.49155/1.13618. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.49813/1.18728. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.47751/1.18156. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.51442/1.12616. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.49954/1.14269. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.47584/1.18185. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.47068/1.20106. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.48408/1.16653. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.48021/1.17792. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.46245/1.18101. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.44448/1.26651. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.46586/1.22993. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.45001/1.28170. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.45072/1.25736. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.43660/1.27967. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.44659/1.35244. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.43614/1.30573. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.44791/1.31465. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.41778/1.29884. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43260/1.32544. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.43396/1.26075. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.43432/1.30162. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.41204/1.34887. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.41019/1.31633. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.42464/1.33782. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.41987/1.33549. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.39416/1.35682. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.40388/1.38771. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.42158/1.29092. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.39457/1.30302. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.38662/1.39273. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.38778/1.43532. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.39472/1.46878. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.35665/1.44526. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.39000/1.43502. Took 0.44 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.70962/0.72304. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69910/0.71663. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68743/0.71346. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69010/0.70984. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68999/0.71212. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68072/0.69570. Took 0.48 sec\n",
      "Epoch 6, Loss(train/val) 0.68367/0.70326. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68186/0.71605. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67609/0.71744. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67215/0.71124. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67260/0.72484. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67420/0.73195. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67104/0.73204. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67839/0.73037. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66500/0.74181. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66212/0.73703. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66196/0.74829. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66784/0.75226. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66388/0.75575. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66282/0.76816. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65772/0.77266. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64963/0.80292. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65709/0.80091. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64660/0.80773. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64688/0.81056. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65065/0.83610. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63810/0.85111. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65069/0.86227. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63220/0.86024. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63679/0.87213. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63291/0.89286. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62729/0.89331. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63354/0.87695. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62485/0.88024. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62351/0.88959. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61566/0.88782. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60981/0.92924. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60727/0.94970. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.60241/0.95314. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.60203/0.97007. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61539/0.94056. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.60231/0.91936. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59854/0.93899. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59781/0.92915. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59883/0.97178. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.60118/0.92734. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.58167/0.96078. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58890/0.97413. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58746/0.93490. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.57705/0.96279. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56371/0.97534. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.57114/0.95956. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.55987/0.92866. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.57109/0.94543. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55719/0.90406. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.56078/0.93109. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55102/0.92152. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54916/0.93966. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.55368/0.87424. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54879/0.91265. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52648/0.93068. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.53949/0.88920. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54180/0.88963. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.52869/0.89724. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.53534/0.91292. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.52346/0.87196. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.52143/0.88058. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.52448/0.86138. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.51852/0.82942. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.52825/0.93634. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.52284/0.88365. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.50249/0.89524. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.49853/0.90398. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.50632/0.88164. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.50946/0.89679. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.50192/0.91825. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.49819/0.97483. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.48209/1.01600. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.48279/1.04932. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.47775/1.00781. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.45926/1.03328. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.45963/1.06089. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.45918/1.08567. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.47379/1.11259. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.46689/1.06885. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.45806/1.06681. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.47060/1.02170. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.47576/1.07119. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.45603/1.03915. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.45599/1.08625. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.45186/1.06648. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.44184/1.08628. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.44012/1.15760. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.42416/1.22692. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.43234/1.22984. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.43883/1.17740. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.43125/1.15924. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.41807/1.20670. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.42010/1.22756. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.39639/1.23942. Took 0.44 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.74759/0.72138. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.72023/0.69462. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70445/0.67681. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69736/0.66985. Took 0.49 sec\n",
      "Epoch 4, Loss(train/val) 0.69174/0.66987. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68918/0.65665. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.68842/0.66021. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.69135/0.65850. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67873/0.65432. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.68272/0.65713. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68421/0.66280. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68316/0.66744. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.68041/0.65394. Took 0.48 sec\n",
      "Epoch 13, Loss(train/val) 0.68504/0.67143. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67819/0.67695. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67212/0.67126. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67086/0.67964. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66870/0.67455. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66122/0.68628. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66386/0.70332. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66350/0.70024. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66854/0.69187. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.67128/0.68671. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65781/0.70736. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66035/0.70883. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65830/0.70248. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65000/0.70607. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64706/0.70505. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64841/0.70105. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.64324/0.71282. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.64750/0.71243. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.64923/0.71963. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.64076/0.71973. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63544/0.73834. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.63266/0.74114. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.63323/0.74799. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.63238/0.72636. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.62334/0.73228. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62565/0.74730. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.61675/0.73298. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62528/0.74150. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61263/0.74020. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.61296/0.75642. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61888/0.75153. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60974/0.75550. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.61190/0.77313. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.60449/0.79205. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58607/0.81518. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.58765/0.81841. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57746/0.83510. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.58394/0.82864. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.58786/0.83199. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.56191/0.84539. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56860/0.85389. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.56259/0.84166. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.56462/0.84469. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56088/0.85046. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54840/0.88390. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.55828/0.88244. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.55482/0.89019. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55863/0.88245. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.54955/0.86598. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54567/0.91186. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.53308/0.89974. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.52530/0.88885. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.51386/0.95256. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.52261/0.96877. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.49760/0.97233. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50050/0.99457. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.49336/0.98454. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48534/1.03443. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.49910/1.00286. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48190/1.00061. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.48035/0.99243. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.47347/1.00903. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.48954/1.05230. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.47921/0.97598. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.47319/1.11985. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.47585/1.09625. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46963/1.06000. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.45305/1.10530. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.47513/1.10470. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.46694/1.06575. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.46079/1.09570. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.45260/1.12526. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.43683/1.17814. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.44554/1.08148. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.44409/1.11062. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.43615/1.18744. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.43075/1.11723. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.45840/1.08682. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.44042/1.06439. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.43401/1.06411. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.42031/1.13823. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.39893/1.16895. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.40060/1.18265. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.38443/1.26640. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.39481/1.27044. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.38764/1.20903. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.37447/1.24412. Took 0.44 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.70987/0.72250. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.71308/0.72069. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.70572/0.75425. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70323/0.73309. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68558/0.75139. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68607/0.72753. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68123/0.73962. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68446/0.73678. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67328/0.74142. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67267/0.74597. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67148/0.75690. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67052/0.77361. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67496/0.76366. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66761/0.76564. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66154/0.77348. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66273/0.77380. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66346/0.76776. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65871/0.78316. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65327/0.79190. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65708/0.78684. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65911/0.79400. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65502/0.75786. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65914/0.78597. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65162/0.77796. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.65588/0.77781. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63594/0.79454. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64180/0.78639. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63045/0.78417. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63346/0.75904. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63544/0.77516. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63856/0.75229. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62608/0.73508. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.63002/0.75442. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62183/0.75758. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.61425/0.74449. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.62629/0.75233. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61925/0.75921. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60069/0.76455. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60896/0.78585. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60386/0.78402. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.59599/0.80239. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60524/0.78217. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59852/0.78125. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58472/0.79505. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.58840/0.84821. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.58521/0.79642. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58793/0.76857. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58108/0.80062. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58174/0.82334. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56489/0.83253. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.58128/0.83287. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.57969/0.82190. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.57358/0.83517. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.55904/0.82900. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54836/0.84394. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.54113/0.87444. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.54605/0.88573. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54967/0.86205. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.54884/0.89299. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.53146/0.88959. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.54432/0.91087. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.53362/0.89342. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51321/0.90669. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.53893/0.88489. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.53529/0.93434. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.52295/0.90786. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51747/0.98203. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.51199/0.97868. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.51574/0.97305. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.51223/0.98006. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.52404/0.93949. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.50939/1.00864. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.51097/0.99357. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.51586/0.96017. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.49805/1.03320. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.51075/1.02839. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.50887/1.00190. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.49819/1.03702. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.48007/1.05944. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.50198/1.05041. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.47348/1.07871. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.47760/1.04881. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.48782/1.03371. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.47683/1.09619. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.50075/1.08096. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.48791/1.00511. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.47637/1.09356. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.45484/1.10146. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.46466/1.11796. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.48776/1.04692. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.45294/1.13983. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.43254/1.17922. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.45547/1.10450. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.41776/1.17837. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.41967/1.21901. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.44084/1.20921. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.43240/1.20714. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.40083/1.23990. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.41593/1.25369. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.42618/1.21051. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.72436/0.68162. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.71703/0.68123. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70796/0.69157. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.70216/0.68900. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.70039/0.69722. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.70174/0.67735. Took 0.48 sec\n",
      "Epoch 6, Loss(train/val) 0.69890/0.68626. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68833/0.69050. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68626/0.70751. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68324/0.69866. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68408/0.71918. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68094/0.71062. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67798/0.73023. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68631/0.70646. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.67886/0.72425. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67404/0.72501. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67080/0.73551. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.66542/0.73447. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66708/0.73317. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66776/0.75297. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66104/0.75887. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66830/0.75753. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66047/0.75263. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66171/0.78041. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65379/0.78834. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.66113/0.78668. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65618/0.78035. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.66168/0.81154. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.66121/0.77467. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.65401/0.81087. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64819/0.81838. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64945/0.81008. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63883/0.85647. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63087/0.81850. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.63755/0.84262. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.63256/0.85390. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.64267/0.83352. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61834/0.88387. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62980/0.87810. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.63207/0.85428. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.63171/0.86398. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.64099/0.80701. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.62204/0.85151. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.61591/0.85970. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.60166/0.87694. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.60574/0.89251. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.61412/0.87548. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.61882/0.86735. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.60845/0.86593. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.60337/0.85343. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.60194/0.89640. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.58019/0.89027. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.58605/0.88634. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.59479/0.90984. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.58213/0.88511. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.57614/0.88493. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.57157/0.93166. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.57025/0.92288. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.57952/0.89096. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.57621/0.89349. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.56451/0.90615. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.56849/0.89347. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.57289/0.89434. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.55529/0.89234. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.55680/0.98930. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.54733/0.89434. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.54959/0.92551. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.54826/0.96185. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.54905/0.93431. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.53116/1.01527. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.53302/0.95450. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.51758/1.03547. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.51899/1.01615. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.53417/0.99053. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.52378/0.98476. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.51788/1.11365. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.51131/1.06920. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.51208/1.04935. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.50047/1.04647. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.51817/1.05345. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.50477/1.02579. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.50106/1.04295. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.48543/1.04591. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.49251/1.20160. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.49073/1.07397. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.48549/1.09925. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.47997/1.09432. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.49292/1.17198. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.47831/1.14913. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.45583/1.05699. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.44993/1.16109. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.45185/1.21987. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.45852/1.15408. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.47128/1.14061. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.46717/1.12017. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.45207/1.09691. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.41213/1.26645. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.44415/1.23814. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.41027/1.15344. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.41995/1.36263. Took 0.43 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69930/0.67412. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69570/0.67179. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69317/0.66979. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69218/0.66866. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68786/0.67064. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68629/0.67510. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68285/0.67332. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67974/0.67071. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67866/0.66860. Took 0.49 sec\n",
      "Epoch 9, Loss(train/val) 0.67641/0.67827. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67208/0.68554. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66673/0.69860. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66749/0.70103. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66489/0.70492. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66339/0.69664. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65782/0.69381. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65076/0.70345. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64909/0.70901. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64258/0.70619. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64246/0.71004. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.63832/0.69363. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63049/0.67476. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.62774/0.68270. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62291/0.70770. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.61333/0.68518. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.62483/0.70090. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.60710/0.68645. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.60306/0.68690. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.59869/0.70543. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.58407/0.70092. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.58790/0.73673. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.58348/0.73531. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.57290/0.72557. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.56066/0.71387. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.54830/0.73960. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.57179/0.67807. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.54619/0.74828. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.54544/0.81309. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.53668/0.77188. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.52499/0.79924. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.54640/0.80225. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.53404/0.82399. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.51304/0.79082. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.50519/0.81811. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.50331/0.82965. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.50570/0.79320. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.51011/0.82331. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.49309/0.83020. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.49551/0.88080. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.48900/0.83469. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.46785/0.82745. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.47045/0.81815. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.47606/0.84043. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.52194/0.84593. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.49120/0.82594. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.47704/0.88186. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.45260/0.88816. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.44514/0.90832. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.45830/0.90721. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.42995/0.92182. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.42758/0.93272. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.42152/0.95210. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.42376/0.94593. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.44479/0.94854. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.40219/1.01138. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.40055/0.96250. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.40795/0.99631. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.40384/0.94260. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.39660/1.05722. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.39164/1.03220. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.38455/0.97286. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.38660/0.98436. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.37988/1.06055. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.38342/1.00545. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.37729/1.12568. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.36677/1.05040. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.35206/1.08212. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.35711/1.11376. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.32102/1.14128. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.34864/1.08155. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.34888/1.21388. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.36046/1.18034. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.32684/1.19398. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.31447/1.18075. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.32221/1.20728. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.30571/1.24828. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.33498/1.20197. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.31859/1.19206. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.31430/1.16497. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.29761/1.11603. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.30353/1.23540. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.28162/1.24171. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.32521/1.21693. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.40981/1.15319. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.35814/1.12885. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.31523/1.13750. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.30564/1.16729. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.29012/1.20296. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.27953/1.25968. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.28924/1.15439. Took 0.43 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69993/0.69368. Took 0.60 sec\n",
      "Epoch 1, Loss(train/val) 0.69511/0.70859. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69776/0.71396. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68928/0.72246. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69140/0.72606. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68730/0.74153. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68609/0.74047. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67829/0.76092. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67866/0.74156. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67618/0.75107. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67852/0.76024. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67259/0.74827. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67684/0.76370. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67054/0.75286. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67423/0.75423. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66779/0.74139. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66236/0.76738. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65577/0.77361. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65180/0.78384. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65267/0.79585. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63967/0.82257. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63878/0.80518. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63951/0.81994. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.63114/0.82343. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62073/0.85571. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62487/0.86775. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62650/0.87438. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.62270/0.89623. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61598/0.90562. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61122/0.90051. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.60806/0.92862. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60624/0.91624. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.60117/0.91063. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.58851/0.93888. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59711/0.90681. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58124/0.94915. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59587/0.91228. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.58793/0.92252. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58575/0.92798. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58289/0.95813. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.56650/0.95990. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.57062/0.99449. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56737/0.96093. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.56076/0.99353. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.55181/1.00489. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56173/1.01222. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.55792/1.03351. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.54899/1.02926. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55999/0.98890. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55045/0.98323. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.54035/1.02184. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53347/1.04738. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52052/1.10384. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.52473/1.08218. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.52837/1.07971. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51642/1.12468. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.49579/1.16758. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.49951/1.22068. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.48255/1.23754. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.49106/1.28082. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.48163/1.22504. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.47977/1.22579. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49092/1.32824. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.48563/1.24662. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.46819/1.24983. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.45773/1.25757. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.45817/1.23819. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.43973/1.29875. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.45379/1.28993. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.44723/1.27121. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.46769/1.34476. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45129/1.32627. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.43525/1.29544. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.42824/1.29329. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.42184/1.29297. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.41935/1.34015. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.40017/1.29948. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.39608/1.36366. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.42201/1.29802. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.39357/1.42880. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.39089/1.46824. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.37282/1.41169. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.35836/1.42249. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.37033/1.45795. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.37448/1.52846. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.36313/1.52642. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.35705/1.46393. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.36314/1.44667. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.34193/1.46882. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.33230/1.57676. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.33486/1.58727. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.32501/1.69402. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.33991/1.72689. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.33258/1.65075. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.30657/1.83960. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.32433/1.77448. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.31043/1.68738. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.30007/1.80443. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.29714/1.76640. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.29952/1.87633. Took 0.43 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.70806/0.69337. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69986/0.70259. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69927/0.70562. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69425/0.68642. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69083/0.69439. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69023/0.69678. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.68153/0.69710. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69071/0.69371. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68158/0.70928. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67761/0.71200. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68280/0.70374. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67813/0.70578. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.68358/0.70667. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68300/0.70724. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67611/0.70358. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67467/0.70796. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67146/0.70384. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66635/0.72523. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66932/0.72908. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66370/0.73571. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.66261/0.73627. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65794/0.75032. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66450/0.74178. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.66690/0.75649. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65341/0.76581. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64990/0.77392. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.64840/0.75559. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64414/0.77277. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64349/0.75642. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.63994/0.76829. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64211/0.78504. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63416/0.76955. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64001/0.78848. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.63237/0.78392. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.63104/0.77998. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62576/0.77849. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.61100/0.78793. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.63568/0.80115. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.61824/0.76889. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61359/0.79470. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60106/0.82249. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59524/0.83094. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.59061/0.85138. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58006/0.88874. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59833/0.85900. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.59078/0.85001. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57286/0.91991. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57979/0.90260. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56644/0.95936. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56432/0.98386. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55344/0.96189. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.55882/0.98702. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.55018/0.92440. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55482/0.96958. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.54625/0.97939. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.53676/0.97891. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53674/0.96827. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.51464/1.04579. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.53074/1.05029. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52011/1.07604. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52391/1.02525. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.50384/1.07447. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51712/1.10278. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.52092/1.06067. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.52764/1.08917. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.51815/1.10809. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.51677/1.07876. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.50402/1.05278. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.49071/1.13069. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.48051/1.17992. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48277/1.13328. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48576/1.11896. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.48836/1.17617. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.49373/1.15934. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.47405/1.20553. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.48262/1.12690. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.45195/1.23399. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.44604/1.24999. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44756/1.20467. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.44584/1.26227. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.43256/1.25568. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.43879/1.26619. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.44844/1.27801. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.44038/1.24000. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.42447/1.28102. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.44270/1.24718. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43505/1.25375. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43535/1.27120. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.40016/1.31351. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.39293/1.33714. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.39270/1.29363. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.41613/1.28933. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.37208/1.42611. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.39068/1.41019. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.42768/1.31774. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.40139/1.36565. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.36664/1.38461. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.36264/1.35649. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.35375/1.39124. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36229/1.46184. Took 0.45 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.74966/0.69594. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.72515/0.72279. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.72756/0.73475. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.72807/0.71130. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.70868/0.70759. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.70913/0.70115. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.70436/0.70688. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.69628/0.70931. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.69539/0.71377. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.69385/0.70580. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.70081/0.71945. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.70813/0.70068. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.70234/0.70265. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.69786/0.71963. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.69309/0.71237. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.69198/0.71687. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.68925/0.72130. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.69551/0.71700. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.68982/0.72534. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.68549/0.72649. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.68639/0.74596. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.69223/0.71143. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.69032/0.71626. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.69488/0.71002. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.68943/0.71316. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.68625/0.70017. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.67763/0.70246. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.68531/0.70074. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.68156/0.71062. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.67590/0.70303. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.67492/0.69007. Took 0.47 sec\n",
      "Epoch 31, Loss(train/val) 0.67405/0.69119. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.67934/0.69148. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.66918/0.68839. Took 0.49 sec\n",
      "Epoch 34, Loss(train/val) 0.66843/0.69390. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.66323/0.70369. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.66443/0.69679. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.66599/0.69979. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.65452/0.71292. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.65435/0.71429. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.66217/0.71748. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.65211/0.71038. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.64914/0.69597. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.63955/0.73624. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.63736/0.72565. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.64322/0.70347. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.63646/0.70297. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.63515/0.71578. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.63611/0.72439. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.62402/0.72792. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.62693/0.73189. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.61169/0.73343. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.62561/0.76880. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.61836/0.74057. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.62955/0.78492. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.60452/0.75842. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.60191/0.81702. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.60400/0.87669. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.59950/0.87133. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.59798/0.93610. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.59066/0.98638. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.59714/0.92487. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.59124/0.95213. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.60040/0.95146. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.57926/0.98418. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.57625/0.96979. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.57749/1.00249. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.57416/0.99544. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.56600/0.94937. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.57282/0.94311. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.57102/0.98121. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.56039/0.99327. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.55681/0.93887. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.55432/1.06118. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.55201/1.00482. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.55147/1.06514. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.53896/0.96382. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.54007/1.07842. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.54782/1.01568. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.50786/1.10781. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.54271/1.09914. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.52120/1.05417. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.51380/1.11072. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.52174/1.09612. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.50523/1.00643. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.51454/1.04714. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.50386/0.99714. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.49434/1.10432. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.47457/1.04507. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.48621/1.09490. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.47995/1.12678. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.45868/1.07528. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.47765/1.13536. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.47617/1.14468. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.46649/1.19039. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.44963/1.16308. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.46331/1.20277. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.45098/1.17777. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.46722/1.07111. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.46373/1.05894. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.72557/0.73415. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.71485/0.73762. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.71211/0.74359. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.70943/0.74378. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.70950/0.74535. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.70767/0.74193. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.70402/0.76705. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.70774/0.77174. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.70043/0.78674. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.69957/0.79107. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.69846/0.77942. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.69529/0.78140. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.69899/0.79552. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.69364/0.80485. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.69033/0.83167. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68186/0.86251. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.68975/0.85646. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.68328/0.85727. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.68087/0.86230. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.68008/0.87571. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67646/0.87731. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66814/0.88814. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66851/0.91195. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.66427/0.89575. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.66124/0.90647. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66389/0.89235. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.66279/0.88778. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.65728/0.89671. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65021/0.91254. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65508/0.88150. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64799/0.89780. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63976/0.91112. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.64026/0.90341. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64411/0.88785. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62940/0.92023. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.62967/0.91917. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62462/0.92021. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61989/0.91474. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.61789/0.91392. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.62693/0.91503. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.61492/0.93139. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61670/0.94746. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.61141/0.95305. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.60522/0.94805. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60462/0.97452. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59452/0.97381. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.58263/1.02328. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58054/1.01293. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58828/1.01672. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.57834/1.02200. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.58971/1.01988. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.56145/1.00739. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.56187/1.09353. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55682/1.04379. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55756/1.05507. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55059/1.06577. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.53860/1.09373. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54779/1.05106. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.53108/1.10476. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.52777/1.10839. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55244/1.07033. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.52358/1.12229. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.52821/1.17326. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.53638/1.08756. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50854/1.10886. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.51893/1.10749. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.51089/1.16831. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.51901/1.19848. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50017/1.21917. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.49567/1.22503. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.47925/1.31679. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48834/1.30083. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.51027/1.28281. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48789/1.25161. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.48238/1.23200. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49886/1.30049. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.47764/1.28728. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46076/1.36972. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.45232/1.34297. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.49253/1.29267. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.46580/1.28416. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.48054/1.32836. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.44220/1.35351. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.44515/1.40839. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.44449/1.39702. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.44568/1.29123. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.44921/1.38306. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.44551/1.31364. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.42668/1.40464. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.41894/1.39764. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.43908/1.33544. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.43499/1.30322. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.41127/1.33948. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.40244/1.44481. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.41578/1.41755. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.40541/1.44660. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.39877/1.48239. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.41777/1.41751. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.37873/1.48871. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.39310/1.49273. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.72768/0.74019. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.74144/0.72507. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.73390/0.72070. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.72094/0.68302. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.71395/0.69789. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.72454/0.69309. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.70077/0.68813. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.70472/0.68424. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.69282/0.69033. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.69529/0.69680. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.70387/0.69277. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.69926/0.70918. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.68572/0.72326. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.69785/0.71131. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.69400/0.70859. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.69076/0.69765. Took 0.47 sec\n",
      "Epoch 16, Loss(train/val) 0.69367/0.70833. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.69118/0.71382. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.68099/0.72132. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.68711/0.72139. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.68683/0.70701. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.68314/0.71852. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.69047/0.72118. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.68658/0.72201. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.68703/0.72557. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.67696/0.73855. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.69281/0.74562. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.68073/0.73797. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.68074/0.73817. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.67958/0.73164. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.68365/0.73969. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.67417/0.71833. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.67497/0.72725. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.67899/0.72452. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.68024/0.72622. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.66469/0.73618. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.67252/0.75090. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.66756/0.75684. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.67248/0.75371. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.67300/0.77328. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.66589/0.76812. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.65723/0.77146. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.65694/0.76949. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.65251/0.77461. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.66356/0.77140. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.65573/0.77380. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.65980/0.75853. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.64905/0.75857. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.64916/0.76197. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.64226/0.75114. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.64865/0.76133. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.64093/0.76572. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.63858/0.77006. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.63301/0.78537. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.62678/0.80823. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.62351/0.81635. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.62577/0.83508. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.61767/0.83539. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.62580/0.84048. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.61329/0.84902. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.61327/0.87834. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.60255/0.86164. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.59254/0.88761. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.59186/0.91837. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.58884/0.92517. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.58553/0.94880. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.56993/0.97061. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.56737/1.00646. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.57303/0.98549. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.56197/1.01849. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.56908/1.00745. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.56640/1.01338. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.56554/0.98503. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.55593/1.04784. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.54634/1.04379. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.55190/1.07879. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.53980/1.08577. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.52981/1.09578. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.53080/1.07410. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.53816/1.08812. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.53605/1.08271. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.53046/1.06965. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.52529/1.07571. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.52396/1.08318. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.50631/1.08562. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.51812/1.04679. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.50373/1.06757. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.49624/1.10989. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.48900/1.17994. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.48183/1.13645. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.48551/1.09952. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.49636/1.12129. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.47977/1.09963. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.48166/1.17892. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.45861/1.16611. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.47005/1.16518. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.45695/1.21248. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.47244/1.18856. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.46522/1.33257. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.48228/1.19884. Took 0.45 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69476/0.70402. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69365/0.71217. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68630/0.71274. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68333/0.72019. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68198/0.71361. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69103/0.71513. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68532/0.71597. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68144/0.71757. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.68344/0.72558. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68514/0.72404. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68107/0.72193. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68145/0.72129. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67664/0.73754. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68138/0.74657. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67569/0.75320. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68014/0.75207. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.67191/0.76314. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67016/0.76518. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.66053/0.77899. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67021/0.78046. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65929/0.77565. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66300/0.75180. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64862/0.77057. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64865/0.79096. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65645/0.78927. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65407/0.79692. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.65029/0.80387. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.65124/0.81923. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64150/0.80385. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.63542/0.80905. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63328/0.81789. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63574/0.84232. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62830/0.84424. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62897/0.87133. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.62097/0.88412. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61771/0.88907. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.61748/0.90668. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61473/0.87501. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.60687/0.88426. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.61272/0.91376. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60718/0.90822. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60006/0.90685. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59040/0.90833. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.57859/0.88302. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58864/0.88712. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57749/0.91232. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.56771/0.90378. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57032/0.91455. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55593/0.93172. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55384/0.93268. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.57190/0.89472. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54357/0.93671. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.54750/0.94242. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.53239/0.94712. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.54516/0.91223. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51527/0.93579. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.51467/0.94536. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.50358/0.95478. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.51766/0.97478. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.51003/0.94121. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50402/1.00966. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.49308/1.01393. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48051/1.07008. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.49210/0.97113. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.47004/1.03960. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.46168/1.02376. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.46693/1.05832. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.45422/1.07149. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.48561/1.04865. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.45481/1.03273. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.44416/1.05468. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.42833/1.07641. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.43149/1.10546. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.43257/1.09041. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.41781/1.14309. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.43684/1.12341. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.40385/1.05332. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.40172/1.09359. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.41173/1.07927. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.40692/1.16829. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.40860/1.08673. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.38116/1.21808. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.39589/1.19560. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.36701/1.25483. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.38582/1.21037. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.37588/1.24655. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.37169/1.24558. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.36725/1.26510. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.35252/1.26984. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.34775/1.27680. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.35978/1.31055. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.32278/1.35338. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.32424/1.36646. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.32538/1.31292. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.32015/1.39400. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.30832/1.38874. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.31394/1.47563. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.30352/1.43877. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.29107/1.45842. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.29916/1.50924. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69080/0.69794. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68541/0.69520. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.68457/0.69745. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68573/0.70398. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.68206/0.70641. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68207/0.71306. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68215/0.71434. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67821/0.71222. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67532/0.72456. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67366/0.74280. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66894/0.74984. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67086/0.74665. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66510/0.75660. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.66056/0.78021. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.65917/0.77545. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65655/0.79995. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65445/0.80747. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65712/0.80033. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64807/0.80446. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64198/0.82879. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.63314/0.85111. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.62190/0.87482. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.62869/0.85725. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62347/0.85694. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.62569/0.85119. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.61950/0.93428. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.60702/0.92402. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.61096/0.90362. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60672/0.90848. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.60299/0.89863. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60305/0.93303. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.58465/0.96681. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.57859/0.93871. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.56251/0.97393. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.55760/1.00841. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.54958/0.99858. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.55697/0.97660. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.55574/0.96601. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.53860/1.00879. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.54605/1.03466. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.53778/0.97683. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.53720/1.01109. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.52624/1.06906. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.51794/1.04137. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.52341/1.03707. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.50921/1.00020. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.50302/1.00024. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.49226/1.05874. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.49153/1.10327. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.50943/0.99032. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.48737/1.05847. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.48794/1.08846. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.51357/1.08530. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.49075/1.03122. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.48552/1.04484. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.47368/1.14372. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.46787/1.10709. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.46879/1.11333. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.47820/1.11306. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.51837/1.01284. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50548/1.03107. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.46925/0.98328. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.47447/1.12488. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.46442/1.15138. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.44640/1.25294. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.43343/1.23557. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.45405/1.17764. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.45506/1.17361. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.43667/1.18310. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.43684/1.26815. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.43319/1.24467. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.43458/1.23517. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.42006/1.20352. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.46132/1.16832. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.42120/1.40146. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.39490/1.42437. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.41509/1.30401. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.40765/1.41455. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.38893/1.41963. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.37730/1.50903. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.38647/1.37125. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.39036/1.46431. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.37670/1.51648. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40472/1.43468. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.39547/1.41847. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.35785/1.61707. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.35779/1.44070. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.36794/1.53123. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.35952/1.63482. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.37383/1.56712. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.33051/1.52996. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.33895/1.55849. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.31941/1.42775. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.33719/1.65320. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.34210/1.60316. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.31797/1.63428. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.32017/1.61949. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.33151/1.62389. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.34625/1.51953. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.31654/1.52539. Took 0.44 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.71566/0.72081. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70494/0.70635. Took 0.50 sec\n",
      "Epoch 2, Loss(train/val) 0.70015/0.69770. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69608/0.68694. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.68791/0.68813. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69292/0.68464. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.68653/0.68599. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68055/0.68775. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68301/0.67909. Took 0.48 sec\n",
      "Epoch 9, Loss(train/val) 0.67883/0.68040. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67530/0.68459. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66762/0.67146. Took 0.50 sec\n",
      "Epoch 12, Loss(train/val) 0.66529/0.67195. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.66153/0.67473. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65677/0.68297. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.65642/0.69098. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66082/0.67957. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.64634/0.67946. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.64630/0.70514. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.63440/0.71866. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.62999/0.73194. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.62647/0.72722. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.63435/0.70769. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62692/0.70994. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62967/0.71295. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.61902/0.71641. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.60816/0.72349. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61451/0.73391. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60604/0.73379. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60199/0.72519. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60562/0.71099. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.60807/0.71515. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60011/0.69641. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59390/0.68138. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59605/0.67062. Took 0.47 sec\n",
      "Epoch 35, Loss(train/val) 0.59423/0.70128. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58647/0.67910. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58567/0.69230. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.58893/0.69817. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57093/0.68577. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59395/0.69695. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59775/0.67756. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58200/0.68740. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.57724/0.66618. Took 0.48 sec\n",
      "Epoch 44, Loss(train/val) 0.57841/0.67289. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.57167/0.68759. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56704/0.66813. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56643/0.69146. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56765/0.68147. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.55897/0.69746. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.54455/0.69529. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54393/0.73441. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.55347/0.73399. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.53510/0.74287. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53012/0.74441. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.52536/0.74402. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.52894/0.74805. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51936/0.72329. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.52839/0.75651. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.52939/0.76791. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52399/0.76927. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.51862/0.81152. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51057/0.82450. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.50679/0.83661. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.51143/0.79481. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.48454/0.81234. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48243/0.84112. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.48085/0.82316. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50119/0.85036. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47020/0.87899. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.46937/0.85392. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.47766/0.82976. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.48049/0.87017. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.47150/0.85555. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.44270/0.90773. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.45821/0.86823. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46278/0.88898. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46702/0.84053. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44483/0.91011. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.45043/0.85325. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.43599/0.94562. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.47935/0.95797. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.46010/0.94389. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.45148/0.87687. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.42245/0.92119. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.41413/0.96968. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.41442/0.95964. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.41376/1.01863. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.41095/0.96346. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.38975/1.01707. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.39976/1.02743. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.40086/1.01543. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.39496/0.98354. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.38972/1.02426. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.44296/1.09504. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.41454/0.99312. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.38824/1.01359. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.37933/1.03830. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.39532/1.03759. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36989/1.03595. Took 0.44 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.70190/0.71035. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69805/0.71314. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69691/0.71936. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69278/0.71009. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.69449/0.70721. Took 0.48 sec\n",
      "Epoch 5, Loss(train/val) 0.68672/0.70784. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68420/0.71545. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68561/0.71751. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67680/0.74737. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67530/0.75002. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67837/0.74458. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66678/0.75196. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66748/0.77321. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.65956/0.77718. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.65645/0.77782. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.64931/0.76664. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.64501/0.78133. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.63396/0.78659. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.63587/0.78648. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.63259/0.79440. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63278/0.79015. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.62232/0.78718. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.62771/0.80481. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.62251/0.78894. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.62045/0.78362. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.60668/0.79733. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61125/0.78148. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.60995/0.79692. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61147/0.80538. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.59803/0.77696. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.59003/0.79599. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.58434/0.81639. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.57957/0.82702. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.57230/0.86225. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.57308/0.85039. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.57418/0.81920. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.56574/0.82815. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57741/0.82423. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.56882/0.81615. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.55012/0.82867. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.55613/0.86479. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.55279/0.83472. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.55615/0.86656. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.55380/0.89568. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.53765/0.87863. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.52671/0.84366. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.53450/0.88324. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.53805/0.87128. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.51883/0.85838. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.51343/0.94856. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.52366/0.89217. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.51035/0.94559. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.50378/0.97894. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.49256/0.97072. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.51593/0.96403. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.50172/0.93522. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.48623/1.00222. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.48550/1.02693. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.48965/1.06495. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.50117/1.00633. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49958/0.96856. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.47177/0.96793. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.47341/0.96814. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.44119/1.00252. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.44666/1.03346. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.44465/1.12463. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.45881/0.99840. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.43594/1.06077. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.40276/1.04870. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.42216/1.02465. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.41353/1.11247. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.40335/1.06905. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.42270/1.12704. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.41980/1.08261. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.39833/1.01271. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.37897/1.08765. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.41675/1.08718. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.40459/1.10843. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.38881/1.04483. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.37663/1.06827. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.37586/1.18095. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.37663/1.17239. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.39413/1.13131. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.36209/1.12304. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.35712/1.14185. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.38554/1.21123. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.38291/1.20596. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.39022/1.23415. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.38451/1.18444. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.35231/1.21219. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36233/1.21487. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.34029/1.29829. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.34070/1.27736. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.33826/1.28332. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.31416/1.35358. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.30888/1.34980. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.30842/1.33421. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.30562/1.28427. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.34751/1.22969. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.30376/1.29337. Took 0.43 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69144/0.70633. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.67995/0.72118. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.67525/0.74722. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.67267/0.74791. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.67153/0.74177. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.66274/0.75995. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.66296/0.76567. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.65719/0.77847. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.65847/0.77996. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.64953/0.79232. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.64852/0.77637. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.65051/0.77193. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.64492/0.77411. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.65206/0.78246. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65053/0.78074. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.64760/0.78075. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.63117/0.79398. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.63012/0.82161. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.62924/0.80956. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.62361/0.82581. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.62109/0.83512. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.61725/0.85173. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.62144/0.88790. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.61810/0.87875. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.60988/0.85900. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.60193/0.85664. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.59605/0.89235. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.59709/0.92060. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.58119/0.91574. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.57856/0.93993. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.56821/0.95641. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.57113/0.96006. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.56927/0.94132. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.56345/0.96390. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.55286/0.95345. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.55321/0.94848. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.54086/0.99517. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.53882/0.99491. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.53809/0.97272. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.53523/0.93828. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.52615/0.97155. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.52038/1.00257. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.51300/0.98302. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.51117/1.02659. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.50308/1.03034. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.49285/0.98842. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.48502/1.02976. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.50421/1.01529. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.48492/1.00605. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.46471/1.03977. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.45714/1.11419. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.46275/1.07403. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.45106/1.14996. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.44971/1.10369. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.43404/1.12066. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.41148/1.11242. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.41507/1.12986. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.41178/1.14268. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.41272/1.15679. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.40903/1.10615. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.37549/1.22127. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.38021/1.17869. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.37705/1.28321. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.38234/1.20900. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.35612/1.20746. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.34092/1.25681. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.34117/1.30966. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.32437/1.31642. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.32315/1.35408. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.33190/1.26732. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.31938/1.38622. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.32423/1.31878. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.32445/1.38702. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.31211/1.38794. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.30599/1.42779. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.26959/1.33767. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.26795/1.46665. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.28980/1.38639. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.31296/1.34506. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.29419/1.39640. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.26834/1.43563. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.25548/1.51215. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.26115/1.49546. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.22241/1.55049. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.25341/1.48744. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.23115/1.55500. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.24868/1.41810. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.21539/1.49552. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.22668/1.54625. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.23092/1.55974. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.24403/1.48343. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.21463/1.58268. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.19401/1.60239. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.20175/1.58579. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.20719/1.72421. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.20635/1.56076. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.17999/1.66529. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.18381/1.68745. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.18952/1.69052. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.17600/1.80748. Took 0.45 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.70671/0.72082. Took 0.62 sec\n",
      "Epoch 1, Loss(train/val) 0.69950/0.71585. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69145/0.72363. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69119/0.72626. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69002/0.72775. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68583/0.73149. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68726/0.72234. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67832/0.73035. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68103/0.73200. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68112/0.74941. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67658/0.74492. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67645/0.75108. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67716/0.75581. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67371/0.75062. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67004/0.76335. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67305/0.78488. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65857/0.77909. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66183/0.77553. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66171/0.77378. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64936/0.82354. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65060/0.82016. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64710/0.80242. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65650/0.82699. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.65115/0.83535. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65001/0.84360. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64190/0.86090. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64602/0.87426. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62874/0.89030. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62487/0.89455. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62650/0.91357. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62421/0.96221. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62757/0.95417. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61882/0.93527. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61100/0.95556. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60929/0.99718. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60292/1.00862. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.61558/1.00002. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61080/1.02407. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.60121/1.04573. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59151/1.05891. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59436/1.09023. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.58454/1.09170. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.59023/1.13420. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.59078/1.10510. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57902/1.11994. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57088/1.10942. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57297/1.11992. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56603/1.14133. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55982/1.14552. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54589/1.18791. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54166/1.22009. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.55568/1.21938. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.53766/1.19183. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.54087/1.23843. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52275/1.29038. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.53090/1.22429. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.51472/1.24156. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53015/1.21809. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.53116/1.21297. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50490/1.25043. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50725/1.27270. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.50778/1.18736. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.50949/1.21867. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.49043/1.26272. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49420/1.24353. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.49728/1.18472. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.50332/1.22656. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.48402/1.31874. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.48173/1.39221. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47360/1.32211. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.44850/1.36939. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45776/1.31977. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.46111/1.32843. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.43809/1.35264. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.44077/1.36525. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.43568/1.41215. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.44325/1.34391. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.41092/1.32679. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.42182/1.36147. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.41926/1.46238. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.39828/1.46778. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.40959/1.42661. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.40632/1.40876. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.42352/1.54802. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.40867/1.49594. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.42477/1.56690. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.38343/1.54483. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.37815/1.51134. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.37377/1.60345. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.36633/1.60705. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.37233/1.58634. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.38198/1.65613. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.37044/1.51563. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.37283/1.57608. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.36330/1.65785. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.34654/1.60286. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.35748/1.57838. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.33637/1.64328. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.32193/1.60862. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.33670/1.67485. Took 0.43 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.70666/0.68896. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69685/0.69562. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69643/0.70549. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69372/0.70498. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68657/0.70925. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68661/0.70571. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68432/0.70674. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68504/0.70563. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68278/0.71656. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67920/0.72714. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67228/0.73612. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67490/0.73241. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66976/0.73712. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66960/0.73999. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66932/0.73092. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66745/0.73601. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66820/0.73331. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67113/0.75073. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66315/0.74352. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66301/0.76209. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66579/0.75923. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.66025/0.74992. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65556/0.76144. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65156/0.74368. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65293/0.75186. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64788/0.74036. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.64465/0.74645. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64072/0.75288. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63935/0.74411. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64186/0.72586. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62986/0.73903. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62420/0.73197. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.62852/0.74232. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62429/0.75874. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62539/0.75929. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62214/0.74042. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.61835/0.77565. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60698/0.77426. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.60752/0.77097. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60610/0.77242. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60711/0.77379. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.58775/0.79056. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.58357/0.78212. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58147/0.79705. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59288/0.78794. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.58221/0.81539. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.57432/0.80454. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56084/0.84326. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55820/0.80864. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.55537/0.84025. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55343/0.79385. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.54368/0.84534. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.55440/0.82786. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.53936/0.87550. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53054/0.85128. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.53935/0.84331. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52116/0.85103. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52202/0.88478. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.51480/0.85758. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.50582/0.88401. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49254/0.91816. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.47834/0.90842. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48283/0.94554. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.48589/0.91053. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.48435/0.91428. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.46540/0.90981. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.45248/0.95392. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.43371/0.94513. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.44182/0.94509. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45322/0.91681. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.44284/0.99764. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.43046/0.98453. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.44936/0.96802. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.40375/0.98290. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.41559/1.02548. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.38422/1.06083. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.40150/1.01519. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.43524/1.00742. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.39351/1.04431. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.40478/0.95630. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.40434/1.01748. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.36996/1.06505. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.38150/1.11793. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.37535/1.07391. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.39280/1.06753. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.36331/1.09984. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.39334/1.08084. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.36742/1.17490. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.38157/1.15490. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.34950/1.13083. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36323/1.13554. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.34544/1.12195. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.35199/1.10503. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.31345/1.10390. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.33080/1.11013. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.35857/1.24499. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.31748/1.15146. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.30258/1.15624. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.31659/1.17976. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.32258/1.17879. Took 0.45 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.71927/0.70239. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.71096/0.70911. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69978/0.70484. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70335/0.71894. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.70002/0.72697. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69473/0.71562. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69122/0.71017. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68627/0.73653. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69215/0.73101. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68556/0.75197. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68351/0.75528. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67901/0.74300. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.68380/0.75904. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68116/0.74925. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.68324/0.77951. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.68706/0.76021. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67588/0.75580. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.67384/0.75767. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67234/0.76178. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67455/0.76547. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.67102/0.76956. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66895/0.78094. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66839/0.79021. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66217/0.81389. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65756/0.80677. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.66129/0.81912. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65334/0.81940. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.65112/0.84305. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64580/0.84264. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63482/0.85327. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.64632/0.85651. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.64257/0.85697. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63133/0.85592. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.63333/0.85842. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62519/0.85732. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61941/0.85867. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.62459/0.86000. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60964/0.88079. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.60538/0.88725. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60518/0.88464. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61355/0.88859. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.60208/0.90102. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59675/0.90606. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59866/0.87222. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58602/0.87673. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.58034/0.89427. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57822/0.90801. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56737/0.89437. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.55927/0.91545. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56037/0.92936. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55856/0.92688. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.57429/0.92675. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.55095/0.92282. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55209/0.90073. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52968/0.92382. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.53300/0.96220. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53420/0.96780. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.52894/0.97955. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.51543/0.99833. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.52152/0.99387. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50872/1.04804. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50897/0.99986. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50056/0.99640. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.49289/1.02120. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49339/1.05953. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.48980/1.03260. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48176/1.08541. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.47199/1.11964. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.47518/1.11952. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46517/1.21885. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.47589/1.15321. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.46608/1.10109. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.44334/1.19841. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46321/1.18468. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.44113/1.20827. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.43171/1.20033. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43670/1.21378. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.42327/1.25893. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.43244/1.22668. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.42206/1.27979. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.42115/1.26897. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.39001/1.28393. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.38533/1.32371. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.39540/1.33385. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.40984/1.34316. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.40014/1.26778. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.37203/1.37669. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.37887/1.32708. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.35772/1.43829. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.37394/1.36517. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36706/1.45473. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.34667/1.46874. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.35386/1.45229. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.35454/1.53436. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.34573/1.50093. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.35649/1.47372. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.35249/1.52046. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.34444/1.45014. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.32151/1.50333. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.32079/1.56307. Took 0.43 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.74058/0.75900. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.72682/0.75792. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.72144/0.74737. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.72770/0.76161. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.70343/0.75474. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.70596/0.75652. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.71285/0.76116. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.70428/0.75459. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.70301/0.75579. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.69999/0.77514. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.70156/0.78683. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.69688/0.79211. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.69340/0.78090. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68247/0.78555. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.69097/0.79847. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67997/0.80620. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.68282/0.83006. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66686/0.82281. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.67248/0.83706. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67914/0.81303. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.67083/0.81551. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.67432/0.80352. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66520/0.82273. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66550/0.81041. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.65681/0.81805. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66370/0.80974. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.65207/0.82055. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64283/0.80054. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63834/0.80486. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65525/0.80016. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63485/0.78254. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.63265/0.80640. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.64371/0.78883. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.63219/0.78645. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62261/0.77266. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.63125/0.77550. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.62088/0.79885. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61451/0.80059. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60830/0.81399. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.61484/0.79868. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59897/0.77725. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.60733/0.80841. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59933/0.79768. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60850/0.81390. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60559/0.80603. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59316/0.79819. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58303/0.81339. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.59175/0.84421. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58468/0.84393. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.58203/0.83697. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.58177/0.82102. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56684/0.85729. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.57165/0.85791. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.56115/0.88891. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.57301/0.86711. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.56817/0.92310. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.55773/0.92988. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.55950/0.92976. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54839/0.95462. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54673/0.91954. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54951/0.94687. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.54857/0.96650. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54646/0.94843. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.53380/0.99740. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.52833/1.00662. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.53893/1.01803. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.52682/0.97764. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.53357/0.98462. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.52654/0.97864. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.51440/0.99361. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.52626/1.01724. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.51856/1.00971. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.51853/0.98652. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.51550/0.98451. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.51361/0.94972. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49769/1.05290. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.51310/1.04048. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.49884/1.02663. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.50009/1.01103. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.48466/1.00060. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.49729/0.98209. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.49110/1.02666. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.48688/1.03658. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.48868/1.01504. Took 0.47 sec\n",
      "Epoch 84, Loss(train/val) 0.48655/1.03789. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.48078/1.05527. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.48852/1.05472. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.46532/1.06115. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.45765/1.14470. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.46205/1.09326. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.44615/1.18232. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.47568/1.09958. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.47401/1.07738. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.44305/1.15089. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.45540/1.14664. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.44277/1.14283. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.43745/1.10369. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.44546/1.16963. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.47981/1.06749. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.46180/1.05457. Took 0.45 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69820/0.69162. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69073/0.70476. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68498/0.70371. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68574/0.70491. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68223/0.71868. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68277/0.72458. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68141/0.72794. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67926/0.71783. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67626/0.72309. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67177/0.72045. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67290/0.72476. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66779/0.71178. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66382/0.72771. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.66507/0.72825. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66576/0.74284. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66330/0.73340. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65877/0.73378. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65814/0.74149. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.65217/0.75331. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.64972/0.73652. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64304/0.74504. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64795/0.77362. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64286/0.75647. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.63326/0.79551. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64226/0.81411. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63295/0.80795. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62106/0.83886. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62289/0.81952. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62484/0.81718. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62310/0.81316. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61633/0.83177. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.62905/0.84584. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61336/0.84104. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.61445/0.85185. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60341/0.85665. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60009/0.85873. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59783/0.85302. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59884/0.83912. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60046/0.81217. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58563/0.83554. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.58228/0.85995. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58104/0.83208. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58712/0.82101. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.59002/0.77292. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57961/0.80591. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56843/0.81471. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56391/0.81446. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56605/0.81905. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.55665/0.80956. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56280/0.84507. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54119/0.83064. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.53425/0.84018. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54158/0.82207. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.54966/0.85731. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53290/0.84158. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.52493/0.85487. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.52028/0.86210. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.55240/0.84223. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.55332/0.81595. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.53471/0.83520. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52948/0.84719. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.51265/0.90338. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.50144/0.91974. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.50354/0.93467. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49303/0.92982. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.49708/0.91133. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.51903/0.87911. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.49647/0.94281. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49174/0.98154. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.48831/0.96380. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.47937/0.98045. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.48431/1.00950. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46278/1.01046. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46570/1.02185. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.46693/0.98218. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.44951/1.02924. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46919/1.01571. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.47158/1.03420. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.42656/1.12515. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.42623/1.17984. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.42391/1.19488. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.42689/1.18815. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.42165/1.14344. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.42072/1.21530. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.40763/1.23761. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.41184/1.21572. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.41926/1.21640. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.41370/1.13626. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.40166/1.26082. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.41993/1.22693. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.38377/1.30190. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.38497/1.34893. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.39061/1.25451. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.40680/1.23998. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.44341/1.29041. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.43329/1.28756. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.41243/1.33201. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.39376/1.39367. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.39219/1.43783. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.39480/1.42999. Took 0.44 sec\n",
      "ACC: 0.6145833333333334\n",
      "Epoch 0, Loss(train/val) 0.70521/0.71684. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.70071/0.70976. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69466/0.69996. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68866/0.70399. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68584/0.69431. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.68091/0.70800. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.67390/0.72011. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67576/0.70167. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.66983/0.71178. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66952/0.71260. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67251/0.72107. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66132/0.70841. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65573/0.73232. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65348/0.71324. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.64208/0.72899. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.63582/0.72163. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.64163/0.73388. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.62971/0.73745. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.62882/0.75612. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.62312/0.76751. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.61520/0.82603. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.61646/0.86224. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.60997/0.89039. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.59999/0.90001. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.60689/0.85099. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.59919/0.83241. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61010/0.85770. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.59265/0.82067. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.59100/0.85794. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.58216/0.85564. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.57150/0.87306. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.56119/0.88177. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.56237/0.91822. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.55321/0.91701. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.53875/0.93492. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.54574/0.94578. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.53596/0.95142. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.53348/0.87675. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.52759/0.88854. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.52506/0.87120. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.51662/0.92650. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.52204/0.97347. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.50276/0.93503. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.50943/0.95962. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.49216/0.99774. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.49381/0.94926. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.48662/0.96449. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.47792/1.03299. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.48719/1.03604. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.46518/1.00254. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.46462/1.01262. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.44668/1.03159. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.44440/0.94961. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.44357/1.05994. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.42938/1.10854. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.44267/1.01341. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.44174/1.04015. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.40629/1.13411. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.41427/1.14226. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.39347/1.16648. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.39735/1.17375. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.38798/1.17157. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.39440/1.15095. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.41447/1.25393. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.36833/1.25640. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.36422/1.30621. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.39843/1.28188. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.37725/1.30227. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.37679/1.26701. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.35204/1.28742. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.34023/1.29479. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.34060/1.31331. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.32531/1.32271. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.32989/1.45197. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.30057/1.47854. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.32180/1.46905. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.28664/1.42451. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.28145/1.48684. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.29193/1.56233. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.27268/1.47532. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.28260/1.61499. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.27401/1.67605. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.27687/1.64103. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.27669/1.72419. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.25177/1.59633. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.25831/1.62650. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.27284/1.75498. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.25681/1.72941. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.25965/1.83073. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.23290/1.80382. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.24852/1.80092. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.25091/1.64876. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.22563/1.87487. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.23484/1.78384. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.21366/1.87719. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.19584/1.82313. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.21363/1.94053. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.18452/1.92880. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.19829/1.87228. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.19669/1.93742. Took 0.44 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.70863/0.71875. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69213/0.72743. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.68911/0.71898. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68802/0.74537. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68493/0.74555. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.67367/0.73017. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68766/0.72729. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67960/0.70990. Took 0.48 sec\n",
      "Epoch 8, Loss(train/val) 0.67518/0.72683. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67239/0.72441. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67115/0.74016. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66709/0.73873. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66871/0.73079. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66507/0.78785. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67301/0.73165. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65922/0.74416. Took 0.47 sec\n",
      "Epoch 16, Loss(train/val) 0.66395/0.74708. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64956/0.74694. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65593/0.77466. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66602/0.83918. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.66675/0.76790. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64710/0.79046. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64095/0.78405. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.63892/0.79382. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63577/0.80707. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63602/0.77673. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62517/0.79888. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62364/0.79252. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62384/0.77817. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62369/0.75727. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61386/0.77406. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61246/0.92292. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63020/0.78390. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60853/0.78234. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61021/0.79063. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.59656/0.77050. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.59346/0.79185. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59333/0.79517. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59299/0.80825. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.58705/0.82941. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58360/0.79561. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58087/0.78955. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56450/0.83722. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.56445/0.83994. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.56692/0.80871. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55765/0.80856. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.54986/0.84205. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.54760/0.81975. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.53542/0.85972. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.54177/0.84053. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.53427/0.95219. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54272/0.83558. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52136/0.90068. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.51629/0.94251. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.52475/0.93054. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.50699/1.04433. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53099/1.02030. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.54251/0.98778. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.51854/0.97736. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.49584/1.03079. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49269/1.04217. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.50099/0.99150. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50413/0.98650. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.47203/1.02291. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.47524/1.00068. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.47485/1.05165. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.46579/1.05603. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.46933/1.09760. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.46412/1.09063. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.44617/1.08356. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.45134/1.10127. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45342/1.10184. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46650/1.02174. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.43560/1.09192. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.43250/1.15093. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.42957/1.12994. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.42810/1.15495. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.41078/1.17645. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.41092/1.17029. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.42616/1.13872. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.43753/1.14591. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.41832/1.13084. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.40082/1.21096. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40306/1.16005. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.37977/1.20548. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.39172/1.29676. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.37943/1.33569. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.39447/1.21237. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.37743/1.38133. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.38928/1.30772. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.37031/1.36503. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.35696/1.37635. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.35545/1.39289. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.34040/1.48810. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.34222/1.41991. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.37564/1.30119. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.34726/1.48706. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.34305/1.59207. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.34738/1.39159. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.31694/1.52323. Took 0.44 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.72017/0.74888. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.70624/0.71118. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.71024/0.71594. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69491/0.74854. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69959/0.74018. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67988/0.74443. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68961/0.76187. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68483/0.75097. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67777/0.75038. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67366/0.76231. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67306/0.74719. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68222/0.77118. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66656/0.78900. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.66857/0.82853. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66658/0.84236. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.66067/0.85923. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65364/0.87316. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66524/0.89284. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65325/0.91279. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65347/0.93596. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65902/0.93419. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64588/0.94037. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64599/0.96720. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.63691/0.94571. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63467/0.96488. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.63342/0.96722. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63461/0.96550. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62855/0.95255. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61247/0.97443. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60132/0.95141. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60858/0.96054. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.59883/1.00796. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61503/1.00686. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.60442/0.98009. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.60133/1.00040. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60017/1.00237. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.60030/0.99909. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59123/1.02828. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59181/1.04306. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59004/1.02762. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.57901/1.04094. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57994/1.07830. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56954/1.09655. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56234/1.09842. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56573/1.11688. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.57087/1.11733. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55060/1.16434. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.55365/1.12228. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.54709/1.15146. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.54819/1.14927. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.53076/1.18605. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.52083/1.24161. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52091/1.26891. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.51580/1.26112. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.50984/1.29367. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51775/1.10817. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53637/1.13970. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51409/1.21649. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.49121/1.26204. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.48384/1.26171. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.49118/1.29495. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.47750/1.30358. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50034/1.24344. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.48741/1.31921. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.48418/1.26795. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.47933/1.15381. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48606/1.26483. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.46701/1.31795. Took 0.47 sec\n",
      "Epoch 68, Loss(train/val) 0.48057/1.27696. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45978/1.26858. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.45883/1.29182. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.43754/1.35452. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.45295/1.20969. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.44382/1.32660. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.43733/1.31769. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.46771/1.40799. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.44780/1.25716. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.44414/1.26691. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44571/1.43704. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.44101/1.39487. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.43097/1.47937. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.41731/1.45912. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.42286/1.39506. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.42442/1.50387. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.40981/1.35452. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.40911/1.34139. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.44938/1.31735. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.41991/1.36670. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.43776/1.40638. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.42292/1.33664. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.38856/1.51770. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.40380/1.57829. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.42325/1.40459. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.39148/1.43373. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.39019/1.42222. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.37565/1.47073. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.37715/1.52378. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.38948/1.57954. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.35627/1.59211. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.35928/1.66476. Took 0.44 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.73251/0.77265. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.71727/0.75666. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.70164/0.76357. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.71141/0.75470. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.70024/0.76190. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.70093/0.77059. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.69299/0.77017. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68182/0.77383. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69335/0.78930. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68303/0.76953. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68552/0.76710. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.69430/0.76364. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67792/0.77737. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67494/0.77931. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67971/0.80425. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67370/0.78251. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.66692/0.79622. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66410/0.80798. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.65859/0.81113. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66047/0.82088. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64477/0.85648. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64906/0.87284. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64054/0.89085. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62986/0.88992. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.64490/0.89573. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.63336/0.90018. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63014/0.89458. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62742/0.91232. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61522/0.92982. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61455/0.91749. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.61686/0.93457. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61326/0.95593. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.61161/0.95813. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60849/0.97631. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60447/0.98062. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60081/0.96411. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59561/0.97388. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60063/0.94687. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59049/0.97079. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.58510/0.96613. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.57674/0.91280. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.56966/0.99463. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56991/0.90952. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.58247/0.90969. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55901/0.92367. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.57767/0.91398. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.55958/0.93574. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56693/0.90422. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.55678/0.88498. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.54756/0.89865. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54451/0.94551. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55061/0.84945. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.53142/0.96046. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.52682/0.89196. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.53901/0.89643. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.53569/0.85143. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53356/0.82975. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.51292/0.84039. Took 0.47 sec\n",
      "Epoch 58, Loss(train/val) 0.52101/0.81284. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52866/0.77407. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51971/0.77442. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.51805/0.79600. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.49688/0.80566. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.48255/0.82173. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.48305/0.83813. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.49820/0.82645. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.49072/0.80508. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.47042/0.85966. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.48563/0.89091. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47002/0.85973. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.45639/0.87086. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.45446/0.91029. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46063/0.89491. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.49942/0.91044. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.46267/0.84438. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.43879/0.93296. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.45638/0.95039. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.42569/0.99549. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44485/1.02217. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.43876/1.02293. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.43188/1.06858. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.40738/1.12651. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.41683/1.08345. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.42277/1.05896. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.41788/1.03783. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.40790/1.10409. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.41129/1.11253. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.38697/1.12112. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.39487/1.17013. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.39812/1.09037. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.38862/1.11058. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.36625/1.16873. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.40163/1.25893. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.36527/1.23599. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.35900/1.28376. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.36444/1.23014. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.35842/1.24141. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.33776/1.37033. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.34689/1.26751. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.34060/1.40022. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70893/0.71183. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70263/0.70463. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69897/0.70593. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69330/0.69820. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.69180/0.71633. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68564/0.71402. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68117/0.72063. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67652/0.72131. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67099/0.73519. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66715/0.73360. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.67060/0.74538. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.65895/0.76565. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65713/0.77151. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65711/0.77229. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.65430/0.75909. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65017/0.76769. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65625/0.78540. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64661/0.78242. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64304/0.79961. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.63571/0.78193. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.63946/0.78232. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63722/0.79512. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.62531/0.80309. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.62619/0.81124. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62331/0.84322. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.63198/0.84006. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62705/0.82344. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.61912/0.83800. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61193/0.87140. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61807/0.84639. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61103/0.87245. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.60804/0.87411. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60247/0.89640. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.59514/0.91635. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.59187/0.95375. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59797/0.89051. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58765/0.88656. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58171/0.91689. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58530/0.93905. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57603/0.95720. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.57701/0.96505. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57419/1.00007. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57230/1.02288. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58824/0.96710. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57557/0.97369. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.57722/0.95971. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57191/0.98024. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.56433/0.99472. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56015/0.97810. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54840/0.99826. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.54228/0.96897. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55205/1.02549. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.55833/0.98614. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.55481/0.94212. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54411/0.97258. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.54220/1.00637. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.53217/0.96562. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.53590/0.95407. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.52708/0.96039. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.54172/0.95358. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.53887/0.92999. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.51712/0.92359. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.51459/0.93074. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51826/0.93209. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50404/0.96605. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50430/0.95841. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.50431/0.93342. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.49817/0.97359. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.48712/0.97572. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.50034/0.94637. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.51122/0.93362. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.48400/0.95660. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.55634/0.88213. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.56757/0.89670. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.53711/0.89999. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.50650/0.92489. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.50007/0.92704. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.48846/0.94362. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.48808/0.92026. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46514/0.98790. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.47726/0.96256. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.46085/0.95442. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.46282/0.94746. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.44074/0.97315. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.44228/1.00629. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.44857/0.98963. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.44787/1.02261. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.43795/0.96645. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.42513/0.93398. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.43946/0.94899. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.43105/0.98097. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.43015/0.98152. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.41626/1.07327. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.42470/0.96791. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.42707/1.02296. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.41052/1.02138. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.41453/1.04244. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.38016/1.09333. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.40131/1.09991. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.39346/1.05821. Took 0.44 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.71041/0.76608. Took 0.64 sec\n",
      "Epoch 1, Loss(train/val) 0.70728/0.75877. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.68787/0.74939. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69647/0.74117. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.67942/0.74005. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.69157/0.72717. Took 0.49 sec\n",
      "Epoch 6, Loss(train/val) 0.69046/0.72041. Took 0.47 sec\n",
      "Epoch 7, Loss(train/val) 0.67718/0.72004. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.67785/0.72866. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.68092/0.73013. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67846/0.73334. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67841/0.73900. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67448/0.74405. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66861/0.76143. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67339/0.77389. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66144/0.78704. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66201/0.78047. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65838/0.79004. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65507/0.79150. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65537/0.77791. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.66122/0.79584. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66549/0.80044. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65212/0.80795. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.65809/0.80418. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.65051/0.81294. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64605/0.81710. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64522/0.81642. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63775/0.84550. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64370/0.84807. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.63999/0.85323. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63721/0.91512. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63718/0.90512. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63652/0.93162. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63271/0.89118. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.63878/0.85424. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.61987/0.91833. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.62837/0.94257. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61805/0.89494. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.61601/0.97325. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61039/0.92027. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61556/0.91989. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61303/0.89655. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59099/0.87614. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61462/0.90015. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59884/0.91895. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59738/0.90462. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.60002/0.92263. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.59757/0.91241. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59318/0.95376. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.58526/0.91283. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.58517/0.93815. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.59717/0.88753. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.59833/0.89037. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.57901/0.90830. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56364/0.93281. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.57554/0.95687. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56793/0.96954. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.58047/0.99554. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.56907/0.98709. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.56863/0.96642. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55616/1.02828. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.55450/1.06200. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.58061/1.01759. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.57413/0.96937. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.55668/1.01300. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.55298/0.99629. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.54303/1.11926. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.52907/1.12124. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.54719/1.06148. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.52823/1.12902. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.53086/1.09286. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.51064/1.17830. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.55566/1.15357. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.51230/1.21060. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.51492/1.16590. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.52411/1.22208. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.52329/1.18357. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.50076/1.24971. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.49875/1.25342. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.49323/1.26311. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.54141/1.16447. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.54424/1.13204. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.51353/1.21321. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.48345/1.31615. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.48576/1.23245. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.47619/1.29494. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.46685/1.25320. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.47572/1.29859. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.48088/1.27555. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.46400/1.33262. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.47226/1.42471. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.45897/1.40467. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.48001/1.36089. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.48954/1.23802. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.47818/1.27771. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.46032/1.30637. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.43475/1.36593. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.47332/1.30907. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.45988/1.34102. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.45533/1.29631. Took 0.44 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.72624/0.72488. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.71174/0.74198. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.71512/0.73690. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.70458/0.73940. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.70737/0.71110. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.69171/0.72222. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68932/0.73032. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68394/0.70916. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.68915/0.75161. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.68221/0.74154. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67265/0.75475. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68442/0.74637. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68215/0.76497. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67906/0.74526. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67515/0.76038. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68157/0.75751. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66799/0.77305. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66668/0.77168. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65883/0.76307. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65717/0.76307. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66199/0.79464. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65686/0.78500. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65069/0.77632. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.66344/0.81222. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66081/0.80073. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65871/0.83072. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65293/0.83254. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64788/0.83223. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.65691/0.84529. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63839/0.86208. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.65164/0.85206. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.65108/0.84652. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64356/0.85666. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64671/0.86556. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.63791/0.89488. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.63981/0.89988. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.64554/0.89507. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.63602/0.90138. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61740/0.91253. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.63137/0.91913. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.61749/0.90504. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61393/0.93138. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.61598/0.94308. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.60687/0.95239. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60905/0.95935. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.59669/1.00356. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.59963/0.95429. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.61017/1.00120. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.60766/0.97151. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.60400/0.98659. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.58914/0.95904. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.60038/0.96006. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.59154/0.96683. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.58221/0.99969. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.57754/0.93886. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.57757/1.06661. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56342/1.00850. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.57465/1.03287. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.56713/1.08856. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.55463/1.11435. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55016/1.07696. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.54742/1.14327. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.55944/1.13096. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.54900/1.14726. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.54791/1.10946. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.53286/1.16579. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.52660/1.17999. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.52625/1.20037. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.53338/1.18884. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.52707/1.15816. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.53450/1.15009. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.52590/1.13410. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.50455/1.23629. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.51338/1.21264. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.52319/1.15449. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.50688/1.22574. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.49658/1.19150. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.49522/1.28087. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.50487/1.22391. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.50691/1.19322. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.48545/1.14216. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.48773/1.28992. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.48000/1.27498. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.47665/1.26959. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.48522/1.30487. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.48414/1.22663. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.45837/1.26996. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.46360/1.28499. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.46326/1.34133. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.48269/1.35449. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.44860/1.43080. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.47260/1.29809. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.46544/1.33099. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.43709/1.36578. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.45457/1.34762. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.45656/1.38867. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.44079/1.58299. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.46104/1.48304. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.41993/1.48613. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.41989/1.47626. Took 0.45 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.70067/0.69590. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69139/0.69764. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68729/0.69670. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68177/0.70599. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68586/0.70234. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68260/0.71465. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.67721/0.72315. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67828/0.71686. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67143/0.72029. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67450/0.72516. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.67079/0.72877. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66891/0.73980. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66221/0.76400. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65606/0.77361. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.65814/0.76467. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.65587/0.77719. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.65340/0.78103. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.64033/0.79165. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.63837/0.79016. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.63541/0.79314. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.62584/0.80080. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63273/0.79095. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.62808/0.78180. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.61765/0.77835. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.60841/0.79564. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.61054/0.81476. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61026/0.80777. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.60050/0.83507. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.59941/0.83843. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.59374/0.85391. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.58917/0.85886. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.57809/0.84503. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.58423/0.87282. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.58061/0.86447. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.56921/0.84843. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.56031/0.86041. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.55377/0.88421. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.56205/0.88791. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.55465/0.89064. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.54891/0.91216. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.54287/0.95345. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.53334/0.96528. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.52912/0.96171. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.52625/0.99145. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.52085/0.99079. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.51473/0.98483. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.51223/0.97829. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.50088/1.01556. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.49891/1.00316. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.48865/1.01458. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.50481/1.00393. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.48379/1.00692. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.48275/0.96841. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.48115/0.97831. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.46255/1.02875. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.47858/1.00863. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.47359/1.08021. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.46306/1.02951. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.44940/1.05529. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.45029/1.06083. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.46857/1.02933. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.44242/1.07562. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.42373/1.08174. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.45081/1.11069. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.43956/1.06893. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.42766/1.05907. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.41766/1.04839. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.40775/1.09052. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.43313/1.07027. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.42414/1.10972. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.43525/1.06334. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.43716/1.04865. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.40910/1.08926. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.40416/1.08267. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.39486/1.08052. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.38689/1.08717. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.36317/1.12427. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.37669/1.12034. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.37666/1.18061. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.39595/1.15786. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.40276/1.13106. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.36734/1.18725. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.35602/1.19058. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.35927/1.24819. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.38836/1.24863. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.35959/1.22350. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.36753/1.20403. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.34306/1.17684. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.35481/1.28113. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.33836/1.17728. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.32455/1.27967. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.32380/1.32660. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.32783/1.30588. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.36686/1.27381. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.33447/1.35106. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.32855/1.28325. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.34083/1.31110. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.30621/1.31647. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.32716/1.32738. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.30446/1.34926. Took 0.45 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.70360/0.69746. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.68768/0.69492. Took 0.49 sec\n",
      "Epoch 2, Loss(train/val) 0.68543/0.68612. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68257/0.69280. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68417/0.68369. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.68297/0.69465. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67726/0.68019. Took 0.47 sec\n",
      "Epoch 7, Loss(train/val) 0.67621/0.68058. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.66876/0.68787. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67662/0.68784. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67021/0.69921. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66524/0.69844. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.66200/0.70036. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66362/0.71090. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.65617/0.70539. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65228/0.70571. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65020/0.72775. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64855/0.70509. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64803/0.69523. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64058/0.69936. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64077/0.67846. Took 0.48 sec\n",
      "Epoch 21, Loss(train/val) 0.64004/0.69138. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.63630/0.68879. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.63516/0.69847. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63699/0.67701. Took 0.49 sec\n",
      "Epoch 25, Loss(train/val) 0.62038/0.67631. Took 0.47 sec\n",
      "Epoch 26, Loss(train/val) 0.62687/0.68554. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62729/0.67763. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62083/0.67134. Took 0.48 sec\n",
      "Epoch 29, Loss(train/val) 0.61482/0.66083. Took 0.47 sec\n",
      "Epoch 30, Loss(train/val) 0.61235/0.67892. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.60568/0.67833. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.60243/0.69591. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59049/0.70308. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.60281/0.70197. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58651/0.70199. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.59488/0.74116. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.57450/0.72186. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.56434/0.74736. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.58648/0.74515. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.57546/0.74257. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.55999/0.76853. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56628/0.76218. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56049/0.77819. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.54438/0.79322. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.53504/0.80893. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.53471/0.80559. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.53779/0.85084. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.52158/0.84893. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52480/0.83681. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.51452/0.88755. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.52551/0.86553. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.50814/0.85408. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.50241/0.86074. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.48419/0.89555. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.51283/0.89732. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.49428/0.86779. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.47778/0.89004. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.47623/0.93396. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.46538/0.92046. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.47122/0.95103. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.46693/0.93774. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.46359/0.95515. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.45156/0.95852. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.44430/1.01995. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.44756/0.99829. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.45859/1.04161. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.44727/0.99108. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.44456/1.00168. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.40859/1.01602. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.41210/1.04492. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.43799/1.02318. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.39626/1.07892. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.42253/1.00350. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.39957/1.03940. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.39235/1.07021. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.41461/1.08251. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.39096/1.11609. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.38266/1.12575. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.36245/1.14420. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.38029/1.14633. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.37564/1.04414. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.37024/1.16603. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.36465/1.22334. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.36078/1.18252. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.37367/1.26886. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.35013/1.19863. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.35025/1.25887. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.36597/1.24576. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.36983/1.22184. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36584/1.19076. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.32403/1.22601. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.32475/1.31831. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.33429/1.21199. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.35814/1.22081. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.31469/1.31666. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.31070/1.20302. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.29947/1.24645. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.32727/1.21189. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.29265/1.30158. Took 0.45 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70645/0.72451. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70193/0.70625. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69193/0.72830. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69339/0.71777. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.67956/0.73170. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67608/0.74271. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.67729/0.73038. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68407/0.74126. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67932/0.74531. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67112/0.74381. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67754/0.73951. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66383/0.74524. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66666/0.75291. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66712/0.75752. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66654/0.75817. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66346/0.75825. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65388/0.77495. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66138/0.76090. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.64412/0.74483. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66203/0.75743. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64852/0.76745. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63672/0.75784. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63898/0.77591. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63658/0.77899. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63106/0.77470. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62697/0.75490. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.62643/0.74373. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.61462/0.74464. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61361/0.76213. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.60842/0.73885. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62212/0.73846. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.60047/0.76641. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.60247/0.78711. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.59330/0.77775. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.59625/0.77878. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.58104/0.79193. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.58784/0.78882. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.57228/0.81807. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.58092/0.77835. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58263/0.79630. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.57490/0.80395. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57856/0.81217. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.56170/0.82084. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.56714/0.82333. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55646/0.80755. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55519/0.84846. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55068/0.85553. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.56060/0.85687. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.56083/0.82709. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.54301/0.83717. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54317/0.83919. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55023/0.86619. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54374/0.83283. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.54620/0.82900. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.52674/0.83668. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51465/0.84846. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52414/0.86786. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.53250/0.88122. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.50669/0.90566. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.52220/0.86969. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51158/0.89713. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50718/0.89993. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50707/0.89626. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.49625/0.91289. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.48894/0.92879. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50175/0.97282. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48300/0.90483. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48844/0.95732. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.50274/0.95824. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.49764/0.98134. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49356/0.92541. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.46636/0.99846. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46372/0.97843. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.46345/0.97413. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.46854/1.06489. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.45485/0.99317. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.45463/0.99990. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.44110/0.99504. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44365/1.07694. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.45140/1.04914. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.46047/1.01764. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.44262/0.96562. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.42918/1.00942. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.42207/1.03053. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.42037/1.03005. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.43283/1.10211. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.42293/1.12814. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.40890/1.11418. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.39332/1.08021. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.41890/1.02544. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.42140/1.09442. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.38610/1.10249. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.41468/1.10122. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.39715/1.10913. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40674/1.17380. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.40306/1.09785. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.38195/1.15241. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.37638/1.15551. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.37218/1.19945. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.38779/1.16333. Took 0.45 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.70076/0.69767. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.69238/0.69892. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68441/0.71205. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68507/0.72501. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.67921/0.73051. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67416/0.73341. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.67254/0.74201. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.66629/0.75951. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.66901/0.78842. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66535/0.77736. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.65986/0.80526. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66080/0.82951. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65767/0.81986. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66154/0.81804. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.65110/0.80331. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.64605/0.83609. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.64437/0.81548. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.64778/0.81918. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.64193/0.82847. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.64212/0.81821. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64005/0.81891. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64112/0.82797. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.62985/0.83464. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63179/0.82694. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.62422/0.83210. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62716/0.81468. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63037/0.81199. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.61058/0.82539. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63067/0.82224. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61305/0.84999. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.60784/0.85344. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60581/0.84627. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60830/0.84417. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60512/0.88429. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59623/0.86934. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59356/0.87872. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.57960/0.87666. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59349/0.90907. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58809/0.90995. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57880/0.91523. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.58090/0.91309. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.56245/0.92258. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56521/0.96207. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.57239/0.97245. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56174/0.98811. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.54512/1.01544. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.53688/1.01679. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.53415/1.02941. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.54988/1.02089. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54125/1.03063. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.53692/1.00257. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.53353/1.04212. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.53119/1.06222. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52981/1.02054. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51640/1.10108. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.50910/1.05929. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.51826/1.08188. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51525/1.07675. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.50677/1.05172. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.49579/1.09077. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.50115/1.16308. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.49866/1.10181. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48877/1.13973. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.47870/1.14606. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.47244/1.18324. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.46639/1.16063. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.45313/1.26618. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.47060/1.25009. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.46131/1.22182. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.45158/1.25225. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.46578/1.25749. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.45572/1.28694. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.44513/1.34859. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.44609/1.25510. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.44118/1.31375. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.42777/1.33053. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.42655/1.31981. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.43821/1.24376. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.42854/1.32075. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.42993/1.34377. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.40292/1.41272. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.38984/1.43654. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.41210/1.42201. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.42530/1.33095. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.38890/1.50308. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.38195/1.48369. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.38687/1.49342. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.37412/1.53369. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.38781/1.52398. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.36197/1.59657. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.36190/1.61295. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.34268/1.61191. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.35152/1.59161. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.34371/1.55307. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.36731/1.53750. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.35393/1.65454. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.37459/1.59534. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.33693/1.66248. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.34264/1.72647. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.31991/1.74435. Took 0.44 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.70713/0.68849. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69663/0.67778. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.68892/0.67793. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68949/0.68429. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.68764/0.70295. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68429/0.70369. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67601/0.71276. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67441/0.71074. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.66821/0.73235. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67161/0.74029. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67074/0.74644. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.67373/0.74437. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66792/0.73636. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66570/0.74599. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66226/0.77000. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65529/0.76224. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.65457/0.78223. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.64896/0.78614. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65706/0.80114. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64917/0.79208. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64172/0.82424. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63901/0.81318. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63889/0.81724. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.64244/0.84575. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.63703/0.81299. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.63522/0.81575. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.63520/0.78810. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62477/0.81442. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62365/0.77280. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62074/0.80784. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61245/0.79519. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60971/0.79216. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.61708/0.78424. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61197/0.78741. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60712/0.78368. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61410/0.77579. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59082/0.78408. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59380/0.78624. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.59372/0.80683. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.59247/0.81086. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.59539/0.81161. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.60426/0.82146. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.58658/0.77197. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.58298/0.79055. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58249/0.76969. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.57417/0.78613. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.56848/0.82488. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55937/0.81273. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.56746/0.84591. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55823/0.82078. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.54903/0.85468. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54037/0.86050. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.53265/0.87717. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.52385/0.91228. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53194/0.88233. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51678/0.91291. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.50061/0.95382. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51783/0.89817. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.51466/0.92299. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.49032/0.95900. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.48435/0.93611. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.47609/1.00611. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.47851/0.98372. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.47395/0.99125. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.49795/0.96933. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.47553/0.96897. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.44542/0.99829. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.45985/1.09968. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.43702/1.00555. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.44428/1.02990. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.45854/1.06824. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.44077/1.02599. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.42262/1.10571. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.42626/1.03999. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.40955/1.10216. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.42197/1.05151. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.42124/1.09482. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.41611/1.08874. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.45317/1.03149. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.42601/1.02286. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.38645/1.04924. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.41941/1.03925. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.40932/1.05728. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40467/1.10824. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.38609/1.13404. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.40018/1.00933. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.44259/1.08815. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.41006/1.09023. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.35091/1.18341. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.33358/1.15618. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.34217/1.25318. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.35008/1.24106. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.32076/1.27196. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.34929/1.15979. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.34222/1.19668. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.32288/1.28673. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.33719/1.21858. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.36447/1.15456. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.35175/1.31424. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.31626/1.22378. Took 0.44 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69426/0.70390. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69156/0.69873. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68152/0.70011. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.67975/0.70592. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.67621/0.70615. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.67137/0.71949. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67142/0.73358. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.66888/0.73076. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.66254/0.74340. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.65780/0.75404. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66232/0.77003. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.65735/0.75862. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.65639/0.74367. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.64894/0.74799. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.64799/0.75410. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.64740/0.76422. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.63289/0.79388. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.63908/0.79713. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.63137/0.81510. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.62728/0.83444. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.62368/0.85978. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63141/0.85344. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.62908/0.82531. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62361/0.81910. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.60904/0.83285. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.61145/0.86140. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61604/0.85648. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.59850/0.83352. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62112/0.82000. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.60620/0.85448. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.59057/0.83427. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.58906/0.85019. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.58916/0.85766. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59423/0.83310. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.58012/0.85538. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.56614/0.84985. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.55736/0.88661. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.57175/0.85130. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.57695/0.85875. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.56443/0.88900. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.55690/0.86414. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.54882/0.86313. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.55522/0.85791. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.54915/0.85123. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.54604/0.84857. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.53082/0.89811. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.54753/0.90538. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.54329/0.89729. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.52208/0.94318. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.52260/1.03000. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.52255/0.99965. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.50144/1.08116. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.49722/1.07683. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.50342/1.08145. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52016/1.08868. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.49151/1.04952. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.49198/1.05822. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.49837/1.00544. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.47618/1.08845. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.46460/1.11884. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.46048/1.05316. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.45816/1.13442. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.45210/1.11322. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.49668/1.10899. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.46462/1.11593. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.44952/1.14544. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.46685/1.15520. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.46440/1.09838. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.45521/0.96527. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45023/1.15436. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.42641/1.09807. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.44245/1.14009. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.42925/1.16750. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.41717/1.06628. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.43409/1.17652. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.40555/1.21807. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.40836/1.20813. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.38742/1.24577. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.39548/1.21470. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.40474/1.10643. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.39293/1.16835. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.41341/1.19494. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.37849/1.17215. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.38715/1.24626. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.38845/1.16981. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.38144/1.16227. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.36098/1.21304. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.34267/1.29241. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.37917/1.15934. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.35687/1.14768. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.37332/1.20493. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.39186/1.12097. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.35291/1.11268. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.34753/1.24182. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.33392/1.34789. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.35078/1.19216. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.34795/1.13032. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.31380/1.29574. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.30714/1.35894. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.30647/1.36263. Took 0.43 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69131/0.67503. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68093/0.68581. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.67891/0.68346. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.67595/0.68132. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.67431/0.68385. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.67117/0.67773. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.66984/0.68423. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.66582/0.68568. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.66421/0.69596. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.66234/0.70128. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.65834/0.68969. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.65043/0.73621. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.64596/0.71592. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.64786/0.72844. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.64163/0.70917. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.63812/0.73851. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.64153/0.74877. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.63766/0.72522. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.62707/0.76319. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.62117/0.76582. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.62223/0.76418. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.62221/0.77644. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.60538/0.77090. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.60833/0.80582. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.60014/0.80520. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.60494/0.81670. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.60081/0.82119. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.60343/0.82986. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.59058/0.81673. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.58928/0.78489. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.57770/0.81331. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.58450/0.82101. Took 0.47 sec\n",
      "Epoch 32, Loss(train/val) 0.58117/0.82141. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.57248/0.83822. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.56339/0.85246. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.54902/0.87166. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.55715/0.84643. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.55399/0.83613. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.54356/0.92109. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.54187/0.84219. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.53274/0.86095. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.51697/0.88168. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.51603/0.89043. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.51149/0.92183. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.50017/0.89918. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.51082/0.86163. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.49952/0.90533. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.48681/0.90711. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.50349/0.84143. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.47116/0.89918. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.47190/0.91815. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.46967/0.96371. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.46495/0.86095. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.46882/0.91409. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.47143/0.85447. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.44345/0.87978. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.44484/0.90633. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.43318/0.89739. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.44139/0.88849. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.42704/0.92926. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.42276/0.92450. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.42248/0.94735. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.41434/0.94634. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.41029/0.93918. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.38286/0.97901. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.40804/0.95341. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.40615/0.98341. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.39336/0.96261. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.37412/1.00413. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.36162/1.02852. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.36275/1.05290. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.36401/1.00080. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.36743/1.01057. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.34979/1.02629. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.36767/0.98281. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.35845/1.05613. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.33863/1.05810. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.33262/1.05778. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.34018/1.10338. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.32595/1.03772. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.31923/1.11007. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.30425/1.17721. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.32870/1.08724. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.30230/1.07645. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.28778/1.14864. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.29907/1.14933. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.28917/1.15701. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.27579/1.18240. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.29045/1.18517. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.26877/1.19781. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.28797/1.23582. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.28006/1.16225. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.31592/1.21228. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.26437/1.24023. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.25325/1.30189. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.26245/1.40061. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.27019/1.23419. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.25324/1.22726. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.23916/1.26744. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.22581/1.29855. Took 0.44 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.70744/0.74191. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69345/0.73102. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.68866/0.73475. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69217/0.71253. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.67824/0.71833. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68921/0.72026. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67723/0.73348. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67791/0.73173. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67291/0.72203. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67457/0.72783. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68656/0.70425. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.67460/0.71834. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67789/0.69939. Took 0.47 sec\n",
      "Epoch 13, Loss(train/val) 0.67227/0.71302. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66700/0.69256. Took 0.48 sec\n",
      "Epoch 15, Loss(train/val) 0.67307/0.69337. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66719/0.69423. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66889/0.71960. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67448/0.72400. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67214/0.73042. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.66528/0.74389. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66178/0.73843. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66220/0.74519. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.65847/0.77270. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66011/0.76721. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64868/0.77657. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.65131/0.76846. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.65026/0.78694. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64603/0.79751. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.64358/0.81465. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64049/0.82861. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63291/0.83641. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63116/0.86574. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63700/0.82665. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62760/0.81230. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.62638/0.81892. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.62334/0.84016. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62026/0.83659. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.61662/0.88556. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.61062/0.91520. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60358/0.91527. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60250/0.94755. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.61853/0.94113. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60501/0.93000. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.61357/0.94536. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.61323/0.95791. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.60961/0.94748. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.60887/0.90625. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.59346/0.90657. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.59310/0.93617. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.59613/0.98511. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.58890/1.00435. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.58712/1.00459. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.58442/1.01587. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.57342/1.03429. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.57630/1.06405. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.57171/1.07314. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.56803/1.01533. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.57496/1.05432. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.56899/1.02313. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.57181/1.01029. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.58035/0.97709. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.56197/1.00691. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.55893/1.02361. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.55965/1.07041. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.56318/1.04119. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.55787/1.05780. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.55491/1.03718. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.54214/1.08979. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.53969/1.07663. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.53859/1.10195. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.53984/1.11204. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.53785/1.11976. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.52791/1.15111. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.52237/1.20569. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.51271/1.19526. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.50726/1.19111. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.51503/1.20184. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.52064/1.20448. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.50915/1.16831. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.49517/1.22567. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.50108/1.26063. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.47677/1.35934. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.48336/1.32298. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.48233/1.39885. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.49660/1.21904. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.46542/1.32051. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.46998/1.32645. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.47737/1.36252. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.46789/1.35382. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.47194/1.28614. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.45135/1.37298. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.45802/1.39120. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.46780/1.23397. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.46689/1.28977. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.46744/1.32694. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.43104/1.40608. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.42857/1.45479. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.44086/1.41337. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.44241/1.39145. Took 0.45 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.70592/0.69420. Took 0.61 sec\n",
      "Epoch 1, Loss(train/val) 0.70365/0.69613. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69026/0.70440. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69509/0.70013. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68981/0.69588. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68907/0.69417. Took 0.48 sec\n",
      "Epoch 6, Loss(train/val) 0.68929/0.70496. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69135/0.69369. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.68128/0.70629. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.68449/0.70957. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67887/0.71140. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67568/0.72395. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67426/0.72750. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67669/0.72736. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67109/0.74127. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66964/0.74984. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67711/0.75232. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66498/0.75588. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66774/0.75318. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66193/0.73227. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65557/0.71885. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65525/0.72396. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66108/0.73480. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65826/0.71561. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65979/0.69668. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64870/0.70913. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64370/0.71576. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64662/0.73928. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.64853/0.74051. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.65095/0.72120. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63342/0.74014. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.63531/0.72989. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63341/0.74263. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.63293/0.71872. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.63486/0.74414. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.62159/0.76866. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61372/0.78781. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61841/0.79122. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60991/0.80253. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.61520/0.81234. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.59457/0.83431. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.60261/0.81076. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59852/0.81330. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61043/0.84786. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.61745/0.82331. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.60788/0.82333. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.60664/0.83271. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.59739/0.85621. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59305/0.86317. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.59298/0.84419. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.59244/0.86647. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.58310/0.84146. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.58663/0.86305. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.58746/0.86207. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.57213/0.87613. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.57136/0.93434. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.56326/0.95490. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.54964/0.98603. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.55856/0.99718. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.55914/0.99696. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54335/0.97211. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.53610/0.96723. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.54656/0.93855. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.52407/1.03563. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.53040/1.01671. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.52041/0.96726. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.51663/1.00425. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.52807/1.01792. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50956/1.04654. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.51340/1.06044. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.51159/1.04673. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.49639/1.02124. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.49043/1.04516. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.50153/1.14648. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.51834/1.04525. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.51251/1.02977. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.48713/1.10522. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.47898/1.06334. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.47914/1.08300. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46561/1.14388. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.47263/1.16072. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.45881/1.14501. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.47257/1.15697. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.44818/1.13387. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.43935/1.18409. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.41822/1.20457. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43619/1.16417. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.43222/1.20271. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.42529/1.22011. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.41232/1.20938. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.39846/1.16619. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.41125/1.26381. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.41099/1.19354. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.40105/1.20413. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.37877/1.26448. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.41681/1.16193. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.40405/1.12237. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.40798/1.14782. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.37565/1.22973. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36629/1.14019. Took 0.45 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.70144/0.68280. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69347/0.69313. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68824/0.70937. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.67986/0.72790. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.67939/0.74069. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.67647/0.73977. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.66886/0.73357. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67553/0.73675. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67421/0.75670. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.66435/0.76630. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.66748/0.77303. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67000/0.77651. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66546/0.80288. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65507/0.80823. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.65518/0.80801. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.64996/0.81100. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65186/0.80366. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.64240/0.82673. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.64406/0.82816. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64522/0.82862. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.62913/0.84678. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.63292/0.81139. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63147/0.81952. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.61373/0.82282. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.61422/0.79951. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.61373/0.79765. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61702/0.82389. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.61169/0.82390. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.60291/0.81603. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61178/0.81179. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60727/0.83892. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.60322/0.84935. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.59797/0.85976. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60546/0.85267. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.58373/0.82887. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.59716/0.80839. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.57980/0.85845. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57696/0.87666. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.56865/0.86371. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.57485/0.87449. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56744/0.87955. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.55036/0.86849. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.55493/0.88380. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.53250/0.86862. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.54629/0.87244. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.54304/0.88077. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.52253/0.91134. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.51899/0.85493. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.53612/0.84792. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.49156/0.84244. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.50668/0.81652. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.48100/0.88498. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.49994/0.86900. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.47665/0.82747. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.48761/0.83745. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.46928/0.83228. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.46037/0.84698. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.46123/0.82055. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.48219/0.85991. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.46379/0.81993. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.45494/0.84193. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.46336/0.81791. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.44643/0.84070. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.45142/0.83503. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.44186/0.84367. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.43277/0.84308. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.41766/0.85015. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.42300/0.83161. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.42578/0.89172. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.43476/0.88961. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.40615/0.89664. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.39152/0.87456. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.40531/0.87872. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.40202/0.94889. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.38350/0.98666. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.38537/0.93857. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.38268/0.98286. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.38356/1.03676. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.37296/1.08134. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.38605/1.00338. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.35451/1.03182. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.36645/0.97223. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.34174/1.01950. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.36491/1.05325. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.37133/1.05376. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.37383/0.99913. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.34787/1.02336. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.33115/1.03555. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.34061/1.06481. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.33556/1.07728. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.34687/1.07010. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.33643/1.19212. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.34384/1.10823. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.32648/1.07396. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.29676/1.13125. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.29412/1.03461. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.30660/1.02466. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.31951/1.11201. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.29525/1.10296. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.28536/1.06898. Took 0.43 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70952/0.73747. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69796/0.73314. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69113/0.72901. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69229/0.72001. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68836/0.72960. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68804/0.72621. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68182/0.73966. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67651/0.75852. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68284/0.74385. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68058/0.74686. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67844/0.75138. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67318/0.75056. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66721/0.76116. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67050/0.76160. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66520/0.77714. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67216/0.76991. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66775/0.77926. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66110/0.78546. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66383/0.75870. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65154/0.77172. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64921/0.78501. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65468/0.77856. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65248/0.76404. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64570/0.78887. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64216/0.80028. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63581/0.78960. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63942/0.79159. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63186/0.80495. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62140/0.83838. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62089/0.81909. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62386/0.80104. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.62029/0.81514. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61399/0.84577. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.60942/0.85399. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61103/0.78727. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.60283/0.82596. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60199/0.86459. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60189/0.84968. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.59809/0.78456. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58479/0.85979. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59184/0.85708. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.58548/0.81763. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57784/0.86294. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57622/0.89709. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.58165/0.90925. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.57248/0.99230. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56577/1.00335. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55807/0.97693. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.56561/0.93010. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.55863/1.01853. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54991/1.01228. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.53692/1.01694. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54620/1.13901. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.55522/1.03798. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52528/1.09815. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.54025/1.10151. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52132/1.05229. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52041/1.08065. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.51023/1.12846. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.51363/1.09723. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.50422/1.18271. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.51405/1.11718. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50916/1.19829. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51713/1.13929. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.49435/1.16276. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.48722/1.17741. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.48363/1.20386. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.50486/1.14274. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49056/1.12037. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.49219/1.13673. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48311/1.23013. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.47164/1.28872. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.47873/1.15200. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.49783/1.19479. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.49180/1.31486. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.48308/1.09609. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.47181/1.20198. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.47909/1.16414. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.47533/1.23410. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.45573/1.24655. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.44801/1.31486. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.44615/1.27466. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.43817/1.26705. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.45010/1.32518. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.44618/1.34346. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.44821/1.37779. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.41853/1.34418. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.42962/1.23162. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.43473/1.35906. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.43586/1.28621. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.40515/1.31300. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.41620/1.29110. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.41191/1.26543. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.41720/1.24186. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.41437/1.22157. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.39747/1.29157. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.40401/1.37854. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.38632/1.40188. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.38590/1.29713. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.39899/1.27319. Took 0.45 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.70402/0.70207. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68993/0.69974. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69085/0.71573. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68760/0.71289. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68250/0.72502. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67840/0.73560. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68014/0.74397. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67774/0.74455. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67266/0.74808. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.66668/0.76921. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66807/0.77084. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66015/0.78434. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66044/0.78039. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65682/0.76896. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66226/0.76457. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.66632/0.74701. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.65023/0.74040. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.65709/0.73564. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64673/0.71940. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64529/0.74868. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63897/0.75571. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.62842/0.77752. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.63490/0.75858. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63373/0.76013. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.61885/0.75255. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.61482/0.78316. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61651/0.78737. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.60840/0.78462. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.60377/0.79476. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60297/0.80294. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.59335/0.79322. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.59864/0.81531. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.58567/0.81624. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58844/0.84103. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.58858/0.82933. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.57760/0.83269. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.57232/0.82533. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58766/0.83037. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.56621/0.83698. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57757/0.85238. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.55907/0.83445. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.56760/0.86177. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.55553/0.86327. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.55146/0.87383. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55109/0.87747. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.53762/0.88601. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.54332/0.90570. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.53189/0.90088. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.52912/0.92578. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.51565/0.96224. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.51846/0.94980. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.51199/0.95831. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52282/0.92698. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.50435/0.94606. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51736/0.92430. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.50435/0.94811. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.50234/0.95051. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.47302/1.01768. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.48970/1.03204. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.49039/0.98536. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.47766/1.00407. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.47219/0.99625. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.46324/1.06499. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.49054/1.02115. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.46405/1.02114. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.46916/1.00279. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.47387/1.01595. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.45635/1.04654. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.45330/1.02323. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.45268/1.02612. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.44545/1.00611. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.44139/1.08515. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.42942/1.08242. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.41420/1.13205. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.42800/1.07072. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.42490/1.12421. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.40895/1.06701. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.40745/1.20974. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43241/1.15506. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.41322/1.18704. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.39645/1.24480. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.41180/1.29318. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.38680/1.19532. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.36937/1.29717. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.38573/1.21626. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.38116/1.25684. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.39811/1.24513. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.37624/1.26666. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.34702/1.30656. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.36329/1.28014. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.37223/1.32785. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.33393/1.44077. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.33689/1.38795. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.33806/1.50484. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.34106/1.36519. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.35621/1.46784. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.32989/1.37316. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.32246/1.49903. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.32311/1.43784. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.31041/1.51981. Took 0.43 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70966/0.69241. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69335/0.69354. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69571/0.69677. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68696/0.70516. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68185/0.72791. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68068/0.73269. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67344/0.73911. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67349/0.75025. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.66594/0.74672. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66700/0.76771. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.66834/0.75828. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67114/0.76711. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66177/0.77149. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67463/0.77537. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66268/0.77590. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65771/0.77106. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66212/0.78456. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65388/0.78963. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66456/0.76839. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65839/0.79098. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65671/0.80147. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64471/0.81651. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64741/0.82093. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64775/0.84403. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.63220/0.87074. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.62874/0.87252. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61747/0.91297. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.61663/0.91469. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62155/0.90709. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.61262/0.94247. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61210/0.95308. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.60188/0.96488. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60811/0.94665. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60833/0.95248. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59536/0.95573. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58493/0.96065. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58074/0.95999. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.57175/0.97568. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.56299/0.98847. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.56418/0.96702. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.55543/0.98222. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.55108/0.97886. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56961/1.01315. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.55421/1.01173. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.54613/1.00879. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.54049/1.02943. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.52450/1.01482. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.52478/1.03781. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.53108/1.05440. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.51592/1.06069. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.52702/1.03351. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.52022/1.07819. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.50475/1.10955. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.50806/1.15105. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52179/1.14588. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51812/1.10007. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.49403/1.14714. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.48801/1.16371. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50011/1.18095. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.46775/1.17877. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.46455/1.21888. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.47280/1.20877. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.47657/1.18255. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.47082/1.15836. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.46832/1.20552. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.46125/1.23892. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.44462/1.22957. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.44078/1.26273. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.43535/1.25843. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45200/1.26258. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.44580/1.29093. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.43022/1.30863. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.43104/1.31814. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.40974/1.34439. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.41680/1.37200. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.42178/1.33613. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.41579/1.38087. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.41509/1.36348. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.39955/1.43301. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.40324/1.37217. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.40156/1.38005. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.38914/1.34149. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.40788/1.35935. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.39647/1.40505. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.37027/1.44935. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.36236/1.47078. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.38664/1.37859. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.36976/1.45061. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.37768/1.45090. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.35927/1.51105. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.35245/1.54982. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.38137/1.50201. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.35263/1.53186. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.33205/1.62058. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.34392/1.63846. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.34262/1.69250. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.35130/1.58927. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.33655/1.65061. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.34633/1.66128. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.32296/1.67645. Took 0.44 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70215/0.71314. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69676/0.71305. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68985/0.71467. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69383/0.71779. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68612/0.72236. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67981/0.72047. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67804/0.72537. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67992/0.72431. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67326/0.73150. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67330/0.71821. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.66927/0.73508. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66969/0.72842. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67061/0.73843. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66093/0.73947. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66887/0.75678. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66696/0.76090. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66022/0.76472. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65047/0.76827. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65012/0.75862. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64865/0.77540. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64360/0.77804. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64936/0.76580. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64442/0.75285. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64367/0.77745. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63205/0.77833. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.63266/0.79564. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62843/0.81462. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62767/0.83147. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61611/0.84487. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61885/0.83096. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.61008/0.84779. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61337/0.85550. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60617/0.84013. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.60234/0.84655. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.60409/0.88256. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60725/0.87665. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.59477/0.87253. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.59003/0.86528. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.58683/0.88004. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.58940/0.87866. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58165/0.86908. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58727/0.89088. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56795/0.89550. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57235/0.91040. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57166/0.89287. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55776/0.90199. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.56039/0.89582. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55530/0.91453. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.54500/0.91783. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55463/0.90800. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53731/0.91683. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.53865/0.90721. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.53917/0.95700. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.52130/0.95852. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.53347/0.93035. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51957/0.93515. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.51226/0.95705. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51614/0.97892. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.51902/0.95401. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52832/0.91952. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.49839/0.94323. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.50981/0.98616. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49958/0.96022. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48320/0.97733. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.48717/0.98948. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.48291/0.97441. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.47458/1.00288. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.46629/0.97765. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.47041/0.98492. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46059/1.00978. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.45993/0.97966. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45101/1.03870. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.45481/0.95510. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.44037/1.01626. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.44121/1.04699. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.45292/1.00104. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.45401/0.98215. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.44247/1.02215. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.41493/1.09081. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.42293/1.05668. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.40044/1.12862. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.42134/0.98568. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.41208/1.03136. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.39681/1.01762. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.39133/1.17404. Took 0.47 sec\n",
      "Epoch 85, Loss(train/val) 0.41831/1.05558. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.44070/0.96672. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.41458/1.01221. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.41623/1.06251. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.41504/1.08299. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.39570/1.13211. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.38306/1.15583. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.36992/1.11844. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.35195/1.21819. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.36488/1.19724. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.37098/1.19541. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.36568/1.20585. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.34907/1.17917. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.34557/1.27879. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.38815/1.17211. Took 0.43 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.70027/0.73611. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69819/0.73988. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69490/0.74320. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69333/0.74719. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69516/0.75246. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69669/0.74265. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68897/0.73865. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68627/0.73782. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.69079/0.73562. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.69249/0.73138. Took 0.48 sec\n",
      "Epoch 10, Loss(train/val) 0.68580/0.73346. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.68835/0.73219. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.68751/0.72708. Took 0.47 sec\n",
      "Epoch 13, Loss(train/val) 0.68577/0.73076. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68241/0.73119. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67955/0.73774. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.68821/0.74104. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.68106/0.73845. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.67890/0.75209. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.68227/0.75710. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67069/0.76158. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.67607/0.76965. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66916/0.77103. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66973/0.78026. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66899/0.78191. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.66627/0.77878. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.66428/0.77504. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.66313/0.78440. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.66716/0.78199. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.65197/0.77703. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64833/0.78904. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.65180/0.80761. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64434/0.80986. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63947/0.81323. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.63517/0.85516. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.63960/0.85154. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.64070/0.85764. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.62896/0.86240. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.63137/0.87098. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.62345/0.85606. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.61981/0.86596. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.61349/0.88109. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60539/0.89788. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60165/0.91393. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.60313/0.90746. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59607/0.94329. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.57969/0.95476. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57960/0.97386. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.58201/0.98722. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.58794/0.96558. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56646/0.99656. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56009/1.03757. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.55156/1.03301. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56604/1.02725. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.55019/1.04235. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.54862/1.05719. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.53326/1.09279. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.53261/1.06702. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.53873/1.07374. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50604/1.06973. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52596/1.08971. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.53133/1.11627. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.50848/1.10780. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51472/1.11500. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.51472/1.14087. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50712/1.13015. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.52000/1.17495. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.49117/1.15909. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.48188/1.20930. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.47798/1.22615. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.47301/1.23715. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48926/1.24217. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.45910/1.22484. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.45201/1.25700. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.44893/1.30234. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.47932/1.24548. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.44971/1.24348. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.46212/1.24291. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.44781/1.23992. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.46353/1.22013. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.42397/1.31506. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.43772/1.25089. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.43754/1.32388. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.42787/1.26450. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43949/1.24061. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.44951/1.29213. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.42931/1.27796. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.41541/1.30528. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.41731/1.35663. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.41411/1.28406. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.39804/1.30365. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.40754/1.34876. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.41336/1.34246. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.40197/1.32500. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40454/1.36581. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.40560/1.37784. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.40745/1.29808. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.38951/1.34334. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.40126/1.30777. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.39404/1.36371. Took 0.43 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.71010/0.70450. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70147/0.71568. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69940/0.72525. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69711/0.73558. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69670/0.71118. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68934/0.70909. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68806/0.70795. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.69151/0.70744. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.69035/0.70286. Took 0.48 sec\n",
      "Epoch 9, Loss(train/val) 0.68560/0.71482. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67881/0.70854. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67555/0.70059. Took 0.47 sec\n",
      "Epoch 12, Loss(train/val) 0.67590/0.71044. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67053/0.71761. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67387/0.72091. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66495/0.74480. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66430/0.75066. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66075/0.73731. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66366/0.74826. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65630/0.75406. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65483/0.74324. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.66109/0.74908. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65050/0.75844. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64634/0.74443. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64688/0.74142. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64355/0.74698. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64137/0.76366. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63699/0.76814. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.64314/0.77779. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63850/0.79660. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.62910/0.79655. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61981/0.81247. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63076/0.81991. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62741/0.83828. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.61387/0.85379. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60822/0.84484. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60751/0.84840. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60173/0.80427. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.60485/0.77562. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.59246/0.79755. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59514/0.76250. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.59137/0.75031. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.57907/0.75892. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.58389/0.72988. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58907/0.77192. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57126/0.78433. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56893/0.77810. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.55254/0.77610. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.56654/0.85130. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.54975/0.81808. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56801/0.83699. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55220/0.81905. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.54747/0.85177. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56016/0.85013. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53415/0.84298. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.53238/0.90281. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.52521/0.90062. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52622/0.87948. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.51199/0.87700. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.52017/0.91106. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49912/0.96259. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.52976/0.95772. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52093/0.91803. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.50571/0.97607. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.48041/0.97184. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.49411/1.05289. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48382/1.01342. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48849/0.99551. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.48986/0.97443. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.48148/1.08205. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.49111/1.00788. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.47410/1.05269. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.44973/1.02397. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.45301/1.20220. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.47470/1.10130. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.44218/1.15439. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43903/1.12897. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.44626/1.21555. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.45570/1.13677. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.43472/1.08759. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.41023/1.11967. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.41337/1.19890. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.40220/1.30394. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40190/1.28691. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.42027/1.34194. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.42684/1.39355. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.41506/1.34807. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.42714/1.42455. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.39324/1.55737. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.37946/1.49671. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.42106/1.35463. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.40903/1.33718. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.39609/1.55250. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.42846/1.49068. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.38075/1.57701. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.38610/1.43271. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.40855/1.52938. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.38237/1.49154. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.35363/1.59834. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36035/1.57871. Took 0.44 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.71127/0.70536. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69648/0.71021. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69247/0.71352. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69832/0.71822. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68782/0.74023. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69062/0.73072. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68932/0.73096. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68275/0.75375. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67810/0.74751. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68080/0.75225. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67998/0.74656. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.68168/0.75909. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68364/0.76983. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67417/0.75237. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.68110/0.75961. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67652/0.74337. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66927/0.73937. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66699/0.74295. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66721/0.75034. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67093/0.74053. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.67018/0.75444. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66434/0.75499. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66262/0.77153. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.65748/0.78288. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65088/0.76861. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64245/0.77180. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.64608/0.75727. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64628/0.74990. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63585/0.76648. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63747/0.78594. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.62588/0.78973. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.62241/0.80078. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.60288/0.80374. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60829/0.81021. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61280/0.83570. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.60819/0.81406. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.60232/0.81373. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59995/0.82801. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59553/0.82889. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59194/0.85133. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.59188/0.86276. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57041/0.90245. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.58251/0.89386. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57233/0.89292. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57145/0.88140. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.57009/0.89455. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.56407/0.86890. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56155/0.93262. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56456/0.89022. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55094/0.93540. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.54168/0.97471. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54672/0.97268. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54458/0.97105. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.53140/0.99372. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54241/0.93547. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.52154/0.98708. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.52426/1.01782. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53073/0.99772. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.52462/1.04671. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.52700/1.05385. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51616/1.06672. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.51516/1.04897. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.48966/1.07826. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48632/1.13110. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49082/1.12272. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.49096/1.13321. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.49203/1.12050. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48341/1.08545. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.45570/1.17099. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47061/1.11735. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49987/1.10079. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45659/1.15502. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46647/1.14416. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.44519/1.21189. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.45852/1.12051. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.45120/1.15158. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.45975/1.13317. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.42935/1.22725. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44378/1.20056. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.42168/1.21495. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.40827/1.23162. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.42331/1.24612. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.41943/1.20952. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.42112/1.16678. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.44637/1.15277. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.41988/1.17900. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.39202/1.18378. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.40515/1.17984. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.39170/1.20542. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.42516/1.16260. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.39634/1.26844. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.38914/1.18834. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.38373/1.24721. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.37710/1.34815. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40384/1.23719. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.36545/1.36008. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.37631/1.32182. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.38085/1.36530. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.34425/1.27338. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.34027/1.33874. Took 0.45 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.70830/0.70026. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69703/0.70031. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69185/0.70173. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69278/0.69707. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69177/0.69713. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68650/0.69927. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68550/0.70098. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68171/0.70015. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68211/0.69914. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67829/0.68638. Took 0.48 sec\n",
      "Epoch 10, Loss(train/val) 0.67593/0.69218. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67382/0.69905. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67051/0.69474. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.66582/0.69680. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.65400/0.69920. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65579/0.70534. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65636/0.71439. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65571/0.71821. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65320/0.71131. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64279/0.72337. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64246/0.72979. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63991/0.72927. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64417/0.72891. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63788/0.74417. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.63453/0.72962. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62494/0.74915. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62527/0.75510. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62136/0.75938. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61228/0.76550. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61005/0.77457. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60152/0.77673. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60408/0.78280. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.59897/0.78267. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58619/0.79508. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.58088/0.83512. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.57665/0.84020. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.57480/0.88013. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57162/0.85058. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.56252/0.85923. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.56594/0.88315. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56040/0.87841. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.54532/0.87881. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.55424/0.87171. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.53535/0.93396. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.52693/0.92823. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.52370/0.98497. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.51589/0.97463. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.52286/0.97157. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.49671/0.97750. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.51264/0.91898. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.49700/0.96468. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.51056/0.93884. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.48518/0.95132. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.48449/1.02089. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.48804/1.01418. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.45960/1.02250. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.47487/1.01221. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.45075/1.04242. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.45101/1.03541. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.44778/1.03404. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.45771/1.08783. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.45036/1.14343. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.44112/1.11854. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.43050/1.17141. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.42409/1.13962. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.43174/1.22256. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.42250/1.17319. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.40985/1.17032. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.38535/1.25065. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.40792/1.34675. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.40819/1.26835. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.38496/1.31073. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.38203/1.32678. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.38251/1.21225. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.39868/1.34165. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.43254/1.23015. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.36882/1.28521. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.35967/1.41355. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.35685/1.33802. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.35170/1.37392. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.34539/1.33446. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.33318/1.43320. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.34888/1.32355. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.35619/1.30127. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.32488/1.37457. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.31862/1.44193. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.32000/1.47303. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.30981/1.44306. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.33290/1.42023. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.34348/1.33779. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.28853/1.46979. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.31660/1.37627. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.29780/1.45954. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.27386/1.43656. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.28457/1.58220. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.29040/1.50127. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.27207/1.45685. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.24610/1.54561. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.27104/1.58314. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.28172/1.54156. Took 0.44 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.70519/0.70795. Took 0.61 sec\n",
      "Epoch 1, Loss(train/val) 0.69706/0.71972. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69734/0.71945. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69701/0.72107. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69221/0.72569. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68143/0.73250. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68437/0.72850. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67921/0.73192. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68012/0.75494. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67750/0.75611. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68145/0.76268. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.65933/0.74481. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67106/0.77204. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66736/0.79061. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66158/0.78607. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65997/0.76780. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65693/0.78946. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65646/0.78803. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65871/0.78003. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65638/0.78257. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65229/0.79400. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64355/0.80437. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65255/0.79189. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.64082/0.79257. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63426/0.81608. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.62981/0.83455. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62525/0.85769. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62360/0.85253. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61520/0.87590. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.61509/0.89237. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60955/0.89754. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.60446/0.88600. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.60781/0.90007. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.59999/0.90051. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.60342/0.93052. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58641/0.92754. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.57373/0.96178. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58679/0.94190. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.58641/0.95840. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.56380/0.96986. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57792/0.99138. Took 0.47 sec\n",
      "Epoch 41, Loss(train/val) 0.55983/0.97789. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56507/0.98266. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.55962/1.00696. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55284/1.03775. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.55217/1.06187. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.53775/1.04916. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.53754/1.08864. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.54658/1.08112. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.52961/1.06279. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.53942/1.04781. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.52404/1.04798. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.52214/1.09817. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.52274/1.14940. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.50727/1.10859. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51426/1.13603. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.49052/1.17059. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.50276/1.15065. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.48880/1.15123. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.49258/1.16095. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.47924/1.20509. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.47436/1.22111. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49207/1.17204. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.47642/1.19174. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.46970/1.24963. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.46776/1.23851. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.47575/1.21612. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.49123/1.16006. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.47667/1.17608. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46092/1.27854. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.45352/1.24012. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.43805/1.27774. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.43904/1.19528. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.43074/1.21170. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.43468/1.35471. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.42827/1.32201. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.41161/1.35112. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.39263/1.38210. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.41866/1.33093. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.42009/1.33891. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.41621/1.36561. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.39279/1.37245. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.41481/1.39306. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.41947/1.40747. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.39979/1.35729. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.38479/1.39734. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.40465/1.32691. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.38462/1.50290. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.38032/1.46674. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.34715/1.53240. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.36194/1.54192. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.36718/1.54703. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.36925/1.48047. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.35462/1.51201. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.34442/1.51741. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.34650/1.60502. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.33017/1.62808. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.34483/1.60621. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.32364/1.61800. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.32999/1.63286. Took 0.43 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69716/0.70516. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69149/0.69377. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68527/0.69619. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68762/0.69886. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68236/0.69789. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68284/0.70233. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67928/0.71190. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67776/0.70624. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67612/0.71404. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67768/0.70791. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67317/0.73069. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67017/0.71715. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66601/0.72007. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.67103/0.71571. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66187/0.71803. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.66537/0.73518. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66049/0.72604. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65927/0.72791. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65481/0.74402. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64687/0.74612. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64395/0.74571. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64011/0.74483. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.63991/0.74895. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.63273/0.76505. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.62124/0.78771. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.61953/0.78666. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.62238/0.79356. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.60959/0.78044. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61468/0.78961. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60468/0.80343. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.59788/0.82244. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.59834/0.79815. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.58712/0.83521. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58435/0.82298. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.56496/0.82307. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.56026/0.83337. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.56122/0.84939. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.54855/0.87623. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.54262/0.88092. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.53676/0.88402. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.53315/0.89616. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.51953/0.95735. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.52155/0.95600. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.52318/0.94729. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.53012/0.93450. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.49903/0.97498. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.50667/0.94596. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.48080/0.98585. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.47681/1.08199. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.48397/1.07004. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.48355/1.04542. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.46232/1.08122. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.46818/1.06905. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.45477/1.09013. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.42752/1.06882. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.44447/1.07809. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.44206/1.05566. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.44269/1.17560. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.41897/1.10323. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.42308/1.14512. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.41082/1.17610. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.42642/1.12707. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.42839/1.13996. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.40289/1.05210. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.39244/1.23752. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.38575/1.19520. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.38396/1.13177. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.37641/1.20446. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.36225/1.29340. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.36311/1.25981. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.35347/1.22923. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.35754/1.33633. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.33724/1.23552. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.32763/1.23796. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.32781/1.28601. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.33166/1.22214. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.34530/1.30555. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.32572/1.35308. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.31468/1.37847. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.29126/1.40516. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.30668/1.40129. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.33389/1.42292. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.32399/1.42766. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.29131/1.43054. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.27722/1.44225. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.26242/1.52916. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.27777/1.51888. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.28231/1.46950. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.28799/1.54506. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.26596/1.43443. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.27364/1.52555. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.24443/1.58085. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.24479/1.50062. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.25471/1.50944. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.27481/1.52188. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.25989/1.65369. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.25850/1.53076. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.23411/1.62383. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.22872/1.53538. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.21501/1.70099. Took 0.44 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69470/0.72386. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69836/0.71968. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69332/0.71591. Took 0.49 sec\n",
      "Epoch 3, Loss(train/val) 0.69207/0.71357. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68511/0.72023. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68522/0.73443. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.67768/0.74388. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67695/0.74944. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67811/0.77520. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67830/0.77623. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67394/0.77072. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.67322/0.77111. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67103/0.77026. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67014/0.77251. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66701/0.78463. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66395/0.78274. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66714/0.78732. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65713/0.79870. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65296/0.82410. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65650/0.81772. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65277/0.82778. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65051/0.83459. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64816/0.83194. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.65127/0.83639. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64511/0.83641. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64049/0.83716. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.63403/0.83877. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64001/0.85045. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62658/0.86898. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62274/0.87469. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62284/0.88438. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61282/0.90766. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60481/0.90595. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.60235/0.94329. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60738/0.88859. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59999/0.87082. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.57966/0.91265. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58293/0.92116. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57484/0.91084. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.58203/0.88622. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.57251/0.87958. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.56208/0.86207. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.55450/0.87266. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.55839/0.86900. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55119/0.84363. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.52841/0.84951. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55182/0.86843. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.52954/0.92736. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.51692/0.88632. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.51508/0.91043. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.50987/0.90541. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.50364/0.87720. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.53790/0.87560. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.48985/0.92485. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.48563/0.92021. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.47100/0.90743. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.46296/0.93800. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.46783/1.00330. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.46327/0.90419. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.44013/0.99709. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.43868/0.97209. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.44461/1.01559. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.43328/1.01071. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.41561/0.99577. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.40438/1.10234. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.41173/1.05905. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.40798/0.96352. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.38453/1.04130. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.38320/1.11384. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.37854/1.13075. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.37536/1.11420. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.35457/1.13647. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.35523/1.14143. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.35091/1.12767. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.33334/1.18791. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.33492/1.14588. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.34385/1.13931. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.34623/1.08475. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.31749/1.18462. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.32966/1.06171. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.34278/1.11980. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.29536/1.13829. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.30973/1.21023. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.29260/1.31129. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.31624/1.26502. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.29518/1.17823. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.28914/1.18125. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.30824/1.21995. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.27565/1.25749. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.26371/1.16571. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.24707/1.18271. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.26435/1.17275. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.26717/1.38242. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.28585/1.30135. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.27178/1.33440. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.27242/1.24180. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.24440/1.23253. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.24248/1.19324. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.24490/1.40012. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.25071/1.23952. Took 0.44 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69881/0.69084. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69715/0.69156. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69299/0.69172. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69104/0.70090. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68805/0.70509. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68935/0.70332. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68379/0.71115. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68291/0.71664. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68032/0.72530. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68185/0.72379. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67624/0.72997. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67181/0.74178. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67643/0.74011. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67535/0.73165. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67278/0.73835. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66822/0.74114. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66545/0.75156. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66381/0.74747. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66172/0.76380. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66715/0.76883. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66003/0.75676. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65501/0.77824. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65779/0.76422. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63759/0.79943. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.63838/0.80367. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63569/0.81800. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63332/0.81306. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62406/0.85198. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62877/0.81558. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62133/0.85984. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.60904/0.89497. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62031/0.84717. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61251/0.88803. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.60972/0.86564. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.60963/0.86932. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60195/0.88916. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.59684/0.89335. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.59036/0.90341. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58841/0.93073. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.58265/0.90583. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56976/0.96102. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57803/0.96929. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.58204/0.93538. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56728/1.01527. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56166/0.92812. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.55010/1.01145. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55672/0.99269. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57142/0.94126. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.54546/0.97087. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.53788/1.02552. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55800/1.01843. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.53472/1.00208. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.51986/0.99333. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52763/1.01672. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52721/1.05841. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.52758/1.04140. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.53256/1.02463. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52303/1.00236. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.49375/1.04915. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.51877/1.00794. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.50671/1.05611. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.51122/1.03075. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.50449/1.00866. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51004/0.98010. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.49441/1.03500. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.48933/1.02715. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.49203/1.03344. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48362/1.08865. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49068/1.08746. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.48296/1.06572. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.48750/1.02019. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.47469/1.04533. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.47328/1.06516. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48893/1.04972. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.46145/1.05653. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.45956/1.06815. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.47189/1.07068. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.44885/1.07519. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44328/1.12151. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.45098/1.11211. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.44255/1.13983. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.44621/1.16065. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.43524/1.11358. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.42334/1.14022. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.44561/1.09989. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.43100/1.12028. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.41832/1.11311. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43799/1.13126. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.42534/1.15767. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.41549/1.16072. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.43399/1.12631. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.40765/1.15488. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.44315/1.07674. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.41025/1.08942. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.39554/1.16000. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.38997/1.18774. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.38776/1.16531. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.37487/1.19651. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.40099/1.15812. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.38785/1.22250. Took 0.45 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.71202/0.72118. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.71140/0.73665. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.71860/0.71750. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.70033/0.71655. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.70308/0.71797. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69867/0.72640. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69242/0.72389. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.69468/0.73343. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69208/0.73419. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68960/0.73961. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68917/0.73300. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.68063/0.72951. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68118/0.73312. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68366/0.72624. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68067/0.72914. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66700/0.73499. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66845/0.74330. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66793/0.73920. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66180/0.74133. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66210/0.74323. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.66327/0.75465. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65809/0.76294. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.66238/0.75406. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65850/0.75322. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65667/0.79898. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.65930/0.77162. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65254/0.77993. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64377/0.79303. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.65730/0.76313. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64936/0.78711. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.65774/0.76499. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.64291/0.76258. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64305/0.80448. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.64279/0.80243. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.63877/0.78896. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.62660/0.79569. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.62830/0.80957. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.62933/0.81355. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62624/0.83957. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.63324/0.78395. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61516/0.81556. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.61120/0.84152. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.61311/0.80134. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59996/0.82287. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.61392/0.82917. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.59274/0.82638. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59513/0.83040. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.59737/0.85551. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59510/0.85942. Took 0.47 sec\n",
      "Epoch 49, Loss(train/val) 0.58892/0.86970. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.58291/0.86114. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.58956/0.86266. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.57444/0.86590. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.57332/0.90144. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.57064/0.91574. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.56325/0.94994. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55917/0.96137. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.56664/0.92397. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.57496/0.93678. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.56400/0.94495. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54602/0.98549. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.54465/0.96767. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54097/1.02596. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.53274/0.97186. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.54341/1.04214. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.52146/1.06946. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.52764/1.04381. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.52099/1.08042. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50684/1.07836. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.51730/1.01259. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48471/1.02423. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.49018/1.06865. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.47981/1.14984. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46058/1.17645. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.46045/1.20197. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.47046/1.20494. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46685/1.10548. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46175/1.16321. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.45606/1.11972. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.50941/1.20441. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.45232/1.21919. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.45556/1.19693. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.44391/1.29269. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.45550/1.10245. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.45230/1.17011. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.44565/1.26051. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.42238/1.21137. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.38461/1.24165. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.40725/1.25418. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.40497/1.23191. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.41137/1.23377. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.41208/1.31787. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.40557/1.25932. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.39891/1.21432. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.41437/1.20399. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.38654/1.28586. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.37111/1.30810. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.37222/1.28550. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.35435/1.23277. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.37873/1.28782. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69970/0.71116. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69734/0.71191. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68996/0.70789. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69045/0.69498. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68138/0.69043. Took 0.48 sec\n",
      "Epoch 5, Loss(train/val) 0.68364/0.69452. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.67776/0.69366. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67612/0.70139. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67528/0.70394. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.67081/0.71731. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67064/0.71254. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.66681/0.71984. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66405/0.73670. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66492/0.72247. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66190/0.71313. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66108/0.71575. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66067/0.72964. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65347/0.73414. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65269/0.73015. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64102/0.75145. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64425/0.77730. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63671/0.76213. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63526/0.77782. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.63504/0.77608. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.62748/0.78874. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.62128/0.79834. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63102/0.74821. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61725/0.79278. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.60144/0.79355. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.60668/0.83201. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.59553/0.78569. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.59450/0.79641. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.58354/0.78145. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.58311/0.79484. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.57767/0.79429. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.56707/0.81266. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.57202/0.80928. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.55879/0.82697. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.54922/0.82037. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.55852/0.90174. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.55263/0.89426. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.54569/0.82435. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.52889/0.88533. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.51871/0.88528. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.52333/0.89316. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.51131/0.87427. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.50520/0.91098. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.48691/0.93410. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.50615/0.90747. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.51426/0.91144. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.51215/0.94766. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.48240/0.96298. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.49387/0.94807. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.47272/1.02687. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.47162/1.04842. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.45697/0.98196. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.46320/1.03666. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.45282/1.06679. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.44090/1.08468. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.45015/1.10151. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.42965/1.15995. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.42107/1.10754. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.40663/1.19550. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.40660/1.21827. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.40767/1.18030. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.41782/1.23442. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.40259/1.22776. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.38866/1.26231. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.38862/1.19822. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.36793/1.16437. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.37233/1.24350. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.37106/1.19941. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.37342/1.21089. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.36661/1.27326. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.35821/1.31424. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.34928/1.33029. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.34201/1.48349. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.34570/1.42784. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.33978/1.40950. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.31699/1.31706. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.32775/1.39263. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.31076/1.49549. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.29059/1.49837. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.30289/1.52083. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.27838/1.57920. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.26545/1.56188. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.27083/1.62433. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.25758/1.60365. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.30631/1.55496. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.30053/1.44084. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.27162/1.51248. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.26199/1.56665. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.26517/1.66145. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.24654/1.60675. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.27381/1.71481. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.23983/1.88042. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.22436/1.84500. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.22947/1.76664. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.21988/1.89135. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.21780/1.97254. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70838/0.68672. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70210/0.68798. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69629/0.69532. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69333/0.70447. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68964/0.70920. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68618/0.70255. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68465/0.71052. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68049/0.71840. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68137/0.72138. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67904/0.72101. Took 0.42 sec\n",
      "Epoch 10, Loss(train/val) 0.67667/0.71672. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67691/0.72857. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67493/0.71969. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67008/0.73645. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66792/0.74708. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66162/0.76698. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66132/0.76279. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66200/0.76947. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65840/0.76774. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65221/0.77747. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65397/0.75993. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64818/0.75994. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.63982/0.77712. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63632/0.78049. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63096/0.78100. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62337/0.77599. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.61223/0.79610. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.60853/0.77537. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60970/0.76559. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.59470/0.80078. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.59592/0.78468. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.59061/0.80179. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.58699/0.79629. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58085/0.78134. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.58226/0.82981. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.57585/0.84517. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.55615/0.82367. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.56015/0.83955. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.56310/0.79075. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.54071/0.80720. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.54357/0.79830. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.52561/0.78814. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.52354/0.80324. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.52800/0.80381. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.50989/0.83089. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.51397/0.85272. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.50932/0.83792. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.50783/0.82192. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.50982/0.84955. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.50066/0.86674. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.49787/0.87384. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.49138/0.87314. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.47676/0.91893. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.48314/0.89901. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.45690/0.92759. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.46731/0.91676. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.45717/0.92049. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.44634/0.95004. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.44469/0.93956. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.43417/0.97306. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.44843/0.92087. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.42632/0.99718. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.43674/0.98570. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.42707/1.02870. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.42719/1.02216. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.39967/1.03379. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.40307/1.02500. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.39260/1.06917. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.40493/1.05832. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.39948/1.05666. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.40692/1.03467. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.43122/1.06500. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.42073/1.04508. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.40063/1.02773. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.37043/1.10628. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.39745/1.09898. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.38013/1.14657. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.35184/1.18387. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.33890/1.20431. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.35278/1.16232. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.34009/1.17924. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.34510/1.17359. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.37309/1.05313. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.37248/1.15067. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.33711/1.10967. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.31887/1.16040. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.32729/1.12412. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.32651/1.16123. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.32602/1.16324. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.31939/1.17077. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.30350/1.25682. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.32161/1.17759. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.30953/1.18084. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.32660/1.17802. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.30406/1.18306. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.28715/1.32054. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.29173/1.24812. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.26139/1.29100. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.31584/1.20560. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.27965/1.30751. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.70695/0.71985. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.71897/0.73837. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.70733/0.71419. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.70075/0.70944. Took 0.49 sec\n",
      "Epoch 4, Loss(train/val) 0.69597/0.71478. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.70250/0.72221. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69063/0.74148. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.69022/0.71399. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.69509/0.70742. Took 0.48 sec\n",
      "Epoch 9, Loss(train/val) 0.69499/0.70476. Took 0.47 sec\n",
      "Epoch 10, Loss(train/val) 0.68698/0.70683. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.68010/0.69754. Took 0.47 sec\n",
      "Epoch 12, Loss(train/val) 0.68149/0.69965. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.68237/0.70428. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67431/0.70383. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66939/0.68880. Took 0.50 sec\n",
      "Epoch 16, Loss(train/val) 0.66279/0.69406. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66542/0.70478. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66662/0.69144. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65831/0.68803. Took 0.48 sec\n",
      "Epoch 20, Loss(train/val) 0.66163/0.70890. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65133/0.72239. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65407/0.70811. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64669/0.71412. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64442/0.73548. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63993/0.74379. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62863/0.75040. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62993/0.76673. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63288/0.77942. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62944/0.77037. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.62287/0.78247. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62020/0.79238. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.61732/0.81534. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.61129/0.81425. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.60818/0.81956. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.60511/0.84073. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.60456/0.84298. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60694/0.84674. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.60415/0.84767. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60152/0.84939. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60161/0.87888. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59314/0.88113. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.58644/0.89178. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60213/0.87876. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59889/0.85701. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.59809/0.87270. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57915/0.86980. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58680/0.87231. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.58954/0.85530. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.57952/0.83992. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57283/0.85355. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.57224/0.87425. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.56607/0.88058. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.56998/0.89757. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56272/0.89235. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.56124/0.91036. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.55320/0.90652. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.56313/0.91735. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.55900/0.92599. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54869/0.92131. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55487/0.92375. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.54322/0.90231. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.55229/0.88903. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.55255/0.91459. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.55592/0.86792. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.54978/0.92445. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.54857/0.92401. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.53331/0.96048. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52896/0.95794. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.51040/0.92747. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.51819/0.99824. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.50750/0.97211. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.51093/1.00088. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.49348/1.04143. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.50269/0.99020. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.50114/1.02773. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.52699/0.98207. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.49531/1.04221. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.48427/1.00509. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.49030/1.06147. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.47989/1.01049. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.49538/1.04280. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.48072/1.02975. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.47672/1.09077. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.48528/1.06649. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.48610/1.01674. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.47568/0.98468. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.46268/1.06446. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.45514/1.01664. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.45011/1.06027. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.42522/1.17288. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.47921/1.16243. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.46185/1.11162. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.46405/1.06381. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.45334/1.09148. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.43459/1.12835. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.41487/1.18619. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.43425/1.18588. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.43916/1.14287. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.39732/1.23496. Took 0.44 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69685/0.69244. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.69392/0.68848. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.69292/0.68650. Took 0.49 sec\n",
      "Epoch 3, Loss(train/val) 0.68940/0.68222. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.68632/0.68060. Took 0.48 sec\n",
      "Epoch 5, Loss(train/val) 0.68577/0.67962. Took 0.48 sec\n",
      "Epoch 6, Loss(train/val) 0.68269/0.68583. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68017/0.69261. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67648/0.69672. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67317/0.69540. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67087/0.69853. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.66354/0.69923. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65953/0.71925. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66293/0.69104. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.65689/0.73116. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65526/0.71528. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.64535/0.70223. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.64309/0.70836. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.64233/0.72216. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.63150/0.70142. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.62764/0.69872. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.62422/0.68545. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.61263/0.71358. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.59912/0.69371. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.61601/0.72379. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.60044/0.73427. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.58878/0.76182. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.59197/0.77075. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.56212/0.79955. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.56779/0.81698. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.57250/0.82210. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.55286/0.79589. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.55720/0.70950. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.53694/0.80259. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.52561/0.83857. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.52353/0.88195. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.52578/0.80753. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.51562/0.85636. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.52309/0.84445. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.49411/0.87446. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.50194/0.87341. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.49637/0.90067. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.47678/0.93473. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.47540/0.95027. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.46390/0.95368. Took 0.47 sec\n",
      "Epoch 45, Loss(train/val) 0.45070/0.99576. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.48215/0.92207. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.44296/0.92813. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.45558/0.89522. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.42783/0.91824. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.43089/0.98367. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.42292/1.00836. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.41255/0.97452. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.40191/0.96765. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.39231/1.05285. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.38713/1.02566. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.38421/1.04816. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.37795/1.02393. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.35324/1.10364. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.38103/1.08325. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.35093/1.08496. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.36190/1.13269. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.37588/1.11309. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.39941/1.09871. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.37102/1.11383. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.37005/1.05070. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.36317/1.05586. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.33831/1.09252. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.35336/1.09246. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.32793/1.08412. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.30682/1.17422. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.31047/1.22752. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.32358/1.20071. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.30762/1.18365. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.30611/1.25817. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.31010/1.28437. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.29552/1.23021. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.27075/1.29307. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.24575/1.26539. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.24894/1.23123. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.30052/1.27509. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.24653/1.22923. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.28472/1.32800. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.24233/1.25821. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.21855/1.33162. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.23417/1.41703. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.23672/1.28398. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.22536/1.21554. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.22745/1.33805. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.22138/1.39201. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.22850/1.31348. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.20437/1.19013. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.24990/1.21255. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.29658/1.34557. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.25763/1.26971. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.22057/1.37401. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.25082/1.23463. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.20870/1.27605. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.18557/1.30232. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.18656/1.36599. Took 0.45 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69705/0.70482. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69220/0.71729. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69090/0.72103. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68746/0.72193. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68626/0.73749. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68430/0.74378. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68296/0.75118. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67660/0.76384. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67543/0.76320. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67638/0.75809. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67312/0.76504. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67344/0.76005. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66781/0.75955. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66553/0.76504. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66618/0.76292. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66162/0.76057. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65953/0.77128. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65694/0.78388. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65115/0.79583. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64449/0.81354. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63961/0.83521. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63806/0.84512. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63356/0.84382. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.62808/0.85014. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.61540/0.87223. Took 0.47 sec\n",
      "Epoch 25, Loss(train/val) 0.61015/0.89137. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.60627/0.88914. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.60174/0.88263. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.58892/0.90358. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.58806/0.87739. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.59175/0.89885. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.57668/0.92038. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.57937/0.91738. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.56482/0.93126. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.56211/0.93048. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.55522/0.90169. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.54085/0.95370. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.54340/0.93625. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.54110/0.94306. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.52954/0.96458. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.52021/1.02048. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.51040/0.98678. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.51393/1.01756. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.51470/0.97545. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.50067/1.08622. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.49439/1.04226. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.49264/1.03058. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.47846/1.11233. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.47031/1.05671. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.45863/1.09109. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.45343/1.12039. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.45310/1.18232. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.46766/1.12562. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.44125/1.17948. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.44019/1.17748. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.43382/1.20342. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.42384/1.31073. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.42678/1.32724. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.42232/1.29282. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.39979/1.35430. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.40175/1.33056. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.41144/1.36441. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.38945/1.41315. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.40788/1.29684. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.39141/1.35380. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.39672/1.36079. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.39172/1.37311. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.39839/1.40086. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.38270/1.31553. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.36143/1.45975. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.37233/1.44587. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.34691/1.45853. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.35011/1.48532. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.36092/1.48200. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.35421/1.51236. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.34816/1.49286. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.33846/1.50100. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.32122/1.55744. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.33196/1.61044. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.35314/1.50399. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.32308/1.58649. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.30057/1.66626. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.30774/1.64507. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.34545/1.63605. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.31507/1.65036. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.30738/1.71377. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.29440/1.66283. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.28909/1.76187. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.25876/1.77645. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.28542/1.71411. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.28171/1.76319. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.26080/1.85704. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.26908/1.89914. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.33844/1.86874. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.29236/1.75046. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.26538/1.80649. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.23658/1.94911. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.27830/1.89543. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.28934/1.91216. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.24915/1.93559. Took 0.43 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.70520/0.67755. Took 0.63 sec\n",
      "Epoch 1, Loss(train/val) 0.69729/0.67869. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69335/0.70036. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68896/0.69773. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68709/0.69863. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68708/0.71020. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68136/0.70599. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.67955/0.70078. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67261/0.70980. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67173/0.72227. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67040/0.72633. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66828/0.71603. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66089/0.71785. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66051/0.69717. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66395/0.70285. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.65950/0.71677. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66120/0.72487. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66212/0.72464. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66194/0.72187. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65439/0.72237. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65015/0.74868. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.64899/0.75583. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63943/0.74076. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64194/0.74955. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64217/0.76248. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.63617/0.76571. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.63094/0.76430. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62723/0.77900. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62171/0.79345. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61064/0.81018. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61960/0.79096. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.60995/0.80150. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61397/0.79409. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.60769/0.81355. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60124/0.81792. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.61087/0.81512. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59897/0.82787. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.59723/0.82658. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.59468/0.82964. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.58305/0.81416. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57964/0.83778. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57950/0.82913. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57823/0.83908. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.58579/0.80965. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57773/0.84115. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56340/0.84847. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.58238/0.84008. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.55889/0.82953. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.56246/0.86843. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55411/0.88824. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55657/0.86590. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55224/0.87712. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.54975/0.87602. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54224/0.88294. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.53291/0.95750. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.53445/0.94866. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53816/0.92138. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53490/0.93078. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.51765/0.95038. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.49822/0.98312. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51118/1.03527. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.49491/1.00920. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.51051/1.01306. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.48303/1.00677. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.48594/1.07144. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.49028/1.12517. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48964/1.09421. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.50677/0.98397. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.48522/1.08237. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46805/1.07384. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.47705/1.03510. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.46638/1.13369. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.47163/1.13535. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.45174/1.11393. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.43985/1.19205. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.44768/1.21591. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.45312/1.08232. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.43584/1.09148. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.42865/1.12010. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.43199/1.10552. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.43852/1.14885. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.41409/1.18497. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.42268/1.14600. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40795/1.16456. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.40207/1.17861. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.41033/1.13432. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.40430/1.13919. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.40977/1.17565. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.39621/1.23073. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.38336/1.20424. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36893/1.28791. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.40160/1.28658. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.38623/1.18818. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.36901/1.24037. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.38955/1.20336. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.36005/1.24043. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.37612/1.31178. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.38036/1.35589. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.40121/1.24542. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.37579/1.32268. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.71452/0.73810. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.71590/0.75997. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.71265/0.70199. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.70466/0.74935. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.70492/0.71583. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.70264/0.70784. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.69796/0.71214. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69678/0.71550. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68813/0.71789. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68647/0.72045. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67968/0.71699. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.69200/0.72582. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67367/0.72652. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68099/0.72969. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67317/0.73197. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66977/0.73519. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67282/0.73315. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67094/0.74012. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66289/0.75905. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.66292/0.74599. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65606/0.75180. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65696/0.77405. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66448/0.78823. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.66204/0.79385. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65549/0.81000. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65693/0.81452. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64238/0.81601. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64643/0.82179. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.64268/0.83134. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64261/0.84813. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64418/0.83516. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.64070/0.84874. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62970/0.85194. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63233/0.86380. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.62252/0.86957. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62582/0.87956. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62487/0.87817. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.62731/0.89358. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61969/0.88925. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.62552/0.89956. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61846/0.88730. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.60952/0.88578. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.61440/0.86689. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.61496/0.90023. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60582/0.87874. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.61354/0.87303. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.61299/0.88667. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.60522/0.86865. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.60385/0.86666. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.60286/0.88201. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.59127/0.88921. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.59800/0.90285. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.59403/0.92010. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.57981/0.93838. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.57476/0.95105. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.59446/0.97158. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.57601/0.97941. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.55690/1.02650. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.57694/1.01113. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.57873/0.99980. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.54895/1.04679. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.56700/1.03573. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.55381/0.99049. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.55820/1.06950. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.55295/1.02568. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.55031/1.04468. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.53971/1.02592. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.53226/1.10924. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.52684/1.07957. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.54351/1.08998. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.54808/1.07490. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.53155/1.10349. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.53401/1.15512. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.52330/1.08008. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.51208/1.10903. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.50620/1.20270. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.50643/1.18401. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.49996/1.21754. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.51135/1.18367. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.50267/1.18672. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.50970/1.18943. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.53032/1.10020. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.51319/1.13673. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.49581/1.13966. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.50417/1.21724. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.49599/1.16245. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.47767/1.14265. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.50454/1.19617. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.47790/1.20539. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.48515/1.20672. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.48400/1.23936. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.47338/1.19582. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.47235/1.21220. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.49005/1.25211. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.48064/1.12908. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.45961/1.19469. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.45659/1.23716. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.45136/1.24607. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.44794/1.27450. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.44750/1.28020. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69350/0.72753. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.68635/0.72601. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68536/0.73173. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68293/0.72096. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.67794/0.73123. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67815/0.73872. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68086/0.74029. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67592/0.73195. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67566/0.73448. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67416/0.74265. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67638/0.76738. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67243/0.74688. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67078/0.75294. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.67190/0.75467. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66924/0.76102. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66775/0.76381. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66437/0.76582. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66314/0.77937. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65774/0.77866. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65933/0.78740. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65665/0.78963. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65308/0.78605. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64955/0.78645. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64754/0.78097. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64406/0.78780. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64130/0.77576. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63696/0.77843. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63606/0.77741. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63585/0.78295. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62595/0.78416. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.61837/0.79038. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.61498/0.79169. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61407/0.77892. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.60874/0.80917. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60996/0.80493. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60462/0.78477. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.59153/0.79083. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58939/0.79142. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58558/0.79349. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.58407/0.80792. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57703/0.78730. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58939/0.78122. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.57049/0.77035. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.55878/0.78928. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.54605/0.80815. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.54300/0.81910. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.53676/0.81272. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.53344/0.84644. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.51711/0.83505. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52238/0.83879. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.50918/0.82538. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.54132/0.81452. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.53457/0.82045. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52096/0.82871. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.50209/0.83293. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.48894/0.88371. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.49180/0.86845. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.47352/0.88551. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.46639/0.91291. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.46992/0.90426. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.45546/0.88944. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.45266/0.89177. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.46860/0.91681. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.44066/0.91515. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.44466/0.89593. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.44243/0.90525. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.41199/0.92257. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.42618/0.99328. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.40816/1.01400. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.38906/1.03436. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.42626/1.00479. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.41126/0.99600. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.40514/0.99776. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.39413/0.93880. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.37848/0.97693. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.36544/1.03265. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.37282/1.07420. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.35257/1.10983. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.37996/1.09402. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.35458/1.09479. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.34330/1.10095. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.37065/1.08033. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.33126/1.10029. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.33480/1.12074. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.35663/1.06437. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.33669/1.14078. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.32856/1.12638. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.31879/1.14849. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.32883/1.13795. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.29355/1.12516. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.28997/1.14478. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.26863/1.20301. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.26185/1.16999. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.25540/1.15008. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.26532/1.18691. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.27804/1.22359. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.25186/1.28641. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.25161/1.20774. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.28167/1.18560. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.25820/1.28050. Took 0.44 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69818/0.70904. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.69458/0.69573. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69570/0.70015. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68793/0.70536. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68587/0.70850. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68333/0.71671. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68668/0.71928. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68731/0.71627. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68822/0.72086. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67626/0.70534. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67952/0.72618. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67624/0.72777. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67378/0.73195. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.66689/0.72971. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66794/0.74732. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67095/0.74668. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67246/0.74400. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66715/0.75467. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66649/0.76241. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66985/0.76101. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66695/0.76089. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65975/0.76677. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.66818/0.77086. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66092/0.77589. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.65833/0.80171. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65372/0.80878. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65758/0.83028. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65331/0.83056. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.65105/0.83539. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65203/0.84752. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.64144/0.87992. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.65349/0.83058. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.64878/0.87057. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64478/0.85248. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.64280/0.85083. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.64100/0.84781. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.63400/0.85170. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.63049/0.86229. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.62436/0.86948. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61921/0.88444. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61992/0.91308. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.61647/0.87787. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.61274/0.89937. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.62014/0.86186. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60183/0.89914. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59729/0.92428. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.59900/0.91612. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.59077/0.94638. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.59611/0.94366. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.59544/0.97263. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.59213/0.94085. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.57481/1.00046. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.57460/0.99422. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.62232/0.96247. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.58591/0.96752. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.56701/1.00013. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55970/1.01505. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.55981/1.03914. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.56081/1.00762. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.54550/1.01287. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55445/1.03809. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.52930/1.03246. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54624/1.07203. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.53440/1.03381. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.52075/1.05792. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.50598/1.09803. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.51104/1.03753. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.54371/1.07554. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.52162/1.01364. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.49242/0.99721. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.51852/1.08093. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.50556/1.04931. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.49512/1.03630. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.49923/1.00711. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.49502/1.05413. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.49354/1.07046. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.47039/1.07461. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.47557/1.04178. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.46386/1.07510. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46216/1.14829. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.47048/1.09947. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.46827/1.05028. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.47638/1.13407. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.46157/1.08149. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.45247/1.13041. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.42247/1.14135. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.44534/1.11176. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.44581/1.09943. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.42663/1.09021. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.42303/1.22024. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.45445/1.24478. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.45846/1.13433. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.41341/1.24068. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.42339/1.24169. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.42663/1.24024. Took 0.47 sec\n",
      "Epoch 95, Loss(train/val) 0.42052/1.26214. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.41027/1.24776. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.42764/1.26516. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.40287/1.23338. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.39789/1.32472. Took 0.43 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.70276/0.69279. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69338/0.68928. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68728/0.68889. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.68494/0.68769. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68099/0.69159. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.67914/0.70653. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67732/0.70223. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67447/0.69983. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67105/0.69167. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67140/0.68929. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66636/0.69245. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66265/0.70040. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66372/0.70332. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65669/0.71556. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65659/0.71483. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.65559/0.72121. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.65765/0.72047. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65268/0.72111. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64899/0.72419. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65450/0.72802. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64639/0.71789. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64270/0.73349. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.63985/0.74312. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63715/0.76198. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63717/0.76419. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.63242/0.75595. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62595/0.78460. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62597/0.77558. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62558/0.78839. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61969/0.77168. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62452/0.75288. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.63253/0.75215. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60925/0.77900. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.61853/0.76041. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.60710/0.76160. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61164/0.76958. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.60881/0.77622. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.59745/0.78012. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59651/0.78855. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59851/0.78913. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.58769/0.80673. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.58859/0.80720. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58639/0.81420. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.57691/0.80184. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.58066/0.78868. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.58091/0.77006. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.57048/0.75892. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57881/0.79268. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.57050/0.79785. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.56715/0.78169. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56455/0.81550. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.58434/0.81939. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.56044/0.82516. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.55414/0.82199. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54704/0.84036. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.54953/0.83871. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.55617/0.82608. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.52960/0.86006. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.52588/0.87603. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52561/0.85991. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.50930/0.88698. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.53125/0.90787. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51624/0.91112. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.52692/0.94358. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.49512/0.94895. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50421/0.92226. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.50630/0.93544. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.50115/0.95788. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49002/0.94373. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.46138/1.01145. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48526/0.98156. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48303/0.99864. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.47365/0.95157. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.47263/0.97801. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.46366/0.97464. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.46945/1.00331. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.45199/1.02593. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.44969/1.03070. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.46434/1.00679. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44006/1.01614. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.42762/1.08052. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.45671/1.01986. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.42449/1.05459. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.41958/1.08252. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.42484/1.05705. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.40773/1.07712. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.41117/1.11271. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.41171/1.05435. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.39797/1.10759. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.41633/1.10901. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.39204/1.12222. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.37602/1.10324. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.36519/1.14751. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.36238/1.19874. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.36115/1.17100. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.34963/1.27104. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.34963/1.25106. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.33992/1.30226. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.36028/1.36380. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34822/1.35131. Took 0.46 sec\n",
      "ACC: 0.6354166666666666\n",
      "Epoch 0, Loss(train/val) 0.69318/0.71377. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.68808/0.69348. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.68883/0.70594. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68256/0.68481. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68567/0.69271. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68181/0.69050. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67397/0.68515. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67925/0.69097. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67139/0.69195. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67312/0.69803. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67170/0.70569. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67427/0.69658. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67247/0.71419. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65982/0.71229. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66071/0.74456. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66029/0.73608. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66163/0.75646. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65888/0.74501. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.67073/0.75349. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64966/0.75362. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65138/0.74874. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65233/0.74712. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65371/0.74597. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64758/0.74071. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64558/0.74928. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64714/0.73877. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65553/0.76307. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64582/0.76214. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63590/0.77070. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64089/0.76949. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64113/0.78714. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62895/0.78601. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.62568/0.81794. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.62538/0.82426. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62497/0.80421. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62225/0.82203. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.61867/0.82421. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.61846/0.83223. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61179/0.82676. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.60666/0.85374. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60371/0.83056. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.62031/0.87297. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.60664/0.88229. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60005/0.91106. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60019/0.87821. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.58614/0.89444. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58961/0.89596. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57940/0.95338. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.57369/0.94055. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.60426/0.91644. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.58273/0.88696. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.57604/0.92779. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.58722/0.90042. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.57622/0.92846. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.58064/0.90161. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.57656/0.88955. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.55321/0.93503. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.56370/0.94806. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.54683/0.94769. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.56221/0.96764. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55114/1.00675. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.53860/0.98656. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54706/0.92543. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.57239/0.93279. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.55646/0.92951. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.53795/0.98337. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.55144/0.96239. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.52442/0.94850. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.53323/0.99638. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.54069/0.97466. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.50760/0.99943. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.52258/1.02720. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.53225/1.01990. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.50426/1.00558. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.51032/0.99262. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.48365/1.04326. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.51190/1.00834. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.49619/1.01873. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.51221/0.96800. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.49019/0.99929. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.48567/1.04088. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.48455/0.99850. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.48071/0.99933. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.47279/1.02535. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.44096/1.06745. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.47524/1.01051. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.46305/1.02427. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.44615/1.03134. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.45382/1.03306. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.42442/1.10265. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.46877/1.03724. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.44213/1.00632. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.45035/1.03873. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.42544/1.06202. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.44371/1.05003. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.43488/1.01604. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.42106/1.04647. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.44696/1.01180. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.39704/1.06345. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.39777/1.03765. Took 0.44 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69294/0.69299. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69198/0.69059. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68730/0.69321. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68944/0.69063. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68778/0.69293. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68506/0.68798. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.68426/0.68846. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.68494/0.69367. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67983/0.68847. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68051/0.69429. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67756/0.68904. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67726/0.68934. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67375/0.67830. Took 0.47 sec\n",
      "Epoch 13, Loss(train/val) 0.67200/0.68316. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67202/0.68280. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66797/0.70227. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66626/0.68068. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66223/0.70755. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65946/0.72370. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65199/0.71587. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65512/0.74253. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65258/0.75010. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65434/0.75563. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64925/0.74880. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64390/0.73781. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64604/0.73044. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.63660/0.73119. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64007/0.73207. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63525/0.73230. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62479/0.75200. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62014/0.80397. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61848/0.79228. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.62739/0.81584. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.62035/0.82948. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62099/0.84590. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62161/0.82565. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60970/0.86592. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61384/0.90111. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60833/0.89159. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.60478/0.86538. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59179/0.91993. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.58711/0.90296. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.57707/0.94747. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58174/0.89040. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59663/0.89322. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57331/0.85967. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57577/0.93361. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56677/0.92403. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.56203/0.91895. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54182/0.98619. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.57171/0.99139. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.54847/0.90308. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.53463/0.91795. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54670/1.00667. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.53980/1.04430. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.52902/1.03591. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52415/1.01634. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52345/1.02849. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.51672/0.98447. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53859/0.92241. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51637/0.96309. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.49226/1.06911. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.49242/1.06157. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.49208/1.05080. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.47476/1.05374. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.48202/1.07490. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.47455/1.10240. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.46663/1.15577. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.46167/1.05373. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.44321/1.11569. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.44923/1.11256. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45379/1.17195. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.44143/1.17459. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.45014/1.17187. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.44309/1.21938. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.44315/1.08385. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.41686/1.22176. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.41332/1.22461. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.41416/1.27522. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.40331/1.28392. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.40392/1.27427. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.38191/1.33185. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.38983/1.35062. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.38648/1.37496. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.36967/1.41101. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.38714/1.37930. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.39087/1.42393. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.36313/1.41182. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.35811/1.39598. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.36389/1.43817. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.35282/1.41301. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.35715/1.39575. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.35703/1.48542. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.33255/1.50769. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.34505/1.41853. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.32906/1.35434. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.33459/1.33411. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.32452/1.41530. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.30438/1.52548. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.30726/1.51879. Took 0.45 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.71161/0.70592. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.70401/0.71337. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69547/0.71869. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69578/0.73352. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69403/0.71756. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68981/0.72095. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.69368/0.72175. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68765/0.73215. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.69137/0.72666. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68753/0.72911. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68632/0.74064. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.68215/0.73346. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.68354/0.73477. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67960/0.74257. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67355/0.75656. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66915/0.76360. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.68022/0.77182. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67588/0.77431. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66566/0.77826. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.67617/0.77651. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.66713/0.78182. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.66501/0.78653. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66137/0.79428. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.66870/0.78276. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66295/0.79886. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65238/0.80931. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65747/0.79884. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65770/0.79625. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.65502/0.80176. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65054/0.80073. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64611/0.79727. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64932/0.79672. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63883/0.81000. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64299/0.78587. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.64117/0.77298. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.62671/0.78857. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.63085/0.78849. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.63274/0.78389. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62513/0.81552. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.62239/0.81323. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.61641/0.81501. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.61971/0.83132. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60072/0.81743. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.60419/0.82592. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.60577/0.83467. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59583/0.85257. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58893/0.86263. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.58255/0.85222. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.58999/0.87802. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.58590/0.90733. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57442/0.88695. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56388/0.92111. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.57199/0.93917. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56475/0.91760. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.55132/1.00863. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55994/0.96821. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55860/0.97470. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54455/0.95358. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54313/1.01177. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.54366/0.94310. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52887/1.00020. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.54351/1.02654. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52286/1.03756. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.51048/1.06517. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50837/1.04517. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.50310/1.06117. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.49264/1.06470. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48461/1.17242. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49272/1.08947. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.49493/1.14779. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.47176/1.07697. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.49713/1.13233. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.45206/1.12200. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.46098/1.17158. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.46032/1.17036. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.46196/1.21149. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46120/1.17692. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.44607/1.23728. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.43214/1.26470. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.44796/1.19221. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.43578/1.17729. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.43103/1.23244. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.42078/1.25029. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.39995/1.27930. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.39986/1.24624. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.38811/1.24503. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.44210/1.16624. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.39081/1.34414. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.38991/1.31810. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.37504/1.36496. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.37287/1.36704. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.36665/1.39534. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.36801/1.41340. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.36593/1.29603. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.36306/1.39363. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.35162/1.37652. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.35597/1.35277. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.35503/1.42197. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.34295/1.29326. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.33399/1.33467. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70143/0.74353. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69865/0.71544. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69560/0.71498. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69328/0.70588. Took 0.49 sec\n",
      "Epoch 4, Loss(train/val) 0.69128/0.70668. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68932/0.70895. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68750/0.70646. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68702/0.70753. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68624/0.70385. Took 0.48 sec\n",
      "Epoch 9, Loss(train/val) 0.68643/0.70240. Took 0.48 sec\n",
      "Epoch 10, Loss(train/val) 0.68031/0.71261. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68066/0.71094. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67523/0.72098. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.68060/0.71612. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67336/0.71604. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67184/0.72912. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.67176/0.72484. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67077/0.73858. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67007/0.72822. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66535/0.72958. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67178/0.73542. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66908/0.73002. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66530/0.73929. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66310/0.74591. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.66212/0.73822. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65714/0.74661. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.65697/0.75790. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.65049/0.75565. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.64940/0.76650. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64832/0.77937. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.64611/0.76463. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64234/0.77581. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63511/0.79507. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63937/0.80586. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.64008/0.81717. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.63319/0.82486. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62695/0.84228. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62600/0.84974. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62674/0.83594. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.61990/0.84625. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61894/0.83431. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61138/0.85191. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.59918/0.85593. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.59424/0.87654. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.58719/0.90237. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.59429/0.86801. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58990/0.85673. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.57865/0.88956. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.58241/0.86694. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.58298/0.88463. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.57161/0.91953. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56554/0.92689. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.56291/0.87679. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55365/0.91896. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55595/0.90732. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.54908/0.93364. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.54327/0.94226. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.54248/0.91630. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54357/0.90542. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.51318/0.95626. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.52717/0.97652. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.52289/0.94647. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52291/0.91519. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.51006/0.98381. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.49526/0.99237. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.50626/0.97534. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.50430/0.94915. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.48442/1.01519. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.47740/1.04717. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.48168/1.07970. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.47142/1.12794. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.47982/1.06016. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.45413/1.10192. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.45484/1.09144. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.45211/1.06072. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.46145/1.08927. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.46392/1.12427. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.45242/1.07708. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.42940/1.16307. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.42775/1.18518. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.42145/1.16128. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.42722/1.12398. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.42342/1.16064. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.41917/1.19323. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.41223/1.18765. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.40404/1.25825. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.39947/1.26547. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.38588/1.27675. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.40278/1.26885. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.38305/1.27243. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.38199/1.22401. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.41139/1.19371. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.36598/1.28191. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.37434/1.26977. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.37580/1.27111. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.37265/1.22463. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.35882/1.25122. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.35703/1.20642. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.34261/1.24409. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36214/1.27526. Took 0.46 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.70018/0.67589. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69137/0.67193. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69334/0.67250. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68803/0.67155. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69051/0.68588. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68646/0.68391. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68949/0.69116. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68013/0.68174. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67940/0.68815. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68128/0.68802. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67883/0.69076. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67531/0.69095. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67886/0.70063. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67262/0.70554. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67169/0.71079. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66348/0.72904. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65322/0.72753. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65669/0.73461. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65267/0.74516. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65410/0.76001. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65437/0.77618. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63953/0.75912. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63849/0.78417. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.63992/0.79201. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63104/0.78158. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.63375/0.78719. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63160/0.79365. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62231/0.82695. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61712/0.79780. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61117/0.84173. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.60211/0.86009. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60625/0.87129. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60033/0.88157. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.59111/0.93103. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.59083/0.93983. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58589/0.96778. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.58823/0.96791. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58688/0.90992. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.56956/0.94422. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.56854/0.99103. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.56482/1.02734. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.55824/0.99090. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.55465/1.00859. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.54517/1.05582. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55419/1.03298. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56277/1.02545. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.51936/1.11645. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.53009/1.07117. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.51503/1.14674. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52029/1.12571. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.51738/1.11097. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.51321/1.15240. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.49127/1.19712. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.49036/1.14201. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.49467/1.15750. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.47634/1.18411. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.49104/1.27202. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.47757/1.21440. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.46380/1.29156. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.46638/1.26162. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.47278/1.19979. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.45569/1.25936. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.44953/1.28375. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.45500/1.22327. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.44548/1.19627. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.44777/1.20419. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.45114/1.24407. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.43150/1.14514. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.42111/1.18617. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.43368/1.23577. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.42254/1.32430. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.43501/1.26946. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.39319/1.29938. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.41331/1.34641. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.39103/1.23651. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.39897/1.36675. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.38361/1.31420. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.39432/1.33789. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.41237/1.33107. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.37389/1.35484. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.36174/1.45956. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.36288/1.36921. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.38603/1.31943. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.35009/1.26023. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.35481/1.39069. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.35199/1.44181. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.35453/1.43548. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.34648/1.36888. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.34773/1.37579. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.34283/1.34751. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.35744/1.31325. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.38967/1.34440. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.34383/1.38079. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.31829/1.45928. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.32755/1.43285. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.32164/1.58109. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.30127/1.54458. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.32922/1.49711. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.31641/1.50387. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.30014/1.62719. Took 0.45 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.71281/0.70909. Took 0.63 sec\n",
      "Epoch 1, Loss(train/val) 0.70346/0.70895. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69657/0.72722. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70150/0.69167. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69111/0.71027. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69679/0.71203. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68958/0.70817. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68372/0.72131. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68169/0.71598. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68596/0.72187. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67619/0.71164. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67677/0.71279. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67398/0.70447. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.68013/0.71706. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67812/0.72088. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.65947/0.74120. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66154/0.73654. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66922/0.75650. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66551/0.75367. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67495/0.75518. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65929/0.75304. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66320/0.74514. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65989/0.73724. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64602/0.73327. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65631/0.75633. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.65125/0.74477. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64626/0.75614. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64727/0.74861. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64228/0.75708. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64123/0.76591. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63785/0.77946. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.62920/0.78520. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60837/0.78984. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62097/0.82904. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.62334/0.86745. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61653/0.84323. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.61367/0.89232. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.61527/0.79220. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.60573/0.80915. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60410/0.91449. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60821/0.83564. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.60188/0.86510. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59965/0.86957. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.62065/0.81510. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.60126/0.85295. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59316/0.86970. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59425/0.86173. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.58838/0.83668. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59557/0.86879. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57995/0.88692. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.57947/0.89057. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.57036/0.91159. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.56447/0.94760. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56843/0.93664. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.57024/0.92316. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.54669/0.90534. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55944/0.94611. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.56823/0.91642. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54861/0.91564. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54252/0.95985. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.53484/0.99263. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.53688/0.98253. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54140/0.97071. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.52730/0.95228. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.53173/0.91362. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.51376/0.96277. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.51692/0.92602. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.51541/0.95593. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.51535/1.00819. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.49284/1.02245. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.50086/1.05324. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.49637/1.02882. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.49626/1.04813. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.50289/1.05891. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.49201/1.04306. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.48717/1.06802. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.49065/1.02621. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46934/1.05199. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.46819/1.09541. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.48107/1.02838. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.47294/1.04913. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.47354/1.01485. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.45762/1.09188. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.43747/1.09527. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.44321/1.14274. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.44154/1.10714. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.41297/1.18036. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.42243/1.22555. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.43071/1.14221. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.40557/1.18021. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.41001/1.24251. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.42402/1.28247. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.42209/1.11218. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.40540/1.20315. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40383/1.17693. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.40679/1.15005. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.39383/1.21274. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.38871/1.19261. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.41729/1.21091. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.38826/1.13306. Took 0.44 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.72432/0.71005. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.72269/0.70180. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.71363/0.71847. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70673/0.71280. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69928/0.70755. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.70166/0.69456. Took 0.48 sec\n",
      "Epoch 6, Loss(train/val) 0.69292/0.69062. Took 0.48 sec\n",
      "Epoch 7, Loss(train/val) 0.69535/0.68892. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.67870/0.70739. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.68291/0.71419. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.69699/0.70976. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68069/0.71426. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68359/0.72513. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68116/0.70804. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.68548/0.70372. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.68382/0.69625. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67959/0.69919. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67463/0.70144. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.67336/0.71190. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66198/0.71254. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65952/0.72919. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66275/0.73552. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65553/0.75756. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.65273/0.76270. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.65317/0.78046. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64432/0.77966. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63951/0.78773. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62825/0.80655. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63238/0.81300. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63600/0.82419. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62349/0.83343. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62466/0.84936. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62069/0.84170. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.61520/0.87501. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61857/0.87088. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.60571/0.89818. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60825/0.87974. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59723/0.90177. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59858/0.91961. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59114/0.97028. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59043/0.97613. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.59454/0.99056. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57847/1.03780. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.58580/1.04529. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58557/1.02550. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.58776/1.04669. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.57955/1.04624. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57021/1.06120. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55616/1.06406. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.56468/1.10027. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.56166/1.09137. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.55558/1.05869. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56586/1.07857. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55921/1.09863. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.55223/1.07421. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.54104/1.10069. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53785/1.10553. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53724/1.17371. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.53053/1.19375. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53938/1.21777. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52979/1.20970. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.52483/1.15398. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51671/1.22012. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.50896/1.26050. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50659/1.25487. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.51584/1.26551. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.50061/1.20833. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.49940/1.19257. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.48407/1.23853. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.49419/1.29596. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.46871/1.35039. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48652/1.32203. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48074/1.34869. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.47880/1.30353. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.47747/1.33196. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.45850/1.37749. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.47064/1.40199. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46369/1.46910. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.44581/1.37202. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44822/1.40252. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.45255/1.39313. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.47040/1.37338. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.43624/1.34616. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.42964/1.42680. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.41996/1.40908. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.42875/1.44866. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.42828/1.55320. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.41702/1.48918. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.42203/1.48533. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.42785/1.37112. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.41256/1.54892. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.39656/1.54702. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.38612/1.51606. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.40469/1.53824. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.42086/1.51884. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.38452/1.65985. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.38472/1.65848. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.36394/1.66958. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.41093/1.53325. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.37757/1.64485. Took 0.43 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.70304/0.70655. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69480/0.69683. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.69121/0.70103. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69059/0.69810. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69275/0.70505. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69270/0.70120. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68621/0.70950. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68055/0.71551. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68493/0.71548. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68135/0.72554. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67704/0.74597. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68065/0.74177. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67626/0.75261. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.66887/0.76537. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67315/0.76952. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.67428/0.77288. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67052/0.77942. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67689/0.77850. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66938/0.76657. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66213/0.79453. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66807/0.79561. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.67278/0.77419. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.66468/0.78614. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.66192/0.78795. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66072/0.79280. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65980/0.77877. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.66151/0.78912. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65493/0.78213. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65092/0.78843. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.65079/0.80986. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.65418/0.80732. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.64315/0.82045. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64033/0.83097. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64292/0.82452. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.63979/0.82496. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.63617/0.80510. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.63295/0.82293. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.62976/0.81178. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.63671/0.79193. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61857/0.80781. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62087/0.85049. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.61912/0.86625. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.60957/0.87819. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60114/0.90490. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60569/0.92590. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.59747/0.92001. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59632/0.93101. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.60613/0.87753. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58233/0.96764. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.59578/0.89103. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.59060/0.88300. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.57843/0.94622. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.57620/0.94036. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.57324/0.92099. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56114/0.98146. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55858/0.93110. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.55988/0.93862. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.56395/0.94682. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54053/1.00584. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.54567/0.99858. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.54812/0.97644. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.55228/0.97049. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53117/0.97695. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.53583/1.01979. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.53007/1.01695. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.52964/0.99144. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.53344/0.99114. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.58978/0.94632. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.56457/0.91841. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.53045/0.98224. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.52692/1.00327. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.51077/1.03048. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.51354/1.07339. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.49530/1.13148. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.49543/1.07287. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48731/1.10590. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.46597/1.18803. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.49019/1.11993. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.50249/1.07139. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.51953/0.99572. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.48214/1.10779. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.47165/1.11009. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.46704/1.21360. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.46951/1.15719. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.47376/1.12070. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.46560/1.09962. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43804/1.23420. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.44501/1.14373. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.44010/1.17071. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.46051/1.09959. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.44336/1.21691. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.44292/1.21270. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.43126/1.20510. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.44340/1.17313. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.43468/1.21503. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.41247/1.28023. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.42160/1.30242. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.41856/1.27106. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.42111/1.27190. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.43419/1.26188. Took 0.44 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69776/0.68807. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69284/0.69315. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69264/0.69505. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68883/0.69026. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69067/0.69371. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69097/0.69970. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68416/0.70104. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68343/0.70381. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68174/0.70721. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67797/0.70257. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67682/0.70973. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67484/0.71055. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67478/0.69797. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66524/0.70677. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66453/0.70133. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.65961/0.72331. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66394/0.70314. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66660/0.72806. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65489/0.73231. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65309/0.75811. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65610/0.75284. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65003/0.76074. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64345/0.76584. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.63941/0.76932. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.63026/0.80540. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64041/0.79416. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.62666/0.80795. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62028/0.81397. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63755/0.77997. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62302/0.83835. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61268/0.86933. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.61117/0.81869. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60266/0.83377. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.60965/0.87532. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59220/0.85166. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59830/0.83092. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.59400/0.85567. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59484/0.83094. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59620/0.82614. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58442/0.81919. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57862/0.85667. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.57865/0.84774. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56996/0.83338. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58822/0.82474. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57641/0.83331. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.56816/0.84490. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.56295/0.86820. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56117/0.89871. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55814/0.82173. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.55358/0.86643. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.55043/0.84822. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56023/0.84252. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54503/0.83372. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.53145/0.89325. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.53266/0.86443. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51998/0.92271. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52711/0.87799. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51601/0.85755. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.51861/0.91828. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.51343/0.88547. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.50890/0.89478. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50482/0.90126. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.49470/0.89490. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.48991/0.97343. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50349/1.05415. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.49045/0.97205. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.49847/0.91948. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.46962/0.93575. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.45866/0.90220. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.45551/0.96631. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.46400/0.97133. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48053/0.95468. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.47086/0.93199. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.45959/0.93379. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.44396/0.97861. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.43093/0.93670. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.47760/0.98742. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.45742/0.95850. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.42703/0.99304. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.43092/0.97812. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.41874/0.97036. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.41159/1.00154. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.40193/1.03661. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.39041/1.05641. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.39297/1.07219. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.40332/1.08010. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.40191/1.12979. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.39476/1.07608. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.37779/1.15187. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.35956/1.11378. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.38379/1.18371. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.37777/1.19373. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.35886/1.13540. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.36310/1.12070. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.36537/1.16449. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.36354/1.15720. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.34277/1.15327. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.34318/1.22894. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.35176/1.18695. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.35939/1.20561. Took 0.43 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.70343/0.71662. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.69177/0.72052. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69470/0.72158. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68617/0.73097. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68531/0.73510. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68325/0.74573. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68211/0.74544. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68204/0.74803. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67956/0.75631. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67696/0.75176. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66917/0.76862. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.67436/0.77438. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66843/0.80213. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66418/0.80711. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66104/0.87107. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65345/0.86127. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65079/0.84092. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64785/0.87332. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.64349/0.86566. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64700/0.85666. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64366/0.84990. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63446/0.88254. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64564/0.89228. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.63535/0.89579. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62690/0.92997. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64049/0.88908. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63607/0.89276. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62626/0.95437. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62694/0.94082. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61981/0.97819. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.62227/0.95100. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61854/0.94838. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61786/0.95473. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.60381/1.01489. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.60439/1.00692. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60799/1.02612. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.60225/1.06304. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.59560/1.04179. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59315/1.05977. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59152/1.06955. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.57659/1.11314. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57692/1.13445. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.57916/1.12971. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57253/1.16119. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.57656/1.13302. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.57627/1.19478. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56209/1.19600. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.56175/1.17494. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55555/1.12613. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55929/1.14815. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.55242/1.20312. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54130/1.22220. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54874/1.15723. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.54169/1.19966. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.53891/1.22735. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.52713/1.22762. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.51908/1.16253. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51591/1.22662. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50069/1.32208. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.50585/1.39146. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.48267/1.31808. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.49520/1.29191. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.48760/1.33638. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.48126/1.35541. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.48079/1.37951. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.47201/1.45197. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.46481/1.37478. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.46839/1.44626. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.45236/1.37570. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46794/1.49810. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.45315/1.40888. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.43182/1.44606. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.43778/1.39903. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.45719/1.47097. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.44840/1.35018. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.44121/1.37105. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43202/1.47771. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.42998/1.48755. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.40708/1.40751. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.41125/1.48344. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.42159/1.48516. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.41762/1.54245. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.41620/1.53651. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.43085/1.49471. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.42172/1.51413. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.41728/1.44119. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.39966/1.43883. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.38826/1.43045. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.38676/1.38783. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.38818/1.52789. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.39703/1.48292. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.41212/1.47348. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.39267/1.54254. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.38469/1.49435. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.37247/1.51791. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.37389/1.50939. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.35776/1.52763. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.35253/1.58991. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.34499/1.52514. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.35870/1.52090. Took 0.45 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.71103/0.68452. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69987/0.68459. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69623/0.68659. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69328/0.67634. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69611/0.68704. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69174/0.68593. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68213/0.68585. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68611/0.68062. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67623/0.69210. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68729/0.66866. Took 0.47 sec\n",
      "Epoch 10, Loss(train/val) 0.67078/0.67226. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67823/0.66842. Took 0.47 sec\n",
      "Epoch 12, Loss(train/val) 0.67191/0.67761. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66999/0.68593. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66994/0.69470. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66857/0.70129. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65909/0.71173. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66635/0.71111. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66329/0.72478. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66615/0.70923. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66084/0.71733. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65324/0.74109. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64930/0.75461. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.65270/0.75926. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.64529/0.76769. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64332/0.76299. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63893/0.78450. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64279/0.77217. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63942/0.77004. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.63687/0.77657. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63429/0.76863. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63266/0.79293. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62924/0.80155. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62918/0.81497. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61909/0.87669. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.61673/0.75375. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.61931/0.85444. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62001/0.76522. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.61394/0.79722. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61387/0.83522. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60319/0.81670. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59464/0.83730. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.59898/0.85564. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.60143/0.81661. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.59143/0.87086. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57991/0.87344. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59078/0.87239. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.58572/0.84280. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.58186/0.83792. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.56003/0.86284. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.57732/0.89255. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55305/0.92411. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56663/0.84844. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.57007/0.87315. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56351/0.93830. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.55950/0.91576. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53614/1.00013. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.56135/0.91399. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.53718/0.92011. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.54299/0.98755. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55564/0.92896. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.54205/0.98840. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.53483/0.93879. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.53208/0.90864. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.51499/0.94250. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.52016/0.96825. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.50799/1.01923. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.51467/0.96767. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.50240/1.02970. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.50674/1.01894. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.51638/0.99549. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.50147/0.93694. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.50689/0.97382. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.50842/1.04690. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.48489/1.01924. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49046/1.08072. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.49938/1.04989. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.47154/1.07714. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.46686/1.17689. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46260/1.14219. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.48301/1.02330. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.47409/1.08940. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.45971/1.14428. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.48502/1.12175. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.46877/1.05086. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.45320/1.11467. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.46574/1.06046. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43681/1.18118. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.45209/1.18178. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.45956/1.11409. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.42370/1.13480. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.40997/1.31377. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.41945/1.32779. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.45775/1.25792. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.46182/1.16107. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.44203/1.26036. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.44064/1.19435. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.43791/1.10491. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.46549/1.00581. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.46313/1.13777. Took 0.44 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.70270/0.68546. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70309/0.69724. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69578/0.69178. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69044/0.69043. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69095/0.68793. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69110/0.69708. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69020/0.69742. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68791/0.69777. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68703/0.68637. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.69178/0.68336. Took 0.48 sec\n",
      "Epoch 10, Loss(train/val) 0.68374/0.68745. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68521/0.68925. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.68374/0.69012. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67753/0.69625. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67774/0.69324. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.67186/0.69926. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.67790/0.71053. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67388/0.71295. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.67278/0.71290. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66749/0.71308. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66449/0.71235. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66735/0.72053. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66219/0.73022. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.66368/0.71266. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66184/0.73004. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.66882/0.71340. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65124/0.70983. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.66044/0.74027. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65354/0.73874. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65035/0.71965. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.66190/0.72535. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.65064/0.69616. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64265/0.73040. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.65295/0.72326. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.64408/0.72707. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.63944/0.72677. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.64546/0.73281. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.63201/0.73339. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.63838/0.72418. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.63563/0.74978. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.62457/0.73361. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.62360/0.72905. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.63207/0.75653. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.62415/0.77161. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.62717/0.74215. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.61667/0.75655. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.61370/0.78237. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.60865/0.80789. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.60678/0.76753. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.61261/0.80317. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.60622/0.78295. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.60230/0.80028. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.59826/0.81603. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.59722/0.85352. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.60678/0.73115. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.59936/0.79797. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.59397/0.77749. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.59741/0.82040. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.59118/0.81804. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.59815/0.76478. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.59547/0.81767. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.57604/0.83839. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.56175/0.86866. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.56878/0.88528. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.58110/0.88529. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.57986/0.85219. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.57162/0.90622. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.55421/0.88057. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.55470/0.90294. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.55691/0.92923. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.55531/0.93711. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.57143/0.87766. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.55336/0.90708. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.55422/0.93085. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.59942/0.84850. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.57206/0.82498. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.56584/0.81962. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.54671/0.91285. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.54579/0.90470. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.51972/0.95548. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.53061/0.95995. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.52346/0.99342. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.52606/0.92923. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.52396/0.93863. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.51099/0.94337. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.50603/0.93890. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.51155/0.93001. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.49984/0.96098. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.48318/1.02097. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.49362/0.94310. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.47542/0.99818. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.47770/0.94777. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.46497/0.92243. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.47126/0.93419. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.47221/0.94733. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.45735/0.99457. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.45858/0.96924. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.44890/1.00836. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.45676/1.09166. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.44862/1.05028. Took 0.44 sec\n",
      "ACC: 0.625\n",
      "Epoch 0, Loss(train/val) 0.71375/0.71642. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70246/0.70179. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69447/0.69259. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69349/0.67696. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69087/0.67610. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.68510/0.68244. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68412/0.68709. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67600/0.69653. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68000/0.69424. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67806/0.69031. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66489/0.69836. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66575/0.71399. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66529/0.70089. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67055/0.68502. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67171/0.68810. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66669/0.70289. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66133/0.69338. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66049/0.69609. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65412/0.68374. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65782/0.71735. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65096/0.71492. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65154/0.72787. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64703/0.72254. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.64527/0.72968. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64526/0.72199. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.63166/0.73598. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63825/0.75587. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64094/0.72350. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63925/0.72756. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63704/0.73183. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63717/0.71486. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.62899/0.72808. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.62877/0.73823. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61932/0.75344. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62385/0.75110. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61774/0.75509. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61122/0.77028. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.61231/0.78051. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60824/0.76964. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60617/0.79069. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60660/0.81357. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.60192/0.79503. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59682/0.80870. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.59874/0.79791. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59118/0.80494. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.58517/0.81628. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58517/0.82148. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57366/0.84894. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.56707/0.84725. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56852/0.83185. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56570/0.88735. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.57437/0.86591. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.57442/0.82813. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54891/0.87710. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.54282/0.89933. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.54409/0.91723. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.54299/0.88716. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54293/0.89549. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54278/0.89496. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.53666/0.92745. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.52519/0.92766. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.53598/0.91227. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52465/0.93025. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.51478/0.96756. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.52646/0.90079. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50783/0.92961. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.50910/0.91173. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.50318/0.91864. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.50517/0.92961. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.49916/0.91834. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.48135/0.90867. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.47663/0.88291. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48360/0.90852. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48951/0.90798. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.48566/0.88981. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.46249/0.91108. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.47069/0.88846. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.46239/0.90751. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.46007/0.93330. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.47418/0.89356. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.46317/0.91023. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.47181/0.89562. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.43748/0.87847. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.46206/0.93627. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.42723/0.91688. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.41293/0.92224. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.44199/0.92822. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43573/0.86088. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.44119/0.87115. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.41575/0.90629. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.40631/0.90480. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.39826/0.94605. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.39204/0.94791. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.37807/0.96621. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.38777/0.94464. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.38879/0.99443. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.38445/0.95865. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.39066/0.90263. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.40669/0.96561. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.36277/0.98197. Took 0.44 sec\n",
      "ACC: 0.3958333333333333\n",
      "Epoch 0, Loss(train/val) 0.70084/0.76527. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.68668/0.72832. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.67918/0.72522. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.67821/0.73835. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.67482/0.73553. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.67284/0.73318. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67213/0.73037. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.66669/0.73342. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.66596/0.74238. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.65590/0.74456. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.65409/0.76075. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.65929/0.75871. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65474/0.76603. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.64291/0.78537. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.64980/0.77993. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.64551/0.79518. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.64390/0.79773. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.64392/0.79931. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64048/0.81823. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.63060/0.83726. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.63896/0.84752. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63470/0.87435. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.63610/0.86358. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.62353/0.86515. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.62110/0.87678. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62662/0.86012. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62994/0.89217. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62027/0.90596. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.61517/0.91189. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60954/0.95497. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61280/0.99962. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.61502/0.94770. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.60295/0.95205. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60916/0.97049. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.60888/0.97072. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61209/0.96850. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61786/0.98428. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60235/0.99853. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60308/0.95616. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59643/0.99157. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59707/1.03919. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.58938/1.03624. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.58506/1.05559. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.59090/1.14480. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.59775/1.06054. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59015/1.02327. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.58174/1.05624. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58341/1.10913. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58495/1.04225. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.57472/1.03974. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56731/1.04590. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.56549/1.08700. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56177/1.08497. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55360/1.07842. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56911/1.06889. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.56125/1.07589. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.55813/1.15062. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.55548/1.07265. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.53975/1.10435. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54842/1.04220. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.53959/1.05148. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.53304/1.08119. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53598/1.08543. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.52162/1.08527. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.51530/1.17150. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.53747/1.04286. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.52506/1.13179. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.52153/1.12300. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.53306/1.15685. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.52017/1.11408. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.51570/1.14912. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.51060/1.13776. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.50557/1.12226. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.49728/1.21849. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.50918/1.17571. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.52076/1.16974. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.50197/1.14992. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.49372/1.21087. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.48773/1.08997. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.48094/1.21444. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.48167/1.12947. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.46923/1.19363. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.46910/1.27114. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.48338/1.18615. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.47303/1.25615. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.44502/1.34510. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.46492/1.23442. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.44991/1.27548. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.44298/1.34799. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.47592/1.21942. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.45027/1.23330. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.44450/1.30681. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.40516/1.38572. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.42435/1.31973. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.42150/1.34651. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.45626/1.22477. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.43320/1.25725. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.41527/1.29170. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.41529/1.36954. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.42517/1.35873. Took 0.44 sec\n",
      "ACC: 0.3854166666666667\n",
      "Epoch 0, Loss(train/val) 0.70335/0.75161. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69284/0.75571. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69003/0.74898. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68893/0.75809. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68350/0.76206. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68163/0.76942. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.67671/0.78616. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67453/0.81086. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67712/0.82183. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67192/0.80864. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66768/0.81904. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67064/0.81642. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66956/0.82754. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.65753/0.81685. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66032/0.82552. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65457/0.79162. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65122/0.80357. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64455/0.79558. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.64618/0.79598. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64395/0.78915. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.63661/0.80785. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63390/0.78483. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63547/0.81207. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62958/0.80954. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62584/0.81272. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.62240/0.83835. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.61924/0.85729. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62052/0.83916. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61215/0.81732. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60960/0.84126. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.60491/0.84511. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61070/0.84340. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59712/0.87283. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.59990/0.87296. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60528/0.89264. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59882/0.84572. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.60305/0.86097. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59767/0.89029. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59721/0.86501. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.57992/0.89919. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58890/0.95292. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58821/0.86688. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.59057/0.93836. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57442/0.93009. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59347/0.92457. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.57735/0.90746. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57033/0.96550. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.57353/0.92606. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.58009/0.97381. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.56788/0.95951. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57106/0.99136. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.56873/0.93978. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.55894/0.94047. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56439/1.03355. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.54289/1.05036. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.54823/1.04222. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.54947/1.02962. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53273/1.04748. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.53149/1.09459. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53293/1.06813. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52276/1.08520. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.50480/1.08031. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50996/1.10472. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51101/1.12911. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.50527/1.10927. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.49325/1.17489. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.48975/1.12427. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.50268/1.14665. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49815/1.09437. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.47579/1.17872. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.50374/1.19403. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.47927/1.19073. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.46248/1.20013. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48836/1.20381. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.48016/1.17780. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.46739/1.21232. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.46379/1.24995. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.45754/1.27707. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.45195/1.28121. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.42933/1.34882. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.43064/1.41347. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.44524/1.36859. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.43707/1.40246. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.43154/1.35166. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.41700/1.35719. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.40398/1.43519. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.40418/1.42428. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.41061/1.39461. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.40625/1.33520. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.39635/1.46234. Took 0.47 sec\n",
      "Epoch 90, Loss(train/val) 0.38713/1.43562. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.39244/1.41751. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.42399/1.42116. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.41507/1.40830. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.39253/1.50487. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.37765/1.45161. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.42027/1.38206. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.40638/1.32365. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.39170/1.39809. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.38371/1.42335. Took 0.44 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69800/0.70850. Took 0.61 sec\n",
      "Epoch 1, Loss(train/val) 0.68866/0.70341. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68500/0.69888. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.68189/0.70126. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68083/0.69421. Took 0.48 sec\n",
      "Epoch 5, Loss(train/val) 0.67779/0.70445. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67427/0.70868. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.66991/0.71825. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.66450/0.72380. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66177/0.71481. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.66400/0.72653. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.65952/0.73009. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65708/0.72166. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.65131/0.73483. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65356/0.72162. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.64724/0.71970. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.64469/0.72795. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64167/0.73361. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.63255/0.73909. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.63398/0.74686. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.63801/0.74756. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63329/0.75008. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.62530/0.76293. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.61703/0.75119. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.61497/0.76908. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.60569/0.75441. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61365/0.75650. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.60012/0.76565. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.59623/0.77327. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.59681/0.78590. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.59329/0.75566. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.58840/0.76000. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.57820/0.78206. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.57589/0.78076. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.56758/0.78379. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.57576/0.79537. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.56358/0.79385. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.55623/0.80660. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.54823/0.80624. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.54215/0.83112. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.53959/0.84645. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.53789/0.84307. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.53358/0.83349. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.52318/0.88138. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.53781/0.86594. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.51758/0.88642. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.51638/0.87110. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.50631/0.88801. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.50796/0.89496. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.49224/0.92990. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.49610/0.90726. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.49051/0.93002. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.50655/0.91614. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.48785/0.89423. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.47714/0.91716. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.46835/0.97545. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.46194/0.95070. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.45457/0.98499. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.43987/0.99246. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.45627/0.97954. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.44762/0.97160. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.44237/0.96484. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.43155/1.01699. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.43702/1.00880. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.42371/1.03826. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.42660/1.03074. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.42697/1.05891. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.42442/1.02525. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.38600/1.11665. Took 0.47 sec\n",
      "Epoch 69, Loss(train/val) 0.40255/1.08755. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.38212/1.09905. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.41450/1.07612. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.40038/1.11701. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.38827/1.11221. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.38070/1.12476. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.37991/1.12626. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.39739/1.17244. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.36473/1.15482. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.36055/1.18956. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.38171/1.16627. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.36211/1.15882. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.36540/1.19331. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.35148/1.23819. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.37161/1.19904. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.36270/1.16937. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.34801/1.17374. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.33180/1.18433. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.32905/1.26413. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.32771/1.23835. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.32915/1.32238. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.32449/1.19092. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.31629/1.27081. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.32655/1.24517. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.31812/1.30077. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.32122/1.29777. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.28823/1.38312. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.29038/1.42283. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.30793/1.40250. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.27684/1.39166. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.28048/1.43277. Took 0.47 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69513/0.76621. Took 0.51 sec\n",
      "Epoch 1, Loss(train/val) 0.68735/0.78087. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.68741/0.76584. Took 0.51 sec\n",
      "Epoch 3, Loss(train/val) 0.68549/0.76610. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.67533/0.77264. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67567/0.76916. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.67216/0.77263. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67298/0.76884. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.66691/0.76664. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67161/0.78570. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66287/0.80366. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66196/0.80528. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.65998/0.83316. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.66665/0.81584. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.65996/0.80493. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66055/0.82359. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.65294/0.85336. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65212/0.82448. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64588/0.87155. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.64150/0.80692. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.62694/0.86507. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63702/0.82020. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.62675/0.87792. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.63726/0.79914. Took 0.80 sec\n",
      "Epoch 24, Loss(train/val) 0.62611/0.84840. Took 0.52 sec\n",
      "Epoch 25, Loss(train/val) 0.62619/0.83267. Took 0.50 sec\n",
      "Epoch 26, Loss(train/val) 0.61919/0.84882. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.61415/0.87402. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.62527/0.85117. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.61547/0.87097. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60495/0.88364. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.60206/0.89423. Took 0.47 sec\n",
      "Epoch 32, Loss(train/val) 0.60485/0.90158. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.60706/0.90042. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.60083/0.88909. Took 0.47 sec\n",
      "Epoch 35, Loss(train/val) 0.59114/0.88404. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.58243/0.93743. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.57866/0.93516. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.57591/0.93884. Took 0.50 sec\n",
      "Epoch 39, Loss(train/val) 0.57550/0.91337. Took 0.47 sec\n",
      "Epoch 40, Loss(train/val) 0.56518/0.93242. Took 0.47 sec\n",
      "Epoch 41, Loss(train/val) 0.56231/0.93847. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.55938/0.97819. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.56857/0.94312. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.56517/0.96333. Took 0.47 sec\n",
      "Epoch 45, Loss(train/val) 0.56266/0.95729. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.55280/0.93545. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.54771/0.95095. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.53982/0.95109. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.55751/0.91706. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53973/0.96881. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.53948/0.98683. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.52879/0.99935. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52520/0.95998. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.51417/0.98306. Took 0.47 sec\n",
      "Epoch 55, Loss(train/val) 0.51477/1.00841. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.50935/0.99507. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.50469/0.97269. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.50492/0.98128. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.49887/0.95815. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.49745/1.00100. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.49387/0.97654. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.48889/0.95144. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.47743/0.99905. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.46895/1.02109. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.46908/1.03749. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.47399/1.01227. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.45342/1.07613. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.44419/1.09577. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.44147/1.08431. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.45553/1.05778. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.44496/1.11354. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.42943/1.07906. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.46598/1.09547. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.43190/1.09480. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.44163/1.13513. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.42543/1.13079. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.40661/1.20993. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.42569/1.24714. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.42802/1.19751. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.41410/1.16506. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.39834/1.22942. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.43527/1.18184. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.40975/1.27059. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.38556/1.25180. Took 0.47 sec\n",
      "Epoch 85, Loss(train/val) 0.39771/1.27419. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.40298/1.24464. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.39230/1.23330. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.41250/1.16713. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.38031/1.26595. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.38225/1.20545. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.36643/1.23478. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.36749/1.23219. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.36973/1.23219. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.36278/1.25464. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.34443/1.23484. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.35595/1.28706. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.35458/1.27717. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.33936/1.29719. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.34064/1.27957. Took 0.45 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.75411/0.71534. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.73414/0.71016. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.72825/0.71347. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.72597/0.70364. Took 0.49 sec\n",
      "Epoch 4, Loss(train/val) 0.72466/0.72773. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.71579/0.71835. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.71752/0.73827. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.72364/0.73530. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.71152/0.73632. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.70271/0.74383. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.70678/0.75230. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.71361/0.75089. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.71947/0.74108. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.70002/0.75407. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.70817/0.74470. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.69980/0.76307. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.69339/0.76628. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.68865/0.79579. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.68784/0.78520. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.68800/0.79995. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67850/0.83885. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.68612/0.85841. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.68407/0.85857. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.68024/0.85400. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.66065/0.88852. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.66020/0.89821. Took 0.47 sec\n",
      "Epoch 26, Loss(train/val) 0.66832/0.92256. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.65356/0.94502. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65562/0.96129. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.65263/0.92681. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64621/0.92658. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.64641/0.94820. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.64649/0.95516. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.64639/0.95582. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.63643/0.97607. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.63894/0.96831. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.63587/1.01112. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.63857/1.04182. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.64664/1.03104. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.63728/0.96019. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.62909/1.01528. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.61789/1.02331. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.63356/1.00185. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.62545/1.04620. Took 0.47 sec\n",
      "Epoch 44, Loss(train/val) 0.62825/1.03124. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.63156/1.09849. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.62713/1.00671. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.62957/1.05827. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.62596/1.03747. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.60960/1.07323. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.61565/1.08412. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.60823/1.07443. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.60947/1.06347. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.60541/1.09457. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.61207/1.08919. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.60307/1.06915. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.60870/1.11983. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.60867/1.09509. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.59810/1.12766. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.59598/1.16668. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.59647/1.14497. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.58745/1.20748. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.61148/1.09323. Took 0.47 sec\n",
      "Epoch 63, Loss(train/val) 0.59562/1.10419. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.58005/1.14124. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.58401/1.14900. Took 0.47 sec\n",
      "Epoch 66, Loss(train/val) 0.58032/1.13571. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.57457/1.11990. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.58173/1.09076. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.57840/1.15183. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.57381/1.21491. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.56420/1.10151. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.56638/1.12206. Took 0.47 sec\n",
      "Epoch 73, Loss(train/val) 0.56569/1.11024. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.55820/1.16391. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.54471/1.21909. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.54438/1.19867. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.56467/1.09925. Took 0.47 sec\n",
      "Epoch 78, Loss(train/val) 0.54634/1.09559. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.55633/1.12402. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.53102/1.17687. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.53402/1.19014. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.54201/1.11777. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.53258/1.22982. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.53494/1.10602. Took 0.47 sec\n",
      "Epoch 85, Loss(train/val) 0.52122/1.25812. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.50972/1.20373. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.52646/1.19874. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.54006/1.17441. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.52463/1.18968. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.49949/1.19097. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.50401/1.32550. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.52357/1.19873. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.50846/1.25743. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.50364/1.22286. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.48991/1.23452. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.51531/1.24140. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.49958/1.27289. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.50052/1.21861. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.49250/1.21418. Took 0.45 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.73706/0.68765. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.72166/0.73119. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.71435/0.74524. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.70433/0.68703. Took 0.49 sec\n",
      "Epoch 4, Loss(train/val) 0.71011/0.70018. Took 0.49 sec\n",
      "Epoch 5, Loss(train/val) 0.70744/0.71425. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.69973/0.68601. Took 0.55 sec\n",
      "Epoch 7, Loss(train/val) 0.70474/0.72889. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.70149/0.73207. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.70241/0.72798. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.70480/0.75551. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.69628/0.72105. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.69123/0.69815. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.69091/0.70495. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68709/0.69112. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.69430/0.70915. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.69274/0.72310. Took 0.47 sec\n",
      "Epoch 17, Loss(train/val) 0.68073/0.71582. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.68678/0.69964. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.68350/0.70217. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67021/0.71259. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.67774/0.66353. Took 0.49 sec\n",
      "Epoch 22, Loss(train/val) 0.68370/0.68082. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.67388/0.67801. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.67587/0.68138. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.67042/0.68423. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.67202/0.66933. Took 0.48 sec\n",
      "Epoch 27, Loss(train/val) 0.67354/0.66968. Took 0.48 sec\n",
      "Epoch 28, Loss(train/val) 0.66927/0.66210. Took 0.51 sec\n",
      "Epoch 29, Loss(train/val) 0.66157/0.64976. Took 0.51 sec\n",
      "Epoch 30, Loss(train/val) 0.65766/0.66928. Took 0.47 sec\n",
      "Epoch 31, Loss(train/val) 0.66753/0.66258. Took 0.47 sec\n",
      "Epoch 32, Loss(train/val) 0.66256/0.66704. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.65879/0.63742. Took 0.49 sec\n",
      "Epoch 34, Loss(train/val) 0.65807/0.65927. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.65703/0.63629. Took 0.50 sec\n",
      "Epoch 36, Loss(train/val) 0.65530/0.68620. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.66190/0.63158. Took 0.50 sec\n",
      "Epoch 38, Loss(train/val) 0.65066/0.66316. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.65244/0.63188. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.64223/0.65612. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.64100/0.65746. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.64411/0.63845. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.63164/0.63827. Took 0.47 sec\n",
      "Epoch 44, Loss(train/val) 0.64258/0.65370. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.63078/0.66543. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.63744/0.68100. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.63626/0.66533. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.63203/0.66445. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.63274/0.69786. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.63072/0.66876. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.62193/0.66669. Took 0.47 sec\n",
      "Epoch 52, Loss(train/val) 0.62379/0.65591. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.62008/0.66732. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.61591/0.69461. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.61543/0.74594. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.61926/0.72616. Took 0.47 sec\n",
      "Epoch 57, Loss(train/val) 0.61871/0.77100. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.60529/0.77591. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.60647/0.82927. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.60955/0.76965. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.60215/0.79394. Took 0.47 sec\n",
      "Epoch 62, Loss(train/val) 0.60892/0.85637. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.59710/0.84660. Took 0.47 sec\n",
      "Epoch 64, Loss(train/val) 0.59915/0.82568. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.59489/0.81870. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.59854/0.92326. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.59147/0.82420. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.59644/0.90854. Took 0.47 sec\n",
      "Epoch 69, Loss(train/val) 0.58454/0.93343. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.57117/0.96982. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.56734/1.01865. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.57565/0.96655. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.57820/1.01274. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.56928/1.02090. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.56531/0.92868. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.56972/0.98230. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.56010/1.00575. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.56422/0.95615. Took 0.47 sec\n",
      "Epoch 79, Loss(train/val) 0.57006/0.93762. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.55750/0.92690. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.56075/0.97655. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.55528/0.97839. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.54542/0.95814. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.53592/1.04209. Took 0.48 sec\n",
      "Epoch 85, Loss(train/val) 0.53464/1.05460. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.54262/1.01310. Took 0.47 sec\n",
      "Epoch 87, Loss(train/val) 0.53203/1.02807. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.51901/1.07571. Took 0.47 sec\n",
      "Epoch 89, Loss(train/val) 0.50912/1.09563. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.52907/1.08381. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.51388/1.15445. Took 0.47 sec\n",
      "Epoch 92, Loss(train/val) 0.52860/1.03124. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.54205/1.00949. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.51264/1.11451. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.51417/1.08456. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.50490/1.14221. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.49162/1.20657. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.50204/1.18985. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.50889/1.19588. Took 0.47 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.70903/0.72841. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70204/0.76256. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.70640/0.73894. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.70766/0.72107. Took 0.49 sec\n",
      "Epoch 4, Loss(train/val) 0.69311/0.70915. Took 0.48 sec\n",
      "Epoch 5, Loss(train/val) 0.69052/0.70947. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.67913/0.70523. Took 0.49 sec\n",
      "Epoch 7, Loss(train/val) 0.68555/0.69326. Took 0.48 sec\n",
      "Epoch 8, Loss(train/val) 0.68860/0.72497. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67305/0.72028. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.67077/0.72116. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66538/0.72897. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66997/0.71883. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.67094/0.72894. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67484/0.72676. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.67288/0.73348. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.67839/0.72341. Took 0.47 sec\n",
      "Epoch 17, Loss(train/val) 0.66182/0.73250. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.67073/0.72928. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66880/0.74452. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.66602/0.74096. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.66651/0.75372. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.66397/0.75937. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.65751/0.76397. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.65548/0.74575. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.65054/0.75517. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.65620/0.76826. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.65473/0.77199. Took 0.47 sec\n",
      "Epoch 28, Loss(train/val) 0.63991/0.77035. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.65328/0.78957. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.64969/0.79431. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.64421/0.77188. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.64066/0.79221. Took 0.47 sec\n",
      "Epoch 33, Loss(train/val) 0.64276/0.78053. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.64648/0.75726. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.64005/0.74939. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.63290/0.78640. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.63250/0.77500. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.62641/0.76847. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.62279/0.77234. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.63455/0.76124. Took 0.47 sec\n",
      "Epoch 41, Loss(train/val) 0.61510/0.77086. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.61392/0.78006. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.62095/0.77521. Took 0.47 sec\n",
      "Epoch 44, Loss(train/val) 0.61596/0.76014. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.61087/0.76838. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.60399/0.79571. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.60307/0.78436. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.59975/0.77922. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.60885/0.79746. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.59703/0.79682. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.59742/0.80125. Took 0.47 sec\n",
      "Epoch 52, Loss(train/val) 0.58228/0.79388. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.57580/0.81326. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.58187/0.78694. Took 0.47 sec\n",
      "Epoch 55, Loss(train/val) 0.58753/0.78843. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.56299/0.80936. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.57207/0.80818. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.56619/0.81042. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.56324/0.80538. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.56871/0.79766. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.55734/0.79389. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.55765/0.80972. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.55117/0.82532. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.55623/0.82773. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.54206/0.83577. Took 0.47 sec\n",
      "Epoch 66, Loss(train/val) 0.54443/0.83071. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.53912/0.83108. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.53516/0.83298. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.52284/0.84632. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.53458/0.82791. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.53423/0.81707. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.51815/0.82189. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.52699/0.77721. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.51933/0.91853. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.53186/0.81331. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.51761/0.80603. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.50122/0.81384. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.49615/0.78994. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.49232/0.82421. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.49365/0.82710. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.48181/0.83554. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.47622/0.83861. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.48672/0.83736. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.47793/0.84292. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.48849/0.78786. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.47287/0.85339. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.45711/0.79891. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.45796/0.83142. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.44571/0.84557. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.44908/0.87094. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.43215/0.84499. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.42222/0.86035. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.44132/0.87142. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.41646/0.88668. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.41382/0.93724. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.41654/0.88478. Took 0.47 sec\n",
      "Epoch 97, Loss(train/val) 0.40848/0.88065. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.40202/0.87955. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.40027/0.91725. Took 0.45 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.71332/0.71565. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69976/0.71025. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69748/0.71759. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69280/0.72349. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.68949/0.72670. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.67677/0.73400. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.66930/0.74214. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.67898/0.74061. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67552/0.73730. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68112/0.72559. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67098/0.73040. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.67193/0.76751. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67547/0.74185. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.67514/0.75116. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67134/0.75774. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.66847/0.77761. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66186/0.77858. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66048/0.78567. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.65921/0.78659. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66053/0.77168. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65326/0.77046. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.65296/0.78585. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64635/0.77928. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.65071/0.79832. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64047/0.78928. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64846/0.79580. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.64094/0.80356. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63991/0.78796. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63884/0.82664. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.63010/0.81683. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64069/0.80764. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.63383/0.80172. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62375/0.83534. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.62258/0.83156. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61088/0.85772. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.62232/0.87156. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.61620/0.85895. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60978/0.85414. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.60968/0.88752. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.61337/0.85812. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60783/0.87210. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.60041/0.86518. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58809/0.84877. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.58581/0.88159. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58355/0.91911. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.57701/0.94751. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.57800/0.93013. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56972/0.90732. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.55876/0.99057. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.55515/0.96780. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.56290/0.93232. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.54410/0.98122. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.56204/0.95656. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.55416/0.93011. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54688/1.00079. Took 0.47 sec\n",
      "Epoch 55, Loss(train/val) 0.54443/0.95511. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53583/0.99640. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.55303/0.97378. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.53713/1.00547. Took 0.47 sec\n",
      "Epoch 59, Loss(train/val) 0.52021/0.99281. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.52018/0.98386. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.52680/0.94284. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.52377/1.04528. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.50031/0.99637. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.52427/0.99022. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.49699/0.97505. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.49085/1.04592. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.50086/1.01273. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.49224/0.98593. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.47953/1.11237. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.49031/1.04699. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.48101/1.06893. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.45675/1.16221. Took 0.47 sec\n",
      "Epoch 73, Loss(train/val) 0.45483/1.09417. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.45896/1.11338. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.44834/1.12879. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.45525/1.13376. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.42964/1.15270. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.44842/1.14786. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.45659/1.16999. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.44542/1.16841. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.43877/1.12623. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.43384/1.17988. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.43066/1.12569. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.43190/1.14415. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.44614/1.17080. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43484/1.17059. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.40298/1.22099. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.40202/1.17793. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.38688/1.23537. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.39342/1.23230. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.39091/1.30128. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.37088/1.29240. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.38749/1.25795. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.35200/1.22318. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.33883/1.28468. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.37933/1.26415. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.34028/1.29909. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.32847/1.33257. Took 0.47 sec\n",
      "Epoch 99, Loss(train/val) 0.33687/1.33844. Took 0.46 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69157/0.68716. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69268/0.68575. Took 0.49 sec\n",
      "Epoch 2, Loss(train/val) 0.69136/0.68253. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.68181/0.67819. Took 0.49 sec\n",
      "Epoch 4, Loss(train/val) 0.68710/0.68126. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68427/0.68055. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.68258/0.68062. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68166/0.68387. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68133/0.68996. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.67846/0.69564. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67858/0.70297. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.67521/0.70265. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.67736/0.71244. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66897/0.72409. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67306/0.72624. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.66936/0.72404. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65816/0.74519. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66562/0.72939. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.65929/0.74340. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65701/0.75149. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65685/0.75333. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.64822/0.76168. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.64601/0.77634. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.63663/0.79543. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.63977/0.80485. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64044/0.80431. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.64621/0.78729. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62672/0.79319. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62324/0.81534. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.61623/0.83289. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.61663/0.86494. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.60524/0.86011. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.60040/0.86295. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.60003/0.87728. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.59637/0.88342. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.58757/0.89971. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58379/0.89606. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.57876/0.91861. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.58157/0.90985. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.57210/0.95701. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.57872/0.90248. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.56311/0.91764. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.56065/0.95020. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.55336/0.92595. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.55820/0.93415. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.54359/0.94571. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.53658/0.96521. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.55025/0.94155. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.54065/0.98895. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.53925/1.00639. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.53196/0.96134. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.52948/0.96551. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.51418/1.02117. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.50525/1.02117. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.50791/1.03586. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.49873/1.02077. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.50849/1.07246. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.47529/1.08332. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.48461/1.08583. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.49893/1.07224. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.47736/1.07792. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.47288/1.09657. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.47062/1.12284. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.46758/1.07792. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.48327/1.09517. Took 0.47 sec\n",
      "Epoch 65, Loss(train/val) 0.44690/1.11778. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.45290/1.13775. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.43892/1.16301. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.45331/1.11680. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.49120/1.04490. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.45519/1.06416. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.44939/1.03929. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.43972/1.07171. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.42465/1.11823. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.43771/1.11852. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.42047/1.14732. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.42322/1.16342. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.40367/1.19037. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.40112/1.21923. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.40221/1.23673. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.39161/1.23713. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.41891/1.24373. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.38164/1.17436. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.37354/1.31674. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.38162/1.23549. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.37655/1.26563. Took 0.48 sec\n",
      "Epoch 86, Loss(train/val) 0.38459/1.22986. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.37822/1.17788. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.37669/1.22391. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.39407/1.19763. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.40730/1.29738. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.36199/1.28799. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.36296/1.25052. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.33884/1.23742. Took 0.47 sec\n",
      "Epoch 94, Loss(train/val) 0.36574/1.21976. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.33671/1.19787. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.35892/1.28177. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.33825/1.29918. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.34101/1.36610. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.33429/1.29758. Took 0.45 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.74915/0.72992. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.73674/0.73234. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.72800/0.72482. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.72707/0.72926. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.71820/0.74275. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.71509/0.71977. Took 0.51 sec\n",
      "Epoch 6, Loss(train/val) 0.70273/0.71780. Took 0.49 sec\n",
      "Epoch 7, Loss(train/val) 0.70880/0.70561. Took 0.49 sec\n",
      "Epoch 8, Loss(train/val) 0.71044/0.73916. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.70674/0.74079. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.69884/0.73486. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.68467/0.73356. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.68975/0.73296. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.68556/0.73642. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67909/0.74264. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.68316/0.75542. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.68067/0.74377. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.68357/0.73641. Took 0.47 sec\n",
      "Epoch 18, Loss(train/val) 0.67404/0.74590. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.67020/0.75478. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.67222/0.75201. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.66621/0.75135. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.66153/0.77839. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66448/0.78606. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.66019/0.77871. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.65948/0.76062. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.66643/0.75902. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.65211/0.79042. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.65566/0.79696. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.65182/0.77510. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63906/0.80472. Took 0.47 sec\n",
      "Epoch 31, Loss(train/val) 0.64003/0.80722. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.64621/0.83017. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63983/0.82382. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.63833/0.81054. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.62637/0.84047. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.63125/0.83763. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.62450/0.83665. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.62070/0.85855. Took 0.47 sec\n",
      "Epoch 39, Loss(train/val) 0.61400/0.85262. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.61208/0.86118. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.60643/0.84479. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.60757/0.84638. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.60392/0.85343. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.59935/0.86701. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.60803/0.86205. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.59662/0.88843. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.59758/0.85689. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.58905/0.86403. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.58588/0.91059. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.58159/0.90495. Took 0.49 sec\n",
      "Epoch 51, Loss(train/val) 0.58932/0.88993. Took 0.47 sec\n",
      "Epoch 52, Loss(train/val) 0.56778/0.90910. Took 0.47 sec\n",
      "Epoch 53, Loss(train/val) 0.57747/0.92781. Took 0.49 sec\n",
      "Epoch 54, Loss(train/val) 0.57191/0.94255. Took 0.47 sec\n",
      "Epoch 55, Loss(train/val) 0.58240/0.99741. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.59230/0.93081. Took 0.47 sec\n",
      "Epoch 57, Loss(train/val) 0.57235/0.94647. Took 0.49 sec\n",
      "Epoch 58, Loss(train/val) 0.57356/0.93974. Took 0.50 sec\n",
      "Epoch 59, Loss(train/val) 0.57284/0.94294. Took 0.50 sec\n",
      "Epoch 60, Loss(train/val) 0.57202/0.95417. Took 0.47 sec\n",
      "Epoch 61, Loss(train/val) 0.55602/0.97256. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.55002/0.94459. Took 0.48 sec\n",
      "Epoch 63, Loss(train/val) 0.56952/0.98740. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.55946/0.93112. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.56032/0.96531. Took 0.49 sec\n",
      "Epoch 66, Loss(train/val) 0.54182/1.02308. Took 0.47 sec\n",
      "Epoch 67, Loss(train/val) 0.53642/1.02366. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.54932/1.03277. Took 0.47 sec\n",
      "Epoch 69, Loss(train/val) 0.53460/1.00256. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.54732/1.05795. Took 0.48 sec\n",
      "Epoch 71, Loss(train/val) 0.52119/0.95956. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.54550/1.03194. Took 0.49 sec\n",
      "Epoch 73, Loss(train/val) 0.51811/1.03528. Took 0.47 sec\n",
      "Epoch 74, Loss(train/val) 0.53472/1.01920. Took 0.49 sec\n",
      "Epoch 75, Loss(train/val) 0.52010/1.03267. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.50970/1.02959. Took 0.48 sec\n",
      "Epoch 77, Loss(train/val) 0.53394/1.09600. Took 0.48 sec\n",
      "Epoch 78, Loss(train/val) 0.50197/1.01352. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.49165/1.13304. Took 0.48 sec\n",
      "Epoch 80, Loss(train/val) 0.51998/1.02728. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.50413/1.06690. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.48562/1.07053. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.49691/1.10391. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.50636/1.14056. Took 0.47 sec\n",
      "Epoch 85, Loss(train/val) 0.49266/1.08629. Took 0.47 sec\n",
      "Epoch 86, Loss(train/val) 0.50349/1.10473. Took 0.48 sec\n",
      "Epoch 87, Loss(train/val) 0.48166/1.05442. Took 0.47 sec\n",
      "Epoch 88, Loss(train/val) 0.47685/1.04254. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.48640/1.03972. Took 0.47 sec\n",
      "Epoch 90, Loss(train/val) 0.45900/1.08898. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.47633/1.05105. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.46443/1.13060. Took 0.47 sec\n",
      "Epoch 93, Loss(train/val) 0.45364/1.10713. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.46544/1.19840. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.46636/1.14368. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.46238/1.07545. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.44684/1.07485. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.44777/1.13364. Took 0.52 sec\n",
      "Epoch 99, Loss(train/val) 0.46548/1.10662. Took 0.46 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.72515/0.68446. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.71623/0.71308. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.70648/0.72407. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.70053/0.71058. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69737/0.72029. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69098/0.74001. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68758/0.72802. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68381/0.74624. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.68471/0.73873. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67665/0.76449. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67603/0.75028. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66667/0.77169. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66782/0.79146. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67036/0.79569. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66689/0.81772. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66147/0.83961. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65463/0.84380. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.64907/0.85110. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65378/0.82829. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.64653/0.84650. Took 0.47 sec\n",
      "Epoch 20, Loss(train/val) 0.64827/0.87201. Took 0.47 sec\n",
      "Epoch 21, Loss(train/val) 0.64236/0.87011. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64540/0.85189. Took 0.47 sec\n",
      "Epoch 23, Loss(train/val) 0.64799/0.85701. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63656/0.87479. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63655/0.84444. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.63282/0.86415. Took 0.47 sec\n",
      "Epoch 27, Loss(train/val) 0.63326/0.88370. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63827/0.86508. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.63116/0.88912. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63582/0.87106. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62638/0.87398. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62037/0.88740. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.62394/0.89022. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62105/0.89188. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.61984/0.88739. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.61648/0.89216. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61283/0.91642. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.60078/0.91536. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.60551/0.91891. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.60292/0.94663. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60348/0.93570. Took 0.49 sec\n",
      "Epoch 42, Loss(train/val) 0.60259/0.95010. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59996/0.93174. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59346/0.95128. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.60087/0.95035. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58448/0.96795. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.58144/0.97118. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.58115/0.99421. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.58750/0.98170. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.58230/0.98592. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.58984/0.99402. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.59349/0.95641. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.56147/1.00041. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.57159/0.99763. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.57478/0.96140. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.56456/0.98889. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.56289/1.00656. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.56160/1.00497. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.56268/0.99865. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55600/1.04973. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.56382/1.02939. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.55672/1.03144. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.54543/1.03058. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.53286/1.06790. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.55020/1.03792. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.54771/1.04493. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.53572/1.07446. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.53989/1.08909. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.52175/1.13614. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.53796/1.07108. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.53743/1.08340. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.52072/1.14446. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.50513/1.17695. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.51759/1.13410. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.51060/1.14072. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.50532/1.15989. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.50585/1.24133. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.50123/1.19834. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.49075/1.28227. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.48524/1.34599. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.49549/1.30596. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.48053/1.36580. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.47219/1.36143. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.46187/1.43979. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.46994/1.46090. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.46221/1.41961. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.45750/1.52362. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.45664/1.49865. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.44788/1.54671. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.45463/1.53985. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.45098/1.51241. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.44609/1.51067. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.43792/1.58287. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.44101/1.61239. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.45227/1.54386. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.44352/1.60155. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.40732/1.63075. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.40717/1.67689. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.41768/1.67458. Took 0.45 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.71611/0.72072. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70327/0.72809. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.70592/0.72056. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68909/0.71622. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.69424/0.72714. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69639/0.72682. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69028/0.72625. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68551/0.72461. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68736/0.72240. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67877/0.70852. Took 0.48 sec\n",
      "Epoch 10, Loss(train/val) 0.68121/0.71805. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68127/0.73671. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67459/0.72940. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68313/0.72403. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67340/0.73336. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67561/0.73531. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.68094/0.73092. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66139/0.73993. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66867/0.74465. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66780/0.74751. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65863/0.74941. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66531/0.75107. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65977/0.73618. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64849/0.75237. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65420/0.76506. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65264/0.75604. Took 0.47 sec\n",
      "Epoch 26, Loss(train/val) 0.64866/0.73659. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64724/0.75291. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64070/0.74910. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64037/0.75173. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63599/0.76247. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64313/0.76764. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.64031/0.77296. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63470/0.77758. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.64543/0.78069. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.63559/0.77762. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62588/0.77669. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.63303/0.81011. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.63578/0.78171. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.62429/0.80184. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.62499/0.81516. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.62293/0.81008. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.62698/0.81608. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.62248/0.79011. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.61855/0.79386. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.61623/0.81351. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.61720/0.79815. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.61701/0.83438. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.60900/0.83432. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.60323/0.85211. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.61614/0.82095. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.61359/0.84332. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.59688/0.84537. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.60928/0.83642. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.60260/0.86057. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.58746/0.84066. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.60675/0.84611. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.59155/0.83898. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.59909/0.84886. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.60923/0.85296. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.58856/0.84918. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.59411/0.87322. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.57782/0.87703. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.57064/0.90593. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.59192/0.91744. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.57252/0.91835. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.57240/0.89878. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.56024/0.91136. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.56440/0.94302. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.56581/0.91413. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.55429/0.94878. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.55714/0.92598. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.55597/0.91660. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.55237/0.93363. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.54752/0.93658. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.53099/0.97974. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.52296/1.03542. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.54727/0.98418. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.53950/0.94797. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.53954/0.99807. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.53237/0.98040. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.51674/0.97948. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.52750/0.98234. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.52802/0.99140. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.50179/1.02937. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.50591/1.02092. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.50539/1.07324. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.50579/1.05145. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.51205/1.09111. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.49271/1.09091. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.47816/1.15908. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.49235/1.04768. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.49519/1.13369. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.50355/1.08622. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.48554/1.02914. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.46870/1.21167. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.50135/1.14888. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.48176/1.13729. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.47239/1.12083. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.46376/1.18354. Took 0.44 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.70392/0.73267. Took 0.60 sec\n",
      "Epoch 1, Loss(train/val) 0.70115/0.72153. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69812/0.71497. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68906/0.72847. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68826/0.71455. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.68706/0.70867. Took 0.48 sec\n",
      "Epoch 6, Loss(train/val) 0.68686/0.70916. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68489/0.72702. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67544/0.72738. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67214/0.72143. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.67784/0.70644. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.67544/0.70737. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67076/0.71255. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66445/0.72044. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67774/0.72893. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66310/0.71928. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66347/0.71570. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66501/0.73602. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65909/0.75052. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65430/0.76962. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64573/0.79464. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65341/0.76872. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.65076/0.78011. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65125/0.77407. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.65217/0.77195. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64966/0.76795. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63915/0.76755. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63641/0.78045. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63759/0.77809. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62997/0.78727. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62927/0.81276. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61859/0.80874. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.61781/0.82253. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60239/0.80510. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.60629/0.81123. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59741/0.83854. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.59758/0.84374. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.59359/0.83394. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58713/0.85776. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.57968/0.86670. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58067/0.85753. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57948/0.86071. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.57888/0.84022. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.55639/0.89253. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56368/0.87578. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56257/0.88072. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.55102/0.86922. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55351/0.93919. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55196/0.95004. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52807/0.92692. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.52362/0.96114. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.52261/0.95146. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.52168/0.92769. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.53134/0.90072. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.50275/0.95691. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.50105/0.91952. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.51073/0.95464. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.48668/0.97118. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.49713/0.94161. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.47369/1.01065. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.48635/1.02196. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.46334/1.05621. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.45809/1.05110. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.45875/1.03935. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.44882/1.09141. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.43742/1.08629. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.44775/1.10581. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.43955/1.13078. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.43652/1.16783. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.44673/1.07992. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.42601/1.12023. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.41556/1.15138. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.42618/1.10942. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.40180/1.17925. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.40938/1.20044. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.40073/1.21727. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.39802/1.23556. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.38416/1.25949. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.37572/1.31138. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.37318/1.24887. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.37391/1.30181. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.38945/1.25521. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.36755/1.41515. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.37312/1.27341. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.36688/1.32728. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.34207/1.42904. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.34701/1.40414. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.35428/1.35673. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.32988/1.56296. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.34870/1.47787. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.36624/1.44183. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.31824/1.40701. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.32058/1.50759. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.29746/1.63678. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.31543/1.57041. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.31090/1.48758. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.33297/1.43659. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.35999/1.44969. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.27997/1.57036. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.31367/1.58281. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.72184/0.68863. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70515/0.71079. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.70040/0.68978. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69715/0.71325. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.68964/0.70263. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69458/0.70643. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.69257/0.70367. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68046/0.71607. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67834/0.71079. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68104/0.70346. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67416/0.71612. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67699/0.70860. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67415/0.73583. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67315/0.72160. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67158/0.73796. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67724/0.72981. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.67116/0.72858. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.67236/0.72984. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66481/0.73078. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66612/0.73490. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.66367/0.73302. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65504/0.75130. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65886/0.76115. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64576/0.77751. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.64162/0.77253. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64532/0.79063. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.63609/0.78692. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64203/0.78083. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62682/0.78308. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63042/0.79473. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.64874/0.77662. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.63130/0.79053. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62941/0.75886. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.62878/0.77425. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62527/0.78112. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62757/0.75278. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.62166/0.78336. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60761/0.72481. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.60810/0.75032. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.61232/0.74500. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61142/0.74924. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.61847/0.73242. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58387/0.76687. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59675/0.77285. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.59626/0.75313. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.58895/0.76051. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.60774/0.77561. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57834/0.77148. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58597/0.76941. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56928/0.78675. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.57316/0.78698. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56205/0.78844. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56546/0.81897. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.56061/0.78517. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.55785/0.77854. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.53737/0.80574. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53334/0.81483. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.54939/0.85307. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.53272/0.86768. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52438/0.87152. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.53189/0.84849. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.50409/0.87361. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.51871/0.88458. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.52814/0.85801. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.50523/0.86222. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.51903/0.87568. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.51670/0.82657. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.52426/0.84006. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.51333/0.81411. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.50306/0.80609. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.50717/0.82245. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.48265/0.84604. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48028/0.87179. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48906/0.85566. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.50967/0.89249. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.47971/0.91957. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.48414/0.91401. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.49874/0.84391. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.48603/0.89249. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.51580/0.89681. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.45998/0.91952. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.47553/0.92891. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.47272/0.91754. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.47481/0.96720. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.45989/0.95782. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.44356/0.90342. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.44668/0.97076. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.44090/1.02351. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.44738/0.95817. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.46248/0.92159. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.43892/1.01197. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.44078/1.04107. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.42466/1.02302. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.42566/1.02023. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.41889/1.02578. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.40873/1.03038. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.41838/1.06756. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.41592/1.09730. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.40137/1.02776. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.41940/1.04365. Took 0.44 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.70401/0.68771. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69521/0.68662. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69443/0.69995. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69147/0.69504. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.69386/0.69928. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68879/0.71136. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68450/0.72219. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68411/0.70541. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68350/0.71401. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68042/0.71579. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67468/0.71617. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66921/0.71658. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66885/0.73215. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67180/0.70789. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.65828/0.70057. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65773/0.69954. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65416/0.68763. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.64729/0.67874. Took 0.47 sec\n",
      "Epoch 18, Loss(train/val) 0.65411/0.68954. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64666/0.68996. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63893/0.70708. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63037/0.70826. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.62466/0.72517. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.61692/0.73338. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.61096/0.77353. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.61529/0.72915. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.60409/0.79030. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.60924/0.78468. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61821/0.79576. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.59762/0.76356. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.57963/0.79412. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.57382/0.75919. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.57454/0.78636. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.57096/0.79752. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.57117/0.84394. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.55756/0.84036. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.56163/0.84189. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.55323/0.88828. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.55237/0.88421. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.54609/0.84655. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.51988/0.90300. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.52425/0.92394. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.52357/0.90381. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.51569/0.91368. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.51664/0.87660. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.49757/0.91418. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.51366/0.95361. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.50644/0.96535. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.48846/0.96098. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.47759/1.00154. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.48557/1.01254. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.48541/1.08070. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.45710/1.08084. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.46293/1.03469. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.44775/1.05759. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.45063/1.15291. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.43736/1.09775. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.44429/1.08719. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.45989/1.09678. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.43863/1.10980. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.42306/1.13003. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.40669/1.21084. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.41500/1.09136. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.39365/1.09191. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.39112/1.20647. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.37847/1.21974. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.40782/1.16013. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.40485/1.22553. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.39768/1.28820. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.39295/1.27019. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.36973/1.26851. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.36310/1.22328. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.36804/1.20871. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.34690/1.37659. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.33578/1.36918. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.34341/1.40828. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.36264/1.26419. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.35996/1.35983. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.33641/1.35787. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.32892/1.40477. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.33229/1.39723. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.31174/1.42840. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.31761/1.40969. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.29428/1.44622. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.28941/1.45957. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.31099/1.48479. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.29030/1.49924. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.30269/1.57873. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.32092/1.48906. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.31030/1.47503. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.29757/1.54637. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.30564/1.49479. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.28578/1.53079. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.27348/1.48974. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.27964/1.47892. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.27836/1.60044. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.27846/1.61624. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.27491/1.58818. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.26209/1.49282. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.27700/1.66672. Took 0.45 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.71763/0.67728. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70790/0.68331. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.70941/0.69207. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70747/0.69090. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69545/0.68800. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.70160/0.68423. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69753/0.68876. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68838/0.69298. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68751/0.70324. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.69498/0.71778. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.69166/0.69711. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68624/0.69856. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.68184/0.70646. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67423/0.71377. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.68094/0.71210. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66838/0.71719. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66795/0.73390. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67312/0.74681. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66172/0.75754. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66025/0.77804. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65858/0.80317. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65827/0.81018. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64938/0.82523. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64782/0.83775. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64068/0.85091. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64869/0.86251. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64108/0.83520. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62870/0.85689. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63526/0.88094. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.63424/0.84830. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.62335/0.91726. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62082/0.89983. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.61567/0.91196. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.60426/0.93764. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.60672/0.91222. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60923/0.91682. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.59976/0.95108. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.60122/0.95539. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.60008/0.95214. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.59756/0.96536. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58911/0.98639. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.57254/0.96521. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57776/0.99752. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56953/0.98459. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58273/0.93387. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.58019/0.97907. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55647/1.03667. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.56490/0.98285. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56247/1.01197. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55557/0.96588. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.55273/0.98007. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.55708/0.98040. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.53551/1.05721. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.54205/1.01317. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55253/1.00725. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.53274/1.01052. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53124/0.99225. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.55431/1.03405. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.53566/1.00713. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.55692/0.95591. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54147/0.98046. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.52739/1.00540. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53953/0.99974. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51071/1.03580. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.52960/1.03118. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50975/1.05806. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51328/1.09285. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.51694/1.00827. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50592/1.08700. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.49732/1.07672. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49671/1.08167. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.48743/1.12047. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.50029/1.07949. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.50401/1.00888. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.49437/1.04319. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.47417/1.12111. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.47883/1.18079. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.48918/1.15179. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.46558/1.15466. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44734/1.22674. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.47105/1.17782. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.47225/1.10374. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.49201/1.10027. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.47845/1.16592. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.45658/1.19334. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.44855/1.16639. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43193/1.22231. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.46225/1.15385. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.44203/1.18239. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.43613/1.19489. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.41399/1.29708. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.44677/1.23652. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.44840/1.17146. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.41674/1.25838. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40951/1.26249. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.42004/1.24816. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.41656/1.21622. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.38890/1.35950. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.42114/1.30420. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.40458/1.29152. Took 0.44 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69875/0.68896. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69462/0.69606. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69514/0.70026. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69300/0.69805. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.69198/0.70357. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68976/0.70709. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68611/0.70826. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68232/0.71156. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68676/0.71433. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68271/0.71548. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68386/0.72047. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.68249/0.72322. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67908/0.73017. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67861/0.71877. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67245/0.72365. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67355/0.72866. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67550/0.72928. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66709/0.73372. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66155/0.75141. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66106/0.75146. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65594/0.76432. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65093/0.76879. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65141/0.78103. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64325/0.77788. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63275/0.77603. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62581/0.79864. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62563/0.80137. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62327/0.77993. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61988/0.80900. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60926/0.79616. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.60301/0.81685. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.59358/0.80875. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.59062/0.80410. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.58971/0.81052. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.58049/0.82079. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.57001/0.83046. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.57630/0.80337. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.57896/0.78623. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57676/0.80162. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.55719/0.81003. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.54627/0.83118. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.56214/0.80512. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.54678/0.81901. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.54410/0.80205. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.53275/0.80749. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.52748/0.82341. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.53229/0.82919. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.52535/0.85554. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.51964/0.82447. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.50678/0.83797. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.50964/0.83258. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.50675/0.82041. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.51350/0.83521. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.49439/0.86374. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.48148/0.86924. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.47523/0.83021. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.47442/0.86087. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.47350/0.87616. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.46002/0.91680. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.46323/0.90000. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.43784/0.92144. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.45561/0.94025. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.44498/0.99522. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.44006/0.92943. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.43351/0.98665. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.45191/0.98794. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.44038/0.98923. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.42157/0.95236. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.42912/1.02113. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.43045/0.99277. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.41625/1.01810. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.42090/0.99323. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.42032/1.00213. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.40588/1.04921. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.40577/1.04238. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.42075/1.00695. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.40967/1.02463. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.44001/1.03789. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.41661/1.03619. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.40392/1.09717. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.39064/1.05493. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.37411/1.10760. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.37932/1.08947. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.38088/1.07398. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.37141/1.08448. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.36863/1.22545. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.36498/1.23457. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.37804/1.23763. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.35651/1.16712. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.33231/1.22347. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.32886/1.26357. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.35995/1.11732. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.37020/1.12916. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.33738/1.20750. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.34849/1.21958. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.33938/1.24088. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.34026/1.22802. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.33130/1.20553. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.34573/1.17721. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.32522/1.23286. Took 0.46 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.71117/0.74940. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70560/0.70986. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.70592/0.70116. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69466/0.69066. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.69211/0.70351. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69677/0.69233. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68842/0.69631. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.69016/0.69847. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.69194/0.70805. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68943/0.71585. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.68126/0.71267. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68367/0.70873. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68364/0.70373. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67989/0.71404. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67811/0.70330. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67468/0.70229. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67805/0.72635. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.67114/0.73513. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66353/0.74199. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66545/0.74883. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65263/0.77143. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65760/0.78625. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.64562/0.81714. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.65901/0.78543. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64712/0.78044. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64070/0.79036. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64437/0.83393. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63413/0.87186. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63503/0.82712. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61576/0.87777. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62785/0.88354. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.61969/0.89335. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.62336/0.86382. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.61489/0.89301. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61166/0.81821. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61605/0.82237. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.60570/0.84575. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59548/0.84460. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59793/0.81333. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.58853/0.80974. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.58766/0.82700. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58541/0.82712. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.59157/0.79913. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.58519/0.82128. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57788/0.81516. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56434/0.86441. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56439/0.85160. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56552/0.83923. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.56470/0.82486. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.56666/0.86119. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53420/0.82577. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.58348/0.88699. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.55010/0.80796. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52932/0.84258. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56130/0.85027. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.53003/0.83297. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.53535/0.84296. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.51415/0.87932. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.51835/0.89460. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50630/0.90386. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.51475/0.92888. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50013/0.95491. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.49341/0.97132. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48495/0.95342. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49056/1.01270. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.49980/0.94936. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48416/1.00474. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.49214/0.96587. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.45064/1.03719. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46817/1.05852. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.47043/1.04163. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.47738/1.02541. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46226/1.08109. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48679/1.03377. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.46022/1.02927. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44369/1.14956. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.45040/1.07735. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.44343/1.12193. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.42830/1.10712. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.43153/1.11150. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.42405/1.18526. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.42508/1.25183. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.41872/1.25814. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40487/1.23926. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.41364/1.28785. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.41529/1.25949. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.39343/1.28172. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.38383/1.34664. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.38531/1.24856. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.42230/1.24775. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.37933/1.23841. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.35245/1.34570. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.37357/1.31443. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.37880/1.36138. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.39310/1.31664. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.40195/1.23275. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.35172/1.23978. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.35641/1.26359. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.34402/1.33462. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.36199/1.43413. Took 0.44 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.71206/0.69720. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69551/0.68819. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69160/0.69019. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69074/0.67915. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68688/0.68068. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68363/0.67845. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.68407/0.67590. Took 0.48 sec\n",
      "Epoch 7, Loss(train/val) 0.68276/0.68356. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67919/0.68129. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67407/0.66870. Took 0.49 sec\n",
      "Epoch 10, Loss(train/val) 0.67035/0.67340. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67319/0.68054. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66576/0.68550. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66444/0.69253. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66408/0.69171. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66039/0.69061. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65852/0.69162. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64969/0.69260. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.64820/0.67678. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.64389/0.69147. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.63941/0.68867. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.63527/0.69544. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.62971/0.70706. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.61990/0.71542. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.61566/0.72837. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.61020/0.72240. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61258/0.74684. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.59778/0.77688. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.59757/0.76554. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.59802/0.78777. Took 0.47 sec\n",
      "Epoch 30, Loss(train/val) 0.58215/0.78883. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.57715/0.80096. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.57602/0.83130. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.57101/0.81666. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.57386/0.78871. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.56980/0.82407. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.55844/0.81383. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.54793/0.83014. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.54188/0.85382. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.54502/0.87014. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.52159/0.85893. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.52151/0.92837. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.51962/0.93746. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.52219/0.90494. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.49948/0.94218. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.50199/0.95084. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.48563/0.99233. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.47202/1.00031. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.48079/0.96689. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.47763/0.94772. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.45495/0.95448. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.44374/1.03635. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.43420/0.97123. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.45788/0.94732. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.42656/0.99119. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.42823/1.04737. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.40901/1.08069. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.41355/1.11033. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.41394/1.04579. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.41423/1.09607. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.39563/1.13932. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.39469/1.09400. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.39277/1.15375. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.38619/1.15795. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.37290/1.17022. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.35066/1.21004. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.35331/1.22459. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.36420/1.24350. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.35377/1.24770. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.33348/1.27407. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.34104/1.27318. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.32470/1.28982. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.31862/1.29680. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.30705/1.25160. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.29946/1.36193. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.28348/1.36810. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.30664/1.41233. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.29050/1.41549. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.30527/1.36386. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.29581/1.37722. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.28209/1.39039. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.27598/1.45500. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.27031/1.34139. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.25480/1.40286. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.26873/1.46999. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.26237/1.45051. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.27579/1.39377. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.27175/1.52709. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.24711/1.48659. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.24484/1.62283. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.24898/1.43411. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.24671/1.50039. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.23071/1.45463. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.21469/1.47636. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.22761/1.47213. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.18916/1.56066. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.20674/1.57565. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.22426/1.53611. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.19015/1.66199. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.21675/1.70743. Took 0.44 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70467/0.71309. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69505/0.67464. Took 0.50 sec\n",
      "Epoch 2, Loss(train/val) 0.69595/0.67833. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68353/0.68241. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68076/0.67822. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.68372/0.68451. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68251/0.68798. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68109/0.68879. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.67856/0.70574. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.66821/0.71112. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67419/0.72053. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66993/0.72247. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67068/0.72478. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67350/0.72772. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66570/0.73195. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65934/0.75013. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66279/0.73334. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65850/0.73759. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66499/0.74622. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66239/0.73423. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66210/0.76721. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64881/0.78654. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64842/0.79841. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64668/0.78277. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.64109/0.79978. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63489/0.83211. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62954/0.87276. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.62375/0.84658. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62106/0.86037. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61912/0.87412. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.61758/0.87772. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61555/0.85874. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.60295/0.86310. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.60620/0.86397. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59649/0.88497. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60256/0.89998. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.59679/0.89765. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57873/0.86833. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57727/0.87942. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58662/0.89995. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56996/0.95398. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.57208/0.95574. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55908/0.94866. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.55248/1.00013. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55232/1.04339. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56544/0.98891. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55042/1.00679. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.53254/1.04618. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.53951/0.97707. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54030/1.03732. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.51683/1.06521. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.51239/1.06239. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.51533/1.12529. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.51178/1.06090. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51527/1.10341. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51386/1.16482. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.50511/1.11261. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51291/1.07935. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50229/1.14613. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.49579/1.12655. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50075/1.11552. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50511/1.15004. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.46934/1.19733. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.46762/1.26347. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.47419/1.17480. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.45621/1.29890. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.46775/1.23501. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.45072/1.29594. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.46257/1.21929. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.45824/1.19063. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.44376/1.22740. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.42909/1.30808. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.43732/1.31230. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.43387/1.34505. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.41511/1.33844. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.42230/1.39094. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.41570/1.28410. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.41661/1.28785. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.40913/1.36629. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.40597/1.41381. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.41521/1.40530. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.41628/1.32661. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.40027/1.46690. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.39941/1.46140. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.38871/1.56813. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.37440/1.49687. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.37298/1.38289. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.36353/1.46355. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.37598/1.52684. Took 0.47 sec\n",
      "Epoch 89, Loss(train/val) 0.36710/1.54192. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.37754/1.48102. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.39003/1.45667. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.36421/1.39468. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.35986/1.41121. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.35729/1.48299. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.33240/1.58423. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.32865/1.53451. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.32063/1.55673. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.33555/1.44038. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.32862/1.59961. Took 0.44 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69495/0.70758. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69176/0.70877. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68214/0.71890. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68849/0.70631. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.67986/0.71056. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.67679/0.72036. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67893/0.72988. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67510/0.73827. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67088/0.74157. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67283/0.74741. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67365/0.75939. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66477/0.75601. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66800/0.77971. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66317/0.78501. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66412/0.76189. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.66343/0.74652. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65729/0.74315. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65861/0.75325. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65790/0.74067. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66007/0.73791. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64897/0.74227. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64176/0.73817. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65006/0.74676. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64238/0.74947. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63835/0.74653. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.63578/0.75818. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63173/0.76595. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62235/0.75496. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62233/0.76766. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62464/0.76406. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61270/0.76322. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60373/0.76515. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60522/0.76856. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59988/0.77704. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.59331/0.77984. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.60640/0.79005. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59987/0.79813. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.57332/0.77124. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58323/0.76583. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.57845/0.76330. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58583/0.77472. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.56525/0.78242. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56347/0.77703. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.56325/0.78115. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55265/0.79450. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.56474/0.80012. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55049/0.83027. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55240/0.80858. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.54943/0.81868. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54011/0.78863. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.52721/0.85995. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.51789/0.85332. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.51990/0.79014. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.53143/0.82952. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52729/0.87962. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.50559/0.85705. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.49746/0.81689. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.49320/0.84990. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.52728/0.79311. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.49464/0.81401. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.47825/0.89340. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.47635/0.90452. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.48760/0.86056. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48136/0.83241. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.46748/0.90924. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.45592/1.00327. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.45877/0.98490. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.45743/0.94249. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.44649/0.95978. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.43415/1.03651. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.43269/1.01631. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.42667/1.06864. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.41249/1.07504. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.41602/1.07510. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.39479/1.01060. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.41352/1.04390. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.38849/1.06234. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.39090/1.12098. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.37535/1.06534. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.37787/1.06768. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.38475/1.13635. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.38009/1.22118. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.37088/1.10238. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.35832/1.16683. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.35655/1.12030. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.34745/1.19129. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.33176/1.19610. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.31803/1.22267. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.33552/1.28797. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.30938/1.29637. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.32999/1.27074. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.33506/1.10751. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.32562/1.21474. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.33112/1.18830. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.35285/1.10136. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.31679/1.15427. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.27497/1.29986. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.28986/1.29662. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.29190/1.28612. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.28812/1.24574. Took 0.44 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.74025/0.74670. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.74114/0.73433. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.74037/0.73281. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.72384/0.73025. Took 0.49 sec\n",
      "Epoch 4, Loss(train/val) 0.71813/0.73531. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.71717/0.72722. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.71699/0.74602. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.71632/0.73535. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.71513/0.73475. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.70569/0.75022. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.70550/0.74720. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.69874/0.75470. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.69920/0.74244. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.70317/0.74315. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.69562/0.74583. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.70430/0.75317. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.69583/0.74372. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.69191/0.74989. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.69770/0.72622. Took 0.48 sec\n",
      "Epoch 19, Loss(train/val) 0.69162/0.74068. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.68577/0.73031. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.68542/0.71596. Took 0.47 sec\n",
      "Epoch 22, Loss(train/val) 0.69798/0.70598. Took 0.48 sec\n",
      "Epoch 23, Loss(train/val) 0.69399/0.71466. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.68465/0.71459. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.68885/0.71948. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.69078/0.73438. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.68836/0.73047. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.68395/0.72131. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.68709/0.73340. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.67690/0.74022. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.66312/0.74354. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.68017/0.71035. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.67987/0.73658. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.67327/0.71927. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.66941/0.75697. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.66119/0.76719. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.66395/0.77998. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.66775/0.78841. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.65416/0.80079. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.64895/0.82973. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.63850/0.84693. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.63644/0.86056. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.63573/0.87889. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.65153/0.84183. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.62697/0.86430. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.62226/0.91694. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.61457/0.92051. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.61410/0.93868. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.59977/0.95907. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.59520/0.95530. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.59976/0.96990. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.60391/0.93448. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.61121/0.97366. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.58474/0.99686. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.58361/1.01861. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.59258/1.05934. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.57786/1.04081. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.56868/1.01983. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54482/1.14291. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55223/1.15425. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.54921/1.16062. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53870/1.22280. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.55002/1.20519. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.52798/1.21546. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.53530/1.25029. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.53144/1.24094. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.53030/1.27756. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52158/1.34454. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.52964/1.26867. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.52390/1.30923. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.52076/1.24770. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.51898/1.32740. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.51890/1.22007. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.50349/1.34966. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.48764/1.25875. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.48893/1.46664. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.50104/1.25882. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.48069/1.43503. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.46937/1.53858. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.47327/1.37579. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.47174/1.44389. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.48640/1.57377. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.46831/1.47785. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.45807/1.57568. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.45349/1.58715. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.44362/1.47085. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43861/1.56074. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.46297/1.50436. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.45339/1.51155. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.45996/1.45563. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.42004/1.56197. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.42321/1.53038. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.41689/1.52102. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.42211/1.58860. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.39846/1.72934. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.40890/1.55186. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.39193/1.64113. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.40741/1.61514. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.39084/1.61927. Took 0.44 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.73306/0.78632. Took 0.60 sec\n",
      "Epoch 1, Loss(train/val) 0.71164/0.76396. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70392/0.73633. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.70149/0.75193. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.69941/0.76557. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69498/0.73925. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68379/0.75744. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.69234/0.74672. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69184/0.72787. Took 0.48 sec\n",
      "Epoch 9, Loss(train/val) 0.69184/0.76067. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68843/0.74731. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67806/0.75265. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.68125/0.74138. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67145/0.71867. Took 0.48 sec\n",
      "Epoch 14, Loss(train/val) 0.67826/0.72116. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67192/0.72649. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.67417/0.74556. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67003/0.72017. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67157/0.74176. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.66996/0.73500. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66212/0.73897. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65445/0.75178. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65415/0.74649. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.65118/0.75447. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64743/0.73898. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.66236/0.74794. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.64060/0.73291. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65481/0.73882. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64245/0.73677. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.63596/0.75839. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.65576/0.74520. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.64337/0.73675. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64347/0.75349. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64135/0.75833. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.64479/0.75882. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.64009/0.76883. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.62689/0.75799. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.63063/0.73754. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.62189/0.76237. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61426/0.75799. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61638/0.77620. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.61583/0.79061. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60499/0.78207. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.60479/0.79378. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60505/0.78650. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.60135/0.78881. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59479/0.78257. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.59253/0.79959. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.58933/0.81026. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.58434/0.81001. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.57216/0.80485. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.57688/0.79353. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.56267/0.84548. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.57550/0.82722. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.57277/0.84562. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.55649/0.83108. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56122/0.81280. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.56040/0.82061. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.54959/0.83720. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54224/0.84137. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.53510/0.83611. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.54535/0.84174. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.53004/0.88088. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.52987/0.89944. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.53013/0.92202. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.51678/0.90177. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.53066/0.94541. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.49953/0.91861. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49433/0.92872. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.51397/0.93582. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.50798/0.96213. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.50896/0.97422. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.50206/0.96600. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.49307/1.01235. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.47291/1.05249. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.47804/1.02197. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.47269/1.04221. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.48577/1.08214. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.47940/1.07512. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46784/1.04778. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.44446/1.10335. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.47129/1.07526. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.45595/1.04383. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.44563/1.09713. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.44437/1.03704. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.43802/1.10997. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43750/1.15460. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.43268/1.17493. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.43061/1.16421. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.43709/1.21509. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.42675/1.13717. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.42271/1.20072. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.40789/1.24615. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.42211/1.15894. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40230/1.19991. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.40329/1.20637. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.41000/1.22935. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.40056/1.14606. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.38876/1.27022. Took 0.47 sec\n",
      "Epoch 99, Loss(train/val) 0.37594/1.29108. Took 0.44 sec\n",
      "ACC: 0.3958333333333333\n",
      "Epoch 0, Loss(train/val) 0.70747/0.74333. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69906/0.75029. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69776/0.76499. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69036/0.76052. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69374/0.76779. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68613/0.77835. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68370/0.79644. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68387/0.79385. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.68182/0.79878. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67138/0.80627. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67227/0.80529. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66518/0.81660. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66608/0.82143. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.65474/0.83820. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65417/0.81524. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65613/0.84328. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.64670/0.85306. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64084/0.84832. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.64522/0.81632. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.62940/0.85187. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.62741/0.87325. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63173/0.85202. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.62577/0.85108. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.62709/0.84996. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.62613/0.87151. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.60906/0.88292. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.61694/0.86497. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.60180/0.91913. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.60337/0.90299. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.58347/0.97580. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.59126/0.89049. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.58776/0.87489. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.57426/0.91334. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.57475/0.94311. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.58591/0.91158. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.56752/0.95261. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.55614/0.97610. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.56429/0.99970. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.55425/0.94663. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.54536/0.93537. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.54308/0.98031. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.55247/0.96746. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.53804/0.99874. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.53259/1.01204. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.53943/0.95255. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.53041/0.96700. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.52044/0.98750. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.52275/1.00087. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.51675/0.96479. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.51163/1.03277. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.50565/0.98347. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.49255/0.97087. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.49546/1.02032. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.50086/1.00981. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.48938/0.99732. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.52095/1.11587. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.49201/1.05349. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.48404/1.01518. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.45507/1.07662. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.46172/1.10430. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.46424/1.09769. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.45556/1.10242. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.47396/1.03090. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.43957/1.16503. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.43500/1.19212. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.42292/1.20674. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.46108/1.14712. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.46942/1.14105. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.43962/1.14456. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.42424/1.23526. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.43123/1.21159. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.41837/1.27403. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.40755/1.24499. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.42958/1.23852. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.38968/1.27042. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.39141/1.25979. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.39698/1.26675. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.38537/1.28249. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.38528/1.22181. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.37033/1.22616. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.38456/1.30139. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.35169/1.42447. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.35072/1.36306. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.33465/1.46310. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.34158/1.42056. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.34419/1.30834. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.32925/1.49760. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.32691/1.39954. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.34928/1.46212. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.32453/1.54424. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.34211/1.38579. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.33233/1.34002. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.30418/1.58866. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.30910/1.54987. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.33886/1.46527. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.29486/1.49595. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.30583/1.49437. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.28783/1.56629. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.32968/1.49639. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.29912/1.53025. Took 0.44 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.70183/0.69736. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69698/0.70204. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69233/0.70346. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.68713/0.70473. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68937/0.70517. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68744/0.70947. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68325/0.71358. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68225/0.72251. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67765/0.71696. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68057/0.71907. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68210/0.72201. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.68009/0.71882. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67506/0.72077. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67392/0.73009. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67571/0.72129. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67072/0.72393. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.66393/0.73235. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66558/0.73919. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66533/0.73149. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65634/0.74070. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65766/0.74252. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65629/0.75016. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65244/0.75923. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64762/0.76716. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64312/0.76436. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65098/0.76637. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.63917/0.76509. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63040/0.77840. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62388/0.79573. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62814/0.80868. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63506/0.81206. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.61824/0.79647. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62384/0.79670. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61655/0.80489. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.60370/0.82498. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60249/0.85505. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.60411/0.82302. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59856/0.83268. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58857/0.85573. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.58189/0.85093. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57846/0.85814. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.57475/0.84053. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57348/0.85115. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56188/0.84749. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.57804/0.86715. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55601/0.83850. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56482/0.86121. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.55227/0.87307. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55438/0.88704. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.54209/0.91136. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53829/0.90934. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.55015/0.87944. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.53076/0.92232. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.53197/0.94069. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.51781/0.93267. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51985/0.97095. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.52146/0.94105. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.50540/0.98296. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.49461/0.96171. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.50530/0.97662. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.48700/0.95961. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.47672/1.02642. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.47560/0.99637. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.47837/1.00070. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.45268/1.02277. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.44991/1.03121. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.46949/1.01117. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.47441/1.00352. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.45096/1.02992. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.44423/1.07535. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.44310/1.05528. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.41848/1.09509. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.42057/1.14299. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.41433/1.16752. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.41377/1.18329. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.40950/1.19703. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.40191/1.15934. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.39599/1.24172. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43345/1.12050. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.40372/1.23158. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.37997/1.23951. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.38583/1.19460. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.38914/1.17016. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.39028/1.21557. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.37849/1.27269. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.36673/1.28733. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.35542/1.27407. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.36933/1.26505. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.35126/1.31072. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.34245/1.26559. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.34660/1.27898. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.36096/1.25667. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.36446/1.29322. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.34037/1.26744. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.33898/1.28867. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.32234/1.29298. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.31926/1.32752. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.33018/1.39817. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.31219/1.37017. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.30239/1.39603. Took 0.45 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.70720/0.70630. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69395/0.70918. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69348/0.72162. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68486/0.73672. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68129/0.74443. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68212/0.74077. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67826/0.75298. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67986/0.75046. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67301/0.76772. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67325/0.77374. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.66305/0.79045. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66601/0.79148. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65677/0.81518. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65086/0.82131. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.65361/0.83348. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.64564/0.84670. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.63655/0.83596. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64066/0.84188. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.62954/0.88032. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.62354/0.87991. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.62007/0.88220. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.61826/0.86528. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.61422/0.90284. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.60638/0.87681. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.60371/0.87256. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.59801/0.93226. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.60902/0.91237. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.58086/0.94695. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.58995/1.00002. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.57800/0.96509. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.58941/0.98958. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.58011/0.99635. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.57388/1.01072. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.56884/1.02367. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.56274/1.11486. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.56953/0.97357. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.55139/1.06515. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.55434/1.02949. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.54677/1.06345. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.54254/1.09546. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.55603/1.07579. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.55028/0.98621. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.54412/1.03120. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.54135/1.02379. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.54226/1.00642. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.53347/0.98724. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.54094/1.00786. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.52459/0.96947. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.52064/0.93514. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.52406/0.93794. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.51509/0.97120. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.50657/0.96724. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.51067/0.97151. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.50234/1.00228. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.47165/1.04895. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.49322/1.04754. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.46872/1.03939. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.48987/1.02854. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.46912/1.10914. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.48381/1.04763. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.48444/1.06966. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.48247/1.06348. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.45950/1.12470. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.45430/1.11268. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.44364/1.10810. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.46385/1.11560. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.44302/1.12543. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.43283/1.17580. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.45587/1.13599. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.43752/1.17591. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.42817/1.19851. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.41762/1.16661. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.41437/1.18758. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.42534/1.22402. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.40257/1.23189. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.40011/1.24818. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.43758/1.23873. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.41744/1.21823. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.41093/1.24087. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.41089/1.23103. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.39374/1.26663. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.38091/1.30753. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.40625/1.26886. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.39495/1.29814. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.37335/1.28567. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.37404/1.34790. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.36380/1.36733. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.36350/1.42465. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.35391/1.37690. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.34939/1.44990. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.34969/1.46569. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.34005/1.43939. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.35302/1.44540. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.33534/1.42805. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.32679/1.49292. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.33039/1.48619. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.35201/1.48931. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.32539/1.44012. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.32754/1.44775. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.28937/1.52868. Took 0.43 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.72561/0.72428. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.72469/0.70185. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70506/0.72815. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69733/0.72421. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69712/0.71591. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.70038/0.71047. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69347/0.72201. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68978/0.71446. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68948/0.72586. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68441/0.72461. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68292/0.71677. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67927/0.73968. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67569/0.73911. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68602/0.72866. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.68328/0.74202. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67612/0.75938. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66493/0.74901. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67100/0.77100. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66678/0.76234. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65877/0.75084. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66282/0.76215. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65752/0.76142. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66197/0.75509. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.66817/0.76617. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.66216/0.76316. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65600/0.76744. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.65170/0.77888. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63971/0.78801. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65293/0.78427. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.64340/0.77711. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64664/0.76526. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.64269/0.77506. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63667/0.78040. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64130/0.79015. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.63390/0.77321. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62641/0.80022. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62766/0.78326. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61520/0.82349. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.62551/0.78501. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61758/0.79012. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60710/0.81241. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.59764/0.80993. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60020/0.82299. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.60150/0.83713. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59810/0.82879. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.59629/0.82581. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59145/0.81232. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.58123/0.86907. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58833/0.86117. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.58059/0.87288. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57737/0.90630. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.57555/0.97288. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.57656/0.91609. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.58498/0.96553. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.58441/0.91325. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.56874/0.99507. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56345/0.93997. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.56253/1.00664. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.56115/0.96582. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.56362/0.94534. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.56102/0.99837. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.54380/0.98394. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.57448/0.97073. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.56461/1.01719. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.55059/1.02692. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.55093/1.05114. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.55670/1.06289. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.55721/1.02004. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52751/1.04415. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.55859/1.01508. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.55738/1.06297. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.54255/1.02654. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.52967/1.06145. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.52242/1.09924. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.53458/1.10255. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.53642/1.02068. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.52272/1.04581. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.50884/1.07543. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.52565/1.09291. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.49972/1.11599. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.50524/1.09750. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.54533/0.98293. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.54785/0.99631. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.50933/1.03628. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.50397/1.06002. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.50250/1.09827. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.48002/1.11423. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.49087/1.12459. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.46480/1.14503. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.47267/1.13918. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.48188/1.11978. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.46390/1.17929. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.44969/1.14884. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.48210/1.20160. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.46486/1.15923. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.45805/1.22007. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.47323/1.10688. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.46110/1.16916. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.44609/1.14331. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.44236/1.18112. Took 0.45 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.70541/0.75420. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70153/0.75197. Took 0.51 sec\n",
      "Epoch 2, Loss(train/val) 0.69523/0.75890. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69903/0.76047. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69713/0.76088. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69374/0.75667. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.69451/0.74524. Took 0.47 sec\n",
      "Epoch 7, Loss(train/val) 0.68732/0.75591. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68497/0.76196. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68677/0.74160. Took 0.47 sec\n",
      "Epoch 10, Loss(train/val) 0.67834/0.75909. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.68208/0.74918. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68069/0.74562. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67256/0.75444. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66388/0.76972. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67394/0.79736. Took 0.47 sec\n",
      "Epoch 16, Loss(train/val) 0.66459/0.78165. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66517/0.79104. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66045/0.78501. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65592/0.79025. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65183/0.78976. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65283/0.79244. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64503/0.77983. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64642/0.79506. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63784/0.78485. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63479/0.79375. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.62643/0.81448. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62941/0.82083. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62939/0.83563. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.61654/0.82871. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.61953/0.83274. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.61088/0.83703. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61239/0.82162. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60329/0.83466. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.60079/0.87082. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58918/0.85921. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.58715/0.83605. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.58485/0.87239. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58797/0.82252. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.57897/0.89341. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.57451/0.85053. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.56081/0.88168. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55841/0.90759. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.56068/0.90923. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55019/0.92679. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.55216/0.92860. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.54885/0.94809. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.53839/1.00128. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.54342/0.96303. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.51721/0.98418. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.52860/0.97501. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.50202/1.02788. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.50619/1.07604. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.50610/1.06209. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.50474/1.06311. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.48346/1.06534. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.48555/1.10275. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.45803/1.14261. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.47167/1.09603. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.46313/1.15970. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.46436/1.11340. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.45757/1.16712. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.44843/1.16299. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.43553/1.14197. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.42945/1.20771. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.43823/1.24516. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.42323/1.16752. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.39877/1.28606. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.44707/1.30123. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.41913/1.28558. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.42791/1.22663. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.40040/1.35301. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.36573/1.38067. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.36684/1.39164. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.37017/1.42891. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.37407/1.52117. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.38256/1.41181. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.36742/1.45823. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.36668/1.42003. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.35397/1.45156. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.33144/1.47514. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.33814/1.57583. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.32145/1.51423. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.33900/1.58242. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.30489/1.61506. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.31714/1.61108. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.30591/1.64606. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.27578/1.63957. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.28290/1.72377. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.27635/1.71801. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.26887/1.73474. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.26594/1.76745. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.25803/1.82299. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.26854/1.85319. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.23459/1.85230. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.25974/1.80307. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.30296/1.78273. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.29485/1.90503. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.24401/1.97569. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.23265/1.88318. Took 0.44 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.70083/0.71323. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69243/0.72169. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68552/0.72689. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.67758/0.73710. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.67957/0.72189. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.67322/0.73311. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67196/0.73119. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67700/0.73464. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.66805/0.74070. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66319/0.73896. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66681/0.74673. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66693/0.73866. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.65923/0.74826. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66420/0.74863. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65956/0.74757. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65394/0.75437. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66077/0.72510. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65987/0.72756. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65421/0.73599. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65287/0.73700. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65080/0.72921. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65266/0.73438. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65545/0.73811. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.63799/0.74782. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64710/0.74628. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63993/0.75351. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.62656/0.75616. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62535/0.75160. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62905/0.76594. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62338/0.76112. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63042/0.76949. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.61191/0.76016. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61554/0.75368. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60579/0.75713. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.59961/0.75754. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59209/0.74433. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60155/0.77639. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.58365/0.78733. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.58331/0.78391. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58070/0.79138. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60248/0.85657. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.57649/0.79670. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56629/0.79893. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.57005/0.81454. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56048/0.83509. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55573/0.82691. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.53898/0.85013. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55306/0.83638. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.55181/0.83702. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55782/0.78020. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.53170/0.82379. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.53665/0.85818. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.53009/0.82491. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.52570/0.86624. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51713/0.88692. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.53102/0.88100. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.51187/0.89681. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.50580/0.88638. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.49133/0.90389. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.49004/0.89478. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.48208/0.94830. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50389/0.96777. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49703/0.96895. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48641/0.90430. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.49032/0.94566. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.48208/0.96819. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48452/0.96044. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.47341/0.97693. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.45675/0.98910. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.46498/0.95680. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.44282/1.00691. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.44906/1.05744. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.47736/1.01039. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.45426/1.08785. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.42303/1.09839. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44087/1.02887. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.44031/1.02189. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.41848/1.05958. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.41788/1.05828. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.41742/1.10451. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.40197/1.11910. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.39723/1.19040. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.41879/1.11193. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.41790/1.19094. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.41328/1.14865. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.40083/1.15677. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.38471/1.21117. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.38690/1.16959. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.37159/1.18971. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.36267/1.19082. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.34289/1.18619. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.35871/1.27375. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.37278/1.25946. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.34881/1.18544. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.35315/1.24430. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.35124/1.25972. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.35043/1.21994. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.34914/1.27394. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.33967/1.31210. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.33835/1.31050. Took 0.44 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.71556/0.68322. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70887/0.68099. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70487/0.67134. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69833/0.67804. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69291/0.68359. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68664/0.68836. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.68912/0.68682. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68264/0.70980. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67132/0.72231. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67697/0.69954. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67594/0.69039. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68524/0.70280. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67302/0.69873. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66309/0.70307. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66275/0.72253. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66583/0.71240. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65105/0.73161. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66256/0.71956. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66057/0.71697. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65528/0.72519. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66072/0.73525. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65384/0.73848. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65742/0.74317. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64951/0.74248. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64930/0.75263. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64136/0.77489. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.63943/0.74230. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63890/0.76475. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63906/0.75849. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.63816/0.75386. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.63559/0.76961. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62008/0.77056. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62578/0.78291. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.62435/0.80150. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61712/0.81394. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61036/0.85242. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60404/0.86337. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.60720/0.80670. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.59998/0.83679. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58920/0.82286. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58795/0.82377. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.59256/0.86274. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58669/0.92533. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.59703/0.86305. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58941/0.80615. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.58159/0.86810. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58638/0.79731. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.57801/0.84005. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.58896/0.85753. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56697/0.88451. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.56917/0.90964. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56776/0.84338. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54768/0.85679. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.58183/0.91057. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.55943/0.90207. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.54673/0.91638. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.55056/0.88906. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54438/0.86647. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.53663/0.86226. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.54493/0.87308. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.53717/0.89829. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.52544/0.95587. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.52568/0.92506. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.53106/0.85945. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.52974/0.92163. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.53137/0.93001. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.52978/0.91727. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.51445/0.94245. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.51937/0.86974. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.52346/0.90914. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.52114/0.89962. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.50216/0.93969. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.50414/0.88139. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.53097/0.89779. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.52334/0.91665. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.52500/0.90923. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.51461/0.89394. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.49441/0.96369. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.50441/0.96433. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.49473/0.99551. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.49345/1.00361. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.47961/1.04199. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.48488/0.99229. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.46444/0.97738. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.48238/1.02219. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.47016/0.99919. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.46757/1.04513. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.46654/1.02960. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.44911/1.02763. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.44271/1.11514. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.45156/1.09810. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.45581/1.08881. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.43198/1.07707. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.44270/1.07011. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.42374/1.09517. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.41585/1.12283. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.43502/1.06332. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.43683/1.00769. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.41723/1.09907. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.41868/1.11555. Took 0.45 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69320/0.71648. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.68095/0.73148. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.67716/0.73264. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.67164/0.74255. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.66825/0.73477. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.67321/0.74002. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.66899/0.75047. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.66194/0.74577. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.66652/0.74734. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66699/0.74591. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.66736/0.75587. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.65860/0.77341. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65863/0.78268. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.65993/0.77659. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65541/0.77532. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.64867/0.78698. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.64199/0.81072. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.64695/0.81397. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.64200/0.83336. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64030/0.81639. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.63215/0.83093. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.62360/0.84845. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63480/0.85331. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62326/0.84897. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.61170/0.87320. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.61832/0.88524. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.60623/0.90247. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.60201/0.90530. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.59778/0.93814. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.60232/0.92689. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.59046/0.94839. Took 0.47 sec\n",
      "Epoch 31, Loss(train/val) 0.59720/0.96080. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59317/0.98076. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59510/0.97297. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.58535/0.95495. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.57443/0.93453. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.56685/1.00219. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.56791/1.00926. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.55921/1.04475. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.55574/1.02676. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.52729/1.06067. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.54273/1.05694. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.53476/1.05847. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.52366/1.08101. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.53485/1.08495. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.51386/1.13141. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.51479/1.11191. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.50387/1.10458. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.51333/1.12462. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.49420/1.06145. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.48276/1.16884. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.49012/1.02319. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.49763/1.11005. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.49362/1.04604. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.48956/1.07908. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.47561/1.04854. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.45942/1.14200. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.46264/1.16635. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.45489/1.10434. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.42900/1.21352. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.44329/1.20481. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.43549/1.26106. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.42444/1.21228. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.40849/1.24377. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.40464/1.33818. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.41199/1.28601. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.39769/1.33929. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.39751/1.36018. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.39578/1.36771. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.40159/1.39995. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.38628/1.33747. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.38207/1.35998. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.37995/1.38658. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.35363/1.32824. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.36045/1.40540. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.34509/1.39203. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.35872/1.42525. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.34689/1.48004. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.32923/1.56068. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.36735/1.46716. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.36933/1.52816. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.31649/1.53184. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.35460/1.53868. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.34464/1.53923. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.31283/1.58001. Took 0.47 sec\n",
      "Epoch 85, Loss(train/val) 0.29468/1.67581. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.31134/1.68510. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.31935/1.65697. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.30440/1.61267. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.28243/1.84669. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.29625/1.76499. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.27339/1.84419. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.25677/1.87146. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.29171/1.84954. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.29717/1.68479. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.27987/1.91952. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.26901/1.88997. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.26685/1.84563. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.26384/1.85207. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.26075/1.83562. Took 0.44 sec\n",
      "ACC: 0.6145833333333334\n",
      "Epoch 0, Loss(train/val) 0.69308/0.66373. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.69604/0.66315. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69023/0.67808. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68268/0.66370. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.68292/0.66885. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67672/0.66139. Took 0.48 sec\n",
      "Epoch 6, Loss(train/val) 0.67171/0.66523. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67867/0.67334. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67673/0.67645. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67128/0.68647. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67317/0.67146. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66451/0.67648. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66871/0.68600. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.66713/0.67005. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66248/0.67970. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66403/0.68373. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65100/0.67940. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65496/0.69498. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65483/0.69152. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.64548/0.69142. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63693/0.69947. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63910/0.71196. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63429/0.70116. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.63919/0.72124. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63592/0.71341. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.63249/0.71489. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.61938/0.73276. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62261/0.76384. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63327/0.76302. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.63494/0.73541. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.61847/0.78239. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62889/0.75416. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.61028/0.75858. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61229/0.77527. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60548/0.76821. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.60986/0.77502. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59274/0.77493. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58961/0.85635. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.58885/0.80770. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59412/0.81462. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.58276/0.85025. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.58383/0.85112. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57600/0.87455. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.56368/0.91261. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57856/0.86711. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56888/0.91525. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.57116/0.84661. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.54491/0.92437. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.55848/0.94179. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55968/0.82588. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.56398/0.93011. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54411/0.94175. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.53619/0.90315. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.52736/0.99566. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.52492/1.00365. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.52246/0.92026. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52022/1.08642. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51250/1.02856. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.51969/0.99154. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52661/1.03952. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.50565/1.06583. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.50861/1.03977. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48815/1.14051. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.51223/1.01341. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.49703/1.02770. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.47979/1.04387. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.49112/1.05825. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.47141/1.03910. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.45676/1.15199. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45702/1.13230. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.46189/1.03888. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.47576/1.15583. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46143/1.17719. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.45499/1.24386. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.44087/1.16143. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.42145/1.20028. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.41836/1.27237. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.42203/1.24815. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.40889/1.25969. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.42309/1.16546. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.40550/1.18786. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.40633/1.17702. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.42629/1.22163. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.41256/1.33801. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.40385/1.28463. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.39069/1.24310. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.38001/1.35194. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.36217/1.45425. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.36229/1.37912. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.36564/1.23707. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.37258/1.38661. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.38885/1.34468. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.37304/1.34921. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.34647/1.34940. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.34120/1.40572. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.34711/1.38009. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.32397/1.42100. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.31367/1.47776. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.31818/1.42750. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34014/1.41404. Took 0.44 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.71115/0.72949. Took 0.65 sec\n",
      "Epoch 1, Loss(train/val) 0.69886/0.71687. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69089/0.74098. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68684/0.74046. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68205/0.75501. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68508/0.75568. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68223/0.75131. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67955/0.75363. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67803/0.75087. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67462/0.75209. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.66842/0.76019. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67326/0.75908. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66614/0.77035. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.65772/0.77753. Took 0.47 sec\n",
      "Epoch 14, Loss(train/val) 0.66486/0.77368. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.65725/0.78003. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65152/0.78368. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64326/0.79055. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64379/0.80183. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.63760/0.79288. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64046/0.83511. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63327/0.82966. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.63233/0.84435. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.62737/0.84355. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62652/0.85110. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.62133/0.84362. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.61406/0.87890. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.61127/0.88114. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60284/0.90946. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60402/0.91918. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.59569/0.92254. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.59621/0.91781. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.59001/0.95281. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58818/0.95329. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.57666/0.98532. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.57072/1.02307. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.56879/1.02845. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.55563/1.04726. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.55592/1.06614. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.54796/1.04343. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.54823/1.11829. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.53528/1.11435. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.54680/1.10567. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.52462/1.15441. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.53936/1.14831. Took 0.47 sec\n",
      "Epoch 45, Loss(train/val) 0.51572/1.18628. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.50750/1.18299. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.51037/1.17897. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.50732/1.14711. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.51449/1.12284. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.48963/1.12461. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.48054/1.20612. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.49391/1.18173. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.48034/1.25348. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.47499/1.23760. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.47401/1.15596. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.46796/1.23577. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.45338/1.31657. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.45309/1.26894. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.45421/1.29887. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.44572/1.26378. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.44459/1.25741. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.46830/1.28571. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.43956/1.28574. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.42683/1.28964. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.42939/1.38420. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.42583/1.28597. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.40545/1.31256. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.40911/1.29734. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.40544/1.31042. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.42628/1.24648. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.39200/1.41347. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.39357/1.29326. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.37811/1.33956. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.37175/1.28798. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.40447/1.36417. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.39463/1.35743. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.35103/1.34055. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.38344/1.54849. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.40012/1.37623. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.37932/1.43368. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.36440/1.60005. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.34141/1.53853. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.34440/1.60916. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.33466/1.51273. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.31952/1.57588. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.34701/1.45233. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.33056/1.49946. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.30896/1.53762. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.34453/1.67272. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.34412/1.47362. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.33349/1.44038. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.31627/1.50796. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.33792/1.64833. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.33252/1.55402. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.28815/1.64562. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.27331/1.70291. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.27303/1.71920. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.26994/1.73658. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.29634/1.73999. Took 0.43 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.71803/0.70943. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.70315/0.71321. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.70249/0.73954. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68917/0.73533. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69472/0.75387. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67847/0.76219. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.67770/0.74820. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68433/0.74289. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68346/0.76611. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67206/0.75234. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67429/0.76080. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66532/0.76957. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66592/0.78936. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65842/0.80802. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67617/0.77939. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65977/0.78452. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65634/0.79591. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65469/0.80235. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65087/0.80749. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.64024/0.80103. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63852/0.77468. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63571/0.77065. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63117/0.78292. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.62816/0.80123. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.62335/0.79918. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.62157/0.78641. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.60172/0.82332. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.59820/0.84036. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.60869/0.82060. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.59417/0.86404. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.58375/0.86480. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.57586/0.88875. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.57914/0.94408. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.57491/0.92799. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.59077/0.90801. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.55647/0.95918. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.56880/0.93992. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.57938/0.91468. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.55493/1.00401. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.53538/0.99301. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.54077/1.00273. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.52763/1.03803. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.53480/1.11531. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.53308/1.06545. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.53787/1.08621. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.51203/1.13977. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.50848/1.06841. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.52558/1.10573. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.51657/1.17384. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.49288/1.10747. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.49158/1.12769. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.46798/1.18082. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.48022/1.16405. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.46638/1.17264. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.47677/1.21913. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.46718/1.22535. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.47582/1.14365. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.45052/1.20296. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.43382/1.25297. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.46929/1.06311. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.46941/1.11931. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.43902/1.19905. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.42333/1.23586. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.42407/1.33319. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.40838/1.28472. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.40475/1.45485. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.41596/1.47952. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.39888/1.42501. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.38795/1.46961. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.38833/1.52357. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.37559/1.48654. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.36954/1.68244. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.36663/1.60837. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.37749/1.47708. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.35523/1.61126. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.33499/1.58083. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.34997/1.56034. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.34568/1.66242. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.33281/1.73542. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.31906/1.67283. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.30850/1.68596. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.31400/1.72904. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.31040/1.65771. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.33110/1.52683. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.29357/1.78147. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.30405/1.79540. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.30587/1.82238. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.30391/1.83251. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.29590/1.75907. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.27428/1.81815. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.27514/1.85960. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.27003/1.81180. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.24264/1.91193. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.25448/1.88289. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.22019/2.01072. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.23665/2.03747. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.24401/1.92808. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.23599/1.90741. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.24469/2.01822. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.25112/2.02800. Took 0.44 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.72803/0.68701. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.71237/0.70864. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.70167/0.72316. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69092/0.71825. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68297/0.74144. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69536/0.74653. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67633/0.74579. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67481/0.74088. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67199/0.74532. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67012/0.75413. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67303/0.77321. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66451/0.76834. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66297/0.78034. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.65143/0.74943. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66672/0.77780. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66348/0.80052. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.65932/0.79825. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65480/0.80195. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.66095/0.75079. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65575/0.79400. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64496/0.78030. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.63969/0.79765. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65680/0.78783. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64378/0.81155. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63862/0.85209. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65706/0.82975. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65263/0.81601. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63611/0.83332. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.63409/0.83298. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.63531/0.89137. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64307/0.81957. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.62623/0.83586. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62925/0.85416. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.60549/0.89269. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61785/0.90043. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59967/0.91126. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.60829/0.94370. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60167/0.94406. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60149/0.94012. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.59947/0.89602. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60184/0.88366. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.58392/0.91401. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.58920/0.93183. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58951/0.90966. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.57402/0.92781. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.56936/0.86622. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59001/0.89622. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.58654/0.86692. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.56551/0.91948. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.58268/0.86603. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.56445/0.90463. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.56764/0.91487. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.55095/0.92409. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.54091/0.93010. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55820/0.93873. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.53945/0.93251. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55345/0.91125. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.53430/0.97993. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.55085/0.95264. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.54663/0.93092. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51825/1.00016. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.52531/1.03765. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.57550/0.91296. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.52542/0.93101. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.51345/0.97331. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.53149/0.96622. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.52299/0.97531. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.53149/0.93359. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.51371/0.95897. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.51116/1.02480. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.50690/0.96486. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48593/1.00545. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.48378/1.03175. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.51418/0.99971. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.48540/0.99600. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49768/0.93770. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.48785/0.95274. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46326/1.00966. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.47250/0.99253. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.48598/1.05369. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.46701/1.05564. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.48032/1.01420. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.46897/1.01911. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.46521/1.05989. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.44651/0.99456. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.47947/0.99006. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.46230/0.96493. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.44851/1.01063. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.44632/1.06791. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.43638/1.06044. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.41502/1.02925. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.43212/1.05817. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.42275/1.08258. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.43360/1.07921. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.43533/1.05367. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.42462/1.07116. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.40364/1.09660. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.44035/1.11006. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.38897/1.14076. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.37912/1.17813. Took 0.44 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.70742/0.69435. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.70320/0.68863. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69664/0.68388. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69485/0.68462. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68894/0.68773. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69271/0.69841. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69068/0.71526. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67695/0.71672. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68721/0.71755. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67955/0.70692. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67190/0.72250. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67256/0.73465. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67164/0.74717. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66376/0.72418. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66290/0.73245. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66294/0.72096. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67224/0.74739. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67642/0.72840. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65947/0.74707. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66171/0.75824. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66003/0.76448. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.66080/0.76090. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65292/0.77100. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64844/0.77915. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64616/0.79164. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65280/0.79185. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64168/0.80988. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64584/0.84591. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63022/0.84107. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62778/0.88545. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62867/0.82811. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.63405/0.86289. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61490/0.89754. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.62607/0.89420. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.63285/0.92370. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.60755/0.86895. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59557/0.96097. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.59929/0.90420. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.59296/0.92193. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58902/0.95226. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60499/0.94041. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.61071/0.86943. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59714/0.92133. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.58151/0.92463. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56970/0.99889. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57929/0.99333. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.58291/0.94512. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57621/0.99184. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.55749/1.01387. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.54583/0.96757. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55590/0.95323. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.53635/1.00824. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.53986/0.97905. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.53262/0.96736. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.53353/1.00497. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55334/0.92860. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.53302/0.90604. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.51640/0.99822. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.51434/0.97095. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.49877/1.03280. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.52536/0.95792. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.52416/1.01682. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50871/1.00441. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.49613/1.06604. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.48405/1.13949. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.48991/1.05239. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.49294/1.03370. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.48918/1.06460. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.48253/1.09069. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47830/1.13324. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.46796/1.16872. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.46350/1.14056. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48684/1.12217. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.47358/1.19828. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.47401/1.08795. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.47318/1.17554. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.45119/1.18934. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.42662/1.23898. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.44227/1.27158. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.42129/1.25959. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.42166/1.26593. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.46880/1.20256. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.43204/1.16849. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.44411/1.15662. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.42056/1.30205. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.40238/1.37951. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.42360/1.26227. Took 0.47 sec\n",
      "Epoch 87, Loss(train/val) 0.44111/1.31337. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.41119/1.35967. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.39844/1.37272. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.39474/1.42611. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.40286/1.36829. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.36736/1.42421. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.36614/1.46426. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.38684/1.44189. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.39586/1.38613. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.39119/1.45783. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.37880/1.46190. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.36054/1.48002. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.37177/1.51047. Took 0.43 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69921/0.69758. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68861/0.69561. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68866/0.69972. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.68256/0.70444. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.67779/0.70658. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.67840/0.70710. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.67822/0.69763. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.66869/0.71545. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67036/0.70902. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66099/0.71111. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.65608/0.70968. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.65622/0.73207. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.64851/0.73954. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.64313/0.73386. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.63759/0.74437. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.64815/0.76858. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.63269/0.76369. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.63207/0.77295. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.63258/0.74820. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.62909/0.74666. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.62111/0.77454. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.61268/0.77361. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.61750/0.79236. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.60674/0.76337. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.59796/0.78675. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.59784/0.77599. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.60410/0.80859. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.58863/0.83365. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.58431/0.84984. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.58323/0.87257. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.58323/0.89927. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.56567/0.90194. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.56586/0.92164. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.56248/0.92225. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.55687/0.87691. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.54365/0.87393. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.55237/0.86677. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.53801/0.89461. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.53372/0.91853. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.53249/0.96632. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.52639/0.91951. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.51140/0.94599. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.52247/0.95170. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.50110/0.94920. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.50442/0.89156. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.51757/0.93525. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.49712/0.89561. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.48065/0.98439. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.48979/0.98424. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.47373/1.01397. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.45823/0.99858. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.47107/1.01968. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.46496/1.00651. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.45216/1.01403. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.44643/1.01252. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.44802/1.01412. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.45130/1.03467. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.43259/1.04738. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.41848/1.05787. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.43544/1.06583. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.41343/1.09134. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.40544/1.05101. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.42790/1.10415. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.42585/1.07882. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.42670/1.03122. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.40249/1.09553. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.40986/1.09121. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.39046/1.14735. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.38586/1.21191. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.38682/1.18190. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.38341/1.11285. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.37867/1.11076. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.36418/1.17690. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.40957/1.05248. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.40462/1.06090. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.37613/1.06731. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.36644/1.10352. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.36858/1.11907. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.34763/1.18196. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.34924/1.23646. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.33421/1.18498. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.36869/1.19933. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.35979/1.20010. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.33277/1.22208. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.31793/1.28339. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.32037/1.30153. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.30747/1.31656. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.30065/1.18953. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.31703/1.27735. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.30912/1.39588. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.29421/1.37007. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.31146/1.32518. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.31512/1.35728. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.30862/1.34413. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.30846/1.36255. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.29138/1.49228. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.27906/1.50240. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.33451/1.42340. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.26780/1.39328. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.25314/1.36315. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.72490/0.72732. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.70460/0.71860. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70218/0.72082. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69265/0.70965. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.68983/0.71012. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69234/0.72552. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69160/0.73221. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68787/0.72154. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68834/0.74047. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.69193/0.71913. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68020/0.72336. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67802/0.72150. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66784/0.73857. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67694/0.72398. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67434/0.73804. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66668/0.73768. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66109/0.73675. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66317/0.75736. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65733/0.74473. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65090/0.75299. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65479/0.74891. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65454/0.76206. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65306/0.74109. Took 0.47 sec\n",
      "Epoch 23, Loss(train/val) 0.64480/0.75653. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.64030/0.75647. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63510/0.76419. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63764/0.74697. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62415/0.76759. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62250/0.78969. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62908/0.79758. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61187/0.78517. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.60623/0.82734. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60692/0.83025. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.60556/0.84627. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.59883/0.84441. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.59742/0.82232. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.58527/0.84841. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57553/0.85311. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.58564/0.85834. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57760/0.86436. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.56839/0.87575. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.55199/0.89989. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56168/0.91202. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.56268/0.93472. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.54113/0.93524. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.54165/0.94730. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.53201/0.98733. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.52832/0.96731. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.51215/0.95172. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.51331/0.96661. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.51460/0.97009. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.49853/1.01066. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.49659/1.05552. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.49652/1.03000. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.49690/1.03348. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.49482/1.05614. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.47693/1.06565. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.48834/1.02210. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.45853/1.05127. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.45396/1.07421. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.47597/1.02247. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.45810/1.09057. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.47127/1.07533. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.47693/0.97498. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.45092/1.08194. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.44810/1.08841. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.45114/1.09766. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.45536/1.10562. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.43713/1.15786. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.43548/1.09900. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.42880/1.14449. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.42357/1.11389. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.42054/1.17172. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.42317/1.05065. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.41885/1.26995. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.41660/1.09633. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.40852/1.10340. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.43068/0.99090. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.38097/1.11374. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.38126/1.12480. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.38583/1.14366. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.39975/1.17462. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.36905/1.25834. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.37697/1.22734. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.37810/1.21103. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.36369/1.27965. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.34104/1.32908. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.34422/1.26193. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.34008/1.33769. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.34033/1.31992. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.34648/1.35975. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.32048/1.46630. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.32061/1.33007. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.32175/1.42884. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.30570/1.46666. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.33919/1.56318. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.30808/1.47329. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.29745/1.51588. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.28654/1.51677. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.29928/1.54077. Took 0.45 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70106/0.70600. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69314/0.69292. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.68393/0.70158. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68499/0.70039. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68246/0.71039. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68436/0.71518. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68207/0.72112. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67385/0.72169. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.66668/0.71624. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67744/0.72575. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67320/0.71304. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66600/0.72184. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65880/0.71979. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65878/0.73169. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.65986/0.72800. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66151/0.73615. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65005/0.72465. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64919/0.72776. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64402/0.75276. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.63980/0.75576. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63641/0.74620. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.62553/0.76031. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.60929/0.79179. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.61472/0.79950. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.61151/0.84026. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.60854/0.83543. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.58952/0.84819. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.58628/0.83335. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.57814/0.83682. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.57805/0.83629. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.57635/0.82912. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.55429/0.87433. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.56980/0.88904. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.55902/0.89692. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.54908/0.89081. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.54955/0.89962. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.54828/0.91035. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.53400/0.91329. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.53920/0.87576. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.54065/0.89868. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.55554/0.84328. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.53330/0.89278. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.52664/0.90541. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.52075/0.92946. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.50808/0.97303. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.50535/0.95523. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.51326/0.96529. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.50548/0.94327. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.49100/0.95197. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.48669/0.98582. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.49060/0.96575. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.49697/0.91498. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.47937/0.98155. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.47613/1.03431. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.45556/1.04903. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.45794/1.05966. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.44817/1.06689. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.44965/1.06481. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.43367/1.11799. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.43744/1.12089. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.43086/1.14936. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.41905/1.24745. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.42143/1.17427. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.40769/1.21291. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.42262/1.24025. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.41337/1.23515. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.42383/1.26779. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.42365/1.19333. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.40791/1.23071. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.39606/1.30179. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.37568/1.22632. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.37672/1.42625. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.35362/1.44381. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.38630/1.48215. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.36789/1.37881. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.37023/1.40079. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.38091/1.39274. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.36260/1.34906. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.36461/1.31615. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.37737/1.25744. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.35876/1.39782. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.36342/1.33701. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.33392/1.46955. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.31631/1.52004. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.32408/1.49968. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.31878/1.49441. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.30777/1.64543. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.32615/1.46924. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.32955/1.51896. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.30629/1.57004. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.29037/1.55864. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.31051/1.56775. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.30847/1.55166. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.27656/1.59788. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.28055/1.63395. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.28606/1.76854. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.28058/1.65156. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.26576/1.66926. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.28518/1.58404. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.26009/1.76167. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.72965/0.69704. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.71944/0.69093. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.70731/0.69393. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70977/0.68564. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.70820/0.69473. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.70116/0.69417. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.70133/0.69763. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69761/0.70093. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.69814/0.70274. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68903/0.70652. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.68412/0.70848. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.69002/0.70679. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68214/0.70710. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.68092/0.71328. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67852/0.71260. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.68107/0.71571. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67105/0.72302. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67202/0.71769. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65869/0.71595. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66703/0.72021. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65941/0.71895. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65560/0.74191. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64447/0.73750. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64534/0.72603. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64903/0.69694. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.63286/0.74678. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.63964/0.71293. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63021/0.74772. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.64082/0.71795. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62701/0.74386. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62695/0.74481. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.62738/0.73032. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61206/0.72579. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.60402/0.77555. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61465/0.75023. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.59903/0.76581. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60473/0.77929. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60297/0.76206. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.60562/0.76628. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.58444/0.79848. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.57878/0.81655. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57869/0.87492. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57296/0.81757. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56865/0.86208. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56168/0.88457. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56130/0.89244. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.56184/0.89110. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56185/0.87374. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56948/0.83202. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.53499/0.89946. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.54765/0.86624. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.53609/0.90044. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54859/0.87680. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.52271/0.90899. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52564/0.87454. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.52789/0.83486. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.51177/0.90322. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.51271/0.89487. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.51332/0.92422. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.49649/0.95616. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.50013/0.95923. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.49289/0.94687. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49160/0.93680. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.48612/0.96873. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.48213/0.97367. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.49812/0.95976. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.49120/0.90973. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48118/0.94841. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.47779/0.92134. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46390/0.92312. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.45299/0.97113. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.44181/1.00553. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.47415/0.94550. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.47575/0.97896. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.47142/0.90756. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44875/0.96100. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.45043/0.95014. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.43770/0.94449. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43191/0.94337. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44102/1.00838. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.43105/0.94942. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.42968/1.00796. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.42094/1.00317. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.41725/1.00499. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.40256/1.04092. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.41819/1.01984. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.41023/1.02454. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.40140/1.01609. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.39366/1.02777. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.40398/1.03832. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.39357/1.02472. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.38808/1.06434. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.37095/1.06104. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.39523/1.15767. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.38662/1.07570. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.38716/1.09014. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.36401/1.09785. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.36024/1.08548. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.36640/1.08061. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.38063/1.16442. Took 0.44 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.70531/0.71099. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70083/0.72416. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69217/0.72141. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68970/0.71972. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68978/0.71239. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68929/0.73526. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.67319/0.73583. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67668/0.73528. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68201/0.74489. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.66919/0.74454. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.66353/0.74923. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67206/0.74882. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66748/0.75648. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66624/0.75403. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66330/0.74542. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65560/0.75593. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65288/0.76062. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.65192/0.77613. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.64488/0.74276. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64671/0.76747. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63163/0.78992. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63473/0.78443. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.63345/0.79311. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.63209/0.78868. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63806/0.78027. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62284/0.78305. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.62097/0.77295. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.61712/0.77537. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62067/0.77291. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61553/0.81199. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.60233/0.79309. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.61161/0.78191. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59474/0.77472. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58648/0.77412. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59441/0.78994. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58689/0.81142. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.58762/0.81524. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58488/0.78983. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57284/0.79644. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.58004/0.78853. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57858/0.79634. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.57308/0.83623. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56011/0.82979. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.56166/0.84048. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.57169/0.81538. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55550/0.83446. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56682/0.84381. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.55133/0.84134. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57417/0.82244. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55884/0.87308. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.54920/0.87985. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.54814/0.85415. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.54918/0.86377. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54100/0.87225. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53381/0.90373. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.54253/0.87892. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.52863/0.91997. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52036/0.92423. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.53369/0.92859. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52368/0.89310. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51931/0.90373. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.51008/0.94544. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52025/0.94491. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.50907/0.95451. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.51934/0.93051. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50424/0.91768. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.48290/0.95451. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48136/0.96091. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.47635/0.99774. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.47906/0.94215. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.47859/0.94976. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.48075/0.98977. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.48540/0.95922. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.47023/1.00474. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.47193/1.01272. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.46461/0.99084. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.47091/0.94929. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.44695/1.01196. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44918/1.06168. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.44554/1.06137. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.43728/1.10907. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.44472/1.09044. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.44048/1.09115. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.43475/1.13957. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43235/1.12668. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.42108/1.08514. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43221/1.15126. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.41223/1.15387. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.40549/1.23147. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.42839/1.22000. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.40591/1.13403. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.39872/1.19545. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.39160/1.21140. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.41407/1.18133. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.36892/1.20329. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.37298/1.25259. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.38081/1.30504. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.39276/1.23753. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.40238/1.23417. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.38977/1.19288. Took 0.43 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69875/0.74126. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69634/0.72477. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69230/0.71411. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69486/0.71836. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69401/0.71783. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68503/0.71482. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.68610/0.71610. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68592/0.71808. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68382/0.72027. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67966/0.72603. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.68096/0.73512. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.68026/0.73791. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.67866/0.74384. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67228/0.74526. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66875/0.75358. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67078/0.74734. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66564/0.74637. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66010/0.75684. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65854/0.75856. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66193/0.76321. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.66211/0.76007. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65670/0.77952. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65681/0.77210. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65118/0.77847. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64755/0.78394. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64557/0.81196. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63814/0.79113. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.64004/0.81079. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63467/0.82586. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62689/0.83181. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.62329/0.82627. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.61857/0.81388. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61844/0.81345. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.61829/0.83457. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61141/0.80231. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60895/0.83814. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.60096/0.84341. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60813/0.85479. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59974/0.87063. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58047/0.89487. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.58616/0.89821. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57857/0.94427. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57578/0.94493. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.57912/0.94089. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58047/0.93643. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.56668/0.97608. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.55656/0.99015. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.56249/0.98837. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.55457/1.00678. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54072/1.03521. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53479/1.01787. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53165/1.02734. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.53679/0.99086. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52783/0.97846. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52445/1.00224. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.52947/0.99601. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.51060/1.03056. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.51467/0.99645. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.51337/1.04445. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.51360/0.99475. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50851/1.06828. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50939/1.05432. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48486/1.09228. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.47969/1.08087. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49738/1.09855. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.48342/1.11843. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.47107/1.08028. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.47733/1.07967. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.48890/1.07022. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.50555/1.03653. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.47632/1.05044. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.48426/1.08122. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.44382/1.15774. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.45525/1.09972. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.46600/1.09968. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44866/1.16277. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.44574/1.09570. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.42634/1.12630. Took 0.47 sec\n",
      "Epoch 78, Loss(train/val) 0.44813/1.19892. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.43585/1.19001. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.42075/1.14000. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.41585/1.18759. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.42503/1.18178. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40831/1.25197. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.44300/1.17753. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.42559/1.19644. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.40478/1.27474. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.37932/1.31977. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.38333/1.29856. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.37334/1.31154. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.40493/1.29092. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.37730/1.31883. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.38287/1.27370. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.34974/1.44510. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.38061/1.42219. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.35242/1.32923. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.33231/1.44491. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.33777/1.41362. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.34118/1.44140. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.34536/1.52832. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.73950/0.68981. Took 0.65 sec\n",
      "Epoch 1, Loss(train/val) 0.71441/0.68014. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69553/0.68015. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69880/0.67887. Took 0.49 sec\n",
      "Epoch 4, Loss(train/val) 0.69024/0.67702. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.68500/0.68715. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69338/0.69576. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68709/0.69988. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.66675/0.70872. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68133/0.69535. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67469/0.70268. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66020/0.72471. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66334/0.71908. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66427/0.73372. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65787/0.74912. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67399/0.71702. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67120/0.69200. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66049/0.69255. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65754/0.68926. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65740/0.70046. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64531/0.71845. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64154/0.70500. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64636/0.72012. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63817/0.69214. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65374/0.70143. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64183/0.70971. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65569/0.72785. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63930/0.73494. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62959/0.77232. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63412/0.75614. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63389/0.74747. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64201/0.74104. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64036/0.73274. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.63250/0.71448. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62952/0.73161. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62529/0.72982. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.62010/0.71066. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62325/0.72607. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.62071/0.72121. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.61484/0.69405. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61056/0.70534. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.60329/0.73640. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60809/0.73111. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.59608/0.75233. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58963/0.73843. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.60588/0.72676. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.58887/0.73766. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.58848/0.75339. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58307/0.75690. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.57967/0.75059. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.57345/0.77431. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.58190/0.76837. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.58020/0.77784. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.56178/0.78752. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56025/0.80682. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.56231/0.82325. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56154/0.79535. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.56170/0.82800. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.55596/0.84085. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.55136/0.83085. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.55129/0.86237. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.54258/0.87261. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53708/0.90423. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.53736/0.91597. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.51898/0.95983. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.53420/0.98103. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51893/0.99248. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.51626/1.00770. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.51812/1.00543. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.50787/1.05356. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.50235/1.06248. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.49929/1.05699. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.49647/1.06778. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.51505/0.99385. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.49195/1.08773. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48534/1.13590. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.48482/1.05388. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.47299/1.15137. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.45763/1.14181. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44552/1.20584. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.46971/1.15751. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.48171/1.15254. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.43968/1.25332. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.44055/1.17654. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.45721/1.16094. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.41967/1.27899. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.41567/1.27614. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43770/1.26352. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.42035/1.24041. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.43350/1.28061. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.43220/1.27884. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.43630/1.24682. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.43167/1.10400. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.44774/1.10036. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.42062/1.15770. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.41213/1.22270. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.40389/1.28209. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.39844/1.33159. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.37136/1.23180. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.37147/1.27242. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.70654/0.69990. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.69356/0.69449. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69123/0.70643. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68838/0.70553. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.68952/0.69356. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.68153/0.69886. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.67769/0.69856. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67747/0.69021. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.66929/0.69135. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67243/0.70174. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67489/0.69947. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66693/0.70194. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65930/0.71086. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66535/0.71309. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65882/0.72389. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65928/0.72694. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65168/0.73329. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65227/0.74514. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64461/0.77128. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.63603/0.78293. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64083/0.79011. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63721/0.81220. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63982/0.80903. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64126/0.80708. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63299/0.78010. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63056/0.80744. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62029/0.84783. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.61965/0.83631. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62049/0.85036. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61517/0.85492. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61036/0.84849. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.60106/0.87144. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.59534/0.88029. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.58904/0.89412. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60023/0.91560. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58636/0.91683. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58092/0.93768. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57977/0.92861. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.56977/0.94296. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.57404/0.96481. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.57483/0.93338. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.55839/0.95040. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55094/0.93137. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.54041/0.92913. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.54903/0.94335. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.53626/0.97342. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.52460/1.00197. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.52477/0.98885. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.54322/0.98486. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.50689/1.01353. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.49491/1.05125. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.48967/1.03917. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.50071/1.08349. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.51121/1.04617. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.48853/1.10844. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.47799/1.17842. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.45685/1.17951. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.47807/1.19889. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.47751/1.24233. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.45513/1.23266. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.46436/1.32477. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.46759/1.33134. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.45155/1.24438. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.44934/1.16878. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.46502/1.21279. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.44620/1.30564. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.41957/1.34232. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.41521/1.25950. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.40168/1.38038. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.41048/1.37517. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.40582/1.24224. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.40393/1.32853. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.40405/1.45023. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.38284/1.33530. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.36565/1.40936. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.41277/1.42169. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.36921/1.48508. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.36798/1.50439. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.35141/1.54590. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.36710/1.55564. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.36228/1.46404. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.32979/1.45254. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.32914/1.50465. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.33638/1.53694. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.35519/1.62785. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.29785/1.72844. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.33584/1.45286. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.33716/1.57787. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.31621/1.60584. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.27571/1.73861. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.32675/1.75371. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.36375/1.62585. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.27782/1.63707. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.28259/1.74951. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.27815/1.69949. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.26136/1.85887. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.26542/1.91428. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.29178/1.86788. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.27079/1.77947. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.24783/1.73086. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69796/0.69734. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69411/0.68509. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69611/0.69265. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69151/0.69423. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68615/0.69179. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68934/0.69206. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68435/0.69638. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.68039/0.69011. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67759/0.68140. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.67633/0.69043. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67801/0.68826. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66570/0.68802. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.67212/0.68098. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.66482/0.66120. Took 0.47 sec\n",
      "Epoch 14, Loss(train/val) 0.66314/0.66878. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.66309/0.65491. Took 0.47 sec\n",
      "Epoch 16, Loss(train/val) 0.65240/0.65713. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65871/0.66587. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65746/0.66987. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64359/0.66431. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64486/0.67663. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63247/0.67472. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.63625/0.68297. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.63109/0.69699. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.62336/0.72014. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62146/0.71247. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.61598/0.73050. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.61588/0.72768. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60544/0.75037. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.60508/0.73116. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60070/0.72074. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.59338/0.73997. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59335/0.74068. Took 0.47 sec\n",
      "Epoch 33, Loss(train/val) 0.59612/0.73558. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.59214/0.74352. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.58419/0.75131. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58794/0.75913. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57586/0.75737. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.56484/0.76606. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.57216/0.76386. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56272/0.78406. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.55881/0.79967. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.54507/0.83023. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.55889/0.79737. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.54714/0.83210. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.54321/0.84767. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.54080/0.79593. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.53905/0.80195. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.54121/0.83452. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.53472/0.80905. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.51927/0.85626. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53330/0.82887. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.50024/0.83680. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.51285/0.85239. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.50689/0.83803. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51878/0.79276. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.49541/0.85545. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.48939/0.88208. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50036/0.86408. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.49500/0.84349. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.48123/0.85810. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.48715/0.84284. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.48046/0.84887. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.47178/0.83260. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.45910/0.83759. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.46469/0.85403. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.45813/0.89216. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.45306/0.93671. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.44844/0.93294. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.44723/0.95283. Took 0.48 sec\n",
      "Epoch 70, Loss(train/val) 0.43935/0.96942. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.42938/0.92467. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.42927/1.01896. Took 0.48 sec\n",
      "Epoch 73, Loss(train/val) 0.41723/0.97368. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.40727/0.96609. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.40520/0.98441. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.40242/1.02261. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.39857/1.14427. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.39660/1.08788. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.37724/1.12858. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.36886/1.12832. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.36147/1.16941. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.35241/1.18361. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.35310/1.18743. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.35311/1.18373. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.32942/1.20351. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.33658/1.22926. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.34870/1.25598. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.36173/1.21925. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.35610/1.20817. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.35736/1.21512. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.32831/1.25348. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.38198/1.25582. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.32717/1.20300. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.34104/1.18328. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.31117/1.20353. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.29359/1.27131. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.32294/1.21204. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.30049/1.32979. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.31314/1.30166. Took 0.43 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.70181/0.70129. Took 0.51 sec\n",
      "Epoch 1, Loss(train/val) 0.69775/0.69764. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69427/0.69437. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69223/0.70027. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68894/0.70037. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68734/0.70567. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68696/0.70777. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68392/0.71190. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68122/0.70960. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67679/0.71621. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67766/0.72532. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67252/0.72093. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66940/0.72877. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66463/0.72839. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66339/0.71761. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.65594/0.71566. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.64956/0.73460. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64775/0.74622. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.64222/0.73034. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.63954/0.73078. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.62963/0.75124. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.61995/0.77632. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.61475/0.78801. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.61098/0.81361. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.60975/0.83281. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.59702/0.84968. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.59916/0.80740. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.59497/0.81841. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.58872/0.86192. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.58248/0.80394. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.58126/0.82042. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.57255/0.79441. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.57598/0.77003. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.56507/0.72149. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.56387/0.76602. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.56336/0.76203. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.56487/0.72479. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.54373/0.74588. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.54117/0.84655. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.53199/0.80394. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.53634/0.76902. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.53327/0.76168. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.54082/0.75890. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.51335/0.75190. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.50036/0.75346. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.49708/0.81969. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.50265/0.81505. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.48478/0.82837. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.46678/0.85924. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.47218/0.85023. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.47040/0.85857. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.45572/0.88908. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.46354/0.86202. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.44837/0.83580. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.44569/0.84901. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.41780/0.87911. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.42002/0.83851. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.43541/0.85772. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.41404/0.93528. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.39871/0.89138. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.39278/0.90187. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.37783/1.02580. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.37004/0.96266. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.40221/1.06050. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.38859/1.08874. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.38359/1.07889. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.39305/1.05762. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.36871/1.01385. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.35040/1.08421. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.35270/1.06957. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.33097/1.09676. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.32385/1.04740. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.31409/1.19552. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.33542/1.07092. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.31446/1.15553. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.33792/1.23103. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.31100/1.18600. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.29236/1.13779. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.27713/1.15167. Took 0.47 sec\n",
      "Epoch 79, Loss(train/val) 0.28772/1.14144. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.26748/1.20025. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.25542/1.31637. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.26002/1.32822. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.26837/1.34094. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.28147/1.33606. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.27321/1.22864. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.24965/1.33771. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.24253/1.42619. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.24012/1.36963. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.24806/1.42568. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.22170/1.37846. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.21489/1.39727. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.21116/1.47846. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.23279/1.39418. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.23291/1.40532. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.24992/1.41335. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.20276/1.39776. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.20085/1.35532. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.23319/1.36969. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.22193/1.41905. Took 0.45 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.71157/0.72311. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70865/0.72709. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69624/0.73326. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69050/0.72002. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.68890/0.73835. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68463/0.73490. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68458/0.75341. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68237/0.75743. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68046/0.76675. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67644/0.77552. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67073/0.78099. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67218/0.77921. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67054/0.78677. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.66541/0.79310. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.66698/0.78372. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66663/0.78678. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66816/0.79777. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66293/0.81292. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67098/0.79785. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.66467/0.80057. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66368/0.80273. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65145/0.80926. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65497/0.81672. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.66420/0.81901. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65872/0.83391. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65695/0.81230. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.65729/0.83010. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64655/0.83206. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63780/0.83023. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64777/0.83557. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64147/0.87640. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.64594/0.86052. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63559/0.83705. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63965/0.85431. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.64762/0.87135. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.63299/0.88328. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.62579/0.84104. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62641/0.84702. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.62786/0.85902. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.62068/0.92575. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.65874/0.88443. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.61998/0.85087. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.62978/0.83901. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.61269/0.86650. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.61245/0.88711. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.59986/0.88728. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.60286/0.86645. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.60224/0.87701. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58852/0.89327. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.59357/0.91870. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.59690/0.96651. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.61024/0.90606. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.58257/0.84010. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.58439/0.86156. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.57570/0.89703. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.57012/0.88648. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.57268/0.89071. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.57040/0.90228. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.56566/0.90323. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.56913/0.88579. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.55194/0.88177. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.55867/0.88171. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.55835/0.90868. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.54807/0.90273. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.54513/0.90594. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.55970/0.97889. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.55276/0.89780. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.52825/0.92325. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.53220/0.94465. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.54327/0.93709. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.53890/0.94355. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.51829/0.94145. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.51977/0.97976. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.53617/0.96322. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.52438/0.90759. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.50380/0.99517. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.50871/1.00944. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.50358/0.97042. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.49328/1.09647. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.51886/1.04696. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.50415/1.05015. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.49419/1.19438. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.53758/1.07062. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.49826/1.06336. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.48569/1.06277. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.47250/1.06976. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.48731/1.05988. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.49413/1.25118. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.49406/1.10652. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.46642/1.16317. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.46544/1.20066. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.45415/1.24428. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.45216/1.21373. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.46914/1.11443. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.45020/1.16858. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.45737/1.27746. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.45549/1.12034. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.44726/1.19295. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.45418/1.15183. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.41947/1.19043. Took 0.44 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69665/0.71531. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68466/0.71808. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.68390/0.71680. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.67612/0.71492. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.67262/0.71963. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.67118/0.70816. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.66370/0.71958. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.66734/0.71915. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.66257/0.73828. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.65868/0.73494. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.66283/0.74216. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.65111/0.75168. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65295/0.76010. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.64754/0.76539. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.64178/0.77534. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.64034/0.76371. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.64138/0.78856. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64401/0.81569. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64153/0.81745. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.62819/0.82904. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.62656/0.84126. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.62906/0.86287. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.62067/0.85237. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.61686/0.85940. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.61514/0.85949. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.60389/0.86125. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.60442/0.86894. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.59877/0.88767. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.60010/0.89240. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.57860/0.93250. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.58179/0.90942. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.57404/0.88976. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.56850/0.90162. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.55916/0.91482. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.56070/0.91091. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.56234/0.89687. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.54831/0.94508. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.54463/0.93163. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.54141/0.94137. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.53549/0.98888. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.51997/1.01328. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.52558/1.02139. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.50791/1.07306. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.51346/1.07387. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.50923/1.10848. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.52184/1.12312. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.48991/1.11954. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.49231/1.12873. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.50439/1.10194. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.48088/1.15659. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.47777/1.14083. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.47899/1.18028. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.46978/1.16349. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.47240/1.16064. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.45284/1.21296. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.45555/1.23626. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.46395/1.23786. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.44964/1.26026. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.45002/1.31030. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.43059/1.31494. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.44626/1.28817. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.41975/1.30686. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.42138/1.30536. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.41173/1.31328. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.41700/1.34813. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.40611/1.38159. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.41582/1.35662. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.39193/1.38537. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.39865/1.37686. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.40700/1.36126. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.37866/1.43124. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.35969/1.50500. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.37942/1.51188. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.37669/1.47134. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.36298/1.52617. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.34197/1.50502. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.36287/1.52166. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.35680/1.42646. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.36309/1.52516. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.36636/1.48169. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.32193/1.53445. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.34389/1.52245. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.30673/1.57771. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.32417/1.63537. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.31872/1.60255. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.29850/1.60157. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.30083/1.67838. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.31308/1.64564. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.30737/1.65993. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.30751/1.67402. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.28836/1.70110. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.30314/1.74346. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.26037/1.73124. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.27889/1.82169. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.26847/1.71219. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.25401/1.83065. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.27042/1.77017. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.26310/1.87789. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.27231/1.84533. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.26679/1.79577. Took 0.45 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.72603/0.69991. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.71043/0.70026. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69613/0.70364. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70192/0.70611. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69305/0.70581. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69271/0.70138. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68868/0.69995. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69071/0.70151. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68844/0.70708. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68350/0.70411. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68127/0.70777. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67666/0.71386. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67859/0.71576. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67675/0.71821. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67940/0.72697. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66787/0.74454. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65842/0.73628. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66145/0.77275. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64481/0.79298. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64406/0.81568. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64385/0.84111. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64283/0.85754. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.62471/0.88598. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.62896/0.91614. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62516/0.93818. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.62111/0.93274. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.62081/0.94142. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.60751/0.95611. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61401/0.98097. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60172/0.99356. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.60531/1.01010. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60008/1.02587. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60373/1.02684. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59944/1.01574. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60556/1.02759. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.60370/1.00089. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.59268/1.02488. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59051/1.02264. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.58261/1.04530. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58353/1.03729. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.56922/1.05280. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.57253/1.04989. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57247/1.05186. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.56610/1.01898. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56843/1.04426. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.54758/1.03970. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.54793/1.05421. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56083/1.06302. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.55229/1.07535. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.53947/1.04594. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.53191/1.05594. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.53469/1.05181. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.53023/1.03647. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.51763/1.05702. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.51641/1.08601. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.50779/1.10672. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.51526/1.11086. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.50775/1.08489. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.49345/1.13616. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.51169/1.11793. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49352/1.12661. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.47854/1.15036. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.48000/1.18746. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.49202/1.13490. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.47180/1.15679. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.46622/1.13909. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.45569/1.14616. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.43172/1.17733. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.46371/1.17162. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45209/1.23388. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.48033/1.18935. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.43600/1.22110. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.43435/1.22551. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.41674/1.22820. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.43258/1.20331. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.41492/1.22875. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.42048/1.28754. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.43180/1.22015. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.41297/1.25474. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.40867/1.29172. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.39080/1.27411. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.39344/1.25556. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.40035/1.34512. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.38087/1.32226. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.39929/1.29073. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.38896/1.24870. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.37208/1.22728. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.38970/1.35940. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.35117/1.27393. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.34227/1.33555. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36510/1.31714. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.34779/1.35079. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.35275/1.31862. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.36355/1.29230. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.36192/1.39163. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.34506/1.32594. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.34552/1.39572. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.31813/1.43259. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.32235/1.38777. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.35206/1.27900. Took 0.45 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.71728/0.68790. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70378/0.70118. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69631/0.70028. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68891/0.71854. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68748/0.72157. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68215/0.72963. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.67984/0.76053. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67761/0.75791. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67476/0.76033. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66919/0.76861. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66609/0.77108. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66575/0.77890. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65990/0.77656. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66110/0.79004. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.65542/0.79789. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66523/0.79683. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65313/0.80292. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.64566/0.82224. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65070/0.81894. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64431/0.81946. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64100/0.81791. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63467/0.83681. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63990/0.83296. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64126/0.81740. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.63566/0.83484. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62992/0.83158. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.62797/0.83295. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62255/0.81531. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.63222/0.80219. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62511/0.78928. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62101/0.78289. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61893/0.79586. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.62275/0.79372. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61677/0.81237. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.61776/0.80428. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61035/0.81340. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.60863/0.83214. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60064/0.81894. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60882/0.82250. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58818/0.85085. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.59111/0.82445. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59394/0.84858. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57889/0.82807. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57578/0.80518. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57233/0.83558. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.58612/0.81644. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.57599/0.81850. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57201/0.82063. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.56194/0.85445. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.56164/0.84104. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.54704/0.85543. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.55450/0.84986. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54747/0.84849. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54498/0.83129. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.54475/0.87020. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.53957/0.89693. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.52730/0.90944. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52548/0.92262. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50654/0.93174. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50516/0.95759. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49758/0.98760. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.50145/0.98261. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49721/0.97636. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48746/1.00345. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.47644/0.98894. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.47983/1.00013. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.50219/0.97644. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.45915/1.08298. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.44927/1.05848. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46001/1.02518. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.45634/1.09770. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.45093/1.08633. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.43803/1.08546. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.44073/1.11922. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.43731/1.12333. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.43884/1.15902. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.44120/1.14867. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.41693/1.23547. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.40947/1.25186. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.43077/1.26448. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.40857/1.23881. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.39347/1.30052. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.39978/1.29451. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.39952/1.32543. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.38104/1.26234. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.39172/1.28733. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.38056/1.20143. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.39341/1.28460. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.39997/1.27942. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.38330/1.33786. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.39119/1.29177. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.36662/1.43777. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.37891/1.40513. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.37329/1.42262. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.36685/1.37028. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.35209/1.46044. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.37154/1.38626. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.37021/1.51257. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.41689/1.29152. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.37000/1.42825. Took 0.44 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70085/0.69028. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69744/0.68995. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69387/0.70262. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.68692/0.69438. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68433/0.70597. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69261/0.72605. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68089/0.72047. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68547/0.73219. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67781/0.73154. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67983/0.73441. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67179/0.76279. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66844/0.78118. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67428/0.78973. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66472/0.79804. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66296/0.79438. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66086/0.79535. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65722/0.80132. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.65451/0.80609. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66064/0.81191. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64542/0.84482. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64468/0.87272. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64400/0.88271. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64557/0.85506. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63508/0.86215. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63745/0.85254. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63011/0.89215. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.62083/0.91891. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63154/0.90886. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62764/0.95297. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61163/0.97466. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62023/0.99090. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.60976/0.96952. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60380/0.96672. Took 0.47 sec\n",
      "Epoch 33, Loss(train/val) 0.60298/0.96919. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59291/0.96214. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.60150/0.99933. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59692/0.98399. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.58335/1.01318. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.59594/1.00694. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59327/0.95964. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58701/0.94186. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58245/0.95836. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58055/0.97366. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.58339/0.98834. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56785/1.00533. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57433/1.00606. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.57417/0.97394. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56838/0.99028. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.55613/0.98623. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.54634/0.99543. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.54728/1.04388. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53362/1.02688. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.54005/1.01438. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.54805/1.02160. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51441/1.03926. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.53954/1.02835. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.53142/1.04803. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.52623/1.04889. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.51823/1.02219. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.52306/1.03937. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51947/1.03753. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.50230/1.05660. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51048/1.08341. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.50435/1.06685. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50635/1.03412. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.47948/1.04605. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.50392/1.04013. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.47496/1.04701. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.48905/1.10193. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46697/1.07139. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.45993/1.10548. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.46750/1.05478. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.46848/1.11255. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.46216/1.08972. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.45622/1.10940. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.44948/1.07898. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46374/1.05399. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.45965/1.11042. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.45052/1.06641. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.46214/1.08004. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.44739/1.08241. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.43780/1.02871. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.42082/1.09398. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.41352/1.12644. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.41749/1.20509. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.42736/1.06554. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.48876/1.10415. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.42216/1.09918. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.41933/1.12587. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.40657/1.09217. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.38707/1.19635. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.41195/1.05366. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.41568/1.22082. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.38883/1.20121. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.38679/1.26741. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.39611/1.11927. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.37211/1.23169. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.34897/1.26664. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.36139/1.34226. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.35926/1.34306. Took 0.44 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69920/0.74767. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69387/0.75919. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68329/0.77436. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68099/0.78416. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69013/0.79247. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67888/0.79977. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68165/0.77611. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67670/0.78135. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67395/0.74751. Took 0.48 sec\n",
      "Epoch 9, Loss(train/val) 0.67194/0.74357. Took 0.49 sec\n",
      "Epoch 10, Loss(train/val) 0.66103/0.74845. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67358/0.73669. Took 0.47 sec\n",
      "Epoch 12, Loss(train/val) 0.65861/0.73808. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66417/0.72649. Took 0.49 sec\n",
      "Epoch 14, Loss(train/val) 0.65496/0.74737. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66035/0.76385. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65254/0.77123. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.64792/0.75903. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.64321/0.75508. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64561/0.73854. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64299/0.71956. Took 0.47 sec\n",
      "Epoch 21, Loss(train/val) 0.63367/0.73958. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64597/0.72561. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62575/0.70656. Took 0.47 sec\n",
      "Epoch 24, Loss(train/val) 0.62661/0.69855. Took 0.48 sec\n",
      "Epoch 25, Loss(train/val) 0.62106/0.70903. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62566/0.71010. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.60866/0.73446. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61917/0.73248. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61347/0.74585. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.62721/0.73064. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61398/0.73072. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.60642/0.73544. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60570/0.74234. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60242/0.71408. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.59763/0.72239. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.58975/0.73440. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.59173/0.74109. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58902/0.70649. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.58407/0.74175. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57416/0.75937. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.57478/0.70297. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.56421/0.76038. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56102/0.74989. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.55621/0.77682. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.56405/0.76979. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56025/0.79054. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.54169/0.79330. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.54715/0.83356. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55172/0.82865. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.54828/0.81742. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54280/0.83543. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.53831/0.82216. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.52335/0.83005. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.53247/0.88063. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51115/0.90623. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.51988/0.93338. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.50540/0.93009. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50938/0.89955. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50151/0.94047. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.48952/0.93135. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50814/0.94619. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.49252/0.93125. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.48556/0.96009. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.48731/0.95759. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50212/0.96571. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48283/0.98679. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.47732/0.96546. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.48560/1.00915. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.48343/0.94933. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.46021/1.05189. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.46658/0.96020. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.45401/0.99451. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.45600/1.05860. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.45633/1.05040. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.46808/1.02301. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.44513/1.03942. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.44770/1.05537. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43933/1.08263. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.42977/1.11733. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.43721/1.04893. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.44059/1.07669. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.45329/1.06141. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.42282/1.08973. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.42872/1.10395. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.40869/1.17509. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.43267/1.11742. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.41477/1.12810. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.41590/1.09286. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.41322/1.09813. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.42549/1.08653. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.42208/1.09976. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.39310/1.20959. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.40022/1.20391. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.39914/1.11512. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.38907/1.20788. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.39573/1.21652. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.39945/1.16328. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.37874/1.15333. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.39223/1.11662. Took 0.45 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70435/0.69798. Took 0.62 sec\n",
      "Epoch 1, Loss(train/val) 0.69760/0.68039. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69676/0.67094. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69261/0.67237. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69003/0.67718. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68405/0.67654. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68241/0.67649. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67913/0.67399. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67959/0.67990. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67136/0.68016. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66635/0.68112. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66603/0.68122. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66298/0.68226. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66049/0.67454. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65848/0.67098. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65697/0.67741. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65087/0.68965. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65033/0.69609. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.64641/0.68947. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65008/0.68913. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64382/0.70041. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63456/0.72423. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63897/0.73886. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64133/0.74008. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62931/0.73682. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.62488/0.74623. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.62525/0.74256. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61124/0.75469. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.62270/0.75584. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.61312/0.77272. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61420/0.78875. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.62074/0.77677. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60593/0.79705. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60923/0.80729. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.60475/0.81983. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59911/0.83394. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.59676/0.82800. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60338/0.82367. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.59795/0.82823. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.57742/0.85983. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.59369/0.84587. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.57506/0.90668. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58210/0.91092. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.56605/0.92027. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56993/0.92608. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55872/0.97026. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.55554/0.95032. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.54542/0.96120. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55436/0.94800. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.53647/0.98973. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54326/0.96387. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53839/0.92188. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.53725/0.95969. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.51715/1.01524. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52240/1.02773. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.53447/1.01224. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.51210/0.96704. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.50546/0.97900. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.50809/0.97606. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.51613/0.99519. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50742/1.03081. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.48468/0.99750. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.48809/0.94372. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.49219/0.98143. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.49262/0.97993. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.47733/1.02102. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.47980/0.99890. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.47384/1.02253. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.45881/1.03245. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.43229/1.06008. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.47315/1.04696. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45780/1.00687. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.44336/1.10450. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.43190/1.02523. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.42723/1.13622. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.42775/1.19141. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.45007/1.16747. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.43292/1.19959. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.41924/1.16187. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.40109/1.14761. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.40139/1.20777. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.40735/1.23268. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.38368/1.27204. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.39876/1.31334. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.38605/1.31011. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.38429/1.33075. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.41177/1.27927. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.39029/1.38689. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.38247/1.31654. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.36881/1.35692. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.34411/1.43937. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.35811/1.35711. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.36437/1.38794. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.34749/1.38690. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.34713/1.38577. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.32127/1.44769. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.32869/1.57307. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.33897/1.56092. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.33177/1.55633. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.34044/1.58829. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.71466/0.70729. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69409/0.72567. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.68690/0.74332. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69255/0.73596. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.69009/0.73855. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67900/0.73941. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68523/0.74263. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67338/0.74271. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67512/0.76733. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67955/0.77339. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67347/0.75200. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.66842/0.77227. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66635/0.76716. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66348/0.77984. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.65974/0.76444. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65592/0.76687. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66096/0.77341. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65955/0.77950. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64337/0.78928. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65847/0.76784. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64625/0.78180. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65253/0.79218. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64017/0.80027. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64966/0.80888. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63289/0.79555. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64365/0.78362. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.63939/0.79534. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63642/0.77569. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.62907/0.81164. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62762/0.83411. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.62383/0.82618. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.62034/0.83504. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.61956/0.82643. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.61189/0.82490. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.61679/0.83115. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61790/0.82786. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61496/0.84967. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.61183/0.85274. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60629/0.84789. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60831/0.82886. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59148/0.83680. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59876/0.84377. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59886/0.85822. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59254/0.85566. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.58780/0.85900. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.58742/0.85224. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58468/0.84719. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.57599/0.85985. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56852/0.88533. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57313/0.87990. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.56899/0.89301. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.57188/0.87889. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.54947/0.91636. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55527/0.91313. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54857/0.93912. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55269/0.91767. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.53843/0.93435. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.55405/0.92480. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54335/0.94691. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.52505/0.94391. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.53366/1.00075. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.54022/0.94829. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52890/0.99504. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.52177/0.99087. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.52745/0.99619. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.50655/1.01932. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.50735/1.04361. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.52494/1.00979. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.49967/1.11853. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.50821/1.07475. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.49082/1.07824. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48712/1.11028. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.47891/1.15838. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.49193/1.11674. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.49577/1.08919. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.47562/1.11767. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.47534/1.04972. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46669/1.13296. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.46061/1.08987. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44771/1.23548. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.47578/1.11855. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.44445/1.12464. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.43477/1.19803. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.42774/1.16653. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.44468/1.23145. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.45742/1.12253. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.44837/1.18027. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.44970/1.16833. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.42585/1.21040. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.45130/1.17492. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.46111/1.18017. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.42985/1.20821. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.41379/1.23853. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.41071/1.27664. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.42882/1.17518. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.44285/1.15646. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.41346/1.19341. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.41038/1.13969. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.39788/1.26872. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.40161/1.12156. Took 0.45 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.71187/0.72742. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70488/0.71824. Took 0.49 sec\n",
      "Epoch 2, Loss(train/val) 0.69752/0.72405. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.70457/0.72167. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69358/0.72093. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69257/0.72850. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68804/0.72355. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68782/0.72432. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68815/0.71925. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68875/0.71784. Took 0.48 sec\n",
      "Epoch 10, Loss(train/val) 0.68783/0.71432. Took 0.48 sec\n",
      "Epoch 11, Loss(train/val) 0.68173/0.72142. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67878/0.71110. Took 0.47 sec\n",
      "Epoch 13, Loss(train/val) 0.67829/0.70436. Took 0.50 sec\n",
      "Epoch 14, Loss(train/val) 0.67963/0.70546. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.68385/0.71006. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67686/0.70691. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.67488/0.70919. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67464/0.71844. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66990/0.72598. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.66760/0.72158. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.66613/0.72115. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65872/0.72500. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.65672/0.73632. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65408/0.73133. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65217/0.72901. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65123/0.73717. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.65607/0.73437. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64746/0.71830. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64995/0.74221. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64845/0.74311. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64854/0.75980. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.64657/0.74938. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64152/0.73918. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.63672/0.73434. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.62268/0.75019. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.61982/0.73998. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.61886/0.73021. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.62111/0.75792. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.60700/0.76072. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59997/0.74948. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60583/0.76422. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60064/0.75690. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60022/0.77100. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.59646/0.75022. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57345/0.75456. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.57753/0.79891. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.56859/0.76200. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.56376/0.78561. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.55851/0.80975. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56620/0.76671. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.56265/0.81693. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.53239/0.85552. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.52875/0.85787. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54385/0.84029. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.53722/0.81908. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.51262/0.89069. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.50473/0.88342. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.51348/0.90184. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.48899/0.88402. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.48163/0.99900. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.47820/0.91946. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.47130/0.94462. Took 0.47 sec\n",
      "Epoch 63, Loss(train/val) 0.46591/0.96537. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.48240/1.00629. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.47416/1.03707. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.46487/0.93565. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.44585/0.95329. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.44314/0.99396. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.42830/1.05278. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.43334/0.95122. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.42136/1.05570. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.42251/1.11992. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.44414/1.02935. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.43872/0.93194. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.41501/1.06168. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.41623/1.08217. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.39250/1.10998. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.41279/1.11479. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.39750/1.19197. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.39745/1.09857. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.36539/1.16108. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.38975/1.24845. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.36139/1.15622. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.40153/1.21329. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.37932/1.15749. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.37579/1.21575. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.36521/1.24520. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.37432/1.17667. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.36165/1.29276. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.36539/1.37764. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.36683/1.20546. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.34301/1.30426. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.36093/1.30059. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.34538/1.32909. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.36504/1.28795. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.34899/1.34327. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.33467/1.34086. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.35275/1.38652. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.38415/1.25438. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.71637/0.69348. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.70760/0.72343. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.71206/0.72932. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69568/0.70041. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69707/0.71267. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69887/0.70810. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.70295/0.70601. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69915/0.70114. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.69351/0.70647. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.69475/0.71196. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68823/0.71189. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.68659/0.71294. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68504/0.70816. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.69145/0.71077. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67822/0.70198. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67488/0.70828. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.68528/0.72534. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.68476/0.72099. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.68097/0.73610. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67658/0.72167. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.67846/0.73215. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.67876/0.70752. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66961/0.72287. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.67359/0.71530. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.67216/0.71555. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66707/0.71510. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.67163/0.72461. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.67120/0.73014. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.66799/0.72561. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.66310/0.73764. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.65919/0.71544. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.66359/0.72099. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.66516/0.73131. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.65720/0.71748. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.65610/0.77083. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.65317/0.73772. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.64743/0.73583. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.64169/0.74995. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.65217/0.76641. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.63854/0.76272. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.64741/0.74752. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.64042/0.75038. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.63893/0.77223. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.64048/0.77182. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.63934/0.76308. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.64050/0.75574. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.63143/0.77188. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.63784/0.76876. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.63184/0.76561. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.62737/0.77150. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.62822/0.77075. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.62301/0.78071. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.62291/0.76038. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.62459/0.77459. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.61748/0.77114. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.61140/0.78333. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.61183/0.80314. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.60789/0.78767. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.61070/0.80555. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.60565/0.80025. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.59263/0.81079. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.59528/0.82467. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.59991/0.82155. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.60061/0.80462. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.60944/0.79060. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.60270/0.81327. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.58275/0.79510. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.58314/0.84410. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.57385/0.82542. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.56796/0.86878. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.58019/0.85610. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.56897/0.80749. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.56341/0.82328. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.57135/0.87753. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.57377/0.85946. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.55892/0.87294. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.56894/0.83719. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.56596/0.85462. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.56397/0.82448. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.55305/0.87833. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.55946/0.87662. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.54313/0.89837. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.54358/0.82587. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.54847/0.88605. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.54350/0.91413. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.55905/0.79502. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.52939/0.84530. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.52593/0.82979. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.52862/0.83488. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.53595/0.82285. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.55517/0.77478. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.55472/0.78513. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.51364/0.81808. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.53310/0.81048. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.51267/0.90385. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.52767/0.82906. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.50363/0.83874. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.49521/0.90784. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.50242/0.82688. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.49297/0.85932. Took 0.44 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.71464/0.69193. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70466/0.70770. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.70046/0.68674. Took 0.49 sec\n",
      "Epoch 3, Loss(train/val) 0.69804/0.69355. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69777/0.69405. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69101/0.69444. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68790/0.69221. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68702/0.68615. Took 0.50 sec\n",
      "Epoch 8, Loss(train/val) 0.68280/0.69176. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68042/0.68635. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68218/0.69387. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67819/0.68071. Took 0.47 sec\n",
      "Epoch 12, Loss(train/val) 0.67799/0.69065. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67351/0.67939. Took 0.47 sec\n",
      "Epoch 14, Loss(train/val) 0.67572/0.68815. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67356/0.68367. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66712/0.67703. Took 0.47 sec\n",
      "Epoch 17, Loss(train/val) 0.66913/0.66784. Took 0.49 sec\n",
      "Epoch 18, Loss(train/val) 0.66591/0.66762. Took 0.47 sec\n",
      "Epoch 19, Loss(train/val) 0.66682/0.66721. Took 0.47 sec\n",
      "Epoch 20, Loss(train/val) 0.66270/0.67223. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.65994/0.66436. Took 0.47 sec\n",
      "Epoch 22, Loss(train/val) 0.66245/0.66217. Took 0.48 sec\n",
      "Epoch 23, Loss(train/val) 0.65582/0.65198. Took 0.47 sec\n",
      "Epoch 24, Loss(train/val) 0.65832/0.66899. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.65562/0.65858. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.64859/0.66633. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64666/0.67161. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.64808/0.67069. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.64359/0.67754. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63988/0.68010. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63728/0.67058. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63873/0.66996. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62373/0.66515. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.62224/0.66107. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61518/0.65026. Took 0.47 sec\n",
      "Epoch 36, Loss(train/val) 0.61833/0.65889. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.61771/0.65169. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62118/0.66163. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61204/0.64612. Took 0.48 sec\n",
      "Epoch 40, Loss(train/val) 0.61221/0.63711. Took 0.51 sec\n",
      "Epoch 41, Loss(train/val) 0.61326/0.63887. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60846/0.64722. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.60092/0.65526. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.60656/0.64567. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.59705/0.63867. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59961/0.65620. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.59391/0.63411. Took 0.48 sec\n",
      "Epoch 48, Loss(train/val) 0.58397/0.65424. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.59711/0.63079. Took 0.47 sec\n",
      "Epoch 50, Loss(train/val) 0.57822/0.64694. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.57869/0.64418. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56467/0.65842. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.57097/0.68359. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56989/0.67182. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.56525/0.67780. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.57101/0.69819. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.56587/0.69524. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.56378/0.68069. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54715/0.70583. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.55290/0.70950. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.54903/0.71261. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54978/0.70863. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.54550/0.70688. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.51803/0.72902. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.54241/0.68101. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.52991/0.70833. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.52664/0.69870. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.53495/0.69321. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.52395/0.71390. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.52197/0.73536. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.52912/0.69646. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.51269/0.71668. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.52699/0.71616. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.52086/0.69771. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.50218/0.68634. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.50708/0.71194. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.49991/0.71276. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.49564/0.72935. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.48526/0.69451. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.51368/0.70041. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.50072/0.70660. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.50154/0.68281. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.49579/0.70457. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.50640/0.72684. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.51563/0.72553. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.49640/0.71443. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.48024/0.76343. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.50070/0.76450. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.46728/0.73377. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.47284/0.74999. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.47814/0.82185. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.46228/0.84167. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.46104/0.77527. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.46916/0.79525. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.43534/0.82022. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.49579/0.89369. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.48181/0.82371. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.45516/0.81621. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.44355/0.83526. Took 0.44 sec\n",
      "ACC: 0.6458333333333334\n",
      "Epoch 0, Loss(train/val) 0.75709/0.74770. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.73530/0.70118. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.72153/0.71816. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.71757/0.70950. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.71935/0.72302. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.72211/0.71275. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.71233/0.72059. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.71609/0.70915. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.70751/0.70739. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.70451/0.72723. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68952/0.73930. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.70568/0.73287. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.70620/0.74588. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.69596/0.73863. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68681/0.73928. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.68739/0.75313. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.67731/0.74638. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.68974/0.75000. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.69373/0.76615. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.67766/0.75891. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67598/0.74819. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66115/0.77028. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66932/0.78096. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.67263/0.78695. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.66355/0.78014. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.65873/0.80589. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.65476/0.78814. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.67199/0.77057. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.66363/0.77413. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.65711/0.77317. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.65456/0.78878. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.65249/0.79964. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.65547/0.77562. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64947/0.76268. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.66002/0.77521. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.65016/0.79811. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.64457/0.81641. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.63342/0.79249. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.64055/0.80552. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.63584/0.79453. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.63410/0.86760. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.65417/0.82791. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.63407/0.82397. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.64052/0.82701. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.62999/0.82562. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.62601/0.85021. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.62596/0.83595. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.60978/0.89294. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.62248/0.89052. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.62643/0.84571. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.61218/0.93908. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.60906/0.96222. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.60896/0.94934. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.60691/0.97786. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.61109/0.93094. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.60880/0.98249. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.57991/1.08935. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.60084/1.00862. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.58896/1.01494. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.59213/0.96145. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.59242/0.94999. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.57586/0.98065. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.57672/0.99088. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.61364/0.92054. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.60584/0.95513. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.58019/0.89566. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.57118/0.95356. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.55869/0.94081. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.56258/0.91223. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.55102/0.97606. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.55183/0.92275. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.56292/0.95150. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.55423/0.94566. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.55258/0.93861. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.53626/0.97480. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.54974/0.90979. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.53312/0.95948. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.53315/0.93974. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.53570/1.02260. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.53406/0.92465. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.52817/0.95405. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.54330/0.88991. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.58464/0.98973. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.56416/0.99896. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.53244/0.91231. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.53501/0.92362. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.52412/0.96321. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.51087/0.94117. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.52436/0.92267. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.51375/0.89926. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.49008/0.97058. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.48504/0.94494. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.47317/0.97799. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.47689/1.00353. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.48046/0.96778. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.48231/0.95613. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.50356/0.96465. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.49310/0.94879. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.47201/0.95631. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.51293/0.95229. Took 0.45 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.70878/0.68965. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.70686/0.69091. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69904/0.72571. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70229/0.71468. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69544/0.71817. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69812/0.73058. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.68849/0.73150. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69713/0.72594. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68943/0.74409. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68354/0.74248. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68651/0.74109. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67732/0.75117. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68135/0.75821. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67495/0.76764. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.68409/0.77396. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67223/0.78524. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.67395/0.78181. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66785/0.79001. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67650/0.75806. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66798/0.75163. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65947/0.76304. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66497/0.76139. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65560/0.75373. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64484/0.75037. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.65501/0.76937. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64607/0.77021. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64621/0.75635. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63466/0.75980. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63631/0.76411. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63352/0.77897. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63174/0.78785. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63180/0.80512. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.62495/0.80033. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.61056/0.79493. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62061/0.80183. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61718/0.82288. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61525/0.80431. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60915/0.79751. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60381/0.82501. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.60573/0.85654. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60591/0.84692. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59427/0.85852. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.59837/0.86664. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60045/0.86396. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.59757/0.86459. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59901/0.86894. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59828/0.84630. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58214/0.88856. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57711/0.88037. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57598/0.89337. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.56743/0.90724. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56979/0.91374. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56544/0.92036. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.55252/0.93063. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.56390/0.91286. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.57659/0.90755. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.56613/0.86773. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.54208/0.94888. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.52622/0.98548. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.55347/0.89578. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.53650/0.92036. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.54004/0.97835. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52997/1.02458. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51799/1.02476. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50391/1.05973. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.51192/1.05510. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51258/1.05334. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.50260/1.07554. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.49763/1.09400. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.50585/1.09246. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.50510/1.06118. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.49000/1.09419. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.48935/1.14146. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.49248/1.10371. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.48666/1.14107. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49320/1.16474. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.46893/1.12701. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.47579/1.15541. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.48131/1.17357. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.46366/1.21489. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.46390/1.19136. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.44796/1.22963. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.43292/1.30916. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.44981/1.23635. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.46553/1.24566. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.45748/1.23499. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.43211/1.31346. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.46589/1.18778. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.43669/1.20057. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.43303/1.19895. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.42664/1.26489. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.41313/1.31752. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.43457/1.27697. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.40384/1.27264. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.41417/1.26498. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.40836/1.32088. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.41254/1.28607. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.40836/1.33330. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.41485/1.30795. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.38772/1.41508. Took 0.46 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70170/0.68784. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69587/0.69306. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69246/0.71352. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69095/0.71933. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68337/0.70869. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68470/0.73506. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68046/0.73520. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67167/0.73950. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67716/0.76701. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67870/0.76568. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67825/0.75957. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67518/0.77734. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67662/0.78034. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67433/0.79581. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67294/0.78552. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67456/0.79125. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67067/0.77678. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.67065/0.79180. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66773/0.78875. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66291/0.78624. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.66189/0.79969. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65893/0.79210. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65839/0.80588. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.65105/0.81692. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64503/0.79430. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64970/0.80508. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.63899/0.82554. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64199/0.82234. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63826/0.85886. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63240/0.84246. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.64228/0.84994. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.62613/0.87127. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61627/0.88023. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62281/0.88129. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.62641/0.89119. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61681/0.88616. Took 0.47 sec\n",
      "Epoch 36, Loss(train/val) 0.61782/0.89844. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.61227/0.91841. Took 0.47 sec\n",
      "Epoch 38, Loss(train/val) 0.60866/0.91391. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.60187/0.91292. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59780/0.97440. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.59402/0.96770. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59264/0.94365. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.58717/0.94407. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57285/0.98030. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57757/0.95837. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.57978/0.97621. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57119/0.97686. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56547/1.01313. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.57745/1.01916. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.57246/1.03201. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.56866/0.97093. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.55455/1.01394. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.56218/0.99007. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55357/1.04395. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.54857/1.02349. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.54030/1.05911. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53892/1.07996. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.54963/1.13509. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53072/1.11299. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54670/1.09981. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.54372/1.11322. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53207/1.08869. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.51911/1.11090. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.53007/1.12929. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.50527/1.16888. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51656/1.13144. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.50590/1.19566. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50432/1.17555. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.52236/1.17894. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.48016/1.21889. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.49361/1.18051. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.48383/1.18780. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.51105/1.14961. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.50537/1.21475. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49317/1.23546. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.48334/1.25604. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.49003/1.32966. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.46899/1.30475. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46055/1.46314. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.44722/1.40458. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.44935/1.41746. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.44396/1.41118. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.43589/1.53465. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.45819/1.40335. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.47337/1.40681. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43256/1.29373. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.45954/1.37013. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.47030/1.37230. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.42183/1.40208. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.41531/1.45348. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.43207/1.45722. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.41548/1.37175. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.42041/1.37993. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.41388/1.46532. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.43862/1.40573. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.41158/1.62675. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.41423/1.48263. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.39594/1.63827. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.38288/1.60691. Took 0.45 sec\n",
      "ACC: 0.6354166666666666\n",
      "Epoch 0, Loss(train/val) 0.70326/0.69553. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69638/0.67952. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.68818/0.67427. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68377/0.67616. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.68558/0.67328. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.68177/0.67483. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67892/0.66208. Took 0.48 sec\n",
      "Epoch 7, Loss(train/val) 0.67491/0.66618. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67144/0.68424. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67859/0.68651. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67519/0.68805. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67120/0.69061. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67354/0.71014. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66880/0.70883. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.66537/0.70903. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66173/0.71058. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65793/0.71753. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65570/0.71599. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65039/0.74125. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65191/0.73411. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65383/0.73681. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65421/0.74348. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64789/0.73148. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65156/0.73517. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.64515/0.73710. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.63815/0.75491. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64516/0.74362. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64054/0.74619. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63954/0.75709. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.63100/0.75143. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63144/0.76436. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62714/0.78641. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.62091/0.78756. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.61885/0.76824. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.61738/0.78241. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61681/0.79469. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.60464/0.81466. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60820/0.81801. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60224/0.81348. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.59845/0.81923. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60482/0.86235. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.59067/0.84101. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59919/0.83106. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.58634/0.88901. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57840/0.89830. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.57232/0.90892. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56878/0.89875. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.56126/0.92232. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.56557/0.94153. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55358/0.93510. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.56135/0.95501. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56123/0.90628. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54111/0.90238. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.52796/0.93519. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52846/0.93075. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.53306/1.03930. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52531/1.05913. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52571/1.05893. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.50183/1.05505. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.51305/1.05827. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.50300/1.07406. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.49420/1.11682. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49683/1.11683. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.48111/1.12428. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49334/1.13410. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.47737/1.16256. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.49598/1.22440. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48426/1.20541. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.47734/1.13206. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47481/1.23663. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.48251/1.20358. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.46756/1.23126. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48890/1.23009. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.47690/1.22425. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.47651/1.12702. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.47204/1.20708. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.45127/1.19613. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.44590/1.26527. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.46854/1.22949. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.43413/1.25967. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.43556/1.25960. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.42178/1.29080. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.43397/1.33820. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.42325/1.31321. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.40529/1.32887. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.41308/1.32298. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.41779/1.28140. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.41294/1.31319. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.41245/1.35869. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.38080/1.37460. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.41261/1.36174. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.36987/1.36101. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.37329/1.37608. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.38883/1.51436. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.43854/1.34082. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.41998/1.43065. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.38084/1.47419. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.35626/1.52617. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.35451/1.49965. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.39806/1.44645. Took 0.45 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70468/0.69909. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70434/0.70626. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69885/0.71449. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69224/0.72709. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68239/0.73807. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68811/0.73512. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68328/0.74973. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67752/0.75701. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67760/0.74658. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67746/0.75731. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67706/0.73685. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.68394/0.74578. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67527/0.76132. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67787/0.76905. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67150/0.77247. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67245/0.77937. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67116/0.77829. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66867/0.77797. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.67166/0.78300. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66836/0.79087. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66975/0.77366. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66026/0.77138. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65937/0.79889. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66449/0.78612. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.66198/0.78482. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65977/0.79812. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.66297/0.80664. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65968/0.81233. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.64650/0.83388. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65907/0.84652. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.64794/0.86629. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.65584/0.83260. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.65310/0.85999. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.64763/0.84568. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.64365/0.89194. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.65190/0.86791. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.64005/0.87085. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.64299/0.86224. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.63793/0.87110. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.63525/0.88887. Took 0.47 sec\n",
      "Epoch 40, Loss(train/val) 0.63336/0.90079. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.62550/0.92137. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.62720/0.92170. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.62776/0.92264. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.61923/0.89846. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.61582/0.92981. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.61572/0.94919. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.60209/0.96062. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.60970/0.93630. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.60453/0.94071. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.60871/0.92566. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.60511/0.94093. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.61116/0.92872. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.59691/0.92011. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.59802/0.94959. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.59907/0.96423. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.59536/0.98289. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.58253/0.99716. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.59162/1.01959. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.57762/1.02050. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.57970/1.07937. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.57174/1.08100. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.57345/1.13435. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.57590/1.13229. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.56458/1.14224. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.55656/1.11253. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.56577/1.11253. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.54600/1.17721. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.54348/1.19341. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.54701/1.16461. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.53828/1.18666. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.54018/1.23367. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.53738/1.19201. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.54462/1.23822. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.53480/1.26785. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.52934/1.26723. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.51759/1.26608. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.51077/1.27630. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.52045/1.33413. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.51415/1.23576. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.51755/1.32279. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.51602/1.27978. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.50766/1.30451. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.50344/1.28435. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.49132/1.33109. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.48164/1.34192. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.48633/1.31287. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.47432/1.33509. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.48353/1.37398. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.46536/1.37798. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.48076/1.36377. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.46930/1.51272. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.47268/1.37363. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.46587/1.44164. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.46242/1.39502. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.45351/1.43575. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.44908/1.39811. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.45664/1.44825. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.44054/1.47903. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.44982/1.41838. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70163/0.67922. Took 0.62 sec\n",
      "Epoch 1, Loss(train/val) 0.69455/0.67732. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68846/0.67574. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69212/0.67604. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68851/0.68205. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68488/0.69007. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68760/0.69518. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.68204/0.68883. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67851/0.69995. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68228/0.70090. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67984/0.70652. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67306/0.70720. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.67855/0.70926. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67214/0.71912. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67078/0.72259. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66151/0.72126. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65780/0.73008. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65752/0.73377. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65748/0.73384. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65260/0.73948. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65216/0.73742. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64162/0.73237. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64818/0.73816. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64608/0.74064. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64082/0.74104. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64790/0.74004. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63440/0.72598. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63769/0.73324. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62766/0.74481. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63864/0.73123. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.62842/0.74436. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.62839/0.73516. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62366/0.75356. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62376/0.75424. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61028/0.75530. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.60007/0.77439. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60277/0.76779. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.59209/0.77392. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.59554/0.77033. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58367/0.78247. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.57833/0.79341. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.57361/0.80641. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.57164/0.81863. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57780/0.83905. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57226/0.81115. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55364/0.85894. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55689/0.82950. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56524/0.84262. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55661/0.82252. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55330/0.81732. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.54910/0.85075. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.54327/0.87236. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.54584/0.84149. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.53272/0.83852. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.51738/0.90035. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.52202/0.85724. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53783/0.87495. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.54307/0.80419. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.51402/0.85152. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52028/0.88912. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.51864/0.85132. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.52714/0.84889. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.52147/0.86950. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.53726/0.86403. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50856/0.89780. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.49705/0.88232. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.50371/0.89917. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.49619/0.89117. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.48588/0.94516. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.46991/0.94577. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.46037/0.97377. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.47476/0.95552. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.45783/0.94433. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.49098/0.94165. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.47745/0.89343. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.46485/0.95708. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.45358/0.95804. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.44574/0.98086. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.45849/0.97049. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44251/0.95892. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.43116/1.02909. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.44802/0.95840. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.43328/0.96851. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.43032/0.99721. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.42120/1.07366. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.41636/1.03795. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.41661/0.99626. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.42935/1.02678. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.41409/1.00949. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.40640/1.05276. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.38809/1.10128. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.40688/1.04609. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.38213/1.08066. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.39466/1.09220. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.41164/1.07197. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.40817/1.06881. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.38328/1.04100. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.37843/1.14273. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.39137/1.14491. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.38196/1.18096. Took 0.44 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.70804/0.71520. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70087/0.72649. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.70699/0.70375. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69749/0.70271. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69357/0.71870. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69295/0.73935. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68861/0.73974. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68787/0.74429. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68773/0.78038. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68490/0.77971. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68782/0.76895. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67837/0.76570. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.67735/0.79048. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67782/0.80089. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67690/0.80320. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67391/0.82730. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67613/0.83609. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.68180/0.78740. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67704/0.79037. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67109/0.76919. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.67694/0.80058. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.67833/0.79024. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.67195/0.77875. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.66212/0.83771. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.66403/0.79791. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.66492/0.87079. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.66402/0.83157. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.66155/0.89621. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65621/0.88213. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.65731/0.86662. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.66241/0.79963. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64413/0.83775. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.65062/0.83162. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64406/0.86395. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.63620/0.92167. Took 0.47 sec\n",
      "Epoch 35, Loss(train/val) 0.63909/0.84104. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.62563/0.84545. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.63218/0.86199. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.63730/0.85300. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.62502/0.91110. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.62312/0.81682. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.62894/0.79616. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.62244/0.82203. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61522/0.95062. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.62224/0.86951. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.61954/0.83994. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.61865/0.82612. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.61349/0.78982. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.60712/0.94868. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.60062/0.80541. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.60108/0.87650. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.59362/0.82466. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.59713/0.91444. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.59856/0.85619. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.58458/0.86116. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.58938/0.89660. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.59641/0.86131. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.58302/0.87609. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.57965/0.85645. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.58371/0.90200. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.57780/0.83765. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.58377/0.83132. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.57798/0.92803. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.58118/0.89741. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.57112/0.86777. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.56146/0.90037. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.55970/0.86240. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.54790/0.88425. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.54867/0.86618. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.55071/0.89011. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.54795/0.85428. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.54310/0.92636. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.53144/0.95877. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.53829/0.86597. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.52353/0.89239. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.52086/0.87954. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.54755/0.87185. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.51955/0.88316. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.51973/0.89337. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.51500/0.87149. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.52049/0.83740. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.52393/0.86194. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.50587/0.87459. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.51484/0.83594. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.50991/0.88918. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.50171/0.83838. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.50088/0.88353. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.49817/0.83755. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.48464/0.86607. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.47123/0.88552. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.48685/0.86128. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.49922/0.86749. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.49908/1.00080. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.51495/0.81256. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.48797/0.88203. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.46679/0.82502. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.46042/0.93015. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.46103/0.86680. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.44768/0.89517. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.45568/0.91736. Took 0.44 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.71340/0.70101. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.70248/0.70150. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.70592/0.68592. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69987/0.68757. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69884/0.68031. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.70248/0.69098. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.69434/0.67880. Took 0.49 sec\n",
      "Epoch 7, Loss(train/val) 0.69498/0.70393. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69344/0.68423. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68991/0.69069. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.69178/0.67272. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.67654/0.68873. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.70062/0.68801. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.68268/0.69445. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68463/0.69844. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68976/0.69347. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.68242/0.69791. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67734/0.68586. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.67456/0.69480. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67283/0.69607. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.67428/0.70454. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.67896/0.70615. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.68028/0.70171. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.67651/0.70715. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.68291/0.70692. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.67639/0.70798. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.66780/0.71260. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.66636/0.71596. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.67197/0.71419. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.66722/0.73393. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.67077/0.72927. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.65809/0.72120. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.66734/0.73888. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.66225/0.74954. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.66375/0.71095. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.66002/0.72447. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.66606/0.71836. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.65897/0.72466. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.65252/0.71739. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.65060/0.70811. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.65380/0.70507. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.63893/0.71675. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.64983/0.69614. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.63506/0.70904. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.64689/0.73171. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.62664/0.70228. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.63800/0.69251. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.62136/0.69061. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.61963/0.69675. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.61002/0.74714. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.60979/0.79192. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.61418/0.76938. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.59813/0.80230. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.58688/0.75872. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.59645/0.82413. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.58346/0.83324. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.59236/0.82447. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.59405/0.83930. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.57979/0.81158. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.57394/0.84950. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.56276/0.82311. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.56291/0.89227. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.57873/0.81452. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.56482/0.81488. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.55012/0.79272. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.54050/0.84520. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.54252/0.91034. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.54125/0.88806. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.54807/0.89541. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.53558/0.92361. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.52790/0.95831. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.50628/0.98257. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.52543/0.98755. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.50258/1.02891. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.51293/0.93563. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.49951/1.00971. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.50205/1.02859. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.49792/1.09489. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.49953/1.18294. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.49275/1.19237. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.49815/1.13228. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.48887/1.09908. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.48959/1.12256. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.47066/1.19138. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.46197/1.20129. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.47505/1.15889. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.47169/1.19145. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.46982/1.17740. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.45343/1.18875. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.46164/1.21306. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.45460/1.13121. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.43742/1.27356. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.44525/1.34037. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.46029/1.09490. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.44917/1.26131. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.45663/1.34476. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.42604/1.33553. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.42322/1.16425. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.41258/1.25044. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.40132/1.35336. Took 0.44 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.72253/0.69940. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.71449/0.70183. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.70556/0.71449. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.70640/0.71082. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69933/0.71085. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69928/0.71583. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68962/0.72046. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68908/0.72834. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68287/0.72063. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67960/0.73525. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68464/0.73737. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67945/0.73587. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67251/0.73452. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67575/0.74051. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66455/0.75246. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66109/0.74779. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66418/0.74386. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65588/0.75496. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66129/0.75727. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65737/0.75518. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64255/0.75830. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.64182/0.75972. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64537/0.75325. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64175/0.76092. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63496/0.75140. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63135/0.76849. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63323/0.79041. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62518/0.78570. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62107/0.77199. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61514/0.78130. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.63026/0.77179. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.61676/0.78247. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61129/0.80643. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.61564/0.80547. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60517/0.82266. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60124/0.83172. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.60482/0.83020. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59699/0.88336. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.58616/0.87184. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.57779/0.87939. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58906/0.88121. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58052/0.89586. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.58935/0.84274. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56675/0.88803. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.56484/0.88812. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56823/0.89915. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55222/0.94154. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.55273/0.97313. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.53796/0.94956. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.54279/1.01855. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.52775/1.02769. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.51439/1.05065. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52079/1.06206. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.50477/1.05743. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.49543/1.11307. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.49311/1.10137. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.50126/1.09489. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.49276/1.11217. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.48440/1.10054. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.48128/1.11835. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.47320/1.18541. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.46804/1.17086. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48301/1.15081. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.46762/1.09777. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.44991/1.14272. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.44686/1.16947. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.43845/1.11156. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.44320/1.20766. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.43913/1.21039. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.43640/1.17095. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.41380/1.20970. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.41447/1.26096. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.42578/1.27422. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.42164/1.24001. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.41925/1.11654. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.39677/1.22385. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.39245/1.17729. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.40296/1.27337. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.40074/1.14646. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.38510/1.19079. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.38307/1.16242. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.38365/1.22395. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.37925/1.19283. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.38674/1.23196. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.35944/1.29173. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.34082/1.27402. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.36562/1.20075. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.36032/1.34581. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.36364/1.23009. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.33081/1.28438. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.33638/1.28614. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.33194/1.27358. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.31772/1.25102. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.31107/1.34231. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.33875/1.30214. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.33266/1.29757. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.29885/1.35928. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.28688/1.35386. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.30544/1.36790. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.27895/1.38725. Took 0.45 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69530/0.69868. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.68587/0.70681. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.68746/0.72847. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.67779/0.72656. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.67760/0.73775. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.67349/0.74102. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68185/0.76059. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68025/0.76932. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67319/0.78378. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67272/0.80734. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.66614/0.81520. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66572/0.79639. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66370/0.80643. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66005/0.81836. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.66086/0.83319. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65900/0.85971. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65437/0.85833. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65450/0.85515. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.64399/0.87235. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64398/0.86219. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64186/0.87166. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64122/0.89304. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64105/0.86554. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.63420/0.88194. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63481/0.91448. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.64002/0.86388. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63870/0.88336. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62624/0.89058. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61744/0.92166. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.61409/0.92963. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.61679/0.86567. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.62017/0.89227. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.61246/0.88663. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60895/0.92371. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.60670/0.88511. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.60094/0.91458. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59922/0.87383. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.58558/0.90715. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.58296/0.96735. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.57197/0.98608. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56760/0.94296. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.57505/0.96973. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56339/0.97392. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.55951/1.02653. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.55749/1.01239. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55778/0.97500. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.55154/1.05766. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.54696/1.05306. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.54675/1.07934. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54232/1.02257. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.53972/1.05479. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54868/1.05332. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.52780/1.04524. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.52386/1.09014. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.51318/1.10872. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.51963/1.02997. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.51099/1.10842. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.49799/1.11956. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.49402/1.11544. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.47379/1.08066. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49045/1.07134. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.49276/1.12720. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.46651/1.09068. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.47285/1.13772. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.47638/1.16579. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.47174/1.10499. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.45913/1.12791. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.45920/1.17238. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.44888/1.07628. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.44992/1.14281. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.42748/1.19452. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.44350/1.12334. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.40908/1.15415. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.40833/1.27444. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.41310/1.23909. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.40252/1.29029. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.42509/1.20918. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.40862/1.23127. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.41789/1.16122. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.40273/1.28350. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.40513/1.26148. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.39514/1.23142. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.41982/1.20196. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.35954/1.21977. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.39304/1.24403. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.37691/1.16778. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.36372/1.26299. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.34863/1.24396. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.34986/1.22457. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.36622/1.31966. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36302/1.30145. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.38154/1.21743. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.36834/1.21985. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.36900/1.25933. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.34984/1.24022. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.36341/1.19771. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.34405/1.21734. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.35170/1.25654. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.33033/1.26347. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.35736/1.29129. Took 0.44 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.70827/0.70275. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70021/0.71109. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69606/0.71323. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68998/0.72022. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68483/0.71733. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68721/0.71909. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68335/0.71829. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68544/0.72975. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67275/0.74682. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67990/0.74461. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67488/0.74106. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66995/0.74529. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66416/0.77474. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66365/0.76887. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66707/0.77742. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66136/0.76045. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65696/0.78027. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65329/0.76857. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65465/0.76156. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65254/0.79074. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65365/0.79495. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64633/0.82380. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64398/0.80347. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64614/0.80551. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64478/0.82624. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63868/0.82722. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.62336/0.80963. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.62636/0.82370. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63231/0.85717. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62307/0.88317. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.62634/0.86018. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62082/0.85092. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61027/0.88045. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61728/0.86334. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.60652/0.85778. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60559/0.86900. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.60454/0.86400. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.60270/0.87321. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.58746/0.92540. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59720/0.92033. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.57970/0.95347. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57819/0.96207. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.57619/0.94447. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.56109/1.00454. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.57163/0.96351. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.57758/1.00595. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56341/1.01676. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.56839/1.01656. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.54761/1.08817. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56661/1.01118. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.55860/1.05154. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.54576/1.06809. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.53092/1.06897. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54807/1.10619. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.53184/1.12473. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.54207/1.13174. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.51647/1.12359. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.51836/1.14562. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.52603/1.19076. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52272/1.20730. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.51958/1.11770. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.50570/1.18746. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.50338/1.24615. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.51709/1.22685. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.49164/1.25861. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.48696/1.28621. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.48257/1.24579. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.47033/1.32748. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.45796/1.35230. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.47129/1.38878. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.47796/1.26523. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.45695/1.31379. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.48017/1.38002. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.44670/1.38004. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.44395/1.37125. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.45482/1.28487. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.44746/1.37016. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.43098/1.33883. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44060/1.37921. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.41078/1.45499. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.40299/1.50390. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.39988/1.44314. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.43649/1.45390. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.42854/1.35950. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.40063/1.41362. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.44491/1.42303. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.39370/1.47418. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.39591/1.47779. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.35258/1.52336. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.36943/1.59241. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.35000/1.68322. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.37064/1.54542. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.40785/1.42225. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.38047/1.54643. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.37331/1.52293. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.37110/1.41752. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.36968/1.45630. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.34413/1.61902. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.33918/1.56210. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.33972/1.60215. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69462/0.69623. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.68575/0.70443. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.68873/0.71668. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68603/0.72622. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68037/0.73656. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68337/0.73857. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.67656/0.74110. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67990/0.73849. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68047/0.74566. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67606/0.74729. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67697/0.74617. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67447/0.74467. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66915/0.75408. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66879/0.75226. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66630/0.74694. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66671/0.75696. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.66032/0.76176. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66336/0.75363. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66030/0.74778. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65254/0.78983. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64866/0.79141. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64671/0.78545. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64859/0.77327. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.64477/0.78008. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64007/0.78553. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.63141/0.79352. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63258/0.80515. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62208/0.79293. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62997/0.79419. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61697/0.80881. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61881/0.81233. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.60859/0.80937. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60993/0.82412. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.59832/0.83679. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59905/0.82379. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.59041/0.84900. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59040/0.84826. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.58528/0.85580. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.57526/0.85324. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.56805/0.88876. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56915/0.86721. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.55212/0.89519. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55138/0.91617. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.54127/0.92663. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.53325/0.95045. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.52571/0.96831. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.51533/0.98178. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.52251/0.99110. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.51845/1.02511. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.49771/1.05775. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.49742/1.04611. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.49126/1.06627. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.47530/1.10584. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.47635/1.11589. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.47998/1.13358. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.47701/1.13288. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.45880/1.17077. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.47202/1.20124. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.47183/1.19737. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.44800/1.21157. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.44499/1.19842. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.43845/1.22289. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.42369/1.24570. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.42733/1.27863. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.41957/1.27180. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.41832/1.30972. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.39723/1.30379. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.38832/1.38806. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.40803/1.35014. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.39948/1.34141. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.40675/1.40279. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.39395/1.31047. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.37923/1.39758. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.35889/1.42477. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.37096/1.41142. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.34984/1.49076. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.35574/1.50864. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.33467/1.58446. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.34865/1.50626. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.35614/1.47463. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.34814/1.60703. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.36382/1.64641. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.36018/1.60631. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.35797/1.59524. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.33256/1.57699. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.30535/1.58715. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.31912/1.55088. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.32496/1.54792. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.33138/1.65104. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.31357/1.64464. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.29466/1.64728. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.31363/1.65681. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.29495/1.60537. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.30725/1.69843. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.27734/1.78013. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.26431/1.79783. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.25995/1.80888. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.26923/1.82188. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.28772/1.80351. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.26521/1.83464. Took 0.44 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69145/0.72484. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68763/0.71488. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68295/0.72741. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68600/0.73681. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68117/0.73867. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.67756/0.74348. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.67771/0.75324. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67506/0.76542. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67118/0.76829. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66558/0.78205. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67169/0.78364. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67210/0.79056. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66008/0.81865. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66051/0.81505. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.65758/0.82137. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65114/0.82785. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.65457/0.82878. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.64847/0.84496. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.64743/0.86157. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64703/0.86441. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63894/0.86106. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63858/0.86237. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64057/0.87140. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64165/0.88234. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63636/0.89747. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63158/0.91594. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62255/0.93964. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.61512/0.94527. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61167/0.95455. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60183/0.98731. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.60396/0.97769. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.61023/0.95456. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61369/0.94863. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59507/0.97355. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.58781/0.99092. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.58852/0.97186. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.57468/1.01444. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57319/0.96343. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.57899/0.95097. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.55313/0.99064. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.55947/0.96480. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.55589/0.96292. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.54491/0.98856. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.54701/1.00170. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.53729/1.00358. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.53251/1.03920. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.52334/1.02941. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.50428/1.04122. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.50399/0.99543. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.49271/1.02941. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.49587/1.03612. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.48084/1.07986. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.47023/1.04783. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.46324/1.07178. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.47854/1.04498. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.46310/1.08684. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.44500/1.14144. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.45082/1.08347. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.43468/1.16032. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.42346/1.16717. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.42776/1.15099. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.43329/1.16404. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.40616/1.16484. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.39459/1.21806. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.42355/1.22985. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.41524/1.22119. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.39439/1.25161. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.38136/1.23592. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.37534/1.27177. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.37384/1.28956. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.37264/1.35984. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.36586/1.34435. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.36731/1.39971. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.34373/1.37788. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.34575/1.39303. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.31517/1.37220. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.32841/1.40380. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.36279/1.38421. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.32033/1.26532. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.30375/1.42025. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.30825/1.44122. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.30920/1.45053. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.34839/1.33929. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.33136/1.36461. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.30626/1.42955. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.32490/1.50869. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.29178/1.54538. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.30017/1.48207. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.25814/1.51952. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.26477/1.55511. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.26482/1.57409. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.30781/1.46999. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.28581/1.58495. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.27319/1.58907. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.29613/1.57658. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.25803/1.59591. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.23567/1.67252. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.25338/1.62864. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.25713/1.67826. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.23012/1.67803. Took 0.45 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.70639/0.71761. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69412/0.71359. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69323/0.72405. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69124/0.73260. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68459/0.74794. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68132/0.76238. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68039/0.76999. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68317/0.77973. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67808/0.78757. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67661/0.78586. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67974/0.77597. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67403/0.78658. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66506/0.77939. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66732/0.79458. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66560/0.78945. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.66373/0.79278. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65547/0.78767. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65955/0.78818. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65428/0.80516. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65078/0.79481. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64728/0.79864. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65354/0.79289. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65161/0.80186. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63698/0.78921. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64221/0.79681. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65007/0.79123. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63514/0.80361. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62969/0.82644. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62957/0.84311. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62736/0.84873. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62231/0.85352. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62160/0.84372. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.62238/0.84355. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61629/0.85189. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61089/0.84658. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.61058/0.86796. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.59861/0.89177. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.61441/0.91266. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60695/0.90931. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59202/0.90605. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59536/0.91358. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58808/0.89648. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57598/0.92372. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.57842/0.92137. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56807/0.90392. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.57172/0.91149. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.57301/0.89111. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.54774/0.90700. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.54781/0.97639. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.56173/0.92760. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54086/0.89159. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53963/1.00527. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.53762/0.89302. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54438/0.94005. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.52272/0.95217. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.50273/1.04257. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.50510/0.99968. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.50473/0.98008. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50391/1.01042. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50294/1.03878. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.49737/0.98500. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.49442/0.98643. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49244/0.99928. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48595/0.91798. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49627/0.96141. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.46411/0.98493. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51236/1.00564. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.52241/0.97765. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.50498/0.97361. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.47598/0.98152. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.47775/1.00360. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.47213/1.02512. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46119/1.03509. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.46837/1.01255. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.45274/1.06657. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.44896/1.07117. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.45235/1.04382. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.43331/1.09907. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44872/1.09121. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.42118/1.13330. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.44088/1.08880. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.43559/1.10209. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.41670/1.14978. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.41806/1.15534. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.40508/1.18263. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.43127/1.15347. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.39822/1.19412. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.41014/1.17311. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.39725/1.17195. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.38787/1.19178. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.39311/1.18806. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.39457/1.21378. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.39205/1.21722. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.37050/1.17636. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.38882/1.25663. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.37806/1.22940. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.35704/1.23935. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.36044/1.29959. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.36245/1.29187. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.36631/1.31870. Took 0.45 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70734/0.69734. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69395/0.70382. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69242/0.70905. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69204/0.71088. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68868/0.70923. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68372/0.72085. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68422/0.72502. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.68183/0.73033. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68232/0.72570. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67947/0.71260. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68043/0.71606. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68015/0.71743. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67413/0.71061. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67674/0.71035. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67423/0.71175. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67639/0.71383. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.67504/0.73133. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.67071/0.74360. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66921/0.75000. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67031/0.73276. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66645/0.72861. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66064/0.71702. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65995/0.71323. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66204/0.72890. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.65421/0.73110. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65372/0.73557. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65339/0.73462. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64811/0.74766. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64660/0.77043. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.65277/0.76044. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63871/0.75972. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.64182/0.78417. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63854/0.77059. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64194/0.76570. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.63185/0.80147. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.63511/0.79312. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62449/0.78676. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.62331/0.78088. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.64145/0.79136. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.62339/0.77959. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62318/0.82930. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61715/0.78524. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.62654/0.79716. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61277/0.78750. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.62582/0.80211. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.60757/0.78771. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.61344/0.78135. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.61453/0.79649. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59767/0.80523. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.59402/0.78783. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.60292/0.82986. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.60109/0.82652. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.59147/0.82295. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.58512/0.85652. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.57092/0.83996. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.59206/0.84874. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.58010/0.82730. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.58790/0.83375. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.57418/0.84124. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.56564/0.85819. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.56726/0.85124. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.57893/0.87186. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54573/0.85797. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.55381/0.82531. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.55453/0.83994. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.53538/0.89105. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.54937/0.83947. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.53594/0.86137. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.54435/0.82340. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.53992/0.82763. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.53304/0.81505. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.51939/0.82398. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.52248/0.83488. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.53094/0.83491. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.52255/0.85780. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.52118/0.82289. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.52327/0.82035. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.49884/0.77934. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.51346/0.76024. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.49887/0.81429. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.51880/0.84046. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.49299/0.77879. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.48488/0.85046. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.49151/0.77297. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.48572/0.81505. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.47778/0.82516. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.46233/0.87422. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.48513/0.81581. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.45892/0.85411. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.47030/0.86388. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.46454/0.81061. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.43016/0.86973. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.47070/0.83698. Took 0.47 sec\n",
      "Epoch 93, Loss(train/val) 0.48241/0.78285. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.45069/0.80113. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.44313/0.84707. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.42968/0.85791. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.43184/0.84806. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.42720/0.89111. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.43287/0.89951. Took 0.45 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.71598/0.71365. Took 0.63 sec\n",
      "Epoch 1, Loss(train/val) 0.70476/0.69168. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69729/0.68616. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69590/0.70896. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69907/0.69751. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69532/0.71895. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69349/0.70273. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69380/0.69264. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.69093/0.70292. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68678/0.71254. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68537/0.72878. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68479/0.71631. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68336/0.72695. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68333/0.72430. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67816/0.72202. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67456/0.74301. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.67303/0.72614. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66861/0.72957. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.67232/0.74697. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66894/0.74665. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66068/0.76969. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65805/0.75816. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65592/0.78812. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.65111/0.79153. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64928/0.80245. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.65421/0.79383. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65152/0.80583. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64586/0.81410. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.64035/0.82176. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.64228/0.82364. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63914/0.80947. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.63563/0.83671. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.63602/0.83890. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.63359/0.82341. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.62148/0.84458. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.62103/0.83279. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61497/0.85982. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62650/0.88190. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.61716/0.89515. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.61637/0.89012. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.61283/0.84376. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61175/0.88481. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59849/0.86071. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60803/0.85520. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60447/0.92972. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.59881/0.85257. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.59275/0.91734. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.58502/0.96529. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.58932/0.97134. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.58426/0.94556. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.58567/0.95884. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.57725/0.97994. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.58121/0.93942. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.57768/0.98180. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.57833/0.95280. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.57154/0.95778. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55354/1.03472. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.56074/1.04597. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.55364/0.99908. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.56461/1.06496. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.54484/1.01967. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.55638/1.05180. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.55649/1.03538. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.55692/1.03870. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.54917/1.01782. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.54587/1.05642. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.55153/1.06496. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.52802/1.12770. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.53942/1.12020. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.53448/1.05862. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.52533/1.17214. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.52984/1.08865. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.51885/1.09252. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.51272/1.13411. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.52067/1.14873. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.50585/1.11771. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.55561/1.11060. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.53020/1.06394. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.52251/1.09519. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.49979/1.07514. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.50056/1.12845. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.49440/1.11172. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.49689/1.11271. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.48484/1.16022. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.47429/1.21461. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.48550/1.17382. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.47721/1.25890. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.49208/1.19964. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.45602/1.18545. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.47051/1.22181. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.47732/1.23539. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.47673/1.18405. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.46450/1.20287. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.44632/1.21620. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.46043/1.22715. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.44862/1.27279. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.44856/1.24192. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.44967/1.22390. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.43750/1.23877. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.45412/1.33431. Took 0.45 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.69837/0.69741. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69470/0.69883. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.68584/0.71690. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68336/0.70838. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68509/0.73119. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68044/0.72980. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67847/0.75232. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67183/0.78254. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67682/0.78617. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.66812/0.80672. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.67634/0.79016. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66069/0.82641. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66983/0.82879. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66550/0.83377. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66647/0.83001. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66230/0.86971. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66333/0.86573. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66125/0.87223. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65185/0.85885. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65419/0.86187. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.64745/0.88283. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64256/0.91239. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64254/0.92682. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64111/0.92949. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63397/0.92718. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.63517/0.93071. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.63549/0.93601. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62462/0.95576. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63358/0.92268. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62704/0.94514. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.62290/0.95834. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.62494/0.92982. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.61672/0.94291. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.61418/0.95341. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60858/0.93848. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61280/0.95343. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60266/0.94580. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60137/0.96981. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59986/0.97961. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59859/0.98985. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.58440/0.99811. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.58337/1.00945. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.58901/0.97817. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.58418/1.02982. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.57460/1.08215. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.57742/1.02495. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.57026/0.99619. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57252/1.01875. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.56398/1.01704. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56468/1.00934. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.55116/1.04609. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.56736/0.99070. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54483/1.06130. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.54609/1.03812. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55806/1.06959. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.53580/1.07998. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53982/1.08942. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.54651/1.08658. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.52449/1.11730. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.52127/1.09073. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.52995/1.06439. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.52219/1.06499. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.50480/1.07920. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51101/1.20456. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.49707/1.13046. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.49055/1.15668. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48867/1.18822. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.49641/1.14667. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.46660/1.21994. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.48327/1.17349. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.47789/1.24909. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.47353/1.25692. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.45296/1.18722. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.45774/1.20704. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.46607/1.23186. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.45356/1.22439. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.44030/1.23696. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.44154/1.29597. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.42956/1.30258. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44589/1.26153. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.41278/1.36221. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.41389/1.29429. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.43429/1.29276. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40934/1.42213. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.42220/1.32166. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.40233/1.35448. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.39191/1.30581. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.39120/1.45664. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.47079/1.32825. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.44706/1.32275. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.42060/1.21881. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.37695/1.26176. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.38947/1.32398. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.38194/1.44976. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.35849/1.40350. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.36957/1.39132. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.35960/1.36345. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.36253/1.29328. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.32814/1.45368. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.33788/1.49137. Took 0.44 sec\n",
      "ACC: 0.3958333333333333\n",
      "Epoch 0, Loss(train/val) 0.71066/0.71908. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69796/0.73451. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69320/0.72795. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69101/0.72450. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69169/0.72197. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68666/0.73231. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68682/0.73931. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68229/0.75073. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68050/0.76018. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67630/0.75963. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.66910/0.77270. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67185/0.77788. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67308/0.77942. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.66853/0.79357. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66010/0.77787. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66190/0.77939. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65884/0.77788. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65856/0.78297. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65472/0.79265. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66259/0.75986. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65625/0.77385. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65542/0.77634. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65355/0.78034. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64237/0.80098. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64564/0.80388. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64367/0.80723. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64267/0.77753. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64240/0.77190. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.63819/0.77332. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62853/0.78810. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63214/0.77804. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63090/0.77296. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.62249/0.76892. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62246/0.79335. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.61549/0.79792. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61431/0.79248. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61268/0.79882. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59781/0.82571. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.60340/0.83449. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.59281/0.83275. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.59776/0.84299. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59939/0.86870. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58566/0.88402. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.59377/0.87240. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59428/0.87934. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.58194/0.89625. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.58884/0.89699. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.58241/0.92681. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.57451/0.92718. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.57503/0.89834. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.57664/0.91927. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.56153/0.93318. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56819/0.89599. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.56473/0.92136. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56110/0.92950. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.56274/0.96629. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53787/0.96237. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54477/0.95986. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.53262/1.01329. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54472/0.95254. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.53685/0.96419. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.53373/0.95356. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.52325/1.00727. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.52473/1.01489. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.53064/1.00365. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.51876/1.00642. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.51348/1.01746. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.50673/1.04514. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50733/1.00724. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.51208/1.03217. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48889/1.04513. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.49664/1.06502. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.48036/1.07628. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48525/1.14484. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.47384/1.13598. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.47123/1.10515. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.47335/1.13531. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.46177/1.09963. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.47496/1.16662. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.45215/1.19009. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.43267/1.17956. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.44104/1.15947. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.43183/1.25073. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.41962/1.18609. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.44247/1.20527. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.42182/1.17940. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.41921/1.16018. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.41955/1.23307. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.41352/1.20139. Took 0.47 sec\n",
      "Epoch 89, Loss(train/val) 0.42608/1.22835. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.41793/1.22741. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.39229/1.21979. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.41578/1.24200. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.39524/1.20318. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.39200/1.26245. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.39701/1.23182. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.37152/1.28355. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.38264/1.30860. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.37776/1.25715. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.35822/1.34035. Took 0.46 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.70150/0.69006. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69848/0.70467. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69628/0.70740. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69553/0.71744. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69485/0.71675. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69332/0.72526. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68956/0.72606. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69210/0.73196. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68884/0.73295. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68811/0.72592. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68018/0.74570. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.68597/0.74685. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68224/0.74825. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68102/0.74058. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67667/0.73220. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67601/0.73730. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67434/0.73707. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67218/0.74193. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66486/0.75249. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66209/0.75485. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65940/0.75344. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65769/0.76172. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65099/0.76342. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65223/0.75444. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64887/0.76395. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64019/0.76632. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.63854/0.76103. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63569/0.78223. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63399/0.77260. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63575/0.76623. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62597/0.76978. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.62521/0.78693. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62622/0.78023. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.62106/0.78549. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61618/0.78861. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61233/0.78863. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.61217/0.79394. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60438/0.82095. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.59794/0.83070. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59586/0.83195. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.59894/0.82843. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58289/0.84648. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60631/0.83647. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58939/0.84248. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.58683/0.86201. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.58873/0.85525. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.57865/0.86823. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.58331/0.84654. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.57856/0.85733. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.58057/0.85357. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.57977/0.88452. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56282/0.93775. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.56496/0.90432. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55761/0.93268. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.56064/0.93232. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.55390/0.95609. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55345/1.01401. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.54147/1.04039. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.53800/1.05256. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.54722/1.03535. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.55243/1.04418. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.54743/1.03531. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52733/1.06328. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.52142/1.10756. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.52391/1.11804. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.52702/1.09363. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.52955/1.17209. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.52046/1.16041. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.51188/1.18514. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.50891/1.20895. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.52217/1.13116. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.51091/1.16686. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.50494/1.13088. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.50571/1.13456. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.48588/1.21048. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.49846/1.25566. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.48657/1.22406. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.47519/1.25916. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.48467/1.25210. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.47204/1.25892. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.46361/1.24880. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.46158/1.32603. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.45350/1.25378. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.46084/1.29035. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.46311/1.30424. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.45846/1.26411. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.44715/1.26324. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.46240/1.29336. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.44670/1.28775. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.41975/1.26657. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.43384/1.33676. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.40726/1.27375. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.42259/1.33561. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.41427/1.31665. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.41470/1.44814. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.40844/1.48994. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.43545/1.37101. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.41366/1.43377. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.39343/1.39701. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.40145/1.45325. Took 0.44 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.70688/0.69551. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69799/0.69747. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69515/0.69718. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69326/0.69533. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.69388/0.69935. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68878/0.69819. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68492/0.70432. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68332/0.70292. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68594/0.70047. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68606/0.70260. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68004/0.70673. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68607/0.70709. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.68119/0.71768. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67610/0.70160. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68214/0.70503. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.68130/0.70514. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.68881/0.70387. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.68247/0.70649. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67807/0.70364. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67718/0.70468. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.67005/0.70809. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.67444/0.70836. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.67335/0.71400. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.67047/0.71558. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.66348/0.72279. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65352/0.73008. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.66505/0.72726. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.65915/0.72521. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65528/0.73263. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.64641/0.73701. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.65067/0.74575. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.65652/0.73309. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64304/0.74615. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64597/0.77228. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.64155/0.76615. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.63603/0.75341. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62715/0.76692. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.62497/0.77803. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.62300/0.78068. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61806/0.78359. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62029/0.78799. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60869/0.79268. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.60865/0.80189. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60616/0.81332. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60093/0.79131. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.60914/0.77606. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59911/0.79352. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.59870/0.79145. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59252/0.81578. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.59267/0.82764. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.58433/0.83446. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.57224/0.87540. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.57026/0.89078. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.56230/0.85105. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.56451/0.87676. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.55727/0.87816. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55224/0.93931. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.54958/0.86485. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.54155/0.91019. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54567/0.92649. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54994/0.91658. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.53342/0.97463. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53093/0.95162. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.54931/0.97430. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50982/0.96350. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50369/1.04420. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.53196/0.99814. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.50732/1.01946. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.51012/1.03948. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.50476/1.01719. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49991/1.03085. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.48487/1.01643. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.49037/1.02200. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.48809/1.00648. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.47042/1.05513. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.46724/1.00788. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.45591/1.07132. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.45957/1.05396. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44753/1.06878. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.45892/1.05686. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.44317/1.03084. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.43446/1.02084. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.44574/1.09995. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.43088/1.06295. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.43178/1.01896. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.42514/1.08479. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.41751/1.13150. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.41139/1.09552. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.42331/1.05856. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.39735/1.14772. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.40122/1.18868. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.37552/1.16068. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.39069/1.20994. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.37065/1.17492. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.37848/1.18310. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.36472/1.21338. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.35333/1.22842. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.38031/1.22485. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.36139/1.16304. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.39090/1.20779. Took 0.45 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.70701/0.71284. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69848/0.70423. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69642/0.70375. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69063/0.69546. Took 0.49 sec\n",
      "Epoch 4, Loss(train/val) 0.68886/0.69779. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68870/0.69597. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.68335/0.70464. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68038/0.70557. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67998/0.70242. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68268/0.70160. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67493/0.70886. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67889/0.71410. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66877/0.72255. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.66620/0.72499. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66180/0.74180. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66247/0.73864. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65945/0.74050. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66219/0.74946. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65355/0.74778. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.64833/0.75251. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65250/0.75858. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64697/0.77155. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64783/0.78451. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.63663/0.79963. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62676/0.79827. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.62195/0.81628. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62528/0.82129. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.63027/0.82705. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62223/0.83295. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61694/0.82147. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61389/0.83391. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.61352/0.83343. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.60035/0.84732. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.59543/0.82833. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.58754/0.85700. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.59160/0.86931. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58411/0.84565. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.57037/0.87389. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.56759/0.86318. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.56824/0.85062. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.55661/0.86060. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.55997/0.86543. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55681/0.86476. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.54657/0.86385. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.53592/0.88489. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.54356/0.88395. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.53924/0.89782. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.53132/0.87546. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.51955/0.89589. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52172/0.91437. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.52324/0.91759. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.49701/0.94164. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.50425/0.93980. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.50967/0.90233. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.48997/0.93296. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.49218/0.91410. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.47860/0.96239. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.47395/0.96050. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.48543/0.92735. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.47188/0.94034. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.46535/0.93042. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.45090/0.95250. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.46085/0.95294. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.46256/0.95102. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.42849/0.96673. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.43324/0.97629. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.43454/0.96489. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.43806/0.94482. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.41320/0.91639. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.41595/0.97272. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.44150/0.96831. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.40848/0.97490. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.39518/0.99338. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.40107/1.01112. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.39562/1.00302. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.42039/0.97571. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.37853/0.98558. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.36480/1.06251. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.38013/1.07258. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.35727/1.02676. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.34812/1.02439. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.35241/1.00746. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.34616/1.08998. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.34777/1.07783. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.37434/1.08384. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.33182/1.02802. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.30951/1.05535. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.32034/1.10322. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.29735/1.13693. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.31846/1.11737. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.29254/1.16264. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.28097/1.11834. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.28487/1.15257. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.28318/1.03465. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.26825/1.20131. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.28138/1.24592. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.28262/1.19630. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.27494/1.15343. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.27288/1.15520. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.25909/1.18688. Took 0.45 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.71464/0.72075. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70735/0.72329. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69462/0.69448. Took 0.49 sec\n",
      "Epoch 3, Loss(train/val) 0.69109/0.68660. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68774/0.70558. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68945/0.69312. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68833/0.71086. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68794/0.69786. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68701/0.70725. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67938/0.70683. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67777/0.70962. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67058/0.71388. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67588/0.71537. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66925/0.72338. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67514/0.72395. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66211/0.72781. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66037/0.73945. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.65703/0.76376. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65418/0.76616. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66277/0.77073. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65002/0.79441. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65648/0.80349. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64396/0.77483. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64684/0.80311. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.64346/0.80479. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.64928/0.81414. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63678/0.82291. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64213/0.83816. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.63804/0.85054. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.63834/0.85211. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63727/0.84048. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62757/0.85088. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.63317/0.86100. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61803/0.85781. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62275/0.88433. Took 0.47 sec\n",
      "Epoch 35, Loss(train/val) 0.60739/0.86098. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61080/0.90849. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61881/0.90683. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.61361/0.94863. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60101/0.91936. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.58792/1.01238. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59497/1.01103. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59904/0.90504. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.59700/0.93094. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59660/0.95362. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.58274/0.95334. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.58967/0.95880. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57488/1.02450. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57396/0.99659. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.57399/1.01908. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56874/1.03945. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.56706/1.04937. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.55806/1.00663. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56345/1.01470. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.56195/1.09375. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.55860/1.02427. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56000/1.03332. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.55753/1.01983. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.55227/1.03953. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54862/1.04344. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.54802/1.06647. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.54151/1.00986. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54120/1.00345. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.53358/1.06000. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.53404/1.03684. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.53379/0.97167. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.52264/1.03851. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.51664/1.00801. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.51023/1.09691. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.52183/1.01094. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.51681/1.03215. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.50175/1.01658. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.49771/1.05851. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.49755/1.07187. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.50521/1.08876. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.50762/1.08207. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.49706/1.06539. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.47481/1.10621. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.47267/1.12021. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46787/1.17519. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.47560/1.12744. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.47046/1.15915. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.46091/1.13554. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.48032/1.14458. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.46090/1.16716. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.45583/1.22908. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.44616/1.24284. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.47178/1.27065. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.45977/1.24404. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.45719/1.25499. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.42915/1.25441. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.43621/1.32447. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.43105/1.33968. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.43798/1.29018. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.42498/1.33995. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.44588/1.22709. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.42904/1.28357. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.43473/1.32070. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.40266/1.39453. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.42146/1.28200. Took 0.45 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.71562/0.75696. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.71074/0.75535. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70209/0.73959. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.70154/0.73932. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69545/0.73333. Took 0.49 sec\n",
      "Epoch 5, Loss(train/val) 0.69013/0.74466. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69294/0.76521. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69108/0.75160. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68750/0.76738. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68489/0.75478. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68851/0.75784. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67638/0.76852. Took 0.47 sec\n",
      "Epoch 12, Loss(train/val) 0.67622/0.76468. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67438/0.79796. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67640/0.75935. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.67727/0.80169. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66704/0.76993. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67362/0.80166. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.66871/0.80366. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66378/0.80890. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66668/0.79034. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.66826/0.80274. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65690/0.83438. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66019/0.82394. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.66459/0.81930. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.66505/0.80142. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.65569/0.85641. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.65102/0.82496. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.64820/0.84860. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65325/0.84415. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64380/0.86395. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.64741/0.89606. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64557/0.89679. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63683/0.89715. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.63280/0.91354. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.64423/0.89424. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.63210/0.92478. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.64280/0.92796. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62589/0.90598. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.62220/0.93490. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61735/0.90986. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61657/0.92853. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.60924/0.94972. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.62217/0.92917. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.60360/0.89811. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.60533/0.90591. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.60197/0.90033. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.60138/0.95562. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59975/0.93795. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.59112/0.91731. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.59009/0.93447. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.59360/0.91576. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.58199/0.94560. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.57148/1.00902. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.58219/0.93355. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.57373/0.97376. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.58981/0.98870. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.56078/1.03722. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.56024/0.94769. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.55081/1.02969. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55866/0.99526. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.55582/1.02032. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.55208/1.08618. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.54890/1.03310. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.53050/1.06356. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.54180/1.02828. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.52296/1.05122. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.53410/1.08825. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.51496/1.07222. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.52251/1.16481. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.51677/1.18447. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.52371/1.09493. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.50727/1.18339. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.49627/1.18565. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.48888/1.11782. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48574/1.14924. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.48348/1.22079. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.49752/1.13614. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.47155/1.13955. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.49541/1.18283. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.47505/1.17280. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.44926/1.28793. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.47438/1.21547. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.47679/1.18490. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.44023/1.24008. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.45034/1.29222. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.45941/1.30152. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.46521/1.38029. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.48960/1.26622. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.44909/1.24411. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.42345/1.33510. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.41229/1.33150. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.41213/1.26684. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.43514/1.29762. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.43903/1.27613. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.43322/1.13861. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.43634/1.11213. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.43480/1.32140. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.44078/1.21783. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.42460/1.25648. Took 0.45 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.73052/0.71033. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70957/0.70501. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.70251/0.71327. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70148/0.70020. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.69664/0.69508. Took 0.50 sec\n",
      "Epoch 5, Loss(train/val) 0.70354/0.70607. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.69497/0.70573. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69207/0.69319. Took 0.48 sec\n",
      "Epoch 8, Loss(train/val) 0.68479/0.72469. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67934/0.72203. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68891/0.73449. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68477/0.71329. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.68193/0.74180. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67956/0.73322. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67757/0.74554. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68340/0.74706. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67203/0.75773. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.67424/0.74122. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67482/0.74725. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.68102/0.73831. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.66919/0.73220. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66626/0.74080. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65999/0.72525. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66704/0.72252. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66418/0.72664. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.66275/0.70933. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.66238/0.72952. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.66021/0.74302. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.65737/0.76051. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.65864/0.77239. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.65324/0.78987. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64308/0.78617. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.64316/0.79095. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64596/0.74616. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.64706/0.76427. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.63821/0.76407. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.63552/0.79089. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.62413/0.78843. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.63586/0.80686. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.62879/0.77477. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62464/0.79770. Took 0.47 sec\n",
      "Epoch 41, Loss(train/val) 0.62325/0.78876. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.61743/0.80837. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61873/0.80969. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60759/0.85301. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.60748/0.85989. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.61367/0.83863. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.60319/0.86702. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58879/0.87390. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.59700/0.87176. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.59422/0.88721. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.58575/0.92425. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.59226/0.87987. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.57135/0.90839. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.58574/0.90731. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.57101/0.90016. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56215/0.90202. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.55862/0.93611. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.58151/0.98486. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.58067/0.95272. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.56106/0.95889. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.56378/0.97167. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.55348/0.94653. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.55599/0.93256. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.53970/0.99582. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.55146/0.94443. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.55444/1.05695. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.55979/1.02276. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.53711/1.00688. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.53975/1.01861. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.53303/1.00747. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.52835/0.99362. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.52266/0.97281. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.52679/1.00941. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.52823/1.04197. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.51483/1.02901. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.51158/1.03781. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.51712/1.06224. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.51293/1.04456. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.50908/1.05943. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.51451/1.06778. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.52592/1.06204. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.50465/1.03845. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.50028/1.09496. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.48939/1.07449. Took 0.47 sec\n",
      "Epoch 85, Loss(train/val) 0.47382/1.17197. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.47332/1.08925. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.48509/1.10317. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.47394/1.18091. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.47809/1.12994. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.49169/1.14505. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.47265/1.18386. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.46716/1.24601. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.47049/1.19743. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.48773/1.17156. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.49181/1.15936. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.47059/1.22562. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.46183/1.21188. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.45507/1.27779. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.44454/1.27475. Took 0.46 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.70707/0.71311. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70497/0.70708. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69367/0.71384. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69422/0.71227. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69253/0.72121. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69306/0.70713. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69294/0.72658. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.68772/0.71371. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68970/0.70971. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68280/0.71938. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.68481/0.71632. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68495/0.70576. Took 0.49 sec\n",
      "Epoch 12, Loss(train/val) 0.67790/0.71483. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67987/0.72243. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.67339/0.72554. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67239/0.71878. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.67334/0.73002. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67185/0.73322. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66562/0.74287. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66663/0.71806. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67135/0.75032. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65756/0.73663. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65548/0.74788. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.65489/0.73660. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65420/0.75092. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64724/0.76111. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.64944/0.74436. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64556/0.75599. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.64143/0.77720. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63839/0.77626. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63450/0.77700. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62421/0.78609. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63126/0.80335. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62277/0.80899. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61914/0.80932. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.62370/0.81499. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.61642/0.83692. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.62645/0.82487. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61934/0.80404. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.61579/0.80301. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62063/0.80972. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.61366/0.80281. Took 0.47 sec\n",
      "Epoch 42, Loss(train/val) 0.61218/0.80845. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.59923/0.82550. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60397/0.84464. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.60259/0.83945. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59634/0.86231. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.60271/0.86333. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.58369/0.89713. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.59263/0.88698. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.57393/0.90928. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.56821/0.91841. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.55980/0.93038. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.57831/0.93160. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56294/0.97554. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.56309/0.97217. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55921/0.96465. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.54064/0.98491. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54326/1.00269. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.54285/0.94706. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.56273/0.94958. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.53799/0.98841. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.54297/0.99193. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51921/1.01810. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.52011/1.02533. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.52599/1.03390. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.52154/1.05888. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.52087/1.01029. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.53613/1.05069. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.52170/1.07746. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.51969/1.03785. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.52277/1.09673. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.50260/1.08799. Took 0.47 sec\n",
      "Epoch 73, Loss(train/val) 0.51173/1.13025. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.50236/1.10230. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48752/1.15239. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.49275/1.09190. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.48334/1.11603. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.49073/1.08428. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.50371/1.09030. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.48045/1.02715. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.49896/1.12707. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.50954/1.07679. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.49406/1.09029. Took 0.47 sec\n",
      "Epoch 84, Loss(train/val) 0.48169/1.08613. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.48589/1.12746. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.45847/1.14915. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.44888/1.17176. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.45207/1.18430. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.45344/1.19766. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.44627/1.22278. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.42536/1.25832. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.43744/1.29491. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.42564/1.27201. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.42711/1.29730. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.42290/1.27138. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.42245/1.31751. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.43609/1.22750. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.43161/1.26486. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.41531/1.25466. Took 0.45 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69048/0.69528. Took 0.60 sec\n",
      "Epoch 1, Loss(train/val) 0.68518/0.71032. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.67801/0.70959. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.67592/0.71630. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.68096/0.72228. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.67464/0.71289. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.67062/0.71572. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.67625/0.71325. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67243/0.70138. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66983/0.70208. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.66089/0.69166. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.65474/0.71212. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.65657/0.70171. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65470/0.70634. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.64470/0.70994. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.64538/0.72242. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.63048/0.74374. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.62238/0.76126. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.61807/0.78106. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.61041/0.77569. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.61738/0.80284. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.60806/0.80048. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.60189/0.80649. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.58899/0.82567. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.58868/0.83206. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.58364/0.83118. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.58412/0.84122. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.57200/0.85229. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.57980/0.85818. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.56257/0.85768. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.56341/0.85178. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.56368/0.83983. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.56934/0.87481. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.54127/0.89105. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.52479/0.88609. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.53190/0.89043. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.53576/0.91264. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.53939/0.88252. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.53050/0.87518. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.51594/0.90537. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.51751/0.86643. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.50808/0.89304. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.50498/0.93804. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.50944/0.91329. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.47763/0.92598. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.46795/0.94674. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.48172/0.91616. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.48371/0.95075. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.45683/0.94942. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.48846/0.92663. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.45934/0.99311. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.46233/0.95135. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.47320/0.93334. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.44013/0.97930. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.43133/0.96447. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.42915/0.99837. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.41345/0.98292. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.42214/1.06229. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.45065/0.90383. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.41573/0.93209. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.42101/0.97776. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.42600/1.01848. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.39546/1.01793. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.41547/0.99357. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.37860/1.02047. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.36354/1.05419. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.35700/1.10060. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.34236/1.21624. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.34981/1.08537. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.35520/1.02486. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.33972/1.11721. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.34279/1.14992. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.35280/1.09792. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.32806/1.09589. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.33984/1.01996. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.32911/1.13439. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.33224/1.12953. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.33905/1.09926. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.28567/1.09794. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.32452/1.06120. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.28546/1.08506. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.29465/1.16082. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.32371/1.08153. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.30196/1.17206. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.26269/1.20607. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.27189/1.26367. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.25723/1.15211. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.26398/1.28132. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.23301/1.18101. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.25648/1.27548. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.26793/1.34426. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.25391/1.24098. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.24043/1.22861. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.25287/1.32356. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.23584/1.25091. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.22209/1.25514. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.27794/1.30249. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.25793/1.20979. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.25697/1.16501. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.24042/1.23979. Took 0.44 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.71177/0.70701. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70695/0.69803. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69117/0.71725. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69341/0.70363. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.68870/0.70425. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68597/0.70758. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68222/0.70359. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67614/0.70507. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67241/0.71764. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67640/0.70625. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67593/0.71844. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67571/0.71254. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66947/0.72341. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67149/0.71576. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66606/0.72316. Took 0.47 sec\n",
      "Epoch 15, Loss(train/val) 0.67671/0.70782. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66456/0.71717. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65720/0.71722. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66121/0.69582. Took 0.48 sec\n",
      "Epoch 19, Loss(train/val) 0.65438/0.68525. Took 0.48 sec\n",
      "Epoch 20, Loss(train/val) 0.65200/0.68469. Took 0.47 sec\n",
      "Epoch 21, Loss(train/val) 0.65154/0.68556. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64038/0.67366. Took 0.47 sec\n",
      "Epoch 23, Loss(train/val) 0.64235/0.66658. Took 0.47 sec\n",
      "Epoch 24, Loss(train/val) 0.63728/0.67419. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.63582/0.69047. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64585/0.67389. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63267/0.67211. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63356/0.68714. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.63557/0.67691. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62627/0.68487. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62082/0.67196. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.62050/0.67311. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61144/0.68424. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.61503/0.68904. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61694/0.69713. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61580/0.70257. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.60031/0.68407. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59017/0.71801. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58662/0.70689. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.58666/0.72288. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59077/0.70424. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.58921/0.70089. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60564/0.70864. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58896/0.73660. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.59208/0.72733. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58021/0.71852. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.57761/0.71308. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.60072/0.77053. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56066/0.75874. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.56811/0.75486. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55921/0.79177. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.55964/0.79412. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.57609/0.81648. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56961/0.75397. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.55862/0.78104. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.54129/0.82607. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.54190/0.82851. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54987/0.80044. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.53938/0.87109. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.53914/0.80588. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.51072/0.85449. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.51013/0.85810. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.51539/0.91089. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.50084/0.81334. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50536/0.90345. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.49823/0.88348. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.48212/0.85515. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.48568/0.95793. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.48749/0.82999. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.47924/0.99655. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.47628/0.87321. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.45621/0.85593. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46597/0.83205. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.48813/0.90995. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.45954/0.89601. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43834/0.97499. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.43733/1.00771. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43022/1.02978. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.40623/0.97320. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.45415/1.01669. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.40249/1.09309. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.39463/1.04907. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.39521/1.05404. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.36749/1.14750. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.37889/1.11175. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.39503/1.06947. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.40495/1.10824. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.38565/1.18363. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.37188/1.09296. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36105/1.15118. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.38766/1.06600. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.41969/0.99846. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.37273/1.05527. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.34929/1.00560. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.37874/1.13048. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.34815/1.13680. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.34307/1.15967. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.35732/1.21478. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.32331/1.19548. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.70508/0.69896. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70320/0.71543. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.69370/0.72288. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69401/0.71671. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68928/0.71674. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68687/0.71305. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68171/0.71475. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68887/0.72338. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68734/0.73640. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67328/0.76557. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67323/0.77331. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66471/0.78305. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.66654/0.77574. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67043/0.77357. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.65852/0.79593. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65396/0.81436. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.64970/0.83636. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65003/0.83232. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.64665/0.85511. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64531/0.88847. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63660/0.87712. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63421/0.88186. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64200/0.87320. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.62674/0.89028. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.62327/0.87079. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.62467/0.92056. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61309/0.93040. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.60879/0.89360. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60463/0.95639. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.60323/0.97188. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.59764/0.93456. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.58897/0.99631. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.58621/0.99589. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.57900/1.01362. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.58473/1.02409. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59465/0.98418. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.56730/0.99489. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.56775/1.03547. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.57931/0.96761. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.56279/1.02470. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56358/1.03125. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.54685/1.04674. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55424/1.00299. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.56246/1.02117. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.54263/1.05719. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55619/1.05992. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.52887/1.08249. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.54631/1.06328. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.53219/1.07088. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52953/1.09772. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.53183/1.05350. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.52534/1.09307. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.53088/1.04643. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.52192/1.09157. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.50531/1.05464. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.49757/1.08218. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.52539/1.06902. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.50760/1.12626. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.50173/1.10937. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50597/1.13381. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.46928/1.17888. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.47162/1.15597. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.47878/1.18621. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.46502/1.18532. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.47350/1.20251. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.47238/1.19540. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.46726/1.26439. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.43891/1.31189. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.45992/1.23957. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.44426/1.22734. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.43773/1.21773. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.44620/1.20872. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.43847/1.24173. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.41979/1.27020. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.41112/1.24384. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.43213/1.21880. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.44488/1.23331. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.44598/1.26624. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.40147/1.35242. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.38823/1.31762. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.40691/1.33573. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.38006/1.37079. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.38530/1.37686. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.41062/1.33930. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.40766/1.28336. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.38281/1.35794. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.38370/1.33347. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.37825/1.35612. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.39316/1.35474. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.39797/1.34107. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.36950/1.46838. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.34809/1.41574. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.34686/1.51844. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.34755/1.49615. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.36200/1.37889. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.33794/1.42400. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.33542/1.40841. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.31535/1.52457. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.33366/1.57927. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.30247/1.54827. Took 0.45 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.70515/0.71027. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.70246/0.71491. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69017/0.71312. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68895/0.72494. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68520/0.73272. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68819/0.73626. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.67848/0.74491. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67995/0.75679. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67063/0.78100. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.66674/0.78676. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67255/0.78698. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.67430/0.79402. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66732/0.80017. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66417/0.80329. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66150/0.81535. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66103/0.81794. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.67392/0.83565. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65737/0.82715. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65603/0.81995. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66239/0.83590. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.65284/0.81881. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65530/0.80217. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64462/0.80966. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64511/0.80021. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64820/0.82940. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.63810/0.82320. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.63759/0.85044. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64240/0.83453. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63885/0.84054. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63175/0.83833. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.63298/0.85354. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.62962/0.84963. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62258/0.86069. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.61674/0.82008. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.61063/0.86747. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60065/0.84950. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60676/0.83098. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60165/0.84571. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59539/0.82990. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58248/0.85278. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.58784/0.87196. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58156/0.88453. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.57753/0.89979. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.57325/0.93196. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.57022/0.95781. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.56654/0.95069. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.56057/0.95540. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57448/0.92719. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.55112/0.95886. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.54590/0.97533. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.52827/1.00931. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.52372/1.01376. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.51495/1.03100. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.49858/1.04444. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.51210/1.08361. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.51991/1.09356. Took 0.47 sec\n",
      "Epoch 56, Loss(train/val) 0.48941/1.10576. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.47960/1.11637. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.49123/1.10045. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.46529/1.10953. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.44699/1.07865. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.46468/1.12332. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.45940/1.06330. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.44412/1.11577. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.43070/1.13109. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.45624/1.12487. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.43768/1.12981. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.44934/1.14355. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.39978/1.20412. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.41248/1.22718. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.40159/1.19058. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.39076/1.15140. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.37877/1.17152. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.39071/1.18496. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.37997/1.22431. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.36182/1.19836. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.37426/1.20854. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.36954/1.19672. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.36807/1.23130. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.33208/1.27290. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.35838/1.24997. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.34503/1.22076. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.32731/1.25763. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.32942/1.20961. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.32541/1.27982. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.31666/1.29604. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.32182/1.27365. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.33068/1.32337. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.30970/1.36806. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.31500/1.31260. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.29073/1.34771. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.28202/1.32682. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.28354/1.27396. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.30464/1.28305. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.26037/1.32932. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.27244/1.31776. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.25980/1.40622. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.25597/1.41077. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.26342/1.38087. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.25513/1.43833. Took 0.44 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.71439/0.71456. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.71131/0.70876. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70646/0.71519. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.70173/0.71081. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.70128/0.71303. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69519/0.72996. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68993/0.73990. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69140/0.73416. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68672/0.73922. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67837/0.75909. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68088/0.75797. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67160/0.77407. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68215/0.77134. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67511/0.77182. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67686/0.75337. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67240/0.74967. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66700/0.75409. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67233/0.74852. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.65569/0.74174. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65518/0.74682. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65120/0.75120. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64610/0.76134. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65379/0.76256. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64343/0.76931. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.63060/0.76719. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63432/0.78770. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62874/0.78071. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62828/0.76276. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.62874/0.78676. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62068/0.80705. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60756/0.79488. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.61509/0.79521. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60654/0.80376. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.60012/0.80296. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59238/0.85697. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.59881/0.81875. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59372/0.81932. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58868/0.80724. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.58246/0.83438. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.56734/0.86045. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56984/0.87373. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.56877/0.87552. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56484/0.88272. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56126/0.87031. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.55617/0.87787. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.54895/0.86699. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.54861/0.88510. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.54354/0.90828. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.54524/0.90712. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.53675/0.90874. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53987/0.94254. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.51252/0.96795. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52343/0.98348. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.51204/0.98269. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.51652/1.00932. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.50198/1.00397. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.49864/1.01638. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.49041/1.04399. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.47785/1.10128. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.47634/1.11022. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.48197/1.13182. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.47322/1.13631. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.46452/1.20181. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.46338/1.17167. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.46043/1.21003. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.45623/1.29387. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.44449/1.24302. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.43623/1.29837. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.44734/1.30171. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.42149/1.33610. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.43029/1.38357. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.42306/1.34785. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.41859/1.46669. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.40995/1.47941. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.39385/1.50915. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.38809/1.60956. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.38749/1.56433. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.36896/1.55260. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.36036/1.53532. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.35454/1.70518. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.34811/1.72163. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.34511/1.72740. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.33828/1.76619. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.35492/1.72632. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.31819/1.89299. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.30258/1.85855. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.31407/1.83953. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.32078/1.92689. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.31021/2.01281. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.32685/1.93772. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.33309/1.88203. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.29723/1.97420. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.30481/1.99889. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.31016/1.94056. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.27116/1.97000. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.28168/1.96928. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.28651/1.93334. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.28929/2.06502. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.24965/2.07562. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.27994/2.03560. Took 0.44 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.70104/0.69049. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69815/0.70705. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69423/0.69676. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69148/0.71437. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69218/0.72370. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68761/0.71395. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68079/0.70878. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68165/0.70108. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67941/0.69244. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67949/0.70413. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67071/0.70647. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67083/0.71112. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67371/0.71871. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67731/0.71765. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67140/0.70972. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67206/0.71888. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.66895/0.71664. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66688/0.72731. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66625/0.73335. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66816/0.73796. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66920/0.72810. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66167/0.74480. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65448/0.74827. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.65806/0.74762. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.65531/0.76824. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65124/0.77161. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65020/0.79233. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64232/0.78731. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.63472/0.78287. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63407/0.80469. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.63547/0.79090. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62859/0.83417. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62708/0.83073. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.63122/0.81685. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.61169/0.85580. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.60758/0.82179. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61248/0.84912. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60621/0.86836. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61846/0.84705. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59289/0.86567. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.59246/0.96999. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59219/0.92458. Took 0.47 sec\n",
      "Epoch 42, Loss(train/val) 0.57590/0.92567. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58541/0.96214. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.58391/0.97259. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57856/0.96946. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56522/0.98412. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.57395/0.98029. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.57178/0.96621. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56003/0.95263. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55084/0.98098. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54998/1.00139. Took 0.47 sec\n",
      "Epoch 52, Loss(train/val) 0.54563/0.99466. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.53092/1.00775. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54054/1.00796. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.53115/1.00359. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.53158/1.00947. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.51460/0.98077. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.52410/0.98604. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.50243/1.00394. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49999/1.09297. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.47362/1.10345. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.47210/1.13442. Took 0.47 sec\n",
      "Epoch 63, Loss(train/val) 0.48066/1.08404. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.47072/1.06248. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.46737/1.13243. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.46683/1.10988. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.47313/1.10733. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.45811/1.15755. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.45885/1.07700. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.43431/1.08187. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.42935/1.10940. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.44944/1.09495. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.43560/1.11326. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.43552/1.17062. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.43000/1.08936. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.41415/1.17759. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.39479/1.16044. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.41196/1.20292. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.39265/1.23832. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.35445/1.33852. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.38571/1.32537. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.37961/1.22517. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.37565/1.27360. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.37538/1.24484. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.35832/1.32864. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.36058/1.31511. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.35893/1.37428. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.34494/1.40288. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.33204/1.34631. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.35092/1.34571. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.34473/1.44134. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.32793/1.37419. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.33463/1.45071. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.29620/1.36949. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.31835/1.38360. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.28311/1.55833. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.33166/1.56295. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.32190/1.58795. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.32187/1.59101. Took 0.45 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.71213/0.70764. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.71574/0.72220. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.70740/0.71298. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69896/0.70476. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.70592/0.70630. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69031/0.74193. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69509/0.74875. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68644/0.75985. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.70105/0.74785. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68818/0.74509. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68469/0.75711. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68677/0.74586. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.68910/0.75269. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67142/0.75374. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67185/0.74382. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.66965/0.77348. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66457/0.77831. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.66574/0.77387. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65393/0.80671. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67055/0.78836. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.65175/0.80707. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64168/0.81162. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64690/0.81039. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63545/0.84501. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65582/0.83635. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.64408/0.83290. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62902/0.82667. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62721/0.84032. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62240/0.84748. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62825/0.83755. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61014/0.86395. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.60762/0.86284. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.60966/0.89399. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.61191/0.92424. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60393/0.90549. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.59818/0.92415. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58028/0.98673. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58467/0.98487. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.56734/1.02477. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.58118/1.01117. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.56974/1.05252. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.56454/1.07629. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55878/1.06547. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.58874/1.01423. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57211/1.01507. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.55652/1.09822. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55378/1.04683. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.56530/1.07898. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.52958/1.10815. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.52996/1.11765. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.52734/1.10163. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55988/1.03056. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.51697/1.12918. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.52007/1.06332. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53929/1.02932. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.51932/1.10086. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52553/1.03798. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.50332/1.04487. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50261/1.11456. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50772/1.04174. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.50654/1.01607. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.48843/1.07689. Took 0.47 sec\n",
      "Epoch 62, Loss(train/val) 0.48460/1.03881. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.49959/1.06772. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.48301/1.10571. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.48325/1.06067. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.49136/1.00817. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.47915/1.08093. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.46864/1.11678. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45169/1.10683. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.47191/1.15382. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.46916/1.15267. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.44941/1.15199. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.44976/1.18664. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.43539/1.23062. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.43049/1.23694. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.41830/1.28178. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.42415/1.26860. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43925/1.30380. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.43321/1.21930. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.42680/1.27626. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.42703/1.34288. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.39240/1.29300. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40764/1.36832. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.41099/1.36693. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.36697/1.49266. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.41834/1.44172. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.42199/1.33336. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.39829/1.27011. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.39360/1.27684. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36963/1.43933. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.35536/1.53292. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.37429/1.57637. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.38414/1.46433. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.36353/1.45322. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.34831/1.66865. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.35394/1.56307. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.34479/1.65617. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.33782/1.61413. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.36884/1.62042. Took 0.44 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.70248/0.66900. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69810/0.68092. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69173/0.67885. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69141/0.68865. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.68952/0.69343. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68199/0.69470. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67770/0.69985. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67825/0.70846. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68231/0.72497. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67359/0.71869. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66820/0.77312. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67168/0.76172. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.66404/0.77989. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66338/0.79416. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.66264/0.80168. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65852/0.80493. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66347/0.83552. Took 0.47 sec\n",
      "Epoch 17, Loss(train/val) 0.65834/0.85504. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65596/0.83411. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.64955/0.86954. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64533/0.85818. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63609/0.88038. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.63385/0.88753. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62909/0.92910. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63197/0.94389. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.62793/0.96336. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62205/0.99843. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.61819/0.99343. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61096/0.99065. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.60569/1.00779. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.59821/1.01309. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.59332/1.07150. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59431/1.07735. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58740/1.07393. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.58415/1.04771. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.58348/1.09769. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.56924/1.14165. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.57200/1.14042. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.56292/1.21582. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57818/1.21693. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.55414/1.25326. Took 0.47 sec\n",
      "Epoch 41, Loss(train/val) 0.55435/1.24800. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55228/1.23158. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.54248/1.25014. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.56747/1.17482. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.54823/1.21134. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.53191/1.23001. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.52190/1.25619. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.51991/1.26739. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.54689/1.12937. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.52984/1.18511. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.52991/1.22747. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.50209/1.27238. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.50674/1.25600. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.50601/1.26066. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.51635/1.28989. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.49429/1.34659. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.49287/1.36592. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.48605/1.32203. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.46502/1.35744. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.47948/1.31494. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.47571/1.39362. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.46186/1.45902. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.46728/1.40384. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.48732/1.33037. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.46571/1.46110. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.45320/1.49612. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.44734/1.46046. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.45648/1.49386. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.44090/1.44011. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.43966/1.53660. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.43400/1.48795. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.40974/1.60767. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.44283/1.53955. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.42690/1.48084. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.41321/1.46538. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.43658/1.48595. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.40085/1.58818. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.39916/1.54517. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.40464/1.53483. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.37355/1.63326. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.37268/1.70909. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.40381/1.64352. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.38485/1.66506. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.36295/1.68140. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.37586/1.72362. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.36066/1.71479. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.38736/1.61359. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.35334/1.64555. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.34686/1.75145. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.33621/1.78205. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.35689/1.57222. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.32203/1.82362. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.35424/1.69190. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.34002/1.87005. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.34572/1.80313. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.32399/2.02416. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.38312/1.74798. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.36172/1.83421. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.32456/1.73949. Took 0.43 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.71389/0.71467. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70586/0.70988. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69720/0.71423. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70010/0.70743. Took 0.49 sec\n",
      "Epoch 4, Loss(train/val) 0.69812/0.71701. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69454/0.71827. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69624/0.72563. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.68851/0.71910. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68547/0.72276. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68750/0.72237. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68772/0.73239. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68114/0.73322. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.68508/0.73847. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67931/0.74164. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67316/0.74521. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67499/0.74376. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.67173/0.75430. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67058/0.76242. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.66752/0.76887. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.66482/0.77658. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66661/0.76777. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66089/0.77504. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65864/0.78032. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66248/0.77532. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.65569/0.77270. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65077/0.79157. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.65174/0.77997. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.65144/0.79794. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63903/0.80916. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.64026/0.79465. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63334/0.79692. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61748/0.82441. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.61714/0.82359. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62175/0.82119. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61693/0.80231. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.60426/0.86265. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60418/0.84539. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59308/0.83424. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.58591/0.86602. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.58566/0.88134. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.58648/0.89503. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57855/0.91253. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56162/0.90436. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57938/0.96379. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.55265/0.96985. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.55012/0.98288. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.53950/0.98769. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.53595/1.02302. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.53460/1.02343. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.53403/1.04774. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.52516/1.01111. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.50598/1.06600. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.50908/1.02786. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.51250/1.10356. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.48977/1.03441. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.49939/1.05670. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.48862/1.03911. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.49333/1.08166. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.48252/1.11408. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.48396/1.07692. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.46126/1.08894. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.46504/1.16591. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.44583/1.12034. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.46401/1.06896. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.43986/1.15034. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.43579/1.15030. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.44869/1.18050. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.41990/1.25427. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.43101/1.18406. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.43539/1.21624. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.43101/1.18580. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.44466/1.17471. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.42158/1.19166. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.39059/1.24231. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.38809/1.27688. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.39290/1.26547. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.38989/1.24471. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.37850/1.29013. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.38421/1.29040. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.39047/1.26146. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.37467/1.31666. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.37417/1.26951. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.37389/1.27192. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.34648/1.32957. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.36916/1.25255. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.35693/1.25067. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.33066/1.32833. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.31654/1.38839. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.33263/1.42282. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.36530/1.23724. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.33743/1.31098. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.33973/1.34840. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.31820/1.42601. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.31000/1.48142. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.32437/1.40042. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.31587/1.42023. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.32934/1.42461. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.30222/1.45366. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.31826/1.49545. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.30095/1.45553. Took 0.44 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.70417/0.71778. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.70296/0.71619. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69975/0.71162. Took 0.49 sec\n",
      "Epoch 3, Loss(train/val) 0.69863/0.70542. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69601/0.71090. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.69544/0.71434. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69219/0.72634. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68960/0.72950. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.69002/0.71727. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68520/0.71863. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68035/0.71914. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67854/0.73680. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67240/0.74208. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66710/0.72704. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66670/0.69714. Took 0.48 sec\n",
      "Epoch 15, Loss(train/val) 0.65551/0.70696. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.65749/0.70456. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65915/0.71848. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65196/0.71120. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64471/0.72997. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63878/0.71860. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63600/0.72683. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.62906/0.72657. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62390/0.76343. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.61843/0.81265. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.61389/0.76794. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.60433/0.79930. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.59388/0.81310. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.58628/0.83727. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.57625/0.84567. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.58331/0.86744. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.56687/0.88252. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.57740/0.85910. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.55339/0.91167. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.55263/0.90172. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.55203/0.89987. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.52103/0.93951. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.52974/0.89006. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.51549/0.91623. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.52477/0.91342. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.51191/0.91518. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.49801/0.91739. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.49183/0.90960. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.49697/0.93939. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.47540/0.91343. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.47806/0.92678. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.47084/0.91290. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.45605/0.95178. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.45611/0.98447. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.48256/0.87957. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.42727/0.93969. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.45530/0.89284. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.44203/0.97357. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.41902/1.02385. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.39554/0.95780. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.44944/0.83282. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.40875/0.96111. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.40041/0.91920. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.40419/0.96662. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.38856/0.97041. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.40129/0.96265. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.39108/0.98864. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.37026/1.02979. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.38040/1.03775. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.37613/0.95962. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.35652/1.01735. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.34383/1.06167. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.35148/1.08062. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.33606/1.17224. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.35242/0.93572. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.37191/1.00356. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.32828/1.05525. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.31337/1.11475. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.31889/1.17643. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.30537/1.15891. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.32572/1.12316. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.30068/1.13776. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.29660/1.14557. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.30913/1.12527. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.29503/1.19986. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.28778/1.14653. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.28606/1.16374. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.27292/1.17625. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.27724/1.22245. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.26786/1.17697. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.26193/1.24788. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.28159/1.25213. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.28194/1.22542. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.26455/1.31888. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.23395/1.29380. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.23093/1.26617. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.23197/1.36198. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.24417/1.31717. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.21528/1.32725. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.21567/1.38559. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.21455/1.44812. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.19130/1.47539. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.20121/1.49381. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.21766/1.34381. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.21966/1.45166. Took 0.44 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.68827/0.69762. Took 0.63 sec\n",
      "Epoch 1, Loss(train/val) 0.68440/0.70844. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68345/0.71226. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68292/0.69822. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68037/0.69950. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67575/0.70910. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.67369/0.69175. Took 0.47 sec\n",
      "Epoch 7, Loss(train/val) 0.67984/0.68737. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.67454/0.69768. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67404/0.70094. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67416/0.69981. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66659/0.71043. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.66903/0.73276. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66781/0.72564. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66416/0.73555. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66362/0.73957. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66261/0.73658. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65752/0.74912. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65574/0.76069. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.64634/0.76924. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.64393/0.77552. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64972/0.75866. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.63902/0.78217. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.63992/0.77291. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.63974/0.79745. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63786/0.78332. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63068/0.80146. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62717/0.80385. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61589/0.83767. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.61364/0.84315. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61247/0.85240. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60887/0.84091. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.60384/0.81894. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59560/0.84534. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.58899/0.88205. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.58448/0.87646. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58492/0.86526. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.57897/0.87289. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57533/0.88922. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.58230/0.89430. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.57654/0.88223. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.55016/0.90350. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56139/0.92486. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.55175/0.95503. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.55262/0.92727. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.54614/0.98968. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.53882/0.88711. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.53931/0.93627. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.53042/0.96440. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.51967/0.92394. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.51894/0.94717. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.49616/0.95858. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.51592/0.92728. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.49551/0.95292. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.47284/0.97048. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.49341/0.96645. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.47601/1.02073. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.48306/0.99601. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.48118/0.94828. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.47137/0.94233. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.45189/1.00013. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.44522/1.00473. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.43731/0.98005. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.43602/0.95101. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.46105/0.91229. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.44487/0.99547. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.43717/1.04890. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.41247/1.08241. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.42242/1.02869. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.39326/1.04691. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.40974/1.03192. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.40125/1.05700. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.38343/1.06279. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.37654/1.08061. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.35493/1.08912. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.34900/1.12326. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.39042/1.10860. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.36768/1.16046. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.38113/1.15106. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.37676/1.13767. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.39162/1.17455. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.40399/1.20152. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.39057/1.15907. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.34615/1.18199. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.35106/1.10352. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.32176/1.14550. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.31677/1.14937. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.30075/1.20382. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.31064/1.20304. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.28421/1.18135. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.30288/1.21724. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.28907/1.21457. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.32240/1.24223. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.29044/1.22315. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.27293/1.24836. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.30156/1.22463. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.31466/1.16041. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.26984/1.19203. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.28040/1.28341. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.26642/1.25673. Took 0.43 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69730/0.69260. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69391/0.68500. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.69619/0.68860. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68819/0.69226. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68246/0.68656. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68650/0.69221. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.68386/0.69366. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68016/0.69515. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67490/0.69190. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67426/0.69327. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67656/0.68602. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67421/0.68803. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67699/0.69386. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66821/0.68098. Took 0.48 sec\n",
      "Epoch 14, Loss(train/val) 0.66835/0.70196. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66979/0.69531. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66687/0.71953. Took 0.47 sec\n",
      "Epoch 17, Loss(train/val) 0.66008/0.70815. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65917/0.72509. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66120/0.72532. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65694/0.74607. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65594/0.74254. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65875/0.72473. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.65225/0.74845. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65363/0.73130. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64867/0.72211. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65117/0.73874. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63998/0.74204. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65084/0.72179. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63491/0.74502. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63663/0.74161. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63412/0.75314. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63206/0.77829. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.61576/0.77392. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60996/0.78445. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.61845/0.79147. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61036/0.79690. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59986/0.80020. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.61084/0.82010. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.59360/0.80472. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59716/0.79832. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.59726/0.82664. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58079/0.81538. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59455/0.82511. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58406/0.81054. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.58169/0.82175. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58882/0.78260. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.57575/0.78654. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.56441/0.79315. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.56538/0.78526. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56260/0.77062. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.56319/0.76322. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.54984/0.75686. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.55026/0.77977. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.53730/0.79029. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.52134/0.81334. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.54174/0.82730. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52083/0.82617. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.53026/0.84338. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52980/0.81734. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51462/0.83599. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.52139/0.83172. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50377/0.83929. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.51181/0.81806. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49166/0.82742. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.48880/0.90692. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.48337/0.89362. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.48684/0.86830. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.46871/0.92903. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.46703/0.94246. Took 0.47 sec\n",
      "Epoch 70, Loss(train/val) 0.45988/0.93470. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.47464/0.96081. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48549/0.98029. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.46219/0.96578. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.46685/0.97274. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.44415/1.00697. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43146/1.04963. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.41300/1.06552. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43593/1.03686. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.41728/1.08307. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.43960/1.21835. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.46584/1.09050. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.43097/1.08212. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.42279/1.18706. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.39322/1.20733. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.41199/1.17993. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.38709/1.17682. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43808/1.16644. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.40242/1.12725. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.38439/1.21238. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.38117/1.20148. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.38436/1.23732. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.37758/1.26157. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.37046/1.26540. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.35303/1.35258. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.35601/1.37974. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.36902/1.28123. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.36444/1.32864. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.35060/1.31576. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.34270/1.38092. Took 0.46 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69181/0.69104. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68866/0.69250. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.68254/0.70056. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68155/0.69601. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.67624/0.70686. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67387/0.71069. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.67428/0.72442. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67202/0.72009. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67151/0.71761. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.66980/0.71765. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66438/0.72039. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66215/0.71797. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.65974/0.72598. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66239/0.72415. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.66541/0.72102. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65686/0.71836. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65539/0.71537. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65179/0.72004. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65088/0.74024. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65167/0.73957. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65028/0.74009. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64954/0.72413. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.63507/0.76881. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65078/0.74558. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.63247/0.75616. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63610/0.77230. Took 0.47 sec\n",
      "Epoch 26, Loss(train/val) 0.62761/0.76986. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62760/0.77299. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62859/0.78455. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.61574/0.78756. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61677/0.80193. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61643/0.78382. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.59933/0.82699. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60260/0.80900. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59254/0.81329. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.59427/0.79176. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59288/0.82606. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59281/0.82409. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.58370/0.82631. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.58218/0.83658. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56719/0.86569. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.56712/0.84123. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.56561/0.87389. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.56151/0.88413. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.56932/0.87911. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.54923/0.90762. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55020/0.90430. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.52962/0.93830. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.54086/0.94017. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.54491/0.93025. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.52136/1.00860. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.52879/0.98118. Took 0.47 sec\n",
      "Epoch 52, Loss(train/val) 0.54326/0.94274. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.53199/0.95480. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.52360/0.91632. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.50989/0.91087. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.50881/0.94164. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.50826/0.94420. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.52118/0.90889. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.52742/0.93574. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.48547/0.98344. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50780/0.93794. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.50412/0.94871. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.49938/0.92973. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.49866/0.95905. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.49349/0.95139. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48984/0.96899. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.48630/0.99049. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.47979/0.95089. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.46138/0.94808. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.47525/0.92795. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.46058/0.98313. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48141/1.03253. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.46482/0.98195. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.46819/0.99516. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.46742/0.92179. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.44966/0.96545. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.44438/0.99324. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44178/0.97605. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44172/1.00735. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.44501/1.07571. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.45714/0.96536. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.44661/1.03234. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.41953/1.04464. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.42802/0.98683. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.43110/1.01500. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43946/1.11792. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43378/1.06023. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.41244/1.03873. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.41788/1.08155. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.41907/1.06148. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.40622/1.06977. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.41192/1.07813. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.41583/1.05206. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.39509/1.09665. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.38560/1.05541. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.37660/1.19392. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.37526/1.19206. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.38943/1.17026. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.41133/1.11886. Took 0.44 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69670/0.69398. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69966/0.71120. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69613/0.70389. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69462/0.70884. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69073/0.71466. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.67469/0.70996. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67906/0.71129. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68437/0.72861. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68558/0.72744. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67719/0.74518. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68094/0.73578. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66975/0.74034. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67830/0.73605. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67182/0.73764. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66557/0.74611. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66473/0.75769. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66219/0.77672. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65745/0.78127. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66043/0.78163. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64384/0.80228. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65243/0.79025. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.65243/0.80115. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65278/0.80627. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64741/0.80134. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63862/0.82025. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64732/0.81026. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64366/0.80442. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64872/0.79124. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64651/0.79563. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.63890/0.78616. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63219/0.79064. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.63158/0.78601. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63531/0.79793. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62097/0.77702. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.63480/0.80800. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62465/0.80595. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61335/0.81125. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60865/0.82347. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61973/0.79515. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.60399/0.81864. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.60311/0.79975. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.60870/0.80617. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60501/0.82100. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.59631/0.82422. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59442/0.81652. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59539/0.80262. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.58726/0.83288. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.58498/0.84043. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57898/0.82726. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57856/0.84147. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.58301/0.89047. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.57575/0.89097. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.57413/0.86716. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.57231/0.89361. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.57053/0.89757. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.57066/0.89584. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55611/0.92734. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.56334/0.93173. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.55326/0.98328. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.55586/0.98655. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.53505/1.00106. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.55005/0.93960. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53332/0.99494. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.53024/0.99880. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.51785/1.04490. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.51884/1.00696. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.52847/1.01967. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.53657/1.02531. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.50910/1.00522. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.52237/1.06354. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.50474/1.10940. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.49848/1.04697. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.49351/1.01348. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.49498/1.07152. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.48249/1.07800. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.49100/1.01776. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.49903/1.04480. Took 0.47 sec\n",
      "Epoch 77, Loss(train/val) 0.47055/1.08641. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.46735/1.05944. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.47500/1.07763. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.46021/1.07263. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.47664/1.13011. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.46410/1.08092. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.45248/1.08405. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.44318/1.07359. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.45537/1.15088. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.45600/1.10870. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.45628/1.07979. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.44492/1.05895. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.43040/1.12831. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.41961/1.10838. Took 0.48 sec\n",
      "Epoch 91, Loss(train/val) 0.42311/1.14668. Took 0.47 sec\n",
      "Epoch 92, Loss(train/val) 0.42533/1.08547. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.43671/1.14125. Took 0.47 sec\n",
      "Epoch 94, Loss(train/val) 0.43314/1.13697. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.43257/1.11378. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.40660/1.11152. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.40569/1.13584. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.41990/1.17332. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.41547/1.13843. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.68904/0.69948. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.68497/0.69884. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.67920/0.69748. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68316/0.70684. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68194/0.71475. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67790/0.73416. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67183/0.72512. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.67402/0.73398. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67919/0.72311. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.67197/0.73559. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67594/0.73183. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66690/0.74217. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66416/0.74112. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66649/0.74362. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.66972/0.74415. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66197/0.73857. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66069/0.74889. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.65353/0.75057. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65954/0.77013. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65723/0.77659. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65088/0.79814. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65822/0.80603. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65412/0.81016. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64480/0.83841. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.64604/0.84153. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63911/0.86204. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64181/0.86487. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64084/0.86419. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62808/0.84923. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.63017/0.85602. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63193/0.84401. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.62027/0.87584. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63178/0.86894. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62155/0.84761. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61763/0.87513. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61460/0.90619. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61286/0.90486. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.61635/0.90787. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.60227/0.91586. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60405/0.89538. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60180/0.91384. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.59522/0.92269. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59698/0.90247. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58707/0.93133. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.59094/0.94596. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.58131/0.93199. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.57519/0.97751. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.57187/0.99143. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.58446/0.93384. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.59141/0.92110. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56939/0.93440. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56533/0.91189. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.56867/0.95249. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54796/0.95284. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.57297/0.92983. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.55737/0.92066. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55044/0.95958. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.55356/0.92976. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54156/0.93334. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.54009/0.97394. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52757/0.96259. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.51526/1.01387. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.52887/0.95753. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.51478/0.98124. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.52636/0.97898. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.51705/0.96887. Took 0.47 sec\n",
      "Epoch 66, Loss(train/val) 0.51258/0.92623. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.50038/0.96046. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.50282/1.00037. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.49887/0.99848. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.49556/0.97309. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48517/0.95139. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.48482/1.03926. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.48235/1.01239. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.46892/1.03858. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.45847/1.07440. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.44422/1.11198. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.45682/1.03804. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.43816/1.17568. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46171/1.11663. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.46250/1.04530. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.45672/1.03726. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.43046/1.12096. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.43980/1.07899. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43443/1.10841. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.42083/1.11608. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.42785/1.15270. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.41257/1.13698. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.41050/1.13106. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.43666/1.09944. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.43748/1.09379. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.41001/1.21127. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.39367/1.22830. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.45414/1.11186. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40107/1.17903. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.38142/1.21754. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.36155/1.22794. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.38313/1.24373. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.37880/1.23032. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.38960/1.18276. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70306/0.73222. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69757/0.71211. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69239/0.71020. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69518/0.69117. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.68910/0.71257. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68447/0.71317. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.68572/0.71957. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67872/0.74218. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67918/0.70518. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67991/0.72436. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68109/0.69731. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67854/0.73617. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67085/0.72926. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.66545/0.74314. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66972/0.75503. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.66084/0.78695. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66001/0.78646. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65515/0.78384. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65517/0.82971. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65744/0.84386. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65234/0.82783. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64907/0.84278. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65117/0.84827. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64872/0.89460. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64635/0.89060. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.63932/0.91555. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63738/0.94391. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.62670/0.97732. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62109/0.99388. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62504/1.00716. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.61484/1.02143. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62057/1.02116. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.61731/1.01881. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61442/1.01249. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.61594/1.03745. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61457/1.04572. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60123/1.06779. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60732/1.10840. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60146/1.08530. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58995/1.13147. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.59639/1.12815. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59587/1.15596. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60349/1.14347. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.59146/1.16590. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58049/1.15980. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56963/1.17479. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.57216/1.20986. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56080/1.19343. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.55653/1.22172. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55899/1.29955. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55639/1.25098. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.55874/1.20176. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54622/1.21775. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.54601/1.27242. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53780/1.31803. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.53819/1.31929. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.52861/1.25774. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52910/1.29656. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.52073/1.38187. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.50621/1.38472. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.50553/1.46965. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.51458/1.39575. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50305/1.39770. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.51197/1.41526. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50303/1.54321. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.49080/1.50188. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.50856/1.40471. Took 0.47 sec\n",
      "Epoch 67, Loss(train/val) 0.48745/1.49396. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.47933/1.45345. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47633/1.48890. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.46492/1.51236. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48256/1.53526. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.46893/1.50066. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.45402/1.56702. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.44460/1.63229. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.46449/1.49833. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.45132/1.59373. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.43606/1.70755. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.44363/1.69616. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.42086/1.62710. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.40573/1.64416. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.42629/1.71839. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.43223/1.80874. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.42524/1.73366. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.41645/1.70323. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.41087/1.67483. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.43066/1.67235. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.40130/1.56364. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.39757/1.64682. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.38233/1.69126. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.40550/1.66182. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.41304/1.65183. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.39888/1.62539. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.36748/1.75231. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.36247/1.71961. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.37010/1.69198. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.36843/1.76175. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.35722/1.72116. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.37511/1.57203. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36964/1.64969. Took 0.45 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.70529/0.71460. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69976/0.71278. Took 0.49 sec\n",
      "Epoch 2, Loss(train/val) 0.69026/0.74763. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69372/0.71738. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68557/0.73802. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68188/0.75806. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68327/0.74673. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68184/0.73931. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67067/0.75430. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67537/0.74683. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67052/0.74478. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67166/0.74600. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66328/0.75750. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66628/0.72998. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.66151/0.73946. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.65567/0.77187. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65130/0.75600. Took 0.47 sec\n",
      "Epoch 17, Loss(train/val) 0.64979/0.79257. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64102/0.76755. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64345/0.77615. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64158/0.78986. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63176/0.80728. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.63656/0.75635. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63059/0.80814. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.63662/0.82816. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.62485/0.84120. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.62579/0.82030. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61838/0.85558. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61823/0.84805. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.62118/0.80433. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61436/0.84018. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.61267/0.84857. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60417/0.87137. Took 0.47 sec\n",
      "Epoch 33, Loss(train/val) 0.59898/0.93299. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.59838/0.84003. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.59105/0.91123. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59442/0.92736. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59330/0.82348. Took 0.47 sec\n",
      "Epoch 38, Loss(train/val) 0.58924/0.88779. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.56954/0.93483. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58345/0.90431. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.58061/0.96690. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.57623/0.90420. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.55569/1.00484. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.54800/0.99508. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.55189/1.04657. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.58615/0.92034. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56114/0.92576. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.56390/0.92768. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54808/0.94234. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.52872/0.99082. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54582/0.96366. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54054/0.95936. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.54374/1.03516. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52445/1.01718. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.53991/1.00653. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52432/0.98742. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.52980/1.01375. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.52286/0.95023. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.51389/0.90150. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51931/1.01959. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.51552/0.97845. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49793/0.98262. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48338/0.94676. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.50626/1.00455. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.48132/1.08839. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.49229/1.11261. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.47615/1.08602. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.44896/1.17725. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.48859/1.05639. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.47124/1.12866. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.47249/1.15171. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.45724/1.17325. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46205/1.09618. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.46570/1.05572. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.45134/1.05138. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.46021/1.08793. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.45002/1.12876. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44426/1.12381. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.47252/1.06884. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.45056/1.10571. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.43574/1.16410. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.45777/1.08534. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.44669/1.08699. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.44871/1.09716. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.42747/1.13610. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.41514/1.14326. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.41048/1.17026. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.43523/1.26034. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.40889/1.16882. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.38596/1.17310. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.41626/1.09193. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.41264/1.11604. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.40151/1.23608. Took 0.47 sec\n",
      "Epoch 94, Loss(train/val) 0.39482/1.28076. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.38240/1.22287. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.39853/1.22585. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.38320/1.25525. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.41193/1.23044. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.39738/1.11935. Took 0.45 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.74185/0.68314. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.71634/0.67346. Took 0.49 sec\n",
      "Epoch 2, Loss(train/val) 0.71002/0.67605. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.71348/0.69112. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.70673/0.67668. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69986/0.67026. Took 0.49 sec\n",
      "Epoch 6, Loss(train/val) 0.69844/0.66448. Took 0.48 sec\n",
      "Epoch 7, Loss(train/val) 0.69331/0.67291. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.69651/0.67159. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.70022/0.66656. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68776/0.66869. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.68316/0.67898. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68154/0.68007. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67120/0.69360. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67473/0.70036. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67340/0.71392. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.67193/0.71289. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65870/0.71005. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65190/0.70950. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.66127/0.70172. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65295/0.70342. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.65319/0.71070. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66078/0.70698. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.65272/0.69541. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64166/0.70641. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.63661/0.71205. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.64455/0.71349. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63831/0.72191. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63156/0.71696. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64001/0.72357. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.63815/0.71853. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62986/0.72073. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63176/0.72407. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62071/0.74142. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61999/0.75252. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60694/0.75321. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61351/0.78167. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61621/0.77023. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60970/0.76520. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.60286/0.77235. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.60150/0.77887. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60706/0.77842. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.60205/0.76430. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59864/0.75983. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.60401/0.76918. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.60657/0.74733. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.59583/0.74737. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.57579/0.76910. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58261/0.77690. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.58282/0.76282. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.59006/0.75460. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.58148/0.72850. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.57642/0.74257. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.57383/0.76135. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.56749/0.75033. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.56791/0.77702. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.57120/0.76878. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.56981/0.74552. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.55206/0.78113. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.55395/0.74178. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.55614/0.74224. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.53796/0.73603. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.55197/0.72426. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.53707/0.77519. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.55371/0.76266. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.55157/0.72140. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.53594/0.76067. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.52418/0.77960. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.53668/0.74456. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.52732/0.75012. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.51987/0.73121. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.52819/0.73333. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.52393/0.74563. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.50620/0.71811. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.51424/0.68526. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.51093/0.74629. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.48467/0.72819. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.48751/0.71892. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.49310/0.74120. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.48769/0.77278. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.47853/0.78842. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.47995/0.79256. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.47534/0.80455. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.47017/0.79738. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.48036/0.82299. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.46798/0.80260. Took 0.47 sec\n",
      "Epoch 86, Loss(train/val) 0.45093/0.79864. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.45399/0.81945. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.44378/0.83227. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.45637/0.84459. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.44289/0.87216. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.44678/0.85185. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.43795/0.80055. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.43495/0.86125. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.44291/0.89750. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.42347/0.81494. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.43067/0.85976. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.42286/0.82236. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.40345/0.86378. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.42897/0.92231. Took 0.44 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.71045/0.72219. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70233/0.72182. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69324/0.73279. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.68893/0.73390. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68766/0.72701. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68166/0.73531. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68261/0.73297. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67733/0.73671. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.67809/0.73713. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67333/0.75421. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67377/0.76576. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67260/0.77665. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67600/0.76863. Took 0.47 sec\n",
      "Epoch 13, Loss(train/val) 0.67165/0.77460. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66643/0.79524. Took 0.47 sec\n",
      "Epoch 15, Loss(train/val) 0.66192/0.79490. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67027/0.78959. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66973/0.78096. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66234/0.79519. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65693/0.80353. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.66085/0.81075. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66088/0.81571. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.66029/0.81179. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64544/0.83427. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64667/0.84654. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64569/0.85213. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64769/0.82536. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.64666/0.83526. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64106/0.84716. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.64172/0.84941. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63643/0.84035. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62520/0.86087. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.63448/0.87274. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63196/0.86875. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.62504/0.84946. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.62546/0.84312. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.61551/0.84495. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62234/0.84167. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61551/0.85075. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.60511/0.83863. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.60369/0.85849. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59874/0.87685. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.60054/0.86948. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60452/0.89662. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59013/0.89762. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.58644/0.90404. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58790/0.91267. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.57337/0.93494. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57677/0.92264. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.56978/0.89517. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55688/0.92822. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55710/0.94381. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.55472/0.91766. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54282/0.94777. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54954/0.95344. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.54734/0.92153. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.52648/0.95338. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52868/0.97249. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.52964/0.90704. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52160/0.95541. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.54172/0.93220. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.51746/0.93951. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50914/0.96825. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.50392/0.96521. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50350/1.00896. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.50404/1.01284. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.49706/0.98743. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.49260/1.02442. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.47837/1.05410. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47563/1.07584. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.49303/1.17713. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48979/1.09822. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.45554/1.14669. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.47586/1.10965. Took 0.47 sec\n",
      "Epoch 74, Loss(train/val) 0.47061/1.07555. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44860/1.15778. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.44769/1.19940. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.45111/1.14619. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44872/1.15120. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.43988/1.13271. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.41471/1.19094. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.45833/1.28997. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.42207/1.27109. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.41534/1.23758. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.41298/1.28099. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.41800/1.23029. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.40966/1.26524. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.40792/1.22833. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.39451/1.31055. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.40955/1.28010. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.38430/1.38098. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.37764/1.36801. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.36332/1.37050. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.41377/1.35118. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.37023/1.31909. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.38226/1.38864. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.37455/1.42323. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.37745/1.25262. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.37495/1.38726. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.34264/1.37571. Took 0.44 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.71610/0.72186. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70754/0.71400. Took 0.49 sec\n",
      "Epoch 2, Loss(train/val) 0.69547/0.73568. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69387/0.75460. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68991/0.75744. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68575/0.75414. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67774/0.76839. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67714/0.76771. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67701/0.77725. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.66942/0.77258. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.67535/0.76146. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67648/0.75651. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67060/0.77465. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67044/0.78809. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66440/0.81568. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66965/0.82639. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.66347/0.82143. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65970/0.84399. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65768/0.87072. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65101/0.85127. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65041/0.87292. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.64859/0.88778. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64596/0.88848. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.63974/0.90025. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63383/0.93392. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.63534/0.94655. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63721/0.95489. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.62311/0.97392. Took 0.47 sec\n",
      "Epoch 28, Loss(train/val) 0.61803/0.93325. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.61879/0.98511. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61086/1.01042. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.60605/0.99507. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60766/1.04290. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.60566/0.98322. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.60467/1.00115. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59302/1.01209. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.59072/1.00481. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58115/0.99646. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.58536/0.97872. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59186/0.99640. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57120/0.97210. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.57352/0.99780. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55873/1.02589. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.57196/1.03007. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55440/1.02993. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.54284/1.00529. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.54410/1.02814. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.54125/1.06543. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.53373/1.06891. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.52572/1.09167. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.51856/1.07639. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.51704/1.10163. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.50669/1.11643. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52158/1.13894. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.50852/1.18745. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.50795/1.16710. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.48767/1.17939. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.50116/1.17105. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.46912/1.19522. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.49071/1.17583. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.47095/1.23516. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.46546/1.26285. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.47301/1.27352. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.47683/1.30084. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.45801/1.29475. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.46756/1.21734. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.47378/1.21237. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.45190/1.33408. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.44995/1.31884. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.45548/1.24165. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.44953/1.26270. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.43607/1.25844. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.42659/1.30106. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.43862/1.29397. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.44023/1.35631. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.41384/1.31372. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.42550/1.35202. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.41572/1.34916. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.40315/1.39520. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.41026/1.33639. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.39572/1.43525. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.39178/1.56734. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.38197/1.40233. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.37661/1.45998. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.38460/1.47765. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.35298/1.44178. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.37126/1.62684. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.37832/1.49801. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.36323/1.57247. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.37092/1.56610. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.34895/1.56970. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.39147/1.43399. Took 0.47 sec\n",
      "Epoch 92, Loss(train/val) 0.36518/1.45505. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.34191/1.56415. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.33754/1.64007. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.33820/1.66284. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.36034/1.49205. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.34251/1.57313. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.33143/1.60647. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34256/1.58637. Took 0.46 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69572/0.68046. Took 0.64 sec\n",
      "Epoch 1, Loss(train/val) 0.69372/0.68831. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68914/0.68999. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.68760/0.68231. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68677/0.69076. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68802/0.68541. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68267/0.68267. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68359/0.68161. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68089/0.68226. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67767/0.68886. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.68169/0.68596. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67488/0.69240. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67803/0.69036. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67393/0.69073. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67132/0.68874. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.66993/0.69199. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66974/0.69297. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66511/0.68616. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66631/0.72400. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65874/0.69851. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65958/0.71996. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.64810/0.73330. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64405/0.75607. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64490/0.76556. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63903/0.76090. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.63776/0.76525. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.63742/0.74285. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63347/0.75616. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62901/0.78036. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62981/0.77173. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61070/0.80786. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.60436/0.82753. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59964/0.81965. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.58535/0.83830. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.58476/0.83883. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59024/0.85237. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.57391/0.83119. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58416/0.83437. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.57300/0.82094. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.56162/0.87167. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.57059/0.84700. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.55222/0.86072. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55085/0.85484. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.55023/0.86959. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.54979/0.86921. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.54070/0.86417. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.52762/0.88994. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.52474/0.86464. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.53253/0.89238. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52063/0.83325. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.51562/0.88206. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.50532/0.92638. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.49837/0.91234. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.49546/0.89143. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.48283/0.89674. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.47902/0.92924. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.48217/0.91302. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.48717/0.89205. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.48560/0.86486. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.47065/0.90439. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.46477/0.87939. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.44465/0.90402. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.46324/0.85927. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.43906/0.91801. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.44257/0.87410. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.42095/0.93282. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.41515/0.95066. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.41960/0.95373. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.40655/1.00037. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.42171/0.98638. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.40177/1.00765. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.40087/1.00446. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.41693/1.09268. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.40744/1.02567. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.39188/1.09773. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.38043/1.16066. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.35913/1.08881. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.36859/1.16021. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.35549/1.09479. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44386/1.02581. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.40741/1.03021. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.37305/1.07873. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.47815/1.04574. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.37088/1.00531. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.35311/1.00758. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.34532/1.12487. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.33320/1.14472. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.31216/1.10824. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.32206/1.12861. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.32526/1.07179. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.35286/1.10967. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.33219/1.19247. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.30352/1.12344. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.28064/1.13164. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.32687/1.13981. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.32641/1.19704. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.29956/1.18976. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.28662/1.27796. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.27434/1.28146. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.28560/1.25915. Took 0.44 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69848/0.69118. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69428/0.69083. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69263/0.69364. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68944/0.69484. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68409/0.69305. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.68364/0.70851. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68241/0.71521. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.68161/0.72329. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67778/0.71443. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67371/0.71978. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.66984/0.72419. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67592/0.72134. Took 0.47 sec\n",
      "Epoch 12, Loss(train/val) 0.66913/0.71585. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66787/0.71521. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66911/0.72464. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66318/0.73673. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65996/0.73635. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66088/0.73733. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66050/0.75321. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65331/0.74911. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64787/0.76079. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65032/0.77858. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64438/0.76869. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.65291/0.78780. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64134/0.78932. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64466/0.77848. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64145/0.78278. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.63524/0.79672. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63425/0.80198. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62839/0.79512. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63648/0.78523. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62434/0.78820. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63294/0.77136. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62659/0.78540. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.62100/0.77983. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61139/0.79246. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.60855/0.81494. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60938/0.81589. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60529/0.81644. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.59538/0.84648. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.58997/0.84796. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.59513/0.83720. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.59562/0.81029. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.58382/0.83684. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57995/0.83085. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.57160/0.86647. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56187/0.83653. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56824/0.87783. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.55343/0.92298. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55009/0.90871. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54548/0.88348. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53093/0.93027. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.53529/0.94607. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.53330/0.93048. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53951/0.90532. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.52363/0.98993. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.51503/0.99054. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.51928/0.96675. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50551/1.05757. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53784/1.01389. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.50440/0.99113. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.49296/1.06952. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.48333/0.94380. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.47316/1.08767. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.48763/1.08947. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.47722/1.05690. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.47789/1.05857. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.44861/1.12749. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.45744/1.13063. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.44757/1.16741. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.43725/1.17139. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.43831/1.23089. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.42698/1.10464. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.45228/1.13726. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.43164/1.23066. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.41700/1.12550. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.40061/1.29164. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.40678/1.24491. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.39509/1.22868. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.40902/1.25382. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.38973/1.29998. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.41268/1.15621. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.39685/1.23778. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.40157/1.30453. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.35558/1.29832. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.36958/1.33320. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.37284/1.37997. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.37249/1.46502. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.36272/1.32970. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.35101/1.34426. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.33555/1.59967. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.34328/1.50206. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.33751/1.55542. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.33660/1.53225. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.32745/1.52781. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.31922/1.39793. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.30735/1.56756. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.32874/1.60161. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.32819/1.48388. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34194/1.45007. Took 0.43 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.71187/0.69656. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.71055/0.69280. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.70311/0.70036. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70020/0.70544. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.70047/0.71157. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69118/0.71785. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68957/0.72930. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68640/0.74473. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68602/0.74823. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68911/0.74008. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68016/0.75063. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.68409/0.75295. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.68085/0.75522. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67344/0.75664. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68029/0.77122. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66903/0.76450. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67656/0.76763. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67518/0.74915. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.67128/0.75336. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66806/0.75348. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67146/0.76728. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66868/0.73994. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.66820/0.75066. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.66293/0.77259. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66605/0.76986. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65722/0.74927. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65529/0.76292. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.65413/0.75031. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65659/0.76222. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.64663/0.77784. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64667/0.78742. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64320/0.77417. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63112/0.79110. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63108/0.79011. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.63246/0.80909. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.63394/0.79783. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.61435/0.83731. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.61981/0.82875. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.62121/0.84940. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61231/0.85926. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.61507/0.86033. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60674/0.88641. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60164/0.87191. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.58753/0.85922. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.59750/0.85841. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.57923/0.86089. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57594/0.84555. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57130/0.88776. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.57621/0.90928. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.57375/0.85272. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.57588/0.88959. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.57761/0.86691. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.55733/0.91352. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.55105/0.95115. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54427/0.97023. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.53431/0.98868. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.54115/1.01177. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53701/0.96233. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.53504/1.04185. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53313/1.00097. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.52142/1.00524. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.51834/0.98252. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51069/1.01531. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.51285/1.05158. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.51087/1.05153. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.50522/1.07159. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.49962/1.03456. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48291/1.08967. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.47301/1.17039. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46112/1.18712. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.47164/1.15086. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.46499/1.12610. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.44582/1.16196. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.44094/1.29766. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.45394/1.17677. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.43237/1.29269. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.45536/1.30130. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.44798/1.23927. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43173/1.31047. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.44956/1.30092. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.47185/1.12642. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.41825/1.22654. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.41449/1.30515. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40164/1.29122. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.40134/1.35940. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.42840/1.28832. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.40285/1.31172. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.39036/1.41706. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.39776/1.35955. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.37401/1.39056. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.36278/1.35337. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.35557/1.33457. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.33752/1.39428. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.35592/1.52618. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.35536/1.51628. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.33029/1.52243. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.34795/1.51975. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.33260/1.44795. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.34109/1.53936. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.37381/1.41800. Took 0.45 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69920/0.70040. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69398/0.68973. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69294/0.69368. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69035/0.69461. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68658/0.69531. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.68559/0.69765. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68851/0.69835. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.68280/0.69537. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67789/0.70342. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68131/0.70923. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67499/0.71607. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67263/0.71469. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67413/0.71989. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67134/0.73746. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66838/0.74277. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66553/0.75678. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66300/0.76284. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66383/0.77102. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66354/0.76616. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65940/0.77580. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64981/0.79083. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64760/0.80689. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64650/0.80158. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63934/0.81175. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.63810/0.81022. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.62951/0.79618. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63037/0.80732. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62464/0.82768. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61955/0.83574. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61781/0.85066. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61474/0.83427. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60842/0.83267. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61125/0.84499. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.59892/0.83669. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59549/0.85690. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60119/0.85862. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.59561/0.84885. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.58340/0.86207. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.58145/0.90328. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57909/0.90053. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.57827/0.92706. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.56346/0.84750. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57235/0.90131. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.55957/0.88119. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55694/0.89212. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.54533/0.90804. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.53271/0.94598. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.52670/0.93505. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.52846/0.92030. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.53045/0.94976. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.52674/0.88557. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.51846/0.95164. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.50114/0.89502. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.49657/0.91622. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.50139/0.94157. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.48479/0.94867. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.48115/0.93936. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.48892/0.98576. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.47992/0.97914. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.46157/1.07018. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.46091/1.07142. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.45514/1.06931. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.45835/1.07720. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.45732/1.01945. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.45571/1.01385. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.45638/1.09218. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.43051/1.17499. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.41106/1.19797. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.44292/1.13117. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.43054/1.06351. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.40998/1.07143. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.43892/1.12357. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.39613/1.17763. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.39360/1.17345. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.38596/1.23380. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.37942/1.11771. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.42862/1.22980. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.42576/1.15041. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.37186/1.20064. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.36905/1.35747. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.35579/1.23143. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.35270/1.25182. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.35121/1.32753. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.34149/1.33677. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.36289/1.30066. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.36521/1.34066. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.34737/1.32537. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.34783/1.52862. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.31508/1.50829. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.33152/1.26459. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.30029/1.40480. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.30934/1.40960. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.29429/1.43629. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.29199/1.62032. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.32489/1.48629. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.27575/1.46952. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.30213/1.55535. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.30982/1.56073. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.28394/1.60106. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.29517/1.68280. Took 0.44 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69497/0.69580. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69024/0.69717. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68798/0.70712. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68921/0.70267. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68598/0.70296. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.68420/0.70590. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68298/0.70438. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67799/0.70800. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67681/0.70048. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68147/0.70997. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.67533/0.72074. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67481/0.73181. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67089/0.75407. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66524/0.76412. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66064/0.78312. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.65622/0.79741. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.65614/0.80602. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65021/0.80045. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65126/0.82021. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64388/0.82108. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64108/0.84130. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.63920/0.83530. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63099/0.83980. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63449/0.83240. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.62073/0.85991. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63148/0.84467. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.62083/0.85085. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61706/0.87010. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.61327/0.88847. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60112/0.90399. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60989/0.89041. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.59946/0.92783. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.59115/0.96210. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.59879/0.98799. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59919/0.99317. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58598/0.99399. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.60117/0.96637. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.59457/0.94398. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58257/0.96240. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58127/0.96743. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.57188/1.00134. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.57089/1.01058. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56702/1.00895. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.55539/1.04954. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.55989/1.00990. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.56665/1.00179. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.55687/1.03434. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55888/0.98918. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.54897/1.02678. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56480/1.03185. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54491/1.04673. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.54050/1.09063. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.53799/1.05078. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.53326/1.02824. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54482/0.98501. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.52743/1.05913. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.52643/1.05679. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.52833/1.03410. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.52037/1.05020. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50199/1.07267. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51500/1.02593. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.50076/1.05895. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50175/1.04251. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.49146/1.09727. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.52082/1.11543. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50105/1.13003. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.48012/1.12160. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.48869/1.10485. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.48825/1.07954. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47456/1.12194. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.48265/1.12687. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45106/1.20672. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.44619/1.20838. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.45013/1.14773. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.45654/1.22508. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.45091/1.14577. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43401/1.23789. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.41951/1.18802. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.42012/1.28046. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.43247/1.27040. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.44321/1.28926. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.40746/1.28113. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.42495/1.27936. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.43543/1.26029. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.40571/1.26708. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.44950/1.18377. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.40913/1.25680. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.40899/1.29447. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.40782/1.28495. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.41131/1.23811. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.39863/1.26980. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.39806/1.24946. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.37889/1.35503. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.38702/1.34338. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.37447/1.31816. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.40366/1.24318. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.38844/1.30068. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.38825/1.38871. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.37672/1.25775. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.39316/1.27929. Took 0.44 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.70202/0.67664. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70002/0.68107. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.70038/0.68265. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69104/0.68661. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68937/0.69549. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69115/0.70399. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68832/0.70930. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.69256/0.71123. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68422/0.70913. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68139/0.72479. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68073/0.73676. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67707/0.73433. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67450/0.72356. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68130/0.72507. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.66885/0.74310. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66508/0.74743. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66832/0.75246. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67058/0.75444. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66341/0.77118. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66291/0.77896. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65843/0.80923. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65522/0.80578. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66049/0.80760. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.65010/0.81528. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64731/0.81234. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64581/0.82714. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64451/0.81962. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64166/0.82573. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63980/0.82960. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64272/0.83309. Took 0.47 sec\n",
      "Epoch 30, Loss(train/val) 0.63830/0.84863. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62828/0.85493. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63194/0.84831. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62310/0.87103. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62261/0.88250. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61550/0.88330. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.61168/0.91790. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60963/0.91569. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.60476/0.94557. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60125/0.97112. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60480/0.99676. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.59506/0.98050. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59209/1.00624. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.59411/1.03197. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58194/1.05844. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.57770/1.04205. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58037/1.03675. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57291/1.11221. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.57630/1.09335. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56663/1.06837. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.56480/1.04369. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54942/1.08998. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56110/1.05416. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.55939/1.06421. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55844/1.10179. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.54583/1.17256. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.54716/1.16521. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.53768/1.18170. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54817/1.10759. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.53641/1.17153. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.53199/1.16471. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.52129/1.17459. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.54647/1.12262. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.54111/1.13459. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.51811/1.17170. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.52394/1.13254. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.50726/1.18518. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.51646/1.11274. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.51509/1.18689. Took 0.48 sec\n",
      "Epoch 69, Loss(train/val) 0.52160/1.16804. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.50014/1.16716. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.48169/1.28182. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48515/1.29156. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48578/1.28275. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.49978/1.33512. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.47673/1.34759. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.48916/1.29157. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.48387/1.27672. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.46807/1.32749. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.46493/1.35851. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.47169/1.34418. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.45843/1.33998. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.44505/1.38927. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.47029/1.34042. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.49393/1.24411. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.45232/1.33830. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.46550/1.33378. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.47004/1.26058. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.44830/1.36810. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.44236/1.32993. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.44814/1.32336. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.42909/1.37949. Took 0.47 sec\n",
      "Epoch 92, Loss(train/val) 0.42776/1.35630. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.43706/1.34862. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.39559/1.52155. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.41513/1.38018. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.43071/1.47924. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.43034/1.37128. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.42588/1.35672. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.40044/1.45980. Took 0.44 sec\n",
      "ACC: 0.3645833333333333\n",
      "Epoch 0, Loss(train/val) 0.69477/0.74292. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69279/0.74279. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68886/0.74560. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68497/0.72883. Took 0.49 sec\n",
      "Epoch 4, Loss(train/val) 0.68448/0.74340. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68257/0.72620. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.68283/0.71806. Took 0.48 sec\n",
      "Epoch 7, Loss(train/val) 0.68224/0.73468. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67758/0.73193. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67341/0.74744. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67983/0.75921. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67799/0.76911. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67119/0.76882. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.67095/0.79867. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66771/0.80294. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66348/0.80767. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65731/0.80495. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65513/0.81425. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.64924/0.82604. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64041/0.85361. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63789/0.84534. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64305/0.84920. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.63232/0.84300. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.61953/0.87985. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.61930/0.84689. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.62156/0.85366. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.60404/0.89924. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.60105/0.89528. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.60300/0.84863. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60383/0.81857. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.58240/0.84543. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.57987/0.84329. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.56958/0.85371. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.57123/0.83520. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.55733/0.85848. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.56397/0.82236. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.56118/0.82611. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.54369/0.84962. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.54702/0.84510. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.53583/0.87973. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.54039/0.84590. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.52950/0.84842. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.50566/0.95089. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.51515/0.89988. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.49241/0.92383. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.50160/0.96494. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.49356/0.96162. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.49393/0.97704. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.47258/0.97642. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.47269/0.98817. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.47978/0.97830. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.46973/0.99868. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.48035/1.05498. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.50293/1.05949. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.46426/1.00104. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.45955/1.03382. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.45209/1.04181. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.43949/1.14068. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.45139/1.11007. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.44109/1.13836. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.42510/1.13881. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.44135/1.10769. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.43369/1.20730. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.42576/1.19708. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.42039/1.32095. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.40444/1.23666. Took 0.47 sec\n",
      "Epoch 66, Loss(train/val) 0.39431/1.25796. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.38235/1.37967. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.40715/1.29600. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.39448/1.29256. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.36587/1.28201. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.37056/1.30694. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.38399/1.33920. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.36750/1.29124. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.37329/1.38073. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.34456/1.46638. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.38066/1.35271. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.36156/1.28874. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.35311/1.37185. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.37072/1.36081. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.34440/1.45810. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.38160/1.32553. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.37805/1.36643. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.39228/1.42418. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.34063/1.42862. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.33228/1.44400. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.31576/1.55873. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.33868/1.41377. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.37008/1.49303. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.34020/1.43897. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.30493/1.47628. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.28189/1.52522. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.29104/1.51990. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.28239/1.67075. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.31226/1.50973. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.29274/1.50370. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.25742/1.60719. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.26234/1.76650. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.29182/1.63576. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.32459/1.69935. Took 0.45 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.72032/0.72417. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.70542/0.72793. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.70289/0.72040. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69803/0.71514. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69278/0.71211. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.69833/0.70780. Took 0.49 sec\n",
      "Epoch 6, Loss(train/val) 0.69249/0.71481. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69788/0.70729. Took 0.48 sec\n",
      "Epoch 8, Loss(train/val) 0.69020/0.71521. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68494/0.72532. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68484/0.72807. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68288/0.73090. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.68088/0.73921. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68097/0.74267. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.68159/0.74315. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68336/0.71507. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67716/0.71411. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.67295/0.71343. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67431/0.71095. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67815/0.72360. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.67379/0.72794. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66687/0.72133. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.66438/0.73320. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66032/0.72598. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.65410/0.73297. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64938/0.75058. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64059/0.75119. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64506/0.76351. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64053/0.76517. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.64367/0.74809. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.63268/0.75921. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.61628/0.78182. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.62249/0.82739. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61502/0.85231. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60682/0.85285. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60474/0.89771. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.60232/0.97018. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59394/0.91668. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60855/0.92781. Took 0.47 sec\n",
      "Epoch 39, Loss(train/val) 0.59713/0.96426. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58613/0.97475. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.57885/0.97295. Took 0.47 sec\n",
      "Epoch 42, Loss(train/val) 0.58387/1.01132. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58220/0.98262. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57830/1.00634. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.58102/0.98512. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57306/1.06761. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.58052/1.06396. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57968/1.02259. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56497/1.00087. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.56622/1.07631. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54643/1.07616. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54461/1.10210. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.52661/1.15167. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55463/1.07384. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.53557/1.14424. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.53660/1.13840. Took 0.47 sec\n",
      "Epoch 57, Loss(train/val) 0.52841/1.19068. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.52363/1.14354. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.51281/1.13745. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.49910/1.17779. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50603/1.14126. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.50351/1.17253. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.50405/1.19996. Took 0.47 sec\n",
      "Epoch 64, Loss(train/val) 0.47841/1.23394. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.47499/1.25284. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.48408/1.26230. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.46313/1.31374. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.48281/1.29923. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.46020/1.33345. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.46628/1.35178. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45182/1.38822. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.47066/1.32336. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.43714/1.39822. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.45097/1.34498. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.42782/1.45079. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.44366/1.40482. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.41908/1.50807. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43198/1.47614. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.41478/1.54052. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.42221/1.51861. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.41509/1.55726. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.40976/1.53627. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40982/1.54217. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.41132/1.48721. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.37432/1.56552. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.39875/1.59771. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.39379/1.70838. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.39645/1.62511. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.39166/1.60002. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.38166/1.66934. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.38386/1.55234. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.38258/1.61709. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.38909/1.60688. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.37581/1.62748. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.34526/1.62499. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.38148/1.70256. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.36398/1.59017. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.34951/1.67950. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34016/1.66437. Took 0.46 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.71457/0.70303. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70719/0.70353. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69462/0.73961. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69982/0.72034. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.70531/0.70536. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69096/0.72458. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.70084/0.70897. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69570/0.70539. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69293/0.71563. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68005/0.72468. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68602/0.73316. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.68075/0.73347. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.68448/0.72053. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68285/0.73795. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67891/0.71829. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67707/0.73859. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66418/0.74304. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.67212/0.75243. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.67202/0.74469. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66499/0.75992. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.67502/0.75654. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66691/0.76976. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.66416/0.77430. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66754/0.78323. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65867/0.79586. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.65726/0.83625. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65103/0.84835. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.65694/0.86744. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.65230/0.87932. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63826/0.89380. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.62967/0.94800. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63554/0.97994. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.64232/0.93598. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.63635/0.98303. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62502/0.95221. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.62881/0.97812. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.62652/0.99385. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61791/0.99734. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.62036/1.03819. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.62882/0.97448. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.60832/1.02245. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.62486/1.00204. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.61157/1.02712. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.60516/1.07502. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.61687/1.03111. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.60180/1.06581. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.60225/1.05970. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.59923/1.06487. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.59793/1.04716. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.60055/1.05737. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.59950/1.04525. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.58981/1.03933. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.57479/1.13386. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.58159/1.09321. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.58970/1.09163. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.57915/1.09523. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.57990/1.12827. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.57673/1.13198. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.55735/1.20335. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.55831/1.19116. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.55468/1.19050. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.53813/1.20569. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.54778/1.26153. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.55281/1.20287. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.54792/1.22293. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.52951/1.25332. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.53954/1.19733. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.52264/1.22659. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.54237/1.24403. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.52876/1.19830. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.50987/1.22897. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.50763/1.29331. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.50792/1.22662. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.50530/1.31057. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.51475/1.28305. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.49294/1.25070. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.49453/1.37457. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.47096/1.33813. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.47925/1.37770. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.49029/1.43274. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.47642/1.41367. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.48609/1.33486. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.46934/1.36423. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.46597/1.40343. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.44434/1.41925. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.45070/1.41425. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.42581/1.45085. Took 0.47 sec\n",
      "Epoch 87, Loss(train/val) 0.43045/1.46734. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.44611/1.57969. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.42352/1.52533. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.40921/1.55049. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.43474/1.54973. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.41624/1.53953. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.40813/1.58156. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40637/1.55446. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.40509/1.63811. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.40732/1.65209. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.39704/1.65663. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.38780/1.61089. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.38425/1.56160. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.70494/0.69267. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69348/0.70421. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69056/0.70206. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68537/0.70250. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69513/0.71079. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69112/0.71954. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.69021/0.73270. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68677/0.74651. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68343/0.75598. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.68311/0.75176. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68344/0.75670. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67674/0.76883. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67650/0.76406. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.67958/0.76560. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67541/0.77681. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67154/0.79365. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.66796/0.79142. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66628/0.79979. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65628/0.83469. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66181/0.82546. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.66000/0.82277. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64006/0.84407. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.65531/0.87464. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64406/0.87047. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.64345/0.92077. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63856/0.92842. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.63622/0.92388. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63557/0.90973. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62600/0.93032. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62829/0.92695. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61675/0.96370. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.61115/0.95265. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.61790/0.93395. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.61642/0.93939. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61527/0.94411. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62329/0.88956. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.61436/0.91542. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59948/0.91506. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61070/0.91192. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.60342/0.93595. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60088/0.92256. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57531/0.95818. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56874/0.96932. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58390/0.96008. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.58104/0.97637. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57518/0.95307. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55972/0.97767. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.57098/1.01852. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55338/1.03988. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.54216/1.05384. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53754/1.06966. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.51896/1.12934. Took 0.47 sec\n",
      "Epoch 52, Loss(train/val) 0.53791/1.13571. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.51978/1.12973. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53120/1.14858. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.52733/1.12870. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.48984/1.16185. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.48796/1.24109. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.51527/1.19280. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.50484/1.23149. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.51176/1.21607. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.49764/1.20630. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48493/1.28343. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.48137/1.29029. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.48978/1.26488. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.47649/1.25170. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.49279/1.22802. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48587/1.22925. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.47189/1.27995. Took 0.47 sec\n",
      "Epoch 69, Loss(train/val) 0.47347/1.32900. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.45418/1.41732. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.46038/1.32654. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.45390/1.37437. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.43148/1.37728. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.45134/1.40841. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.43038/1.38998. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.43056/1.36496. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.42070/1.37350. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.41963/1.46034. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.42459/1.51225. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.42927/1.52716. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.42405/1.46091. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.41724/1.41545. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.41161/1.46013. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.40227/1.46768. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.40759/1.52212. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.42310/1.46037. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.40197/1.51425. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.37219/1.52941. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.37695/1.49894. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.38406/1.55037. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.37777/1.62382. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.39086/1.55301. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.38120/1.61014. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40375/1.63910. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.36741/1.49056. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.35869/1.57646. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.38932/1.52608. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.34431/1.62513. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.33927/1.72018. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69856/0.70064. Took 0.62 sec\n",
      "Epoch 1, Loss(train/val) 0.68883/0.69116. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68901/0.68674. Took 0.49 sec\n",
      "Epoch 3, Loss(train/val) 0.68857/0.68535. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68353/0.68255. Took 0.48 sec\n",
      "Epoch 5, Loss(train/val) 0.68397/0.67763. Took 0.48 sec\n",
      "Epoch 6, Loss(train/val) 0.68029/0.67702. Took 0.48 sec\n",
      "Epoch 7, Loss(train/val) 0.68172/0.67978. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67742/0.67994. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67431/0.67790. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67545/0.66992. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.67134/0.67572. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67271/0.67815. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66853/0.68276. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66267/0.68052. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66745/0.68060. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66361/0.68893. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66106/0.70114. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66090/0.68280. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65450/0.69389. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65384/0.69099. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64733/0.70417. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64980/0.70244. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64437/0.71222. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.64830/0.71436. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64055/0.72618. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63593/0.73937. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62767/0.72482. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62062/0.75342. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61937/0.75525. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.61896/0.74596. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60686/0.76687. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.60754/0.77702. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61067/0.79106. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60043/0.79175. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.59900/0.78321. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.59459/0.76985. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.58863/0.77461. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.58038/0.78348. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.56740/0.77455. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57653/0.78496. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.57151/0.78704. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55582/0.80081. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.55868/0.78214. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.54113/0.81154. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.53967/0.82848. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.53365/0.82012. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.53091/0.89857. Took 0.48 sec\n",
      "Epoch 48, Loss(train/val) 0.54528/0.93176. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.53582/0.89176. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.54519/0.88912. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.51240/0.88553. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.53174/0.94656. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.51558/0.93211. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.50972/0.95075. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.49030/0.95901. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.50959/0.98289. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.49545/1.03713. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.48744/1.03839. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.47449/1.01798. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.46238/1.10464. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.45863/1.05853. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.46344/1.13401. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.46049/1.13956. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.44530/1.12577. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.43972/1.15333. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.43028/1.20882. Took 0.47 sec\n",
      "Epoch 67, Loss(train/val) 0.40816/1.21382. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.43954/1.17050. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.40235/1.26865. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.41655/1.28763. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.39674/1.26544. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.39263/1.30911. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.39216/1.26481. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.38654/1.30379. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.39773/1.33189. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.36901/1.27927. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.36499/1.39604. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.37071/1.39649. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.36493/1.42715. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.38129/1.29602. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.32923/1.33562. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.33611/1.41774. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.32975/1.41033. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.33375/1.57389. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.32848/1.35037. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.33092/1.47608. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.34048/1.46188. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.30899/1.50554. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.30837/1.58600. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.33230/1.48381. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.29510/1.59590. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.30210/1.55686. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.29176/1.68931. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.28430/1.69241. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.28083/1.63964. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.29411/1.62318. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.30931/1.57669. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.26827/1.59297. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.26970/1.71532. Took 0.45 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70666/0.68689. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.70019/0.67689. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69508/0.68026. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69460/0.68707. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68806/0.69266. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.68977/0.69669. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68084/0.69706. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68015/0.69973. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.67966/0.69912. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67425/0.70571. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67196/0.73018. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67130/0.73665. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.65924/0.75224. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65990/0.78343. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65459/0.76854. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66594/0.76140. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.64774/0.77452. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65407/0.77443. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.64766/0.77956. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.64568/0.79393. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64181/0.80561. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63158/0.81457. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.62779/0.80425. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64347/0.81148. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.61856/0.82558. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.61609/0.84444. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.60921/0.83127. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61925/0.87204. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.59799/0.87304. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60079/0.89767. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.59423/0.89870. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.59390/0.88375. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.59604/0.90320. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.58348/0.88890. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.57997/0.91073. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.57566/0.90408. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.58427/0.90442. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.55922/0.95102. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.56602/0.95071. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.55871/0.94955. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.55372/0.97053. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.53618/0.98070. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.54146/0.99376. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.54129/0.99258. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.52930/0.99562. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.54953/1.00409. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.53732/1.03445. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.51704/1.03338. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.52729/1.06331. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.53094/1.01453. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.50597/1.11139. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.50916/1.10311. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.51955/1.04550. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.50267/1.10959. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.49662/1.10261. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.50499/1.14510. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.49360/1.16277. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.50083/1.15461. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.48796/1.17297. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.47175/1.19535. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.45973/1.19303. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.49025/1.23196. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.45941/1.19684. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.47301/1.20268. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.47619/1.20458. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.46826/1.20891. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.44759/1.19272. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.45858/1.21065. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.44069/1.24331. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.42693/1.24880. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.42355/1.33714. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.42431/1.31396. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.43680/1.33677. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.40027/1.34702. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.40085/1.38258. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.40346/1.39912. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.39581/1.36939. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.39406/1.41123. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.39198/1.39671. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.39246/1.40213. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.36874/1.51813. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.38039/1.40761. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.37203/1.47619. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.37493/1.48882. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.38041/1.45122. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.37455/1.48612. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.41553/1.42015. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.40956/1.41450. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.36992/1.47253. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.37881/1.36351. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.34077/1.45710. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.31699/1.51150. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.32556/1.65045. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.32164/1.61672. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.32086/1.55892. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.33839/1.60596. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.31841/1.68139. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.31125/1.70071. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.33253/1.51546. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.29712/1.58782. Took 0.46 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69836/0.70443. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68974/0.70920. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.68941/0.70846. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.67872/0.71760. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68109/0.71778. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.67840/0.71990. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67716/0.74729. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67307/0.73449. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.67395/0.73903. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66023/0.75755. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.66366/0.76440. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66574/0.77759. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.65988/0.81422. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.66181/0.80738. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.64989/0.85000. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.64138/0.87728. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.64041/0.87577. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.63569/0.87050. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.63367/0.89829. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.63987/0.92040. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.62440/0.93871. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.62917/0.93302. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.61056/0.96910. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.61408/0.95156. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.61373/0.97789. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.61691/0.98095. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61505/0.95934. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.59874/0.99037. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.60006/0.93749. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.58882/0.93486. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.58509/0.98133. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.56200/1.05182. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.56927/1.09666. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.56540/1.07487. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.56421/1.08127. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.56321/1.05308. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.55703/1.07589. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.55379/1.09977. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.54135/1.08337. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.54467/1.10896. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.53216/1.07198. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.53424/1.10927. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.52702/1.09381. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.52014/1.13315. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.52321/1.15025. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.50524/1.10545. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.50944/1.15046. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.50147/1.13860. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.49036/1.13183. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.49862/1.13913. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.47796/1.15072. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.49177/1.15449. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.48510/1.13860. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.47247/1.13946. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.45448/1.18925. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.44900/1.22886. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.45061/1.21012. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.45051/1.18607. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.46397/1.17917. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.42792/1.15359. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.43183/1.20342. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.43364/1.30408. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.45212/1.11077. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.42028/1.20676. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.41387/1.15457. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.41856/1.25259. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.42697/1.20481. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.39848/1.21306. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.41270/1.19623. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.39335/1.27290. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.40832/1.20959. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.38222/1.31879. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.37582/1.29303. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.39073/1.33473. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.39267/1.28761. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.33650/1.39550. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.34646/1.38880. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.34954/1.39387. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.36285/1.35806. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.37412/1.39645. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.39055/1.31809. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.36409/1.25936. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.37473/1.31905. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.33880/1.35656. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.33805/1.35821. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.38994/1.30139. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.35368/1.36226. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.31518/1.46179. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.34871/1.37928. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.31672/1.43642. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.30057/1.54928. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.30374/1.47234. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.31623/1.52241. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.30859/1.51252. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.30839/1.52669. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.27275/1.58140. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.28395/1.53774. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.26882/1.57742. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.27151/1.66145. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.24647/1.61768. Took 0.45 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.71631/0.69206. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.71123/0.70419. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.69005/0.69429. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69374/0.70850. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.69965/0.70968. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69146/0.70756. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68026/0.70851. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68845/0.70516. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68225/0.69964. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68962/0.71500. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68060/0.73666. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.68542/0.71830. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68389/0.72900. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68168/0.73313. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67931/0.73031. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67566/0.72038. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67191/0.72803. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.67788/0.73298. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.67078/0.73369. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.66661/0.75812. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.67465/0.76383. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66424/0.75567. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66130/0.77534. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66259/0.76642. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.65907/0.77187. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65981/0.77358. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65903/0.76570. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65244/0.77755. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.65154/0.80426. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.65731/0.79687. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.64956/0.80127. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.65059/0.79778. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64759/0.80848. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.64530/0.81830. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.64237/0.84608. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.63701/0.85220. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62949/0.84733. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.62924/0.85311. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.62820/0.88527. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.63310/0.89207. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.63591/0.88400. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.62226/0.89270. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.62196/0.91726. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.61714/0.89630. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.62034/0.93241. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.62564/0.90105. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.60597/0.92201. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.62189/0.91114. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.62243/0.89194. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.61451/0.90366. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.61080/0.92781. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.60542/0.96570. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.59246/0.97475. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.61045/0.94539. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.61512/0.92638. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.60424/0.92264. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.59987/0.92448. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.59663/0.90616. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.60731/0.87698. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.59086/0.90507. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.57978/0.95563. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.58211/0.96249. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.58890/0.93348. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.57592/0.97355. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.58039/0.99234. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.55853/0.98447. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.57393/1.00320. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.56980/1.02062. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.55890/1.06917. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.58730/0.99530. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.57517/0.99230. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.56547/1.03721. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.55300/1.02594. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.53642/1.11986. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.54982/1.03868. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.54370/1.05113. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.53715/1.03425. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.52668/1.04707. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.52719/1.11500. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.51099/1.14766. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.51288/1.17825. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.50367/1.16094. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.49754/1.15431. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.52963/1.07968. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.48898/1.14847. Took 0.47 sec\n",
      "Epoch 85, Loss(train/val) 0.50995/1.18341. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.49435/1.12343. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.51782/1.10159. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.47862/1.15861. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.49151/1.17046. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.46676/1.17687. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.45947/1.29649. Took 0.47 sec\n",
      "Epoch 92, Loss(train/val) 0.47979/1.20656. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.47577/1.16295. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.45168/1.22667. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.44837/1.23613. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.48390/1.17940. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.46239/1.15608. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.45812/1.30723. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.48208/1.12198. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69800/0.69795. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69119/0.69952. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69155/0.69506. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.68800/0.69493. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68991/0.68986. Took 0.48 sec\n",
      "Epoch 5, Loss(train/val) 0.68167/0.68705. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.68166/0.69937. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68039/0.69371. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67739/0.69362. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67436/0.68812. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67954/0.69145. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67195/0.68707. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66868/0.69203. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66466/0.68777. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66801/0.68820. Took 0.47 sec\n",
      "Epoch 15, Loss(train/val) 0.66660/0.69230. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66615/0.69358. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66360/0.69338. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66140/0.69424. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66417/0.69666. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.66590/0.69905. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65820/0.70964. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.65120/0.72161. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64736/0.72756. Took 0.47 sec\n",
      "Epoch 24, Loss(train/val) 0.65203/0.71854. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64776/0.72068. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.64451/0.71496. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.62638/0.72706. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64769/0.71986. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.64527/0.70911. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63598/0.69923. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62573/0.71816. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.61466/0.73258. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61243/0.73551. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.62794/0.72635. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61218/0.74648. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60545/0.75175. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.61037/0.76769. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59418/0.74846. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.59915/0.77014. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60670/0.75627. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58916/0.79009. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.59868/0.77417. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60371/0.77007. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.57812/0.77927. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.59235/0.77399. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58820/0.77142. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.57608/0.81046. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57914/0.77775. Took 0.48 sec\n",
      "Epoch 49, Loss(train/val) 0.56615/0.80257. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.55509/0.79918. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.58346/0.80123. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.57775/0.82325. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.57390/0.79164. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.55906/0.84977. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55051/0.85268. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55210/0.84150. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.53901/0.86031. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.55568/0.86834. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.55576/0.83910. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.53560/0.87740. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.52331/0.88196. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.55330/0.88013. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51728/0.92737. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.52414/0.90402. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.51147/0.87397. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.50299/0.92303. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.50977/0.93665. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49535/0.95694. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.50280/0.92717. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.51532/0.94246. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.49724/0.93786. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.49215/0.95497. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.50126/0.97360. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.49040/0.98955. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.50130/0.95943. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.49280/0.93755. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.47676/0.99050. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.47768/0.96626. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.47893/1.00154. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.48343/1.00907. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.47205/0.98704. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.44096/1.03388. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.45149/1.04520. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.44795/1.01680. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.48020/1.00010. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43191/1.05336. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43058/1.06593. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.43317/1.10032. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.42584/1.09028. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.41025/1.13683. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.44988/1.11534. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.41438/1.09832. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.40972/1.12804. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.42231/1.16419. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.40164/1.09922. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.40351/1.11574. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.40726/1.14634. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.40235/1.12973. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.42599/1.07811. Took 0.44 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.70575/0.69720. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69931/0.70720. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69521/0.70784. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69211/0.71210. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68668/0.71887. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68778/0.71626. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67830/0.72850. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67656/0.73703. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67207/0.75084. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66816/0.74822. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.66113/0.74996. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.65983/0.76922. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65476/0.78243. Took 0.47 sec\n",
      "Epoch 13, Loss(train/val) 0.64915/0.79322. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.64733/0.80984. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.64133/0.80649. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.64412/0.82664. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.63458/0.83360. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.63264/0.81093. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.63022/0.82384. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.62423/0.82240. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.62536/0.82350. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.60874/0.83098. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.60488/0.80701. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.60387/0.82403. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.59835/0.85543. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.60032/0.86650. Took 0.47 sec\n",
      "Epoch 27, Loss(train/val) 0.58654/0.89041. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.58850/0.90510. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.58974/0.89635. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.57851/0.91163. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.56686/0.91496. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.55837/0.96264. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.55075/0.95827. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.54341/0.96165. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.55345/0.99995. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.54185/0.99611. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.54138/1.02469. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.53952/1.01838. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.52400/1.00159. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.52852/1.04104. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.50936/1.01020. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.50339/1.04365. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.49794/1.06051. Took 0.47 sec\n",
      "Epoch 44, Loss(train/val) 0.48634/1.07052. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.50437/1.08144. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.50391/1.06789. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.47765/1.14133. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.48749/1.10030. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.48282/1.05632. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.45509/1.13711. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.47984/1.15943. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.47884/1.13096. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.45373/1.11244. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.46819/1.08517. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.45204/1.15426. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.43937/1.18285. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.44603/1.15875. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.43661/1.21758. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.44046/1.32069. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.47837/1.16197. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.45064/1.29529. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.40772/1.32681. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.40057/1.31049. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.41269/1.35056. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.41946/1.36183. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.38634/1.31025. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.40712/1.33285. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.37100/1.40530. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.38479/1.44493. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.38614/1.40334. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.39298/1.40150. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.38221/1.38198. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.37932/1.46591. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.35248/1.44865. Took 0.47 sec\n",
      "Epoch 75, Loss(train/val) 0.36737/1.42985. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.37505/1.41356. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.36754/1.52028. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.35775/1.46249. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.36223/1.47467. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.34857/1.53854. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.35753/1.48918. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.32341/1.52895. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.33551/1.45466. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.33164/1.50062. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.33747/1.50701. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.33116/1.45233. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.35191/1.54921. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.31913/1.54775. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.32079/1.59297. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.30634/1.56081. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.31651/1.54105. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.28499/1.57002. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.30361/1.60671. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.31235/1.62220. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.30768/1.51549. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.26121/1.64279. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.25680/1.60722. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.24421/1.61391. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.27843/1.62926. Took 0.45 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.71145/0.69380. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69634/0.67814. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.68751/0.66777. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.68221/0.67454. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68072/0.67326. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67688/0.69234. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.67678/0.69963. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67396/0.69735. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67279/0.69296. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.66712/0.70447. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66387/0.70261. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.65555/0.71502. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66725/0.70562. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65539/0.70864. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.64741/0.69775. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65404/0.70991. Took 0.47 sec\n",
      "Epoch 16, Loss(train/val) 0.65116/0.69475. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64182/0.72714. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65230/0.71456. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.64335/0.71746. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64305/0.68361. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64456/0.69001. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64358/0.68649. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62880/0.69506. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.61840/0.68518. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62059/0.69195. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.61170/0.68154. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.61756/0.68293. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.61079/0.67220. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60468/0.66699. Took 0.50 sec\n",
      "Epoch 30, Loss(train/val) 0.59359/0.65398. Took 0.47 sec\n",
      "Epoch 31, Loss(train/val) 0.59366/0.63766. Took 0.48 sec\n",
      "Epoch 32, Loss(train/val) 0.57887/0.65956. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.58804/0.69035. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.58225/0.68527. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.57600/0.67813. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.56759/0.67469. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.57393/0.70879. Took 0.47 sec\n",
      "Epoch 38, Loss(train/val) 0.58035/0.67136. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57992/0.71903. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56013/0.71473. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.54931/0.71226. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.54547/0.73177. Took 0.47 sec\n",
      "Epoch 43, Loss(train/val) 0.54511/0.76410. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.53738/0.73760. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.52260/0.72279. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.51841/0.75931. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.52431/0.76407. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.51848/0.78608. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.51083/0.79260. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.49380/0.82861. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.49003/0.80470. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.51820/0.86949. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.53259/0.84754. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.48389/0.84301. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.48451/0.84688. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.48621/0.92407. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.48757/0.90453. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.48135/0.90172. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.48459/0.90746. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51319/0.81829. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.48226/0.85785. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.47555/0.86703. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.47049/0.88484. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.45462/0.87672. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.46748/0.88082. Took 0.47 sec\n",
      "Epoch 66, Loss(train/val) 0.44818/0.86126. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.44113/0.93435. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.44902/0.91998. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.43078/0.91018. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.43170/0.91355. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.42636/0.94925. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.42441/0.94699. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.42887/0.94757. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.44242/0.94579. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.42418/0.98528. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.42431/0.99566. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.41943/1.04740. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.42230/1.03254. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.40092/1.07586. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.42336/1.05919. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.39680/0.97214. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.41512/1.02079. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.41479/1.07512. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.40875/1.06672. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.39694/1.10479. Took 0.48 sec\n",
      "Epoch 86, Loss(train/val) 0.38897/1.06856. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.39085/1.08709. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.37592/1.04844. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.40990/0.99541. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.40127/1.05230. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.37198/1.19399. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.37177/1.12944. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.36866/1.14383. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.37412/1.08823. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.36364/1.12951. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.35734/1.13099. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.35958/1.08092. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.34424/1.16676. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36392/1.14734. Took 0.44 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.70039/0.70322. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.68691/0.69794. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68056/0.69534. Took 0.49 sec\n",
      "Epoch 3, Loss(train/val) 0.68379/0.69625. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68034/0.70678. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67693/0.70491. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.67921/0.71082. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67531/0.70759. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67120/0.71426. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67645/0.72002. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67012/0.72279. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66807/0.70953. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66647/0.71719. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66737/0.72949. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.65757/0.73275. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66304/0.71260. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66034/0.70311. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65480/0.70494. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65292/0.70683. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65044/0.72381. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64941/0.73933. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64143/0.74427. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.63819/0.75422. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62885/0.75380. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63551/0.75649. Took 0.47 sec\n",
      "Epoch 25, Loss(train/val) 0.63026/0.77948. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63667/0.77661. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62245/0.79028. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61463/0.80375. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61602/0.79035. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.61502/0.81837. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.59207/0.83061. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60713/0.82573. Took 0.47 sec\n",
      "Epoch 33, Loss(train/val) 0.59689/0.85965. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59688/0.84478. Took 0.47 sec\n",
      "Epoch 35, Loss(train/val) 0.59482/0.85563. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58210/0.87827. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57040/0.92916. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.55584/0.94919. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.56136/0.94588. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.55450/0.95184. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.56025/0.99726. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55901/0.98006. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.54666/1.01994. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.53954/1.01467. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.52908/1.02928. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.51428/1.05000. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.51652/1.06050. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.50954/1.06453. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.50765/1.05895. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.48866/1.03826. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.49573/1.08944. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.49204/1.05297. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.47334/1.08564. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.49715/1.03649. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.48221/1.09008. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.47463/1.04408. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.44551/1.15207. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.44084/1.14923. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.43929/1.16173. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.42817/1.18605. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.48436/1.08809. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.46100/1.11613. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.42632/1.10330. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.42676/1.05535. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.42004/1.09847. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.41035/1.09396. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.41992/1.13146. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.42162/1.14635. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.39814/1.12853. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.42499/1.14730. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.37275/1.24161. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.37895/1.22401. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.38626/1.24928. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.38057/1.26075. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.36808/1.25054. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.37664/1.30604. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.36853/1.27816. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.36094/1.27366. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.37013/1.33370. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.34194/1.31115. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.34092/1.34343. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.44353/1.14720. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.39451/1.20440. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.36514/1.19807. Took 0.47 sec\n",
      "Epoch 85, Loss(train/val) 0.33395/1.24192. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.31836/1.36705. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.33579/1.33850. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.33361/1.32189. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.33148/1.40001. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.29695/1.41196. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.27683/1.40641. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.27034/1.45572. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.28226/1.39756. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.28297/1.46216. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.28557/1.49082. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.26863/1.54799. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.28271/1.42511. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.26423/1.42392. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.27266/1.44484. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.71705/0.71000. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.70567/0.70679. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69982/0.70873. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69845/0.70829. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69155/0.71018. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69956/0.71128. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68315/0.71480. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69457/0.71721. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68417/0.70926. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68283/0.71017. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68158/0.70670. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.68060/0.70211. Took 0.48 sec\n",
      "Epoch 12, Loss(train/val) 0.68635/0.70130. Took 0.49 sec\n",
      "Epoch 13, Loss(train/val) 0.67669/0.69939. Took 0.49 sec\n",
      "Epoch 14, Loss(train/val) 0.68090/0.69740. Took 0.48 sec\n",
      "Epoch 15, Loss(train/val) 0.67344/0.69948. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66876/0.71589. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.67156/0.71145. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67409/0.71201. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66409/0.71164. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66557/0.74111. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66723/0.72814. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.66342/0.73140. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65695/0.73351. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.65743/0.75119. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66086/0.74770. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.64109/0.75069. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65439/0.75946. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.64928/0.75748. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64380/0.77740. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64762/0.77104. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.64691/0.77041. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64464/0.78191. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.63183/0.81192. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.63537/0.82063. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.64376/0.78332. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.63827/0.78595. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.63204/0.79363. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.62170/0.80937. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.62088/0.79578. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.61122/0.79474. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61702/0.82588. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.61030/0.82707. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.60454/0.84350. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59083/0.85820. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.59601/0.85877. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58326/0.88379. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.58799/0.89425. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59503/0.87756. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57841/0.89466. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.58975/0.89333. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.58829/0.88369. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.59641/0.87955. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.59389/0.88266. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.58051/0.88779. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.57439/0.89635. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.57121/0.90226. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.55870/0.95416. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.55544/0.94490. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.55698/0.95703. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.53740/1.01944. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.53466/1.04764. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.52980/1.04554. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.51887/1.06836. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.53083/1.02746. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.52266/1.04549. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.53059/1.03061. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.52556/1.04175. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.49913/1.06035. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.49847/1.07398. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.48526/1.07910. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.49911/1.04062. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.49239/1.03159. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.48744/1.03596. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.48191/1.08989. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.49404/1.06474. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.47400/1.01855. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.46049/1.01043. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.48178/1.03499. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.47833/0.98491. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.45933/0.98940. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.44618/1.04684. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.43919/1.05403. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.43543/1.03197. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43711/0.99302. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.43852/1.04573. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.42966/1.03841. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.44399/1.03027. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.41936/0.97769. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.42641/0.98477. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.42358/1.00701. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.40782/1.04404. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.42929/1.04897. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.40675/1.01470. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.41447/1.00477. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.39442/1.02773. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.37886/1.02428. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.38496/1.04693. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.38168/1.07774. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.37105/1.03583. Took 0.46 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.71871/0.70741. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.71184/0.70611. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70904/0.70609. Took 0.49 sec\n",
      "Epoch 3, Loss(train/val) 0.70709/0.70501. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.70571/0.70488. Took 0.49 sec\n",
      "Epoch 5, Loss(train/val) 0.70218/0.69640. Took 0.48 sec\n",
      "Epoch 6, Loss(train/val) 0.69912/0.69694. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69540/0.69890. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.69179/0.69198. Took 0.51 sec\n",
      "Epoch 9, Loss(train/val) 0.69092/0.68954. Took 0.47 sec\n",
      "Epoch 10, Loss(train/val) 0.68664/0.70595. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68087/0.71316. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.68460/0.73022. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66806/0.73920. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.67414/0.73493. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67037/0.73734. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.66487/0.77722. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66648/0.77858. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65825/0.80039. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66239/0.80516. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65845/0.80865. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65278/0.81254. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64559/0.83438. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63490/0.85821. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63318/0.86094. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.63417/0.85503. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.62556/0.84414. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62379/0.84994. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.61951/0.86373. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62463/0.85038. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.60962/0.85557. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.60688/0.87224. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60727/0.84934. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58588/0.88214. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.58719/0.89690. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.58451/0.88820. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.57776/0.89324. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59048/0.86936. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58181/0.89644. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.56082/0.89806. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57446/0.91975. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.56529/0.94332. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55706/0.93990. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.54141/0.94151. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55439/0.98684. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.55040/0.94538. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.53388/0.93377. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.53398/0.96290. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.52781/1.00664. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.51682/0.97712. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.52792/1.02498. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.52538/1.02103. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.51532/1.04192. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.51567/1.02411. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.54117/0.96320. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.56788/0.94084. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53814/0.97623. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.53147/0.96229. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.51360/0.98554. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.51224/1.02056. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50140/1.01850. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.48303/1.06082. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.47267/1.08218. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.46557/1.08744. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.47785/1.09923. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.46612/1.11925. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.46136/1.11242. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.46059/1.11512. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.45031/1.08286. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.47055/1.09926. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.46262/1.11787. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45744/1.13866. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.45233/1.05240. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.44169/1.08453. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.44192/1.09347. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44590/1.12426. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43826/1.13250. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.42681/1.10624. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.41326/1.21427. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44054/1.27661. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.43405/1.21202. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.39688/1.15014. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.40245/1.14751. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40186/1.13940. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.40254/1.17360. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.38450/1.15420. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.38187/1.14141. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.37061/1.15235. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.35716/1.13750. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.36902/1.09444. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.38110/1.11622. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.36052/1.08381. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.35287/1.12540. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.34839/1.10971. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.33141/1.12250. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.33753/1.12230. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.32979/1.13911. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.35615/1.13406. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.36328/1.13844. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.34359/1.17313. Took 0.43 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.70898/0.71325. Took 0.65 sec\n",
      "Epoch 1, Loss(train/val) 0.70321/0.70173. Took 0.49 sec\n",
      "Epoch 2, Loss(train/val) 0.69706/0.70464. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69896/0.70655. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68418/0.70782. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68796/0.70346. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.69186/0.70166. Took 0.47 sec\n",
      "Epoch 7, Loss(train/val) 0.67927/0.70740. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68071/0.71780. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68501/0.71367. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67972/0.72265. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67249/0.72155. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.68146/0.73316. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67361/0.72698. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67641/0.74072. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.66914/0.73804. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66939/0.75510. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.67289/0.74384. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67305/0.73777. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.66545/0.72277. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66587/0.73293. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66497/0.74773. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.66167/0.75114. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66409/0.75467. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66225/0.76683. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65555/0.77191. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.65735/0.78368. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.66193/0.78114. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.65585/0.79784. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.66357/0.77710. Took 0.47 sec\n",
      "Epoch 30, Loss(train/val) 0.65384/0.80360. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64698/0.78701. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64165/0.82611. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.65073/0.84143. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.66034/0.80255. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.64980/0.81601. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.64375/0.84489. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.64488/0.86741. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.65206/0.84146. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.63818/0.83764. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.63759/0.83617. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.63859/0.84672. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.63397/0.82581. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.62726/0.85322. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.62249/0.87379. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.62366/0.87643. Took 0.47 sec\n",
      "Epoch 46, Loss(train/val) 0.61029/0.89385. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.61774/0.86562. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.61444/0.88643. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.61063/0.87702. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.59882/0.89639. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.60391/0.88736. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.59770/0.88424. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.60443/0.88966. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.58991/0.90157. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.56688/0.89451. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.58482/0.86523. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.56710/0.91875. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.56357/0.91110. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.56317/0.88656. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.56537/0.90551. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.54634/0.90748. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.55891/0.93306. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.56018/0.94718. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.53629/0.97483. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.52908/0.95884. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.54129/0.95827. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.53372/1.00145. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.52338/0.96887. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.52463/0.99116. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.50426/0.99642. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.50449/0.98314. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.51093/1.05001. Took 0.47 sec\n",
      "Epoch 73, Loss(train/val) 0.47399/0.96671. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.49145/1.06979. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.48664/1.08109. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.48889/1.08233. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.47642/1.07893. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.48416/1.09989. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.45489/1.14517. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.46966/1.16411. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.45647/1.20691. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.46314/1.19288. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.44736/1.11722. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.44707/1.06616. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.43433/1.16113. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.42356/1.18351. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.42699/1.15901. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.41125/1.29866. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.42508/1.25540. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.41602/1.20459. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.40344/1.30745. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.41925/1.18550. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.39599/1.26238. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40441/1.27232. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.41625/1.25454. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.40663/1.17943. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.38707/1.33647. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.38438/1.29806. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.38715/1.28782. Took 0.45 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.72022/0.68518. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70579/0.68880. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69256/0.69136. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69260/0.69563. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69030/0.69494. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69007/0.70736. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68777/0.71650. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.68752/0.71411. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68174/0.71469. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68119/0.71985. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67274/0.72234. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67716/0.73164. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66798/0.71930. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66748/0.72895. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67274/0.73629. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67468/0.71764. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.67006/0.72483. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66896/0.72128. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66969/0.73172. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66017/0.73753. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66662/0.74151. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.66561/0.72919. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66398/0.74494. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.66028/0.75303. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65226/0.74208. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64966/0.73770. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64702/0.72773. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64563/0.73315. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63497/0.74270. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.64318/0.74344. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.62771/0.74010. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.62423/0.74558. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62847/0.76279. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.61729/0.74239. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62022/0.77105. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61287/0.75942. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.60629/0.77068. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60505/0.77498. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60711/0.76047. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.59709/0.75027. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59425/0.77620. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.58863/0.75478. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58309/0.76038. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58619/0.80285. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.58557/0.78207. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57441/0.80376. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58075/0.79028. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.57782/0.74774. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.56603/0.78560. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56924/0.80424. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57334/0.78868. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.56066/0.78473. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.55576/0.84451. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.57071/0.72671. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.53574/0.80954. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.53280/0.76613. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53603/0.83236. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.52912/0.81032. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.53827/0.81054. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.54853/0.76889. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.52657/0.77629. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.52659/0.77909. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.51417/0.70890. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.50481/0.70558. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49680/0.78187. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.51113/0.82745. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.48585/0.84831. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48669/0.76403. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50245/0.80424. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.48987/0.78980. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.48981/0.73494. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.47898/0.73485. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.48112/0.84000. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.45865/0.86467. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.45822/0.83500. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.45710/0.82885. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.45411/0.83494. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.46947/0.87817. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44151/0.85435. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.45325/0.81768. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.43558/0.89480. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.42801/0.82223. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.43004/0.95381. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.41358/0.90179. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43898/0.95130. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.42625/0.87470. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.41221/0.99773. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43854/0.90535. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.43882/0.96461. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.42110/0.92934. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.41218/0.95068. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.41098/0.98157. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.39915/1.06760. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.39419/1.08498. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.37828/1.03255. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.38398/1.09290. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.40957/1.03398. Took 0.47 sec\n",
      "Epoch 97, Loss(train/val) 0.37091/1.05204. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.37900/1.09948. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.37701/1.10345. Took 0.45 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.71711/0.67627. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.71771/0.68455. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69754/0.68966. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69069/0.68960. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68347/0.69311. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68736/0.70742. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68454/0.71141. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67901/0.71476. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68806/0.68492. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.69144/0.68941. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.69650/0.70372. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.68424/0.67983. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68672/0.71025. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.68038/0.71155. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67756/0.69562. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67656/0.71200. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.67577/0.68311. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.68327/0.71164. Took 0.47 sec\n",
      "Epoch 18, Loss(train/val) 0.67592/0.70340. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67965/0.70966. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67142/0.71761. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.68122/0.70668. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66847/0.71378. Took 0.47 sec\n",
      "Epoch 23, Loss(train/val) 0.66842/0.71203. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.67589/0.72282. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.66363/0.71669. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.67312/0.72536. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.66473/0.73677. Took 0.47 sec\n",
      "Epoch 28, Loss(train/val) 0.66304/0.73857. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65948/0.73331. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.66260/0.70543. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.66362/0.73044. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.66047/0.72594. Took 0.47 sec\n",
      "Epoch 33, Loss(train/val) 0.65750/0.72047. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.65408/0.73091. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.65178/0.72587. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.65577/0.71877. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.63904/0.73644. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.63708/0.71451. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.63947/0.73371. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.63539/0.72820. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.63921/0.73401. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.63464/0.72981. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.62886/0.76102. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.62904/0.79423. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.61588/0.76693. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.61562/0.75033. Took 0.47 sec\n",
      "Epoch 47, Loss(train/val) 0.61664/0.76386. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.61321/0.75379. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.61186/0.74311. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.61002/0.74421. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.60231/0.78623. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.60439/0.79256. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.59549/0.81795. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.58489/0.82407. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.58600/0.80447. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.58060/0.83697. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.58503/0.82185. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.58474/0.83988. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.57779/0.83447. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.56658/0.84174. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.55981/0.87887. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.54664/0.92830. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.57390/0.86205. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.56483/0.84835. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.54764/0.88833. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.54652/0.87712. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.54764/0.87696. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.55109/0.90848. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.53640/0.91532. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.54445/0.89617. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.53025/0.93436. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.51911/0.96323. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.52726/0.98878. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.52456/1.07020. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.51723/1.03627. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.52063/0.98266. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.49812/1.11825. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.50721/1.18222. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.51043/1.16374. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.50445/1.08439. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.49019/1.03430. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.51255/1.08957. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.47587/1.09601. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.50017/1.08870. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.49431/1.10894. Took 0.47 sec\n",
      "Epoch 86, Loss(train/val) 0.48637/1.13228. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.48628/1.17135. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.47909/1.14838. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.46107/1.12595. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.46808/1.18513. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.46324/1.22317. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.47898/1.20820. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.45127/1.15128. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.45989/1.15572. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.43719/1.24339. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.45892/1.14797. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.43705/1.23066. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.44456/1.21824. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.42668/1.21400. Took 0.46 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69779/0.70632. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69495/0.70166. Took 0.49 sec\n",
      "Epoch 2, Loss(train/val) 0.69010/0.70080. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.68652/0.71079. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68363/0.71520. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68493/0.70890. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.67926/0.72783. Took 0.47 sec\n",
      "Epoch 7, Loss(train/val) 0.67451/0.72452. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67912/0.73178. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67372/0.73266. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67247/0.71566. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67168/0.70912. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.67289/0.71196. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67034/0.70637. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66800/0.71775. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.66634/0.70892. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66422/0.71754. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.65941/0.72268. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65311/0.70776. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65600/0.70482. Took 0.47 sec\n",
      "Epoch 20, Loss(train/val) 0.65197/0.72591. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65126/0.71094. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.64760/0.71278. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63932/0.71207. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.63443/0.70496. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62720/0.71008. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.63893/0.71302. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62812/0.72473. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62974/0.74492. Took 0.47 sec\n",
      "Epoch 29, Loss(train/val) 0.61573/0.77058. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61931/0.76010. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61071/0.76648. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.60126/0.77296. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.59494/0.78835. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.60945/0.79557. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59070/0.83757. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59324/0.80840. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59600/0.78262. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.58214/0.81190. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57183/0.84488. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.58056/0.82966. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58006/0.80526. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56274/0.82922. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.56608/0.80566. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.55654/0.78629. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.54701/0.85201. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56111/0.77181. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.53957/0.84614. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.53083/0.83311. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.52111/0.82940. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.52279/0.87151. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.51530/0.82697. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.49946/0.87441. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.49381/0.85254. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.49339/0.88894. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.49290/0.87390. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.47665/0.90350. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.47068/0.88073. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.48485/0.92090. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.46604/0.94266. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.44401/0.89472. Took 0.47 sec\n",
      "Epoch 61, Loss(train/val) 0.44135/0.96919. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.44341/0.99105. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.44069/1.03221. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.42642/1.01245. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.41565/1.03081. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.42143/1.07616. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.42572/1.10432. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.39927/1.13558. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.41810/1.12379. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.38872/1.06590. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.39789/1.19110. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.39023/1.14303. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.37265/1.08641. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.38686/1.07764. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.34006/1.16421. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.34336/1.19145. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.32973/1.20218. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.37245/1.26904. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.36816/1.15294. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.36087/1.11195. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.31227/1.16458. Took 0.47 sec\n",
      "Epoch 82, Loss(train/val) 0.30309/1.19766. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.30336/1.26935. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.31081/1.20503. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.31217/1.27936. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.30265/1.20371. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.29758/1.20173. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.29666/1.26866. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.28621/1.26804. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.28388/1.32690. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.26926/1.27288. Took 0.47 sec\n",
      "Epoch 92, Loss(train/val) 0.28227/1.25162. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.28381/1.31214. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.27817/1.32987. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.23959/1.33195. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.23476/1.42138. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.24401/1.35743. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.26516/1.46401. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.24476/1.42362. Took 0.47 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.70357/0.71239. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69629/0.70595. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69678/0.70193. Took 0.50 sec\n",
      "Epoch 3, Loss(train/val) 0.69335/0.69622. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68924/0.68525. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.68066/0.68921. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.68154/0.68696. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68208/0.70913. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.67214/0.71844. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66971/0.71106. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67099/0.70033. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67253/0.69583. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67484/0.70375. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66375/0.70606. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66545/0.71700. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.67328/0.72379. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67030/0.71802. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66189/0.70754. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65832/0.71243. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65680/0.71614. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.66518/0.71683. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65553/0.71715. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65129/0.72397. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65733/0.69383. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.64907/0.72724. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65122/0.74102. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65247/0.72855. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64263/0.74157. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.64434/0.72768. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.63873/0.72773. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64415/0.73332. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.62923/0.73922. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63156/0.73788. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.63298/0.75038. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.62754/0.74416. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61866/0.72912. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.61371/0.75249. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.62101/0.77559. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.61765/0.80822. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61161/0.76930. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.61165/0.77261. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60850/0.73632. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.59941/0.76579. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58657/0.78848. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.57538/0.83423. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.56991/0.80153. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.58629/0.78358. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.56791/0.79648. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56758/0.78911. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.55375/0.76398. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.56714/0.81923. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.59213/0.77527. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.57010/0.81027. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55719/0.86175. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55289/0.82881. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.54370/0.79272. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52915/0.83411. Took 0.47 sec\n",
      "Epoch 57, Loss(train/val) 0.55759/0.81888. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.56580/0.80774. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.56090/0.81984. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.54483/0.82444. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.52871/0.83026. Took 0.47 sec\n",
      "Epoch 62, Loss(train/val) 0.52994/0.82855. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.52464/0.79889. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.53440/0.83734. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.52004/0.85566. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.52562/0.81990. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.53262/0.85390. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.53418/0.81116. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.54181/0.76361. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.52332/0.79321. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.50519/0.82847. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.51830/0.83800. Took 0.47 sec\n",
      "Epoch 73, Loss(train/val) 0.50306/0.80235. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.50082/0.82292. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49526/0.86444. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.52120/0.83503. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.50032/0.84279. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.48651/0.85941. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.47230/0.90205. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.48772/0.87347. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.47907/0.87194. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.49211/0.87343. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.47858/0.89253. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.47914/0.90605. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.45495/0.94003. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.44518/0.92834. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.44449/0.97034. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.45222/0.96121. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.43735/0.98852. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.43792/0.97029. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.43133/0.98036. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.42629/0.92386. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.41720/0.96785. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.39975/0.98450. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.41451/1.06910. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.41791/1.01989. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.40719/1.06676. Took 0.47 sec\n",
      "Epoch 98, Loss(train/val) 0.39414/1.15104. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.38698/1.07297. Took 0.45 sec\n",
      "ACC: 0.375\n",
      "Epoch 0, Loss(train/val) 0.70951/0.70631. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69909/0.69645. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70012/0.69234. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69177/0.70953. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68696/0.70508. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68917/0.70478. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.69075/0.70734. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68251/0.74345. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67989/0.72746. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67846/0.77026. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67675/0.77736. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67402/0.79738. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66509/0.78189. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66899/0.80044. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66695/0.81164. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66540/0.78380. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66846/0.80144. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.66646/0.79919. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66033/0.81764. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.65141/0.82961. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64835/0.85458. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65928/0.84764. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65822/0.82025. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64900/0.83724. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64754/0.84397. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64203/0.84461. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63686/0.86080. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64041/0.84318. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63889/0.84414. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.63171/0.85850. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.62339/0.86647. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61991/0.88015. Took 0.47 sec\n",
      "Epoch 32, Loss(train/val) 0.62339/0.89159. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.62036/0.91221. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.61456/0.91295. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60973/0.91142. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60026/0.91970. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60163/0.90196. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.59699/0.92731. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58424/0.96692. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.58482/0.93260. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58759/0.96146. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58457/0.95300. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.57641/0.99284. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.57231/0.95118. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.57068/0.98082. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56051/0.95634. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55910/0.96885. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.55671/0.97978. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.54902/0.98140. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.54761/0.98112. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53827/0.94241. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52969/1.00299. Took 0.47 sec\n",
      "Epoch 53, Loss(train/val) 0.54205/0.96093. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52898/0.96044. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.52186/0.99178. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.52505/0.96395. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.49328/0.94538. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.49869/1.01470. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.50222/1.00091. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.48314/1.01139. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.50266/1.07984. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.49296/1.03438. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.47482/1.07335. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49205/1.08075. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.46346/1.10321. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.46957/1.11924. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.47273/1.17374. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.45873/1.19742. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.44917/1.23834. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.47140/1.19359. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.46052/1.18843. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.43972/1.20962. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.44441/1.18169. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.44504/1.29592. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.44406/1.25008. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43683/1.32108. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.44890/1.34265. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.42243/1.36095. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.42692/1.41824. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.41276/1.35055. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.40546/1.44747. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.39850/1.46294. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.37704/1.46001. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.39866/1.50581. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.41494/1.51073. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.40006/1.57316. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.38105/1.62524. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.40561/1.67529. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.37651/1.64440. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36866/1.67736. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.41185/1.73116. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.43140/1.51073. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.43382/1.41964. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.41297/1.34340. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.37436/1.45066. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.35514/1.50877. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.34103/1.62093. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.35003/1.60450. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34405/1.62032. Took 0.44 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70812/0.72596. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.70531/0.69152. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.70342/0.68381. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69810/0.70591. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.70102/0.69676. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69214/0.70681. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.69217/0.69324. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68966/0.70906. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.68963/0.70139. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68076/0.71725. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.69069/0.68531. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68932/0.69409. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.68762/0.69952. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67831/0.69322. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67988/0.69284. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.68119/0.69816. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67792/0.69701. Took 0.47 sec\n",
      "Epoch 17, Loss(train/val) 0.67355/0.73163. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67035/0.69616. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.67926/0.70067. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.67301/0.71559. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66453/0.70519. Took 0.47 sec\n",
      "Epoch 22, Loss(train/val) 0.66021/0.70727. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66441/0.71442. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66101/0.72247. Took 0.47 sec\n",
      "Epoch 25, Loss(train/val) 0.65998/0.70143. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65063/0.72472. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65049/0.74679. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.65229/0.75529. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64974/0.73403. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.64298/0.72385. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.64568/0.72350. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63951/0.75996. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.63558/0.73902. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.62748/0.75673. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.62088/0.76892. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61635/0.75825. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.62151/0.73494. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.62156/0.76220. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61582/0.77038. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.61159/0.78589. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60917/0.78808. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.61701/0.81867. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.60816/0.85159. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60550/0.81584. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.60019/0.81604. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.59726/0.82859. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58963/0.81099. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.59151/0.82083. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.57896/0.81873. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.58885/0.82368. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.57296/0.82535. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56477/0.89020. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.56253/0.85417. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.57307/0.87503. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.57033/0.82701. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.56476/0.82177. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.56575/0.84409. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.57845/0.84398. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.56468/0.84826. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55079/0.89475. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.54370/0.91755. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.54327/0.91543. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.53861/0.87999. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.53474/0.88859. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.54481/0.93434. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.53710/0.90683. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.54642/0.91377. Took 0.47 sec\n",
      "Epoch 68, Loss(train/val) 0.52396/0.88435. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.51896/0.93534. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.55578/0.94696. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.52925/0.95038. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.50434/0.91916. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.52346/0.94046. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.50233/1.01251. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.50216/1.05371. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.51205/0.98197. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.49491/0.97179. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.50354/0.99375. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.49182/1.02364. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.49727/0.98396. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.49291/0.99254. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.46882/1.04350. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.46654/1.13325. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.48671/1.09025. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.47188/1.06296. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.45165/1.06289. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.45524/1.04752. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.48507/1.05957. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.47284/1.03534. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.46621/1.02021. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.46452/1.06647. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.45209/1.04294. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.44726/1.04755. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.44128/1.08646. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.45769/1.07730. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.42820/1.12902. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.42252/1.04053. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.42805/1.11032. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.43115/1.06612. Took 0.44 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.71155/0.69349. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.70711/0.70413. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.70432/0.69253. Took 0.49 sec\n",
      "Epoch 3, Loss(train/val) 0.69519/0.69744. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68986/0.68367. Took 0.49 sec\n",
      "Epoch 5, Loss(train/val) 0.69359/0.67635. Took 0.48 sec\n",
      "Epoch 6, Loss(train/val) 0.68537/0.68879. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68218/0.67322. Took 0.49 sec\n",
      "Epoch 8, Loss(train/val) 0.68659/0.68489. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67930/0.68655. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.67405/0.67057. Took 0.48 sec\n",
      "Epoch 11, Loss(train/val) 0.67713/0.68237. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.67740/0.68472. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66764/0.70311. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67402/0.70281. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66544/0.69612. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66914/0.71207. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65987/0.70876. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66086/0.72715. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.66733/0.75472. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.66078/0.72092. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65442/0.71446. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64854/0.72202. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65821/0.72830. Took 0.47 sec\n",
      "Epoch 24, Loss(train/val) 0.65041/0.73866. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65148/0.75518. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65137/0.73267. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64195/0.74221. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63953/0.73496. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.64526/0.71755. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63319/0.74418. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64116/0.74381. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.63686/0.73969. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.63786/0.73774. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.61991/0.75472. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.61847/0.76481. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.62865/0.77554. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.62548/0.76670. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.61820/0.75841. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.61445/0.77378. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.62202/0.76138. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.62206/0.77115. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.61610/0.78112. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59427/0.79291. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.59532/0.76642. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59240/0.80999. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.58695/0.82892. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.58569/0.83351. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.58930/0.79903. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.59274/0.83283. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.58623/0.79401. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.58252/0.84917. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.58158/0.79312. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.56668/0.82202. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.56672/0.79178. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.55810/0.79040. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.56804/0.77723. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.56965/0.84820. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.55206/0.80428. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.54941/0.77778. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.55024/0.81263. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.51824/0.80069. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.55280/0.74477. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.56454/0.86731. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.54132/0.79940. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.53238/0.83629. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.52127/0.83428. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.52366/0.85715. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.52085/0.86512. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.50378/0.83717. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.50303/0.86465. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.50140/0.87135. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.50950/0.91357. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.51510/0.91917. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.49249/0.90024. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49082/0.92234. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.49147/0.87629. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.48810/0.95981. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.47204/0.95499. Took 0.47 sec\n",
      "Epoch 79, Loss(train/val) 0.50298/0.91079. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.45237/0.93623. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.47605/0.89471. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.50845/0.89952. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.47560/0.93580. Took 0.47 sec\n",
      "Epoch 84, Loss(train/val) 0.48401/0.89103. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.45725/0.90518. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.45625/0.94276. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.46680/0.96619. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.44538/0.93847. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.44150/0.97103. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.45044/0.94031. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.42622/1.00790. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.48166/0.91656. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.46848/0.93923. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.43653/0.99441. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.42330/0.98541. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.42578/1.01913. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.44043/0.95164. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.41964/0.94773. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.44175/0.98265. Took 0.44 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.70468/0.70922. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.69794/0.71401. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69834/0.72997. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69030/0.72238. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68458/0.70433. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.68963/0.72033. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67578/0.71244. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67668/0.72127. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68231/0.72754. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67601/0.73035. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66633/0.73592. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67172/0.74161. Took 0.47 sec\n",
      "Epoch 12, Loss(train/val) 0.66831/0.73261. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66474/0.73367. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.65828/0.74465. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65765/0.75129. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65782/0.73625. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65460/0.76446. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64809/0.74954. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65452/0.74911. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64540/0.74818. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64834/0.76213. Took 0.47 sec\n",
      "Epoch 22, Loss(train/val) 0.64256/0.76970. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63371/0.77687. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.62779/0.77772. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63335/0.79206. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.62723/0.79032. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.61637/0.80383. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60749/0.82011. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.60199/0.82866. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.60006/0.83197. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.59387/0.86496. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.59559/0.85282. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.59197/0.87488. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.59686/0.88191. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.59417/0.86530. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.57916/0.87332. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.56929/0.90475. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57730/0.89269. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.56417/0.89782. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58055/0.92581. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.56667/0.92514. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.55490/0.96261. Took 0.47 sec\n",
      "Epoch 43, Loss(train/val) 0.55985/0.96744. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55504/0.96567. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.55195/0.93071. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.56548/0.95626. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.56440/0.94230. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.54997/0.96818. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.53592/1.04945. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.53349/1.03050. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53052/1.07689. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.52189/1.10681. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54418/1.06705. Took 0.47 sec\n",
      "Epoch 54, Loss(train/val) 0.51540/1.06122. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.53007/1.04135. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52753/1.02175. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.51866/1.06023. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.51957/1.02595. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.50447/1.01166. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.51150/1.03520. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.50979/1.05432. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52391/1.07805. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.50643/1.04415. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49838/1.04180. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.48292/1.07776. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.50990/1.06049. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.48675/1.07882. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49249/1.14407. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.48582/1.09145. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48570/1.08691. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.47693/1.11333. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.46800/1.10699. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.46080/1.14886. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.47330/1.06612. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.45611/1.08438. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46675/1.12152. Took 0.47 sec\n",
      "Epoch 77, Loss(train/val) 0.46932/1.11394. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.46544/1.10394. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.45169/1.14159. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.44383/1.13920. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.44970/1.18713. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.44775/1.14021. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.47909/1.12925. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.46124/1.16164. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.42080/1.17140. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.41244/1.20931. Took 0.47 sec\n",
      "Epoch 87, Loss(train/val) 0.43484/1.19322. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.43146/1.16953. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.44087/1.12145. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.42001/1.20241. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.42494/1.21382. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.41181/1.20900. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.40392/1.20938. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.46466/1.09378. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.41511/1.15638. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.41983/1.11268. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.41769/1.17894. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.39576/1.19188. Took 0.47 sec\n",
      "Epoch 99, Loss(train/val) 0.38442/1.27236. Took 0.44 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.70438/0.70048. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70016/0.71395. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69867/0.72868. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68803/0.74950. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.68964/0.73335. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68771/0.73001. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68926/0.72733. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.68560/0.72507. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67984/0.72564. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68412/0.72698. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68400/0.73089. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67541/0.72815. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.68086/0.74145. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67325/0.74127. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.66494/0.74166. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67295/0.74060. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66470/0.75257. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66904/0.73899. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66196/0.74282. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.66041/0.74823. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66303/0.75425. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.65644/0.73770. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65747/0.74223. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66187/0.75353. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.65713/0.75105. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65663/0.76922. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.65049/0.75943. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64689/0.78648. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65665/0.76970. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.65169/0.77457. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.64094/0.79698. Took 0.47 sec\n",
      "Epoch 31, Loss(train/val) 0.64216/0.80112. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64656/0.79586. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64025/0.78290. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.64511/0.78267. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.64296/0.78533. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62817/0.82670. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.63140/0.81554. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.63335/0.80532. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.63260/0.81626. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.62679/0.82127. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.62277/0.81233. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.62407/0.81814. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.62626/0.80912. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.61984/0.80927. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.60930/0.83172. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.61457/0.82136. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.61971/0.81744. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.60247/0.81732. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.60033/0.81891. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.60117/0.82630. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.59001/0.84206. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.59530/0.82451. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.59564/0.81968. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.58932/0.81080. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.58589/0.84684. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.57296/0.84727. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.58707/0.81756. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.58960/0.78744. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.56244/0.78878. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.58437/0.78622. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.56863/0.80918. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.56522/0.78010. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.56655/0.80871. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.56616/0.79374. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.56701/0.79553. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.54547/0.76677. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.54623/0.81139. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.53455/0.83369. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.53980/0.83736. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.54337/0.81874. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.54397/0.80070. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.53093/0.82965. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.52832/0.86035. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.53391/0.86829. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.52489/0.83546. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.53576/0.86824. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.52897/0.87273. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.51523/0.91698. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.50572/0.88509. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.51845/0.91447. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.49379/0.89610. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.50713/0.91862. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.50715/0.93081. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.48621/0.93777. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.49106/0.89972. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.49316/0.93421. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.48431/0.94078. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.47307/0.95526. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.46640/1.01635. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.47350/1.06675. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.48267/0.93887. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.47612/0.95622. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.44263/0.95546. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.44549/1.01899. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.43624/1.10225. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.46158/1.02941. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.43405/1.03755. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.45351/0.98798. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.42612/1.03788. Took 0.46 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.70720/0.70069. Took 0.59 sec\n",
      "Epoch 1, Loss(train/val) 0.70556/0.70862. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.70624/0.68449. Took 0.49 sec\n",
      "Epoch 3, Loss(train/val) 0.70323/0.69184. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.69402/0.68827. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69821/0.69139. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.69304/0.69469. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.68890/0.68453. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68993/0.69552. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.68158/0.73448. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.69611/0.67394. Took 0.48 sec\n",
      "Epoch 11, Loss(train/val) 0.68132/0.70802. Took 0.47 sec\n",
      "Epoch 12, Loss(train/val) 0.67917/0.71880. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.68040/0.70042. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66652/0.71139. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67565/0.71526. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66477/0.70844. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66308/0.71201. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.66209/0.70709. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65874/0.71278. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65785/0.70230. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.65089/0.70493. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66149/0.69786. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64977/0.69954. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.64605/0.71084. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64158/0.72086. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.64503/0.73139. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64062/0.70945. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.63252/0.70194. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62477/0.72614. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.62864/0.72548. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62993/0.75172. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62938/0.74614. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.61858/0.75585. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.62170/0.73570. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61771/0.78048. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62353/0.74260. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.60850/0.76599. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.60654/0.79589. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60587/0.79630. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.60248/0.77015. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59893/0.77888. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.59595/0.80891. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.59241/0.79847. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58962/0.81485. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.59003/0.80486. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58212/0.82167. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.57705/0.83189. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.57127/0.84840. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.56271/0.83518. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56271/0.87469. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.56049/0.82802. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.56083/0.86449. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54862/0.86116. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.55915/0.83705. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.54839/0.88754. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.54872/0.90500. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53858/0.92885. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54686/0.92185. Took 0.47 sec\n",
      "Epoch 59, Loss(train/val) 0.52982/0.89755. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.53224/0.89914. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.51683/0.91666. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.52123/0.94143. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.50550/1.01245. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.52269/0.95139. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.52196/1.01694. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.51151/0.98895. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.49747/1.00542. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.48916/1.00414. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.50336/1.02168. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49908/1.01373. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.47485/1.05267. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.47346/1.05207. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.47036/1.09465. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.46730/1.04054. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.45975/1.06528. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.46572/1.08079. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.45632/1.05371. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.44973/1.09451. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.43718/1.11625. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.44615/1.05528. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.46739/1.04442. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.43327/1.05410. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.43029/1.11023. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.45000/1.03305. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.43595/1.08224. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43059/1.12562. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.42482/1.09180. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.42091/1.15014. Took 0.47 sec\n",
      "Epoch 89, Loss(train/val) 0.41729/1.11873. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.40019/1.12360. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.38567/1.15055. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.37850/1.20434. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.36926/1.20858. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.37644/1.22568. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.36924/1.20214. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.37095/1.21457. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.36998/1.19827. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.36416/1.22856. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.37554/1.25461. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70155/0.69128. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69548/0.68465. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69226/0.68956. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68848/0.70300. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.68365/0.69039. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68548/0.69243. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68016/0.69120. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67794/0.70639. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67854/0.70577. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67540/0.70027. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67865/0.70419. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67690/0.70419. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66811/0.72140. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.65868/0.72035. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.66096/0.74845. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66128/0.76409. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66380/0.74999. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65017/0.75962. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65323/0.75410. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64522/0.74844. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.63923/0.75772. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64886/0.75219. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.63771/0.75279. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.62650/0.75718. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.62987/0.75410. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62312/0.76159. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.62658/0.77921. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61920/0.75780. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62204/0.73682. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.61454/0.75887. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60818/0.75918. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.60268/0.74862. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59928/0.73952. Took 0.47 sec\n",
      "Epoch 33, Loss(train/val) 0.58479/0.72927. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.60500/0.70563. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60118/0.76290. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.58569/0.76498. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.57735/0.72607. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.58229/0.74607. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57451/0.77299. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56995/0.74738. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.56803/0.77191. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.57104/0.79031. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.55366/0.79575. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.54864/0.77791. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.54549/0.75591. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.53385/0.77662. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.54310/0.76503. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.53601/0.76697. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.53125/0.82142. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.54014/0.80453. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.51500/0.83442. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.51449/0.79724. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.52077/0.78635. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.49972/0.84107. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.50162/0.82513. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.49350/0.83991. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.48968/0.87677. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.47749/0.87189. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.47298/0.92761. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.47537/0.93545. Took 0.47 sec\n",
      "Epoch 61, Loss(train/val) 0.47568/0.92265. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.46878/0.91860. Took 0.47 sec\n",
      "Epoch 63, Loss(train/val) 0.48600/0.95045. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.46039/0.96893. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.46126/1.00003. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.44831/1.00014. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.48160/0.97652. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.44605/0.99403. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45117/1.00646. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.42634/1.00466. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.43238/1.05262. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.39848/1.02790. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.41546/1.06875. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.42339/1.02316. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.41596/1.03703. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.38946/1.08943. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.40859/1.05600. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.43233/1.06166. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.37861/1.08924. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.39679/1.07044. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.36659/1.05440. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.36881/1.13413. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.35838/1.09967. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.35176/1.10041. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.37073/1.10768. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.35583/1.12806. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.34153/1.07742. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.36948/1.11811. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.35602/1.02970. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.35225/1.08313. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.32056/1.08921. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.33041/1.16175. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.34141/1.10980. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.35834/1.15021. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.31957/1.08566. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.31659/1.09347. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.31621/1.09025. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.33443/1.15069. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.29841/1.12062. Took 0.45 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.72288/0.72494. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.71294/0.71323. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.70844/0.71162. Took 0.49 sec\n",
      "Epoch 3, Loss(train/val) 0.70458/0.71369. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69834/0.71556. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.70251/0.71724. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69872/0.72095. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.70139/0.72958. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.70044/0.74370. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.70324/0.73528. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.69001/0.72886. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68987/0.72855. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.69747/0.73162. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.69028/0.74007. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.68796/0.74612. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67762/0.75689. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.68536/0.75821. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67078/0.75809. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.68043/0.76157. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67685/0.76275. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67298/0.77621. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.67034/0.78079. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66681/0.76511. Took 0.47 sec\n",
      "Epoch 23, Loss(train/val) 0.67198/0.76596. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66047/0.76961. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66143/0.78577. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.66704/0.77725. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.66720/0.79139. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65245/0.79651. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.65419/0.79383. Took 0.47 sec\n",
      "Epoch 30, Loss(train/val) 0.66015/0.78761. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.65334/0.77843. Took 0.47 sec\n",
      "Epoch 32, Loss(train/val) 0.64526/0.76178. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64304/0.75960. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.65334/0.75378. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.64491/0.75658. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.63227/0.77193. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.64624/0.79771. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.64243/0.80524. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.64046/0.79580. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.63949/0.78647. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.63245/0.78926. Took 0.47 sec\n",
      "Epoch 42, Loss(train/val) 0.63403/0.80224. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.63212/0.80798. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.62862/0.83048. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.61957/0.83413. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.61979/0.82508. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.61265/0.85302. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.61439/0.85400. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.61885/0.85752. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.61059/0.85084. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.59377/0.88584. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.61239/0.87466. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.60236/0.86791. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.58845/0.87680. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.59731/0.89798. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.58549/0.91057. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.58401/0.89713. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.58067/0.90744. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.58123/0.89780. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.56988/0.91642. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.56677/0.91022. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.56684/0.91733. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.54953/0.95252. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.55452/0.98888. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.53570/0.95446. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.54319/0.99461. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.54496/0.98560. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.53458/1.03730. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.53635/1.00075. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.53939/1.01757. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.51970/1.05579. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.50987/1.05257. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.51690/1.05103. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.51486/1.04973. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.50387/0.99866. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.50151/1.06653. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.50665/1.11876. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.50397/1.12291. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.49405/1.10774. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.49301/1.12688. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.48543/1.14954. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.47438/1.06499. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.47563/1.22317. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.48447/1.17274. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.47622/1.19646. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.47943/1.19007. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.48067/1.19407. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.45694/1.22100. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.45010/1.22638. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.47934/1.14609. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.45790/1.15466. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.44352/1.21589. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.44304/1.26474. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.44742/1.27387. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.42739/1.30359. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.45511/1.26078. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.42427/1.32865. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.43023/1.29394. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.40794/1.33459. Took 0.45 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.70367/0.70464. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.70025/0.71003. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.70014/0.71123. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69052/0.70565. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69172/0.70618. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69190/0.71062. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.69455/0.71275. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68518/0.70606. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68547/0.70964. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.68626/0.71186. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68583/0.70817. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.68390/0.72453. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.68049/0.71715. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67045/0.72117. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.67919/0.71863. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66969/0.71297. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.66681/0.72527. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66283/0.74878. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66549/0.75511. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65616/0.77626. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.66210/0.77989. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65522/0.77939. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64816/0.78019. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64992/0.80032. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.65166/0.80930. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64777/0.80703. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64594/0.82314. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.64027/0.84380. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63844/0.84147. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.64618/0.83586. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63704/0.82690. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.63630/0.84682. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.63886/0.84162. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62683/0.83463. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.62288/0.85186. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.62165/0.85615. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61385/0.86979. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.62546/0.88606. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61450/0.89863. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.60523/0.92072. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60604/0.88640. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60630/0.92884. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.60506/0.90877. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60259/0.94316. Took 0.47 sec\n",
      "Epoch 44, Loss(train/val) 0.58981/0.95279. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.58986/0.96190. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.58006/0.95112. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57949/1.01786. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57888/1.04772. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.57846/1.01256. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56503/1.01582. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.57128/1.02170. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.55250/1.05439. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.56553/1.06196. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.56830/1.02486. Took 0.47 sec\n",
      "Epoch 55, Loss(train/val) 0.56508/1.02920. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55661/1.04064. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.55708/1.03685. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.54124/1.04805. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.54630/1.03200. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.53815/1.02124. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.52771/1.05784. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53618/1.08247. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.53989/1.05466. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.53821/1.02460. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.52347/1.03080. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.51699/1.07520. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.51665/1.07407. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.50665/1.11541. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.51996/1.08676. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.50726/1.09345. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.50599/1.09261. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.49635/1.16816. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.51098/1.09567. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.48345/1.18547. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49356/1.13367. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.49413/1.12839. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.48357/1.12039. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.46378/1.17837. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.47738/1.21622. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.49004/1.16619. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.48952/1.22894. Took 0.47 sec\n",
      "Epoch 82, Loss(train/val) 0.46283/1.22301. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.46076/1.23279. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.48034/1.19318. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.46592/1.22925. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.45219/1.20808. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.44665/1.24187. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.44992/1.23319. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.44704/1.27502. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.44806/1.29940. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.44117/1.30916. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.41252/1.27333. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.43262/1.29292. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.44541/1.32780. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.41744/1.35901. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.43530/1.29566. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.40240/1.38491. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.41620/1.38689. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.43241/1.32008. Took 0.45 sec\n",
      "ACC: 0.6458333333333334\n",
      "Epoch 0, Loss(train/val) 0.69767/0.68737. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.69226/0.67627. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69294/0.68000. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69094/0.67953. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68902/0.68267. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68320/0.68515. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.68302/0.69170. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68222/0.69895. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67835/0.70807. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67685/0.70141. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68027/0.69953. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66832/0.71396. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66964/0.71021. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67090/0.70509. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66836/0.70577. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66733/0.73428. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66853/0.73200. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.67081/0.73740. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.67021/0.73298. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66196/0.73896. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.66149/0.73138. Took 0.47 sec\n",
      "Epoch 21, Loss(train/val) 0.65667/0.74016. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65820/0.74277. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.65496/0.76112. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64746/0.76628. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64797/0.76876. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.63515/0.78620. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63139/0.80149. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63436/0.82055. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62512/0.83576. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62765/0.83438. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.62169/0.84657. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61695/0.83986. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.61321/0.85560. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62360/0.84459. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61370/0.86247. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.60115/0.89756. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60967/0.92978. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.59100/0.89513. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.60433/0.88628. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.59759/0.90068. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58341/0.89658. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.59363/0.91538. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58245/0.92078. Took 0.47 sec\n",
      "Epoch 44, Loss(train/val) 0.59398/0.88705. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.58326/0.91716. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57856/0.91316. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.58446/0.92311. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56890/0.92447. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57090/0.93305. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.55863/0.94343. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.56600/0.93940. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56476/0.97454. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.55983/0.99468. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.55901/1.01140. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.57113/1.01469. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.55259/0.99734. Took 0.47 sec\n",
      "Epoch 57, Loss(train/val) 0.55936/1.01597. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.55872/1.00002. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.54294/1.04534. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.53830/1.03665. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.54069/1.03703. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53706/1.02613. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.53788/1.03736. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.52793/1.02432. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.51847/1.04237. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51325/1.06404. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.53082/1.04598. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.51607/1.05587. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.53656/1.05733. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.51227/1.06021. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.51388/1.07940. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.51553/1.05230. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.49428/1.11203. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.49573/1.09502. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48124/1.08272. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.49895/1.04186. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.49551/1.05231. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.48859/1.04440. Took 0.47 sec\n",
      "Epoch 79, Loss(train/val) 0.48318/1.02595. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.47001/1.04803. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.47280/1.06404. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.46312/1.13433. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.45476/1.07348. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.46345/1.11452. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.44194/1.09718. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.44577/1.12168. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.43225/1.15156. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.42827/1.15430. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.45307/1.08836. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.45428/1.06199. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.42024/1.10195. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.44499/1.11396. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.41386/1.10886. Took 0.47 sec\n",
      "Epoch 94, Loss(train/val) 0.41169/1.13764. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.41330/1.12993. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.41554/1.10201. Took 0.47 sec\n",
      "Epoch 97, Loss(train/val) 0.42698/1.08633. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.38846/1.10924. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.39659/1.12764. Took 0.44 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69634/0.68130. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69167/0.68417. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.68923/0.68179. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69095/0.68628. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68633/0.68311. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68196/0.69070. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.68466/0.69672. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68166/0.70526. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67928/0.70315. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.68134/0.71147. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67832/0.70995. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.67477/0.71737. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66969/0.72399. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67617/0.73631. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67408/0.73938. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66946/0.74991. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.66788/0.74965. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66437/0.74211. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66724/0.73274. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66063/0.75830. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.66361/0.74618. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65667/0.76538. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65799/0.76169. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.65651/0.76973. Took 0.47 sec\n",
      "Epoch 24, Loss(train/val) 0.64994/0.80187. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65032/0.79411. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64338/0.82225. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64514/0.81538. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64233/0.84427. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64383/0.81056. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.64176/0.80748. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63603/0.81594. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63507/0.80676. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.63491/0.80679. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62742/0.82834. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62546/0.84851. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.61965/0.84703. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61911/0.86738. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.60474/0.86700. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61495/0.85857. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.61481/0.86974. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60761/0.87142. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.60081/0.87238. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.60134/0.89635. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59432/0.89370. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.58793/0.88050. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.58374/0.94792. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.57584/0.94015. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.56770/0.94190. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56782/0.93716. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55858/0.94118. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.55744/0.93667. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56443/0.93869. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.54713/0.91797. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54979/0.95419. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.54347/0.92641. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.54626/0.94282. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.53432/0.94899. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.53569/0.94466. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.52632/0.87933. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.51507/0.92071. Took 0.47 sec\n",
      "Epoch 61, Loss(train/val) 0.53747/0.97844. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53037/0.96156. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.52082/0.96191. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.52101/1.00678. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.50596/1.05898. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.51921/1.00358. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.50520/1.03107. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.49931/1.04741. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.49464/1.02807. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.48606/1.05058. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48308/1.05988. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.50595/0.99640. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.49797/1.03504. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.48192/0.99127. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.46397/1.07896. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.46693/1.04661. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.47004/1.02655. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.45237/1.05630. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.43035/1.06462. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.43949/1.12755. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.43922/1.04163. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.44134/1.08113. Took 0.47 sec\n",
      "Epoch 83, Loss(train/val) 0.43180/1.16228. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.41751/1.07931. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.42159/1.00179. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.39989/1.10179. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.41260/1.10314. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.40478/1.09027. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.38759/1.08589. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.37885/1.07439. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.42659/1.03258. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.37664/1.06813. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.35349/1.13433. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.39227/1.15557. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.39487/1.05346. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.45331/1.05797. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.37903/1.14548. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.37438/1.18342. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.39646/1.11713. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69729/0.70088. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.68798/0.71735. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.68534/0.72497. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.68365/0.72690. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68154/0.72982. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.68029/0.73383. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.67824/0.74120. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67769/0.74634. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67413/0.75645. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67714/0.75799. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67514/0.76289. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.67377/0.77255. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67141/0.76971. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66582/0.77965. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.66509/0.80455. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66131/0.80173. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.66048/0.81335. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65557/0.82073. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.65238/0.82796. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65266/0.82944. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64414/0.83511. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64006/0.83044. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.63887/0.85351. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63630/0.84424. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.62430/0.87915. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.61993/0.87760. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.62475/0.87963. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61408/0.91703. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60900/0.91488. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.60632/0.94051. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.59758/0.93370. Took 0.50 sec\n",
      "Epoch 31, Loss(train/val) 0.59214/0.95069. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.58762/0.94030. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58461/0.96375. Took 0.51 sec\n",
      "Epoch 34, Loss(train/val) 0.57376/0.96551. Took 0.48 sec\n",
      "Epoch 35, Loss(train/val) 0.59175/0.92317. Took 0.47 sec\n",
      "Epoch 36, Loss(train/val) 0.58948/0.93095. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57200/0.94605. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.56602/0.94356. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.57455/0.93822. Took 0.78 sec\n",
      "Epoch 40, Loss(train/val) 0.55609/0.96114. Took 0.52 sec\n",
      "Epoch 41, Loss(train/val) 0.55450/0.96447. Took 0.49 sec\n",
      "Epoch 42, Loss(train/val) 0.54539/0.94739. Took 0.49 sec\n",
      "Epoch 43, Loss(train/val) 0.54854/0.96499. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.54480/1.01585. Took 0.47 sec\n",
      "Epoch 45, Loss(train/val) 0.54795/1.01929. Took 0.47 sec\n",
      "Epoch 46, Loss(train/val) 0.54945/0.98231. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.55149/1.00502. Took 0.49 sec\n",
      "Epoch 48, Loss(train/val) 0.53217/1.00940. Took 0.48 sec\n",
      "Epoch 49, Loss(train/val) 0.52014/1.02766. Took 0.49 sec\n",
      "Epoch 50, Loss(train/val) 0.52488/1.03214. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.51466/1.03937. Took 0.48 sec\n",
      "Epoch 52, Loss(train/val) 0.50974/1.04164. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.49907/1.06631. Took 0.48 sec\n",
      "Epoch 54, Loss(train/val) 0.51566/1.07493. Took 0.47 sec\n",
      "Epoch 55, Loss(train/val) 0.52174/1.01475. Took 0.48 sec\n",
      "Epoch 56, Loss(train/val) 0.48911/1.07023. Took 0.47 sec\n",
      "Epoch 57, Loss(train/val) 0.49249/1.06326. Took 0.47 sec\n",
      "Epoch 58, Loss(train/val) 0.49893/1.08957. Took 0.47 sec\n",
      "Epoch 59, Loss(train/val) 0.49473/1.09685. Took 0.48 sec\n",
      "Epoch 60, Loss(train/val) 0.48558/1.04508. Took 0.47 sec\n",
      "Epoch 61, Loss(train/val) 0.48801/1.07249. Took 0.49 sec\n",
      "Epoch 62, Loss(train/val) 0.48085/1.07959. Took 0.47 sec\n",
      "Epoch 63, Loss(train/val) 0.46124/1.13101. Took 0.48 sec\n",
      "Epoch 64, Loss(train/val) 0.45298/1.14262. Took 0.47 sec\n",
      "Epoch 65, Loss(train/val) 0.45341/1.16684. Took 0.50 sec\n",
      "Epoch 66, Loss(train/val) 0.45703/1.14947. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.43767/1.14545. Took 0.49 sec\n",
      "Epoch 68, Loss(train/val) 0.43801/1.16216. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.43710/1.17317. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.44630/1.21671. Took 0.49 sec\n",
      "Epoch 71, Loss(train/val) 0.43165/1.17416. Took 0.48 sec\n",
      "Epoch 72, Loss(train/val) 0.43182/1.20369. Took 0.47 sec\n",
      "Epoch 73, Loss(train/val) 0.40513/1.17626. Took 0.47 sec\n",
      "Epoch 74, Loss(train/val) 0.40940/1.21769. Took 0.47 sec\n",
      "Epoch 75, Loss(train/val) 0.43568/1.20816. Took 0.47 sec\n",
      "Epoch 76, Loss(train/val) 0.41524/1.25919. Took 0.47 sec\n",
      "Epoch 77, Loss(train/val) 0.41673/1.25911. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.39045/1.24022. Took 0.48 sec\n",
      "Epoch 79, Loss(train/val) 0.37461/1.34024. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.39051/1.28660. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.37543/1.26467. Took 0.49 sec\n",
      "Epoch 82, Loss(train/val) 0.35467/1.30818. Took 0.48 sec\n",
      "Epoch 83, Loss(train/val) 0.36769/1.33461. Took 0.47 sec\n",
      "Epoch 84, Loss(train/val) 0.37649/1.36055. Took 0.49 sec\n",
      "Epoch 85, Loss(train/val) 0.36840/1.41641. Took 0.48 sec\n",
      "Epoch 86, Loss(train/val) 0.35326/1.32121. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.36284/1.47587. Took 0.48 sec\n",
      "Epoch 88, Loss(train/val) 0.34472/1.40293. Took 0.47 sec\n",
      "Epoch 89, Loss(train/val) 0.32262/1.42456. Took 0.47 sec\n",
      "Epoch 90, Loss(train/val) 0.32089/1.35674. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.33233/1.39283. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.31592/1.44243. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.30051/1.45551. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.31535/1.45976. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.30410/1.45935. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.31535/1.50150. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.32808/1.42678. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.30867/1.43425. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.29082/1.57216. Took 0.45 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69301/0.70131. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68550/0.71280. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.68700/0.71778. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68231/0.71425. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68290/0.71357. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68058/0.71640. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.67943/0.72660. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67545/0.72555. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67556/0.73255. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67484/0.73548. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67386/0.75105. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66545/0.75394. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66511/0.74808. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66789/0.76227. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66019/0.77426. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66186/0.77539. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65571/0.78583. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.65709/0.80725. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65085/0.81836. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.64971/0.81579. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64390/0.82648. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64065/0.82591. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.63683/0.83339. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63306/0.84891. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63359/0.85352. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.62695/0.85644. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62119/0.85848. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62239/0.86637. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62143/0.85665. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.61501/0.86118. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.60609/0.86042. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.60105/0.87577. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.59763/0.87942. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.59522/0.84666. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.58777/0.83929. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58028/0.84037. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.57856/0.86884. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.57488/0.83789. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.55975/0.85823. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.56599/0.82934. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.55663/0.84409. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.55148/0.86844. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.54625/0.86786. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.53119/0.87454. Took 0.42 sec\n",
      "Epoch 44, Loss(train/val) 0.52836/0.89525. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.53321/0.87947. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.51517/0.91943. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.53001/0.92289. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.51312/0.90226. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.48877/0.91221. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.49169/0.92614. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.50023/0.93690. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.47683/0.95349. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.47868/0.94514. Took 0.42 sec\n",
      "Epoch 54, Loss(train/val) 0.46651/0.92966. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.47767/0.96013. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.46462/0.98269. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.44236/0.99482. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.44356/0.99954. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.44783/1.00390. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.44304/0.96083. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.44373/1.02991. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.43917/1.01392. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.44413/1.04002. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.40830/1.11546. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.40824/1.09658. Took 0.42 sec\n",
      "Epoch 66, Loss(train/val) 0.41083/1.10896. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.40922/1.07402. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.40327/1.11688. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.39652/1.10786. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.39174/1.16247. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.38336/1.12962. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.35750/1.13340. Took 0.57 sec\n",
      "Epoch 73, Loss(train/val) 0.35936/1.22077. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.37380/1.20561. Took 0.62 sec\n",
      "Epoch 75, Loss(train/val) 0.36307/1.16888. Took 0.53 sec\n",
      "Epoch 76, Loss(train/val) 0.34755/1.22158. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.35018/1.20896. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.34754/1.17167. Took 0.49 sec\n",
      "Epoch 79, Loss(train/val) 0.35791/1.18187. Took 0.50 sec\n",
      "Epoch 80, Loss(train/val) 0.37347/1.18732. Took 0.47 sec\n",
      "Epoch 81, Loss(train/val) 0.33399/1.17354. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.32383/1.21916. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.32261/1.25773. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.30783/1.28109. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.35415/1.30278. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.29763/1.30249. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.29189/1.30349. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.31175/1.36484. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.29950/1.28652. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.29879/1.38990. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.26936/1.51997. Took 0.47 sec\n",
      "Epoch 92, Loss(train/val) 0.29691/1.34386. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.28682/1.41227. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.28109/1.32341. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.29630/1.51407. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.27735/1.46445. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.36592/1.55334. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.29724/1.57287. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.26040/1.48347. Took 0.46 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69584/0.69040. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69285/0.69441. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.68891/0.69173. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68407/0.69091. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68542/0.67822. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.68624/0.69245. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.68428/0.68954. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68502/0.69579. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68143/0.69601. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.67397/0.69511. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67948/0.70311. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67173/0.69979. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66929/0.70019. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67029/0.69695. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.66332/0.71135. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66563/0.70713. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66032/0.71791. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65753/0.71338. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.66064/0.71773. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64815/0.75722. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65199/0.74709. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64047/0.77500. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64487/0.76748. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64136/0.74987. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.63921/0.74233. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63256/0.75555. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62746/0.74990. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.62286/0.75673. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.61656/0.75062. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.61301/0.74082. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61827/0.74028. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.61128/0.75213. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60089/0.74863. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.60914/0.75643. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.60205/0.74744. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59336/0.75576. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.58812/0.76570. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.57784/0.75990. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.59036/0.77322. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58618/0.77104. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.57421/0.82675. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.55844/0.79350. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.57695/0.83140. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.55542/0.78803. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55330/0.83485. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.55898/0.83756. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55449/0.80475. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55758/0.86506. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.53775/0.83926. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.53091/0.85065. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53945/0.87096. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.53104/0.85560. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.52455/0.86314. Took 0.42 sec\n",
      "Epoch 53, Loss(train/val) 0.52640/0.89474. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.52206/0.92413. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.52352/0.91833. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.52864/0.87357. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.49048/0.90055. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.49807/0.92152. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50469/0.92882. Took 0.49 sec\n",
      "Epoch 60, Loss(train/val) 0.49683/0.89810. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50250/0.89606. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.47699/0.87642. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.47809/0.93077. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.48284/0.92586. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.49038/0.89930. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.48613/0.95155. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.47581/0.98506. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.47026/0.97275. Took 0.48 sec\n",
      "Epoch 69, Loss(train/val) 0.46387/0.95890. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.46909/0.95914. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.46065/0.94147. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.43062/1.00351. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46365/0.98288. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.44216/0.99714. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44467/1.01941. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.42297/1.04151. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.41956/1.07965. Took 0.47 sec\n",
      "Epoch 78, Loss(train/val) 0.45771/1.07711. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44427/0.99289. Took 0.49 sec\n",
      "Epoch 80, Loss(train/val) 0.41763/1.03985. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.41452/1.02557. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.43623/1.02024. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.41530/1.02988. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.42220/1.08930. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.40524/1.04444. Took 0.48 sec\n",
      "Epoch 86, Loss(train/val) 0.40813/1.08408. Took 0.50 sec\n",
      "Epoch 87, Loss(train/val) 0.38060/1.15903. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.44155/1.03643. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.40539/1.16145. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.36962/1.27682. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.38422/1.17955. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.35382/1.17773. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.37978/1.13796. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.37409/1.10734. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.36081/1.18119. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.33813/1.27354. Took 0.42 sec\n",
      "Epoch 97, Loss(train/val) 0.34643/1.24475. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.35852/1.29922. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.37557/1.17980. Took 0.43 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.72070/0.71484. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.70927/0.71724. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.70413/0.70401. Took 0.50 sec\n",
      "Epoch 3, Loss(train/val) 0.70633/0.70366. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.68887/0.71518. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69128/0.71052. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.69793/0.73149. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68929/0.72250. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69029/0.71691. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.69317/0.72173. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68120/0.71037. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68254/0.70446. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.68552/0.70965. Took 0.42 sec\n",
      "Epoch 13, Loss(train/val) 0.67675/0.69912. Took 0.47 sec\n",
      "Epoch 14, Loss(train/val) 0.66914/0.70605. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67822/0.70371. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66924/0.72342. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67363/0.72969. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66199/0.77782. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.67030/0.78316. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67347/0.78047. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65844/0.77060. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65978/0.76447. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.65614/0.79273. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65908/0.79389. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65439/0.80903. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.65192/0.81837. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65409/0.82568. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65286/0.82521. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.64625/0.80277. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.64301/0.80044. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.63880/0.79962. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63852/0.84514. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.63886/0.81308. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.63781/0.82581. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.62762/0.83326. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.63146/0.85438. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.63275/0.83700. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.62702/0.85719. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.62405/0.83932. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62160/0.84535. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.62221/0.87351. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.61862/0.82446. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61288/0.83675. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.60742/0.89179. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.60235/0.87965. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.61261/0.91965. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.60835/0.93476. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59771/0.88232. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.59118/0.95891. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.59977/0.88085. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.58675/0.97473. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.59182/0.92179. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.59093/0.96969. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.57692/0.96136. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.58084/0.99952. Took 0.42 sec\n",
      "Epoch 56, Loss(train/val) 0.57509/1.03182. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.56973/1.03653. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.56426/1.05629. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.56000/1.01296. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.56963/1.09995. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.56590/1.05628. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.56063/1.00967. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.55336/1.08677. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.53879/1.08131. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.53763/1.08290. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.55569/1.06785. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.55834/1.07818. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.55060/1.01165. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.54009/1.14728. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.52535/1.11683. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.52910/1.21600. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.53359/1.08281. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.53236/1.14569. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.52546/1.06919. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.52674/1.15767. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.52583/1.16895. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.51734/1.21088. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.49870/1.15356. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.51976/1.10129. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.51793/1.18642. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.50391/1.12020. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.49618/1.17523. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.51902/1.19708. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.49783/1.24912. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.47773/1.20686. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.49030/1.28600. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.48109/1.27610. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.47699/1.26704. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.46672/1.21386. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.49625/1.20481. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.47240/1.26166. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.46638/1.23457. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.46125/1.19521. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.44696/1.28987. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.46557/1.19898. Took 0.42 sec\n",
      "Epoch 96, Loss(train/val) 0.44450/1.30797. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.44505/1.28982. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.43294/1.37395. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.43750/1.31790. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.70842/0.70818. Took 0.58 sec\n",
      "Epoch 1, Loss(train/val) 0.70408/0.72994. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69484/0.73473. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69291/0.72573. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69055/0.72758. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68471/0.73447. Took 0.42 sec\n",
      "Epoch 6, Loss(train/val) 0.68471/0.73535. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68170/0.74090. Took 0.42 sec\n",
      "Epoch 8, Loss(train/val) 0.67598/0.74879. Took 0.42 sec\n",
      "Epoch 9, Loss(train/val) 0.67844/0.77589. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67186/0.79272. Took 0.42 sec\n",
      "Epoch 11, Loss(train/val) 0.67607/0.79630. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66971/0.80254. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67144/0.81946. Took 0.42 sec\n",
      "Epoch 14, Loss(train/val) 0.66159/0.83080. Took 0.42 sec\n",
      "Epoch 15, Loss(train/val) 0.66388/0.81311. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66388/0.80863. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.66350/0.83009. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66217/0.81199. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65500/0.82077. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.65819/0.84168. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.65708/0.83258. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64635/0.83053. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.63939/0.85855. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.64576/0.90549. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64763/0.91894. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64468/0.92541. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63213/0.88541. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62131/0.91634. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.61512/0.92498. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61592/0.94284. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.62167/0.98014. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62185/0.98147. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60414/0.99036. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.60031/1.00212. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.59240/1.03221. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.59048/1.05273. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.58229/1.04290. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.59599/1.00743. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58069/1.03322. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.57549/1.06454. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.56984/1.11615. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56175/1.08251. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.56138/1.08255. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.56559/1.07319. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.57140/1.11066. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.55264/1.12315. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.54045/1.12697. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.53925/1.13048. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.54080/1.12201. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.53290/1.12653. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.52206/1.07878. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.52373/1.08873. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.52795/1.07850. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52878/1.09928. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.52671/1.08398. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.50796/1.10590. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.49517/1.14815. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.49285/1.12143. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.49502/1.16737. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49049/1.22059. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.46689/1.20544. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.45395/1.19021. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.48572/1.20545. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.46609/1.18456. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.47351/1.19508. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.45225/1.27412. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.47018/1.30084. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.44479/1.26388. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46674/1.38555. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.45527/1.29630. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.43917/1.31405. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.42658/1.34718. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.43888/1.32799. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.43524/1.40755. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.43315/1.30169. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.43476/1.41361. Took 0.43 sec\n",
      "Epoch 77, Loss(train/val) 0.42642/1.47380. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.43608/1.43102. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.40618/1.49097. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.43232/1.33169. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.44831/1.34705. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.42404/1.33188. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.39529/1.48907. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.40674/1.31034. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.38428/1.45749. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.40002/1.49977. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.38324/1.52064. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.37783/1.63080. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.38380/1.58259. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.37738/1.57292. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.38188/1.45275. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.37551/1.57375. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.37608/1.49339. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.36472/1.61105. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.34977/1.59669. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.37201/1.59259. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.34763/1.70293. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.34819/1.72830. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.38989/1.69830. Took 0.43 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.71715/0.70885. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.71157/0.70818. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.69683/0.69920. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69530/0.71014. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.70063/0.71396. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.69056/0.70310. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68947/0.70411. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68099/0.74057. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.68733/0.74772. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.68346/0.72916. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68142/0.73274. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67742/0.72271. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67208/0.72105. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67738/0.74276. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.67551/0.73269. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67036/0.74016. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.67640/0.76043. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.67336/0.75218. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66383/0.75943. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65886/0.77062. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.67109/0.78859. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66493/0.78856. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66045/0.78931. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65921/0.78308. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.65785/0.78139. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.65333/0.78447. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.64535/0.79607. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65555/0.80942. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.64385/0.83975. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.64096/0.82019. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.65218/0.84139. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.63846/0.84106. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63548/0.84949. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62902/0.86665. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.62781/0.85764. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.62463/0.86701. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.61966/0.85946. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.62687/0.84883. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.61894/0.85228. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60854/0.84105. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.60745/0.87225. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60147/0.88414. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59786/0.89082. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60047/0.90486. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59255/0.91364. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.60480/0.91690. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.58999/0.91471. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58907/0.89840. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.57661/0.92380. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57597/0.91249. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57347/0.91707. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.56062/0.94104. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.57234/0.92573. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.56363/0.95550. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.55849/0.96475. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.55403/0.92429. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.56163/0.99491. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.55044/0.94089. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.53978/0.98149. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.53498/1.02381. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.53369/0.98757. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.55074/0.92872. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.53293/0.99488. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.53404/0.96177. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.52823/0.95565. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.51340/1.01994. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.51958/0.96797. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.50858/0.98050. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.51939/0.96299. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.50807/0.95657. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.49667/0.94758. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.49921/0.91043. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.49138/1.03791. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.51373/0.95002. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.50690/0.95985. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.50538/0.92589. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.47678/1.01486. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46609/1.01389. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.48017/1.01233. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46156/1.06287. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.48316/0.99373. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.48191/0.91290. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.47658/0.99416. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.45604/1.07579. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.45975/1.06298. Took 0.42 sec\n",
      "Epoch 85, Loss(train/val) 0.44901/0.99404. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.45086/1.11397. Took 0.42 sec\n",
      "Epoch 87, Loss(train/val) 0.45548/1.11083. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.45559/1.13280. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.45637/1.10647. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.47662/1.05638. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.44792/1.08091. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.43214/1.10907. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.42802/1.12413. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.42794/1.06702. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.42859/1.10296. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.40565/1.08159. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.41810/1.10651. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.44137/1.04397. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.45284/1.07115. Took 0.43 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69808/0.68780. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69512/0.70200. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69104/0.71578. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.69162/0.71874. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68761/0.72869. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68763/0.74325. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68105/0.74419. Took 0.42 sec\n",
      "Epoch 7, Loss(train/val) 0.68165/0.74784. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68287/0.76947. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67502/0.77322. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67783/0.77422. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67374/0.76646. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67397/0.77304. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67473/0.78358. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66993/0.77973. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.67274/0.79033. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66646/0.79887. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66561/0.81928. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66489/0.81664. Took 0.42 sec\n",
      "Epoch 19, Loss(train/val) 0.65751/0.83674. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65829/0.85861. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.65744/0.85524. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.65006/0.85840. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.64765/0.87876. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.65003/0.89295. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.63682/0.89029. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62802/0.90937. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.62814/0.91706. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62448/0.95651. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.61659/0.96188. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.61518/0.97227. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60438/1.00227. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.60647/0.97688. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.59947/0.99200. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.59791/0.99508. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.59819/0.97691. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59425/1.01968. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.58812/1.02103. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.58264/1.03652. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.56640/1.06352. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.57673/1.03939. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.54826/1.09440. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55399/1.08378. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.55609/1.06523. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.54117/1.07345. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55468/1.07065. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.53475/1.11344. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.51985/1.19192. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.53210/1.16963. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52592/1.14956. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.52946/1.16150. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.52206/1.12219. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.50141/1.18629. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.48889/1.19316. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.49279/1.24880. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.50231/1.22325. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.48968/1.12718. Took 0.42 sec\n",
      "Epoch 57, Loss(train/val) 0.47701/1.16429. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.47466/1.18800. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.49098/1.27029. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.47775/1.27670. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.46117/1.26341. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.46784/1.28618. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.46328/1.28649. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.46007/1.25954. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.44578/1.25073. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.45693/1.45848. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.43859/1.24642. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.43804/1.31705. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.42559/1.34567. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.41471/1.43688. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.42406/1.30544. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.43993/1.37157. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.41412/1.33108. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.41339/1.44137. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.41588/1.31093. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.38038/1.44692. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.38044/1.37571. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.39007/1.42868. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.36442/1.37242. Took 0.42 sec\n",
      "Epoch 80, Loss(train/val) 0.38945/1.52928. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.38642/1.50945. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.38219/1.33343. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.38365/1.42075. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.35222/1.51806. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.35680/1.43107. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.34951/1.49618. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.35917/1.45040. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.35253/1.60690. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.34763/1.52560. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.33548/1.52050. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.32102/1.58640. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.36032/1.37114. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.34189/1.64002. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.37806/1.57739. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.35369/1.51626. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.33072/1.60424. Took 0.43 sec\n",
      "Epoch 97, Loss(train/val) 0.35153/1.46090. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.32922/1.55909. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.37188/1.45111. Took 0.43 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.70721/0.73005. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69962/0.72151. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69134/0.71387. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.68964/0.73669. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69169/0.72401. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68115/0.73451. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68309/0.73396. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68340/0.74348. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67860/0.75415. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67392/0.76131. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67789/0.76606. Took 0.43 sec\n",
      "Epoch 11, Loss(train/val) 0.67440/0.77833. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67026/0.77745. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.67145/0.78657. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66533/0.79499. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66983/0.76924. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66319/0.77801. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65067/0.78544. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65200/0.79559. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65542/0.80834. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64731/0.82030. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64989/0.81666. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64678/0.82876. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63990/0.84870. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63884/0.84734. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.62898/0.85605. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63086/0.88344. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63131/0.87700. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62825/0.87867. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62346/0.90798. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63282/0.89427. Took 0.42 sec\n",
      "Epoch 31, Loss(train/val) 0.61619/0.89050. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.62072/0.88548. Took 0.42 sec\n",
      "Epoch 33, Loss(train/val) 0.61147/0.88037. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.61251/0.88769. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60559/0.91666. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.60092/0.93128. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59101/0.95980. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60100/0.97188. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.59908/0.95163. Took 0.42 sec\n",
      "Epoch 40, Loss(train/val) 0.59911/0.96523. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.59385/0.99170. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58558/1.01085. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.57218/0.97531. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.57375/0.96186. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.56693/0.96977. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.56654/0.99169. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.56779/0.96882. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.56454/0.96760. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.56559/0.97436. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54184/1.01200. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.55098/0.98749. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.54967/1.00354. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.53504/1.00263. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.53990/0.98477. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.52463/1.00688. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.52942/1.00258. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.53412/0.97762. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.51375/1.00652. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.52420/0.97156. Took 0.42 sec\n",
      "Epoch 60, Loss(train/val) 0.51719/0.98020. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.50587/1.02588. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.51169/1.03298. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.48961/1.10439. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.48414/1.07487. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.48057/1.07642. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.48008/1.07892. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48044/1.06721. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.46273/1.10903. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46602/1.10172. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.45238/1.10647. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.45252/1.16432. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48329/1.08332. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.45926/1.11130. Took 0.42 sec\n",
      "Epoch 74, Loss(train/val) 0.43391/1.13863. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44481/1.15003. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.42139/1.19726. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.40672/1.18599. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.43860/1.28835. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.41682/1.25714. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.41194/1.25482. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.41156/1.27484. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.40889/1.27063. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.41968/1.22085. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.40065/1.28424. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.39500/1.27166. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.39313/1.25302. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.39721/1.23401. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.36693/1.31632. Took 0.42 sec\n",
      "Epoch 89, Loss(train/val) 0.39224/1.27013. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.36355/1.30445. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.36805/1.30440. Took 0.42 sec\n",
      "Epoch 92, Loss(train/val) 0.36979/1.37681. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.35560/1.37475. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.36138/1.31420. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.34961/1.40044. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.34320/1.38360. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.34201/1.38893. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.34985/1.40761. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.34307/1.42320. Took 0.43 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.70376/0.68497. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68755/0.69278. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69603/0.69056. Took 0.42 sec\n",
      "Epoch 3, Loss(train/val) 0.68795/0.67303. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.68302/0.67701. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.68470/0.68047. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68057/0.68652. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68133/0.68025. Took 0.42 sec\n",
      "Epoch 8, Loss(train/val) 0.68208/0.68687. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67416/0.69018. Took 0.42 sec\n",
      "Epoch 10, Loss(train/val) 0.67521/0.69983. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67091/0.71183. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67257/0.71370. Took 0.43 sec\n",
      "Epoch 13, Loss(train/val) 0.66385/0.70588. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66707/0.70587. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65341/0.70543. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.65794/0.73530. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65490/0.71629. Took 0.42 sec\n",
      "Epoch 18, Loss(train/val) 0.65321/0.73501. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.64425/0.73186. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63886/0.75483. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.63662/0.79268. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64094/0.81818. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63784/0.79958. Took 0.42 sec\n",
      "Epoch 24, Loss(train/val) 0.64014/0.80419. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.62627/0.81218. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.61337/0.85545. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.62326/0.83403. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61764/0.84012. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60530/0.88879. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.59832/0.90702. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.58942/0.87912. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.58412/0.89918. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.58408/0.89964. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.58255/0.92525. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.57533/0.91736. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.56481/0.92931. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.55731/0.99029. Took 0.42 sec\n",
      "Epoch 38, Loss(train/val) 0.54352/1.00888. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.55200/0.97867. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.55467/0.96845. Took 0.43 sec\n",
      "Epoch 41, Loss(train/val) 0.54138/0.93806. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.53662/0.96423. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.53877/0.98165. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.53344/0.95529. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.53124/0.95229. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.53126/0.97389. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.51237/1.02410. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.49716/1.05020. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.50844/1.08081. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.51250/1.01243. Took 0.42 sec\n",
      "Epoch 51, Loss(train/val) 0.48478/1.14329. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.49521/1.14427. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.48758/1.08603. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.47348/1.09984. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.47611/1.13564. Took 0.42 sec\n",
      "Epoch 56, Loss(train/val) 0.48064/1.18648. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.46236/1.18075. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.47113/1.09530. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.46270/1.18484. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.46976/1.19369. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.46781/1.19465. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.46019/1.27913. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.44395/1.27182. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.43615/1.32612. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.43892/1.32121. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.42100/1.36884. Took 0.42 sec\n",
      "Epoch 67, Loss(train/val) 0.43239/1.30813. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.42558/1.37475. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.41003/1.35387. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.42202/1.34516. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.40180/1.39625. Took 0.42 sec\n",
      "Epoch 72, Loss(train/val) 0.39930/1.39564. Took 0.42 sec\n",
      "Epoch 73, Loss(train/val) 0.38849/1.43523. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.42808/1.30588. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.39167/1.43447. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.39664/1.43062. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.38493/1.41129. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.37652/1.40197. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.37013/1.43991. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.35504/1.47134. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.35516/1.42824. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.36255/1.47859. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.36544/1.44292. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.35603/1.44587. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.34584/1.48824. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.35583/1.35257. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.36026/1.41183. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.33233/1.43878. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.32428/1.56496. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.32083/1.50591. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.29543/1.58504. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.28744/1.56204. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.32012/1.55771. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.28917/1.65444. Took 0.42 sec\n",
      "Epoch 95, Loss(train/val) 0.29630/1.74604. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.30858/1.66926. Took 0.42 sec\n",
      "Epoch 97, Loss(train/val) 0.33078/1.60200. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.29651/1.55580. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.27908/1.64971. Took 0.43 sec\n",
      "ACC: 0.6458333333333334\n",
      "Epoch 0, Loss(train/val) 0.69373/0.69540. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68915/0.70632. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.68212/0.71039. Took 0.42 sec\n",
      "Epoch 3, Loss(train/val) 0.67680/0.71284. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.67290/0.70961. Took 0.43 sec\n",
      "Epoch 5, Loss(train/val) 0.67729/0.71332. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.67127/0.72147. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67061/0.71376. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67358/0.71767. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.66827/0.69227. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.66262/0.73835. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66786/0.70105. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66456/0.69220. Took 0.47 sec\n",
      "Epoch 13, Loss(train/val) 0.66364/0.70517. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66401/0.71447. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.65965/0.71305. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.65867/0.70388. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65908/0.71066. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65711/0.70116. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.64919/0.70713. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64876/0.72102. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64261/0.73035. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.63004/0.76539. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.63668/0.75037. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63398/0.77618. Took 0.42 sec\n",
      "Epoch 25, Loss(train/val) 0.63882/0.76895. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63160/0.76934. Took 0.42 sec\n",
      "Epoch 27, Loss(train/val) 0.62627/0.77843. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62141/0.79049. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.61791/0.80369. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.61428/0.79522. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.60440/0.82221. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.59495/0.81443. Took 0.42 sec\n",
      "Epoch 33, Loss(train/val) 0.59821/0.86097. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.59291/0.88168. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60048/0.88231. Took 0.42 sec\n",
      "Epoch 36, Loss(train/val) 0.59265/0.91994. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.58886/0.89902. Took 0.43 sec\n",
      "Epoch 38, Loss(train/val) 0.57795/0.91430. Took 0.42 sec\n",
      "Epoch 39, Loss(train/val) 0.58652/0.91382. Took 0.42 sec\n",
      "Epoch 40, Loss(train/val) 0.58029/0.93391. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58013/0.90346. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.57695/0.91183. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.56560/0.91412. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56819/0.93832. Took 0.42 sec\n",
      "Epoch 45, Loss(train/val) 0.55322/0.97590. Took 0.42 sec\n",
      "Epoch 46, Loss(train/val) 0.55619/0.96994. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55367/0.94815. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.54565/0.94990. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.55035/1.00556. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55130/0.99498. Took 0.42 sec\n",
      "Epoch 51, Loss(train/val) 0.54576/0.95960. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.53669/0.99124. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.52912/1.01314. Took 0.42 sec\n",
      "Epoch 54, Loss(train/val) 0.50384/0.96484. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.53245/1.03761. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.51154/1.04441. Took 0.42 sec\n",
      "Epoch 57, Loss(train/val) 0.50017/1.08964. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.50031/1.10085. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.51265/1.07358. Took 0.42 sec\n",
      "Epoch 60, Loss(train/val) 0.49973/1.08475. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.49548/1.02947. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48187/0.99444. Took 0.42 sec\n",
      "Epoch 63, Loss(train/val) 0.48618/1.07758. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.48041/1.10341. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.48353/1.02977. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.48154/1.10258. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.45587/1.16988. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.45929/1.17373. Took 0.42 sec\n",
      "Epoch 69, Loss(train/val) 0.44623/1.10228. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.45749/1.06816. Took 0.42 sec\n",
      "Epoch 71, Loss(train/val) 0.44693/1.12963. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.43446/1.21029. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.43891/1.19735. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.41786/1.16792. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44166/1.18200. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.42210/1.15700. Took 0.42 sec\n",
      "Epoch 77, Loss(train/val) 0.42514/1.21987. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.43717/1.11138. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.40965/1.13857. Took 0.42 sec\n",
      "Epoch 80, Loss(train/val) 0.40360/1.15576. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.39673/1.21875. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.41768/1.13675. Took 0.42 sec\n",
      "Epoch 83, Loss(train/val) 0.38641/1.17953. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.38869/1.15033. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.38794/1.17100. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.36732/1.19728. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.36199/1.30311. Took 0.42 sec\n",
      "Epoch 88, Loss(train/val) 0.37311/1.21936. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.40169/1.13407. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.36484/1.25487. Took 0.43 sec\n",
      "Epoch 91, Loss(train/val) 0.34887/1.25072. Took 0.42 sec\n",
      "Epoch 92, Loss(train/val) 0.34519/1.22751. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.33903/1.31911. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.33987/1.29151. Took 0.42 sec\n",
      "Epoch 95, Loss(train/val) 0.34717/1.32344. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.33369/1.38981. Took 0.42 sec\n",
      "Epoch 97, Loss(train/val) 0.31130/1.45400. Took 0.42 sec\n",
      "Epoch 98, Loss(train/val) 0.34267/1.32996. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.32437/1.27716. Took 0.44 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.70357/0.69192. Took 0.46 sec\n",
      "Epoch 1, Loss(train/val) 0.69453/0.69911. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69300/0.69641. Took 0.42 sec\n",
      "Epoch 3, Loss(train/val) 0.68512/0.69200. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68893/0.69166. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.68390/0.68045. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.68278/0.69609. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.68309/0.68370. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67516/0.70329. Took 0.42 sec\n",
      "Epoch 9, Loss(train/val) 0.67891/0.70608. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.68179/0.69839. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67426/0.69843. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67577/0.69857. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66963/0.70409. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67075/0.71630. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66924/0.70588. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66361/0.71572. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66743/0.71622. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66155/0.73318. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.65492/0.73699. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64904/0.74170. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64736/0.73201. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64697/0.74392. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.64866/0.73456. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64437/0.73186. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63284/0.74685. Took 0.42 sec\n",
      "Epoch 26, Loss(train/val) 0.62641/0.75184. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63146/0.75232. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.61503/0.77236. Took 0.42 sec\n",
      "Epoch 29, Loss(train/val) 0.61661/0.77662. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.61191/0.79101. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.60556/0.81716. Took 0.42 sec\n",
      "Epoch 32, Loss(train/val) 0.59735/0.82117. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.60748/0.81598. Took 0.42 sec\n",
      "Epoch 34, Loss(train/val) 0.59572/0.81494. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.58823/0.82463. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.59374/0.81627. Took 0.42 sec\n",
      "Epoch 37, Loss(train/val) 0.58329/0.83761. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57928/0.84808. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57847/0.86240. Took 0.42 sec\n",
      "Epoch 40, Loss(train/val) 0.56460/0.83491. Took 0.42 sec\n",
      "Epoch 41, Loss(train/val) 0.56375/0.86449. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.54486/0.91800. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.54263/0.91081. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.55237/0.93843. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.54727/0.92663. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.53169/0.97819. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.53130/0.96138. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.52520/1.00507. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.52612/0.94281. Took 0.43 sec\n",
      "Epoch 50, Loss(train/val) 0.51477/1.00970. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.52039/1.00728. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.49185/1.01065. Took 0.42 sec\n",
      "Epoch 53, Loss(train/val) 0.49164/1.00554. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.51191/1.00884. Took 0.43 sec\n",
      "Epoch 55, Loss(train/val) 0.50935/0.90599. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.48606/1.01946. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.47840/1.08838. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.47090/1.08136. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.47593/1.17587. Took 0.42 sec\n",
      "Epoch 60, Loss(train/val) 0.47732/1.11480. Took 0.43 sec\n",
      "Epoch 61, Loss(train/val) 0.45988/1.13172. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.46420/1.09674. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.46250/1.07351. Took 0.42 sec\n",
      "Epoch 64, Loss(train/val) 0.43773/1.18343. Took 0.42 sec\n",
      "Epoch 65, Loss(train/val) 0.45784/1.08994. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.44516/1.11933. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.43331/1.13346. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.44050/1.13576. Took 0.43 sec\n",
      "Epoch 69, Loss(train/val) 0.40869/1.19428. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.42164/1.24579. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.43898/1.13630. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.45456/1.17565. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.40408/1.26096. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.41953/1.22523. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.45790/1.22197. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.41474/1.20217. Took 0.42 sec\n",
      "Epoch 77, Loss(train/val) 0.39083/1.23703. Took 0.42 sec\n",
      "Epoch 78, Loss(train/val) 0.39121/1.21936. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.37358/1.25108. Took 0.42 sec\n",
      "Epoch 80, Loss(train/val) 0.38731/1.31179. Took 0.42 sec\n",
      "Epoch 81, Loss(train/val) 0.38384/1.31828. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.37072/1.37472. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.35329/1.28549. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.39134/1.29202. Took 0.42 sec\n",
      "Epoch 85, Loss(train/val) 0.37761/1.28099. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.39645/1.29170. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.40125/1.31132. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.36855/1.37807. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.34761/1.40535. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.39236/1.33478. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.35270/1.40061. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.33108/1.48426. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.35309/1.30595. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.37171/1.39501. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.35024/1.46085. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.35238/1.39727. Took 0.42 sec\n",
      "Epoch 97, Loss(train/val) 0.37143/1.45488. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.33152/1.47862. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34613/1.49516. Took 0.43 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.70115/0.69747. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69060/0.70164. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.68499/0.70714. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68367/0.72444. Took 0.42 sec\n",
      "Epoch 4, Loss(train/val) 0.68104/0.72678. Took 0.42 sec\n",
      "Epoch 5, Loss(train/val) 0.67923/0.73641. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67556/0.73816. Took 0.43 sec\n",
      "Epoch 7, Loss(train/val) 0.67494/0.73663. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.67621/0.74055. Took 0.42 sec\n",
      "Epoch 9, Loss(train/val) 0.67507/0.73415. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67110/0.73714. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67552/0.73125. Took 0.42 sec\n",
      "Epoch 12, Loss(train/val) 0.66758/0.73904. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66558/0.73740. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66807/0.72737. Took 0.42 sec\n",
      "Epoch 15, Loss(train/val) 0.66324/0.73303. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.66594/0.73135. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66194/0.74824. Took 0.43 sec\n",
      "Epoch 18, Loss(train/val) 0.65298/0.74680. Took 0.42 sec\n",
      "Epoch 19, Loss(train/val) 0.65919/0.75394. Took 0.42 sec\n",
      "Epoch 20, Loss(train/val) 0.65761/0.75938. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64777/0.76004. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64453/0.76436. Took 0.42 sec\n",
      "Epoch 23, Loss(train/val) 0.63845/0.78227. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.63976/0.76953. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63623/0.78410. Took 0.43 sec\n",
      "Epoch 26, Loss(train/val) 0.63911/0.77061. Took 0.42 sec\n",
      "Epoch 27, Loss(train/val) 0.62602/0.79859. Took 0.42 sec\n",
      "Epoch 28, Loss(train/val) 0.62891/0.81859. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.61949/0.81074. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.62237/0.81902. Took 0.43 sec\n",
      "Epoch 31, Loss(train/val) 0.62522/0.84033. Took 0.42 sec\n",
      "Epoch 32, Loss(train/val) 0.61858/0.85312. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.60160/0.84835. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.59956/0.85841. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60415/0.86009. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.60127/0.86842. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.59355/0.86432. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58304/0.88169. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58632/0.87040. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.57331/0.86963. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58124/0.88480. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.58047/0.87863. Took 0.42 sec\n",
      "Epoch 43, Loss(train/val) 0.56298/0.88382. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.57016/0.87023. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56177/0.90157. Took 0.42 sec\n",
      "Epoch 46, Loss(train/val) 0.54966/0.93007. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55347/0.92229. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.54184/0.91652. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.54547/0.94753. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.52237/0.92601. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.52030/0.94348. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.51574/0.97025. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.51033/0.97093. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.51197/0.99969. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.50694/1.02854. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.50616/0.99793. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.49695/0.98991. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.49628/0.98050. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.48501/0.99304. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.48290/1.00484. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.47882/1.01282. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.46349/1.00929. Took 0.43 sec\n",
      "Epoch 63, Loss(train/val) 0.46833/1.00241. Took 0.42 sec\n",
      "Epoch 64, Loss(train/val) 0.45758/1.01766. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.44463/1.06603. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.44054/1.05389. Took 0.42 sec\n",
      "Epoch 67, Loss(train/val) 0.43810/1.02608. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.42175/1.03031. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.40646/1.10903. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.43684/1.07187. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.43671/1.07092. Took 0.42 sec\n",
      "Epoch 72, Loss(train/val) 0.43164/1.11866. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.43451/1.07347. Took 0.42 sec\n",
      "Epoch 74, Loss(train/val) 0.40890/1.04723. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.39490/1.05310. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.39774/1.06728. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.40788/1.09438. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.40980/1.07132. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.39744/1.09346. Took 0.42 sec\n",
      "Epoch 80, Loss(train/val) 0.37875/1.08571. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.37049/1.10173. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.36236/1.20231. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.37527/1.17528. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.35474/1.14222. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.35781/1.19231. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.35893/1.19659. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.36781/1.24333. Took 0.49 sec\n",
      "Epoch 88, Loss(train/val) 0.36117/1.17557. Took 0.47 sec\n",
      "Epoch 89, Loss(train/val) 0.36312/1.20270. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.32835/1.22587. Took 0.51 sec\n",
      "Epoch 91, Loss(train/val) 0.33042/1.22549. Took 0.48 sec\n",
      "Epoch 92, Loss(train/val) 0.31174/1.17359. Took 0.48 sec\n",
      "Epoch 93, Loss(train/val) 0.31747/1.24252. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.33935/1.29943. Took 0.48 sec\n",
      "Epoch 95, Loss(train/val) 0.31064/1.33432. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.32794/1.29686. Took 0.47 sec\n",
      "Epoch 97, Loss(train/val) 0.30641/1.22609. Took 0.48 sec\n",
      "Epoch 98, Loss(train/val) 0.29908/1.30353. Took 0.47 sec\n",
      "Epoch 99, Loss(train/val) 0.29175/1.13971. Took 0.46 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.71458/0.69786. Took 0.53 sec\n",
      "Epoch 1, Loss(train/val) 0.70831/0.69127. Took 0.50 sec\n",
      "Epoch 2, Loss(train/val) 0.70364/0.69230. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69844/0.68840. Took 0.49 sec\n",
      "Epoch 4, Loss(train/val) 0.69761/0.67285. Took 0.50 sec\n",
      "Epoch 5, Loss(train/val) 0.69813/0.69184. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.69602/0.68531. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.68614/0.68884. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.68475/0.68848. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68148/0.68331. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67884/0.69770. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.67521/0.70649. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67653/0.71141. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.67316/0.71441. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.66665/0.70959. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66863/0.71166. Took 0.47 sec\n",
      "Epoch 16, Loss(train/val) 0.66273/0.70190. Took 0.47 sec\n",
      "Epoch 17, Loss(train/val) 0.65934/0.73608. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65860/0.74671. Took 0.47 sec\n",
      "Epoch 19, Loss(train/val) 0.66504/0.73449. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.65565/0.76574. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.65172/0.77268. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.64791/0.74974. Took 0.47 sec\n",
      "Epoch 23, Loss(train/val) 0.65670/0.74192. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.63883/0.76810. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65184/0.72773. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.63835/0.73835. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63782/0.74309. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.63551/0.73054. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62772/0.75119. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63971/0.75826. Took 0.47 sec\n",
      "Epoch 31, Loss(train/val) 0.62312/0.73877. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.61733/0.78627. Took 0.47 sec\n",
      "Epoch 33, Loss(train/val) 0.60926/0.80343. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.62225/0.77299. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.62264/0.79463. Took 0.47 sec\n",
      "Epoch 36, Loss(train/val) 0.62234/0.78500. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60531/0.81748. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60407/0.85396. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.59895/0.85476. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.60179/0.84446. Took 0.47 sec\n",
      "Epoch 41, Loss(train/val) 0.59321/0.85574. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.58634/0.88300. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58893/0.84867. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.58636/0.87382. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.58551/0.88067. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.57589/0.89769. Took 0.47 sec\n",
      "Epoch 47, Loss(train/val) 0.57431/0.90896. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57171/0.90657. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.57134/0.90879. Took 0.47 sec\n",
      "Epoch 50, Loss(train/val) 0.56245/0.92109. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55696/0.92213. Took 0.48 sec\n",
      "Epoch 52, Loss(train/val) 0.56160/0.87998. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.55713/0.86112. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.55490/0.88461. Took 0.48 sec\n",
      "Epoch 55, Loss(train/val) 0.53783/0.90649. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.56120/0.87658. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.55131/0.89066. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.54523/0.88642. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.55577/0.91159. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.55143/0.89255. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.53949/0.92452. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.54281/0.88626. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.52900/0.89968. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.51108/0.96275. Took 0.47 sec\n",
      "Epoch 65, Loss(train/val) 0.51711/0.96514. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51359/0.96817. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.52882/0.92814. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52190/0.95889. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.52503/0.98374. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.50111/1.02718. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.52312/1.03167. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.51175/1.05132. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.50466/1.06679. Took 0.47 sec\n",
      "Epoch 74, Loss(train/val) 0.53367/0.98690. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.51963/1.06909. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.49156/1.11669. Took 0.47 sec\n",
      "Epoch 77, Loss(train/val) 0.49075/1.15119. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.48334/1.13762. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.50070/1.09392. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.47194/1.16435. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.47052/1.15552. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.46145/1.16474. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.48052/1.27683. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.47203/1.25414. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.45600/1.19970. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.42507/1.35426. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.47552/1.28566. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.48298/1.17589. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.44924/1.20991. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.44306/1.27835. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.44804/1.22724. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.42636/1.32992. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.44161/1.25303. Took 0.47 sec\n",
      "Epoch 94, Loss(train/val) 0.42564/1.31896. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.45794/1.28659. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.43674/1.34496. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.41620/1.38812. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.43086/1.26897. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.42913/1.27153. Took 0.46 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.71027/0.68769. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.68920/0.70729. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69311/0.71441. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69708/0.69162. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68777/0.68863. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68023/0.70279. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.68506/0.72394. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.67814/0.72992. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67801/0.73420. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67269/0.73999. Took 0.47 sec\n",
      "Epoch 10, Loss(train/val) 0.67087/0.73503. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66332/0.75171. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67501/0.74380. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.66938/0.73636. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.65980/0.73819. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.65555/0.74196. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65792/0.74356. Took 0.47 sec\n",
      "Epoch 17, Loss(train/val) 0.65050/0.73296. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65875/0.73085. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.64577/0.74841. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.65455/0.74981. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.64996/0.75891. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.64622/0.74203. Took 0.48 sec\n",
      "Epoch 23, Loss(train/val) 0.64231/0.75813. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64687/0.77300. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.64265/0.78871. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.64421/0.77804. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63865/0.78082. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63093/0.77214. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62593/0.80977. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.63475/0.80089. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.62254/0.84035. Took 0.47 sec\n",
      "Epoch 32, Loss(train/val) 0.62932/0.79406. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.61925/0.79570. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.60371/0.80337. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.62295/0.79687. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.61164/0.83327. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59995/0.84104. Took 0.47 sec\n",
      "Epoch 38, Loss(train/val) 0.60125/0.84297. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.59067/0.85046. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.59685/0.86432. Took 0.48 sec\n",
      "Epoch 41, Loss(train/val) 0.57570/0.88967. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57433/0.89578. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.57616/0.88950. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.56041/0.90691. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.57657/0.92569. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.57670/0.90791. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.56701/0.94436. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.55371/0.95011. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.55439/0.95449. Took 0.48 sec\n",
      "Epoch 50, Loss(train/val) 0.56225/0.96768. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.57417/0.99663. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.53539/0.99370. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.54583/1.01529. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.54817/1.03184. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.53948/1.09000. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.51731/1.15165. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.54701/1.10326. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.53039/1.12455. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.51525/1.16271. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.52034/1.21923. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.52360/1.20085. Took 0.47 sec\n",
      "Epoch 62, Loss(train/val) 0.52891/1.23170. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.52080/1.22338. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.48893/1.27131. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.49367/1.27826. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.48980/1.24831. Took 0.47 sec\n",
      "Epoch 67, Loss(train/val) 0.49214/1.30067. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.50133/1.24988. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.48913/1.30485. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.47625/1.29036. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.45828/1.38441. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.46397/1.36206. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.47053/1.39743. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.45909/1.40207. Took 0.47 sec\n",
      "Epoch 75, Loss(train/val) 0.46367/1.38822. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.46409/1.43490. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.51296/1.34056. Took 0.47 sec\n",
      "Epoch 78, Loss(train/val) 0.50659/1.28485. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.46363/1.37626. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.45139/1.39061. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.44252/1.45919. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.43315/1.52108. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.43926/1.48181. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43833/1.43886. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.42239/1.61051. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.44163/1.51566. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.45246/1.46009. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.40667/1.48737. Took 0.47 sec\n",
      "Epoch 89, Loss(train/val) 0.42932/1.49674. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.41897/1.48502. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.39909/1.50971. Took 0.48 sec\n",
      "Epoch 92, Loss(train/val) 0.45271/1.54135. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.42631/1.42102. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.41721/1.54263. Took 0.47 sec\n",
      "Epoch 95, Loss(train/val) 0.43952/1.48835. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.39300/1.66503. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.40063/1.54471. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.38302/1.67336. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.36538/1.64326. Took 0.45 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.71148/0.68702. Took 0.63 sec\n",
      "Epoch 1, Loss(train/val) 0.69975/0.67157. Took 0.49 sec\n",
      "Epoch 2, Loss(train/val) 0.69840/0.69662. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69373/0.70925. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69186/0.70711. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69065/0.70807. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.68803/0.71141. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.68200/0.72979. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67826/0.73425. Took 0.48 sec\n",
      "Epoch 9, Loss(train/val) 0.68245/0.74452. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68501/0.73339. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67405/0.73575. Took 0.47 sec\n",
      "Epoch 12, Loss(train/val) 0.68146/0.72980. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.67701/0.75319. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66321/0.76282. Took 0.47 sec\n",
      "Epoch 15, Loss(train/val) 0.66464/0.77481. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66347/0.78239. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65916/0.80252. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.66334/0.79742. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66238/0.79693. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65628/0.79072. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65539/0.79280. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65223/0.78653. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.64684/0.80037. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64648/0.81382. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.64112/0.81607. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.64284/0.84240. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.64130/0.84393. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.63101/0.84150. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63737/0.84401. Took 0.47 sec\n",
      "Epoch 30, Loss(train/val) 0.63249/0.84126. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62994/0.85502. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.61969/0.90969. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.63374/0.88478. Took 0.47 sec\n",
      "Epoch 34, Loss(train/val) 0.62002/0.90596. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61938/0.88965. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.61793/0.88010. Took 0.47 sec\n",
      "Epoch 37, Loss(train/val) 0.61403/0.93278. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60785/0.92721. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61016/0.93852. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.61555/0.90984. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.60243/1.01875. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.60686/0.99816. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.59485/0.99191. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.59799/1.00818. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.59878/1.03069. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.58635/1.03273. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.58685/1.03483. Took 0.48 sec\n",
      "Epoch 48, Loss(train/val) 0.58001/1.09359. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.58287/1.10708. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.57330/1.11394. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.57898/1.06095. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.57065/1.10389. Took 0.47 sec\n",
      "Epoch 53, Loss(train/val) 0.55296/1.13833. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.56114/1.13462. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.55503/1.18346. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.54985/1.19097. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.56878/1.14873. Took 0.47 sec\n",
      "Epoch 58, Loss(train/val) 0.55181/1.19804. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.53647/1.19664. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.55092/1.26879. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.53597/1.24194. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.53631/1.22180. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51797/1.24771. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.54158/1.27700. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.53085/1.26328. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.52754/1.20759. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.53360/1.25399. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.50489/1.35013. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.49374/1.25764. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.50877/1.33145. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.49803/1.33413. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.51459/1.27866. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.49371/1.31790. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.49782/1.31865. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.47457/1.27416. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.47794/1.43371. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.48698/1.33458. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.47273/1.43355. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.45887/1.48384. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.48168/1.37376. Took 0.47 sec\n",
      "Epoch 81, Loss(train/val) 0.47454/1.43638. Took 0.48 sec\n",
      "Epoch 82, Loss(train/val) 0.46608/1.34603. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.47739/1.35193. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.45124/1.48535. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.44195/1.54582. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.43799/1.54473. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.43113/1.47755. Took 0.47 sec\n",
      "Epoch 88, Loss(train/val) 0.43623/1.53014. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.45249/1.50068. Took 0.47 sec\n",
      "Epoch 90, Loss(train/val) 0.42815/1.51433. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.42790/1.53964. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.42940/1.53956. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.42325/1.50578. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.41015/1.61175. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.40092/1.53100. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.42801/1.56793. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.41346/1.62928. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.44525/1.50181. Took 0.47 sec\n",
      "Epoch 99, Loss(train/val) 0.41817/1.49661. Took 0.45 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69869/0.70592. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69467/0.70893. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69198/0.70644. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69194/0.70964. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69049/0.70558. Took 0.49 sec\n",
      "Epoch 5, Loss(train/val) 0.68941/0.69970. Took 0.50 sec\n",
      "Epoch 6, Loss(train/val) 0.68805/0.71964. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68331/0.73148. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.68492/0.73997. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.68376/0.73686. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.68295/0.73551. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.68046/0.73472. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67559/0.73664. Took 0.47 sec\n",
      "Epoch 13, Loss(train/val) 0.67622/0.72842. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.67071/0.73615. Took 0.47 sec\n",
      "Epoch 15, Loss(train/val) 0.67638/0.73911. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.67117/0.75091. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66798/0.74955. Took 0.47 sec\n",
      "Epoch 18, Loss(train/val) 0.65691/0.77567. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66269/0.77678. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.65572/0.78841. Took 0.47 sec\n",
      "Epoch 21, Loss(train/val) 0.64753/0.79466. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.66050/0.80221. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.64502/0.80714. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.63822/0.81333. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.63836/0.81761. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.61867/0.83663. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62766/0.84144. Took 0.47 sec\n",
      "Epoch 28, Loss(train/val) 0.62445/0.83188. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.62440/0.84405. Took 0.47 sec\n",
      "Epoch 30, Loss(train/val) 0.62031/0.85532. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.61268/0.87883. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.60812/0.87100. Took 0.47 sec\n",
      "Epoch 33, Loss(train/val) 0.60258/0.91077. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.60447/0.92094. Took 0.47 sec\n",
      "Epoch 35, Loss(train/val) 0.59496/0.94130. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.58401/0.98196. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.59119/0.96783. Took 0.48 sec\n",
      "Epoch 38, Loss(train/val) 0.58789/0.93645. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.57951/0.99584. Took 0.47 sec\n",
      "Epoch 40, Loss(train/val) 0.57030/0.99273. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.56538/1.02500. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.57641/0.99311. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.56743/1.01685. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.56357/1.02433. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.55738/1.03262. Took 0.47 sec\n",
      "Epoch 46, Loss(train/val) 0.54374/1.04513. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.54837/1.05277. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.55052/1.07935. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.53098/1.13822. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.53788/1.05437. Took 0.47 sec\n",
      "Epoch 51, Loss(train/val) 0.52391/1.02525. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.51702/1.07616. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.54071/1.04854. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.52744/1.02570. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.52727/1.01599. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.52824/1.03625. Took 0.47 sec\n",
      "Epoch 57, Loss(train/val) 0.49873/1.01703. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.51413/1.03726. Took 0.47 sec\n",
      "Epoch 59, Loss(train/val) 0.50381/1.06488. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.48915/1.08731. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.49224/1.11102. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.47316/1.16846. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.49539/1.08174. Took 0.47 sec\n",
      "Epoch 64, Loss(train/val) 0.48371/1.09289. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.47937/1.09525. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.46264/1.10403. Took 0.48 sec\n",
      "Epoch 67, Loss(train/val) 0.45647/1.11666. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.46277/1.09535. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.45599/1.14450. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.43279/1.17136. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.46792/1.07741. Took 0.47 sec\n",
      "Epoch 72, Loss(train/val) 0.45301/1.13015. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.44290/1.20003. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.43095/1.08589. Took 0.47 sec\n",
      "Epoch 75, Loss(train/val) 0.41909/1.15219. Took 0.47 sec\n",
      "Epoch 76, Loss(train/val) 0.42310/1.11948. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.40090/1.19630. Took 0.47 sec\n",
      "Epoch 78, Loss(train/val) 0.41849/1.16189. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.40644/1.10992. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.39191/1.16772. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.39687/1.11689. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.39554/1.10674. Took 0.47 sec\n",
      "Epoch 83, Loss(train/val) 0.40142/1.14311. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.39875/1.14017. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.37426/1.18921. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.35537/1.26785. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.35592/1.19950. Took 0.48 sec\n",
      "Epoch 88, Loss(train/val) 0.36389/1.23811. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.36797/1.17472. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.34546/1.29016. Took 0.47 sec\n",
      "Epoch 91, Loss(train/val) 0.34172/1.18384. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.32803/1.21361. Took 0.47 sec\n",
      "Epoch 93, Loss(train/val) 0.31249/1.38364. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.33026/1.24899. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.36012/1.29792. Took 0.47 sec\n",
      "Epoch 96, Loss(train/val) 0.31162/1.28061. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.31308/1.22223. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.31890/1.20603. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.29963/1.22885. Took 0.46 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.70630/0.69127. Took 0.51 sec\n",
      "Epoch 1, Loss(train/val) 0.69909/0.70986. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69188/0.72126. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68912/0.73459. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.68776/0.71768. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.68871/0.71354. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68363/0.72749. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.68968/0.71511. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.68726/0.72746. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67368/0.72887. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.67926/0.73364. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.68142/0.73572. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.67924/0.73609. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.68295/0.73229. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67172/0.73180. Took 0.47 sec\n",
      "Epoch 15, Loss(train/val) 0.67460/0.75220. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.67392/0.74715. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.66885/0.73298. Took 0.47 sec\n",
      "Epoch 18, Loss(train/val) 0.66901/0.71570. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.66765/0.72253. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.66607/0.72369. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.66659/0.73347. Took 0.47 sec\n",
      "Epoch 22, Loss(train/val) 0.65903/0.74209. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.66027/0.74137. Took 0.47 sec\n",
      "Epoch 24, Loss(train/val) 0.65743/0.75047. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65640/0.76039. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.66004/0.75934. Took 0.48 sec\n",
      "Epoch 27, Loss(train/val) 0.64977/0.78372. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.65098/0.78117. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.64943/0.79558. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.64612/0.79083. Took 0.47 sec\n",
      "Epoch 31, Loss(train/val) 0.63731/0.80308. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63955/0.83262. Took 0.48 sec\n",
      "Epoch 33, Loss(train/val) 0.63992/0.83313. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.63126/0.85112. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.62583/0.86502. Took 0.48 sec\n",
      "Epoch 36, Loss(train/val) 0.62174/0.87028. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.62050/0.88339. Took 0.47 sec\n",
      "Epoch 38, Loss(train/val) 0.62409/0.87744. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.61322/0.88852. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.61088/0.88721. Took 0.47 sec\n",
      "Epoch 41, Loss(train/val) 0.61721/0.85729. Took 0.47 sec\n",
      "Epoch 42, Loss(train/val) 0.62900/0.88390. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.61095/0.91084. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.60976/0.93069. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.61195/0.89590. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.60434/0.92639. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.60688/0.92268. Took 0.47 sec\n",
      "Epoch 48, Loss(train/val) 0.60287/0.92738. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.59494/0.96375. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.59438/0.98536. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.59717/0.98371. Took 0.47 sec\n",
      "Epoch 52, Loss(train/val) 0.59274/0.98989. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.58960/1.00186. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.59217/0.98486. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.59291/0.99613. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.58556/0.98931. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.57957/0.97444. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.58551/0.95664. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.57636/0.95311. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.57557/0.95714. Took 0.47 sec\n",
      "Epoch 61, Loss(train/val) 0.57905/0.97188. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.57433/0.97826. Took 0.47 sec\n",
      "Epoch 63, Loss(train/val) 0.57215/0.94373. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.57246/0.95490. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.56512/0.96471. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.55437/0.99397. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.55780/1.01734. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.55070/1.01935. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.54396/1.00996. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.55208/1.00611. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.53819/1.05844. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.55522/1.02650. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.52045/1.03871. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.53327/1.08064. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.53648/1.07235. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.53040/1.11771. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.53254/1.12570. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.53723/1.09672. Took 0.47 sec\n",
      "Epoch 79, Loss(train/val) 0.50778/1.13932. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.51787/1.16024. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.50065/1.15367. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.50202/1.23790. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.50193/1.13842. Took 0.47 sec\n",
      "Epoch 84, Loss(train/val) 0.49106/1.15793. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.49616/1.17953. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.48777/1.21242. Took 0.47 sec\n",
      "Epoch 87, Loss(train/val) 0.46851/1.19506. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.48389/1.20546. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.48015/1.20609. Took 0.47 sec\n",
      "Epoch 90, Loss(train/val) 0.47291/1.24253. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.46487/1.22854. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.45604/1.22667. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.46542/1.24890. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.45898/1.30582. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.45544/1.31100. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.43114/1.39317. Took 0.47 sec\n",
      "Epoch 97, Loss(train/val) 0.43974/1.37127. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.44352/1.38285. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.41926/1.42269. Took 0.46 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.71891/0.71108. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.70132/0.71248. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.70183/0.72324. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69791/0.71302. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69171/0.70601. Took 0.48 sec\n",
      "Epoch 5, Loss(train/val) 0.69051/0.70959. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.69042/0.70656. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68521/0.71630. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68820/0.72015. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.68345/0.72905. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68265/0.72560. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.67469/0.74180. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.68080/0.74154. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67267/0.73406. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66787/0.73910. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.66276/0.75408. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66496/0.75764. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.66411/0.74881. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.66170/0.76828. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65529/0.76649. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65787/0.78851. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65691/0.79150. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64625/0.77919. Took 0.48 sec\n",
      "Epoch 23, Loss(train/val) 0.64271/0.80071. Took 0.47 sec\n",
      "Epoch 24, Loss(train/val) 0.64871/0.80015. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64070/0.80748. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.64577/0.82177. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64039/0.79062. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.63598/0.81376. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62814/0.81594. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63062/0.82483. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.62247/0.82413. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.62503/0.85216. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.62789/0.83114. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.61336/0.83149. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.61406/0.87262. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.60968/0.84198. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60948/0.86288. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.60912/0.84378. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.60387/0.86131. Took 0.47 sec\n",
      "Epoch 40, Loss(train/val) 0.59541/0.87736. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60787/0.85284. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.59991/0.84383. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.60338/0.85378. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.59664/0.84318. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.58327/0.87693. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.58606/0.87925. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.59319/0.86789. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.58621/0.87470. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.57365/0.88933. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.58717/0.89150. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.57487/0.91450. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.57446/0.88506. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.56249/0.92408. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.56669/0.90953. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.56375/0.92198. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.55224/0.91905. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.53245/0.95507. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.55452/0.93725. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.55044/0.93219. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.52749/0.93762. Took 0.47 sec\n",
      "Epoch 61, Loss(train/val) 0.52112/0.99522. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.51757/0.96552. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.52716/0.97757. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.51723/0.98039. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.51132/0.98372. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.50255/0.99873. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.50401/1.01165. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.49663/0.99992. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.49905/1.02521. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.50221/1.04409. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.49220/1.00341. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.49413/1.05007. Took 0.47 sec\n",
      "Epoch 73, Loss(train/val) 0.49024/1.00375. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.47776/1.08261. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.46446/1.05944. Took 0.47 sec\n",
      "Epoch 76, Loss(train/val) 0.45583/1.07168. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.46417/1.12807. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.45231/1.04597. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.45103/1.10077. Took 0.49 sec\n",
      "Epoch 80, Loss(train/val) 0.44604/1.15500. Took 0.50 sec\n",
      "Epoch 81, Loss(train/val) 0.44004/1.12491. Took 0.47 sec\n",
      "Epoch 82, Loss(train/val) 0.43741/1.07518. Took 0.49 sec\n",
      "Epoch 83, Loss(train/val) 0.43758/1.14627. Took 0.48 sec\n",
      "Epoch 84, Loss(train/val) 0.44433/1.16440. Took 0.47 sec\n",
      "Epoch 85, Loss(train/val) 0.40992/1.26111. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.41475/1.23361. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.41123/1.24737. Took 0.49 sec\n",
      "Epoch 88, Loss(train/val) 0.39746/1.26153. Took 0.49 sec\n",
      "Epoch 89, Loss(train/val) 0.41500/1.30307. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.41539/1.22768. Took 0.47 sec\n",
      "Epoch 91, Loss(train/val) 0.39983/1.20666. Took 0.47 sec\n",
      "Epoch 92, Loss(train/val) 0.39671/1.28452. Took 0.48 sec\n",
      "Epoch 93, Loss(train/val) 0.39975/1.24791. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.40182/1.34645. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.41410/1.30974. Took 0.47 sec\n",
      "Epoch 96, Loss(train/val) 0.40313/1.30352. Took 0.47 sec\n",
      "Epoch 97, Loss(train/val) 0.38888/1.31441. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.40206/1.27476. Took 0.47 sec\n",
      "Epoch 99, Loss(train/val) 0.36663/1.38924. Took 0.45 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70693/0.69221. Took 0.51 sec\n",
      "Epoch 1, Loss(train/val) 0.69760/0.68482. Took 0.52 sec\n",
      "Epoch 2, Loss(train/val) 0.69326/0.68298. Took 0.50 sec\n",
      "Epoch 3, Loss(train/val) 0.69453/0.68309. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.69592/0.67708. Took 0.49 sec\n",
      "Epoch 5, Loss(train/val) 0.68834/0.69026. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.69241/0.69853. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.68282/0.70272. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.68139/0.71233. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.67645/0.71823. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.67931/0.71350. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.67195/0.72078. Took 0.49 sec\n",
      "Epoch 12, Loss(train/val) 0.67095/0.72124. Took 0.48 sec\n",
      "Epoch 13, Loss(train/val) 0.66238/0.72071. Took 0.48 sec\n",
      "Epoch 14, Loss(train/val) 0.66252/0.72355. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.67013/0.71016. Took 0.47 sec\n",
      "Epoch 16, Loss(train/val) 0.65734/0.71806. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65247/0.72792. Took 0.50 sec\n",
      "Epoch 18, Loss(train/val) 0.65200/0.72393. Took 0.49 sec\n",
      "Epoch 19, Loss(train/val) 0.64669/0.74357. Took 0.47 sec\n",
      "Epoch 20, Loss(train/val) 0.63778/0.76529. Took 0.47 sec\n",
      "Epoch 21, Loss(train/val) 0.64352/0.76996. Took 0.48 sec\n",
      "Epoch 22, Loss(train/val) 0.63300/0.78105. Took 0.47 sec\n",
      "Epoch 23, Loss(train/val) 0.63736/0.77802. Took 0.48 sec\n",
      "Epoch 24, Loss(train/val) 0.63301/0.78777. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.62792/0.79510. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.62110/0.79065. Took 0.48 sec\n",
      "Epoch 27, Loss(train/val) 0.61941/0.81349. Took 0.48 sec\n",
      "Epoch 28, Loss(train/val) 0.62568/0.81950. Took 0.48 sec\n",
      "Epoch 29, Loss(train/val) 0.61706/0.81062. Took 0.47 sec\n",
      "Epoch 30, Loss(train/val) 0.61233/0.80028. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.61194/0.83657. Took 0.48 sec\n",
      "Epoch 32, Loss(train/val) 0.60734/0.80922. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.59663/0.84528. Took 0.47 sec\n",
      "Epoch 34, Loss(train/val) 0.61356/0.85521. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.58772/0.87582. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.59147/0.87901. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.58542/0.86373. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.58561/0.86190. Took 0.53 sec\n",
      "Epoch 39, Loss(train/val) 0.57392/0.90715. Took 0.47 sec\n",
      "Epoch 40, Loss(train/val) 0.58792/0.89858. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58569/0.92619. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.57237/0.92702. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.56146/0.94923. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55200/0.96748. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.55992/0.95226. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.55341/0.95863. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.53473/0.98912. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.53070/1.03176. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52853/1.05370. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.54437/1.04803. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53704/0.99540. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.52609/1.03904. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.52396/1.02692. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51587/1.03244. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.52586/1.01138. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.50230/1.03865. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.52765/1.07761. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50191/1.07798. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52032/1.03251. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.49849/1.03910. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.48287/1.08719. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48464/1.09606. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48608/1.08248. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.47819/1.11286. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.47342/1.10697. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.46641/1.14894. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.47639/1.11079. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.44890/1.13662. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.45665/1.12687. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.45666/1.15569. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.44469/1.14832. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46069/1.16501. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.43264/1.12034. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.43605/1.11654. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.41104/1.17502. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.42653/1.13457. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.44775/1.19980. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.42206/1.22243. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.42171/1.19121. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.41505/1.19754. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.39187/1.24207. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.41740/1.20681. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.39548/1.28083. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.39640/1.28038. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.38100/1.23835. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.38034/1.21771. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.38059/1.29038. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.37080/1.26113. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.36670/1.40712. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36658/1.34952. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.36446/1.34042. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.37041/1.35503. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.36489/1.29554. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.36774/1.32516. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.34595/1.37415. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.34628/1.44480. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.33800/1.44433. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.33227/1.49600. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34820/1.38530. Took 0.44 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70623/0.71180. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.70196/0.68515. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69976/0.68021. Took 0.49 sec\n",
      "Epoch 3, Loss(train/val) 0.69183/0.69210. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.68974/0.68532. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69450/0.68884. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68941/0.68121. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68760/0.68971. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68382/0.71426. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68493/0.70996. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68438/0.70978. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.68040/0.73081. Took 0.47 sec\n",
      "Epoch 12, Loss(train/val) 0.67248/0.74539. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67325/0.74120. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66701/0.77514. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66321/0.78437. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66420/0.79809. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66932/0.80985. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66104/0.81977. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66009/0.81576. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.65799/0.83910. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65186/0.85058. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.65764/0.86453. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.64128/0.86600. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64694/0.87648. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64184/0.88453. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63975/0.88660. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.63292/0.92291. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63575/0.94097. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63170/0.95319. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.62585/0.97668. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61978/0.96779. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.61103/0.97883. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62064/1.03210. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60698/1.05007. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.61639/1.02395. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60340/1.07401. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60672/1.05370. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.60327/1.05417. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.60402/1.06139. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60161/1.06781. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.60206/1.14528. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.60519/1.03112. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61102/1.04951. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.59707/1.05260. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.60132/1.02744. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.58518/1.04833. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57601/1.11344. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58752/1.12006. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.58168/1.10793. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56906/1.10749. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56694/1.12667. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.55823/1.14841. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.57288/1.12988. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.57564/1.07287. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.57812/1.09317. Took 0.47 sec\n",
      "Epoch 56, Loss(train/val) 0.55027/1.13513. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.56299/1.12034. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.54295/1.10593. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.53484/1.11355. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54518/1.13456. Took 0.47 sec\n",
      "Epoch 61, Loss(train/val) 0.54423/1.17128. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.52626/1.20692. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.53663/1.19412. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.51924/1.21228. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.51682/1.27279. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.51637/1.23387. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.52720/1.11998. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52040/1.22787. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.51427/1.22968. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.51694/1.18097. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.51270/1.17257. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.50227/1.32002. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.50790/1.23505. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.50699/1.19106. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.48890/1.29677. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.48513/1.29516. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.49719/1.21873. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.48168/1.30317. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.47581/1.37557. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.48678/1.17108. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.48980/1.23504. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.45736/1.26022. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.45169/1.39376. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.47585/1.40956. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.49434/1.22867. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.50129/1.18233. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.46956/1.21968. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.45469/1.24197. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.44693/1.28736. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.45041/1.25285. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.45545/1.26938. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.42682/1.34157. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.43603/1.44511. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.44075/1.28498. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.45502/1.31008. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.46523/1.39712. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.45764/1.27364. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.43375/1.33549. Took 0.47 sec\n",
      "Epoch 99, Loss(train/val) 0.41818/1.32676. Took 0.45 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69689/0.69865. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69880/0.69770. Took 0.49 sec\n",
      "Epoch 2, Loss(train/val) 0.69069/0.69812. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68908/0.70599. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68969/0.70302. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68941/0.70477. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69023/0.70443. Took 0.47 sec\n",
      "Epoch 7, Loss(train/val) 0.68792/0.70436. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68767/0.70609. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68709/0.71234. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68380/0.71157. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67943/0.71453. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.68226/0.72318. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.68175/0.72540. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67890/0.72019. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67630/0.72665. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.67532/0.71900. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.67033/0.72650. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67074/0.72971. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66458/0.74206. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.66889/0.72532. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66375/0.72903. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66403/0.72690. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66058/0.72193. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.65249/0.71412. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64647/0.72841. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64625/0.73901. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.64538/0.73706. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63316/0.77539. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62821/0.76598. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63650/0.75738. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.62674/0.76681. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61591/0.78565. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60496/0.83291. Took 0.47 sec\n",
      "Epoch 34, Loss(train/val) 0.61493/0.81857. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.60018/0.85064. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60856/0.82084. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59546/0.85813. Took 0.47 sec\n",
      "Epoch 38, Loss(train/val) 0.59830/0.85534. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58369/0.87155. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.57621/0.87211. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.59184/0.87932. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57338/0.89666. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.56609/0.89265. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56638/0.90484. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57680/0.93486. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.56277/0.89973. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57973/0.92972. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.56551/0.90046. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54644/0.95401. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54784/0.90524. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.54409/0.95812. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54347/0.98257. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.54032/0.93775. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.53211/0.94910. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.51040/0.97626. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.54234/0.92584. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52167/0.94119. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.51916/0.94271. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.52382/0.96440. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.53027/0.93592. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.51617/0.94796. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50784/0.97299. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.51565/0.95192. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49473/0.96687. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.49820/1.03980. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.49909/0.98328. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.49683/0.99928. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49201/1.00548. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47893/0.95643. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.47900/1.00814. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48486/1.02062. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.47292/1.05660. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.47373/1.02695. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.47022/1.03545. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.47146/1.03682. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.44001/1.07363. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.43364/1.05881. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.44215/1.08770. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.42358/1.07373. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.44149/1.07924. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.44563/1.12797. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.47744/1.12117. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.43239/1.02812. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.40841/1.18301. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.40162/1.22006. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.40226/1.16190. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.43150/1.16475. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.44633/1.07440. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.40207/1.08628. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.39699/1.23793. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.37720/1.21653. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.36963/1.22260. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.39632/1.26796. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.42450/1.13232. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.37894/1.19588. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.39569/1.22728. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.39235/1.24739. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.37848/1.18439. Took 0.47 sec\n",
      "Epoch 99, Loss(train/val) 0.36412/1.31839. Took 0.45 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.71319/0.72887. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.71050/0.70154. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69824/0.69676. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.70263/0.70655. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.69517/0.71229. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69130/0.71624. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.68453/0.73530. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68668/0.72885. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68286/0.72583. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68482/0.72469. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67834/0.72498. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67686/0.73780. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67988/0.73782. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67215/0.74502. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.66971/0.75407. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.67697/0.76228. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66366/0.77727. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.65765/0.79339. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67091/0.78813. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65176/0.78905. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65093/0.80353. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64474/0.81278. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.65360/0.77450. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65032/0.77256. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64326/0.79916. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.64063/0.78303. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64254/0.79868. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62918/0.82616. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63568/0.81658. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62821/0.84067. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.61425/0.81756. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60851/0.84499. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.61054/0.86712. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.60652/0.86931. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61153/0.89341. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.59812/0.86920. Took 0.47 sec\n",
      "Epoch 36, Loss(train/val) 0.59206/0.89925. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59284/0.92982. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.59143/0.88380. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58654/0.87753. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59293/0.88866. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.58856/0.91814. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58960/0.90890. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57788/0.87498. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.56270/0.91767. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.56161/0.94220. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57175/0.93557. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55583/0.91843. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.56642/0.93289. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.55809/0.94208. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.55679/0.96377. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55592/1.00695. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.55237/1.02498. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.52863/1.02874. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.53318/0.99866. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.53955/1.00021. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.53221/0.98919. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52856/1.03006. Took 0.47 sec\n",
      "Epoch 58, Loss(train/val) 0.50677/1.01516. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.51581/1.02738. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52781/0.99366. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.50670/1.04176. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50515/1.01273. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.52014/0.99496. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.50770/0.98109. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.49523/1.05976. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.49800/1.00344. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48815/1.02293. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.48272/1.02487. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.47929/1.16144. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.50054/1.02928. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.47660/1.13373. Took 0.47 sec\n",
      "Epoch 72, Loss(train/val) 0.48606/1.13251. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.47261/1.09009. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.47192/1.05938. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.45862/1.06424. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.44563/1.08896. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.45128/1.19038. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.44099/1.13589. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44943/1.17001. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.43833/1.18740. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.43110/1.23229. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.42492/1.23179. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.42892/1.19767. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43337/1.18131. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.42800/1.14539. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.40842/1.21876. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.40582/1.22406. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.39110/1.21124. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.39270/1.21906. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.38646/1.23091. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.40328/1.17106. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.37662/1.27797. Took 0.47 sec\n",
      "Epoch 93, Loss(train/val) 0.37371/1.31346. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.37622/1.28917. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.35551/1.37071. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.38892/1.31103. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.35717/1.45211. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.35097/1.44932. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.35169/1.45527. Took 0.44 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.73256/0.69554. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.70906/0.69906. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.71093/0.70032. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.70997/0.70438. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.70688/0.70572. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.70846/0.70886. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.70784/0.71084. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.69385/0.71488. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.69214/0.71335. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.70496/0.70686. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.69296/0.71507. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.68946/0.72276. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68408/0.72018. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.68528/0.72235. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68563/0.73172. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67854/0.73154. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.68312/0.73291. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67256/0.72807. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.66915/0.72727. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66122/0.71423. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.67076/0.72080. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.66551/0.72639. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66130/0.70220. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.66185/0.73297. Took 0.43 sec\n",
      "Epoch 24, Loss(train/val) 0.65672/0.70831. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.65837/0.70801. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65484/0.70391. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.65693/0.68535. Took 0.48 sec\n",
      "Epoch 28, Loss(train/val) 0.65060/0.68601. Took 0.47 sec\n",
      "Epoch 29, Loss(train/val) 0.64327/0.68276. Took 0.48 sec\n",
      "Epoch 30, Loss(train/val) 0.64867/0.68128. Took 0.49 sec\n",
      "Epoch 31, Loss(train/val) 0.64252/0.68048. Took 0.48 sec\n",
      "Epoch 32, Loss(train/val) 0.63757/0.67236. Took 0.48 sec\n",
      "Epoch 33, Loss(train/val) 0.63449/0.67696. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.63788/0.67244. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.62609/0.67756. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.63668/0.66369. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.62742/0.69387. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.63347/0.69078. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.61643/0.69190. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61973/0.68658. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.62457/0.68096. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.61571/0.66891. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.61713/0.67954. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60611/0.66561. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.62111/0.66591. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.60504/0.65662. Took 0.47 sec\n",
      "Epoch 47, Loss(train/val) 0.60146/0.66083. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.59862/0.66993. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.60312/0.68712. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.60487/0.67969. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.58772/0.69715. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.58804/0.69376. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.58445/0.72063. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.57975/0.73625. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.58923/0.71231. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.58658/0.70614. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.58762/0.70778. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.56374/0.73004. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.56822/0.74951. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.57010/0.76842. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.55453/0.77266. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.55470/0.79182. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.55580/0.78257. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.55132/0.76745. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.55635/0.80642. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.53973/0.81361. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.54968/0.87120. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52149/0.83008. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.52265/0.85879. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.52766/0.87535. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.51245/0.86834. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.51370/0.92163. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.49762/0.93524. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.51248/0.89390. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.51175/0.92467. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.50210/0.99946. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.49894/1.00520. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.50708/0.95538. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.49318/0.93604. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.48179/1.01079. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.51148/0.89446. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.47636/0.99275. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.47883/0.97787. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.46406/1.05272. Took 0.47 sec\n",
      "Epoch 85, Loss(train/val) 0.45387/1.08708. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.46207/1.07689. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.46200/1.05802. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.46286/1.06583. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.44256/1.17290. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.44256/1.11183. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.44839/1.11932. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.44180/1.04549. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.41842/1.16206. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.43744/1.10213. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.41518/1.18622. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.44069/1.07781. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.41092/1.11842. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.40463/1.29456. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.40867/1.15132. Took 0.44 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.70444/0.69904. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.70086/0.69803. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69736/0.69232. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69485/0.69110. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69016/0.68859. Took 0.48 sec\n",
      "Epoch 5, Loss(train/val) 0.68820/0.68481. Took 0.49 sec\n",
      "Epoch 6, Loss(train/val) 0.68323/0.69299. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.67952/0.70086. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67502/0.69311. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67446/0.69063. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.66697/0.69225. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66211/0.70224. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.65852/0.70275. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.65056/0.69537. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.64829/0.70079. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.64986/0.70691. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.64009/0.70862. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64011/0.70676. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.63559/0.70359. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.62429/0.70824. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.62152/0.70242. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.61893/0.71039. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.61164/0.71085. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.62158/0.71927. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.60929/0.73834. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.60393/0.73773. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.60715/0.73389. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.59646/0.73221. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.58737/0.75705. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.58556/0.73537. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.57443/0.71731. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.57050/0.73854. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.56104/0.72443. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.55668/0.73249. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.55144/0.73539. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.54814/0.72399. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.54480/0.73221. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.54678/0.75603. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.54230/0.71332. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.53171/0.74066. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.52158/0.75863. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.54693/0.76091. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.52197/0.78569. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.52532/0.77746. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.50925/0.79121. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.52024/0.80833. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.48929/0.82451. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.49484/0.85440. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.49929/0.83902. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.47889/0.85729. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.47697/0.88220. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.47076/0.85071. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.46847/0.84577. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.45469/0.84802. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.45900/0.83502. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.46343/0.84802. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.46365/0.85945. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.42473/0.91349. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.43700/0.89347. Took 0.47 sec\n",
      "Epoch 59, Loss(train/val) 0.41704/0.90803. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.44291/0.89370. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.42575/0.90166. Took 0.48 sec\n",
      "Epoch 62, Loss(train/val) 0.41821/0.96215. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.42840/0.93552. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.40082/0.87423. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.41176/0.95019. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.40996/0.95945. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.38532/0.93811. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.38803/0.95239. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.40422/0.94201. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.37525/0.95296. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.36031/0.99574. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.36960/1.04810. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.37497/1.02721. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.36820/1.03499. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.36435/1.05078. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.36049/1.07945. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.34603/1.10228. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.33707/1.10083. Took 0.43 sec\n",
      "Epoch 79, Loss(train/val) 0.34129/1.06991. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.33015/1.13312. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.31757/1.10090. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.33335/1.12848. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.34073/1.12188. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.32559/1.15325. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.30711/1.12697. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.29544/1.13864. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.29845/1.10054. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.30258/1.12598. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.30959/1.24979. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.30194/1.18365. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.28909/1.19240. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.28015/1.19889. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.27442/1.15783. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.25427/1.16863. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.26858/1.17468. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.25219/1.23502. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.30005/1.27587. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.26843/1.22299. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.27791/1.51138. Took 0.46 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.70620/0.70043. Took 0.64 sec\n",
      "Epoch 1, Loss(train/val) 0.70277/0.68885. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69728/0.68760. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69768/0.68612. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.69794/0.69630. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.69656/0.70481. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68640/0.70821. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69351/0.69106. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.69665/0.68186. Took 0.48 sec\n",
      "Epoch 9, Loss(train/val) 0.69038/0.68624. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.69455/0.67582. Took 0.49 sec\n",
      "Epoch 11, Loss(train/val) 0.69362/0.68719. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68812/0.69080. Took 0.47 sec\n",
      "Epoch 13, Loss(train/val) 0.68471/0.69869. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.68677/0.70348. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.68951/0.71518. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.69086/0.72217. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.68247/0.74370. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.68175/0.71818. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.67587/0.72859. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.67861/0.73826. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.68152/0.73871. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.67268/0.73538. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.67051/0.74948. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.66913/0.75926. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66631/0.73521. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.66547/0.75911. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.66541/0.74430. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.66488/0.75746. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.65129/0.77151. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.65219/0.80177. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.64236/0.81271. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64015/0.80171. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.63712/0.81861. Took 0.47 sec\n",
      "Epoch 34, Loss(train/val) 0.63774/0.82413. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62542/0.82913. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.62370/0.89741. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.62586/0.88558. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61728/0.91387. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.61522/0.90896. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60866/0.90644. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.59306/0.96078. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.60234/0.94258. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58579/0.95120. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.58973/0.95272. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.58665/0.93070. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57510/0.95144. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.57340/0.97629. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57330/1.01102. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57126/0.98991. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.55852/1.01556. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.56720/0.99796. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.57407/0.99562. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.55448/1.00320. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53849/1.00186. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.55044/1.01360. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.53615/1.03252. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.53744/1.02770. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.53685/1.00675. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.55132/0.99776. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.51723/1.03357. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.50217/1.06772. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50607/1.09934. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.51128/1.04061. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49591/1.10089. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.50240/1.08348. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.49805/1.03600. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.57494/0.92806. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52871/0.96393. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.49444/1.04383. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.50615/1.03116. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48341/1.05709. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.48692/1.01961. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46169/1.04487. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.45855/1.07445. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.47565/1.01918. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.45294/1.06197. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.45762/1.11671. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.45198/1.11605. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.44566/1.08242. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.43072/1.10417. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.42470/1.12691. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.47176/1.03384. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.44019/1.11129. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43468/1.03991. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.41773/1.15027. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.42574/1.16798. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43153/1.11120. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.39466/1.16410. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.42045/1.13292. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.38632/1.21486. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.39838/1.18676. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.39376/1.22267. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.38931/1.14426. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.40807/1.15344. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.40160/1.19781. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.37553/1.20791. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.35583/1.20725. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.36755/1.22126. Took 0.43 sec\n",
      "Epoch 99, Loss(train/val) 0.36105/1.19994. Took 0.46 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69455/0.70127. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69356/0.70764. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69107/0.71320. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.68630/0.72432. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68251/0.73144. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68719/0.71824. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67783/0.73431. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68458/0.72228. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67772/0.72852. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67835/0.73118. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.67478/0.72783. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67166/0.74108. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.67126/0.75429. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67469/0.73961. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66581/0.75923. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67006/0.73788. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66066/0.76159. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.65408/0.76137. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66016/0.75837. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65772/0.73006. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65611/0.73170. Took 0.43 sec\n",
      "Epoch 21, Loss(train/val) 0.64905/0.75591. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64053/0.74815. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.63694/0.76778. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64028/0.75679. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.63180/0.74899. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63464/0.73963. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63177/0.74086. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62225/0.73949. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62744/0.72293. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62787/0.74314. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62050/0.72502. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.61805/0.71108. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61257/0.73889. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.61484/0.71468. Took 0.47 sec\n",
      "Epoch 35, Loss(train/val) 0.60463/0.70116. Took 0.49 sec\n",
      "Epoch 36, Loss(train/val) 0.60055/0.69535. Took 0.48 sec\n",
      "Epoch 37, Loss(train/val) 0.60263/0.68788. Took 0.49 sec\n",
      "Epoch 38, Loss(train/val) 0.59308/0.69364. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57647/0.68369. Took 0.50 sec\n",
      "Epoch 40, Loss(train/val) 0.59071/0.69861. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.59025/0.69709. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58400/0.73268. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57808/0.69983. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.57712/0.70927. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56722/0.74299. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.55765/0.65863. Took 0.48 sec\n",
      "Epoch 47, Loss(train/val) 0.55765/0.69795. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56354/0.73463. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.56215/0.69597. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55919/0.67598. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.56518/0.70968. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56362/0.68557. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55174/0.68562. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.55195/0.67954. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55757/0.69006. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.54577/0.68531. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.54911/0.71047. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54225/0.71328. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.54579/0.68858. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54119/0.69285. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.53051/0.70103. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.52873/0.69397. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51830/0.71522. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.52373/0.71135. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.51983/0.68656. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.51667/0.69343. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.51953/0.68991. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50096/0.71617. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.51241/0.69346. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.48677/0.73698. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.49611/0.74305. Took 0.43 sec\n",
      "Epoch 72, Loss(train/val) 0.49818/0.68059. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.50337/0.71173. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.50699/0.72291. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48780/0.73724. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.48496/0.71172. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.47047/0.70302. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.46913/0.71790. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.48076/0.71434. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.47825/0.75648. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.46704/0.71234. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.47636/0.72891. Took 0.43 sec\n",
      "Epoch 83, Loss(train/val) 0.45899/0.76182. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.46789/0.76834. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.46109/0.73796. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.43659/0.74733. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.45846/0.81884. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.43638/0.76934. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.43941/0.81738. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.43604/0.79732. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.44647/0.84547. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.45474/0.85721. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.44957/0.85230. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.42442/0.82172. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.42273/0.87890. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.44471/0.77763. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.42589/0.81829. Took 0.47 sec\n",
      "Epoch 98, Loss(train/val) 0.41845/0.83802. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.40425/0.87023. Took 0.44 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.70386/0.70925. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69394/0.70751. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69456/0.71933. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.68992/0.71681. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68927/0.73627. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68627/0.74567. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.68620/0.74435. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67925/0.74435. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.67415/0.75083. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67677/0.76039. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67508/0.75066. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66764/0.75959. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66502/0.76101. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66398/0.75766. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66574/0.75509. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.65881/0.76493. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66134/0.75710. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65263/0.76009. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65244/0.76200. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65133/0.76266. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.64963/0.76720. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65164/0.77651. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64203/0.79882. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64261/0.80852. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.64211/0.83660. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63853/0.85408. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.63804/0.83656. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63082/0.85704. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.62098/0.88106. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62002/0.88611. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60928/0.89417. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.61070/0.91097. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61013/0.92462. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.60508/0.92979. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59298/0.95665. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60916/0.95426. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.59353/0.94436. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58350/0.99713. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58362/0.98869. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.58244/1.01179. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56943/1.03137. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.56755/1.04647. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56063/1.06598. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57167/1.03042. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.56883/1.06026. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.56075/1.06219. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.54489/1.10270. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.54844/1.17697. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.54644/1.14707. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.53200/1.21797. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.52519/1.20017. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53332/1.23274. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52432/1.22321. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.51615/1.28381. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51286/1.25973. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.50436/1.31638. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.49518/1.29484. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.49136/1.26585. Took 0.48 sec\n",
      "Epoch 58, Loss(train/val) 0.48759/1.25059. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.50110/1.33728. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.47869/1.33719. Took 0.47 sec\n",
      "Epoch 61, Loss(train/val) 0.49778/1.31528. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.46641/1.30597. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.48656/1.27930. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.48083/1.27184. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.46122/1.28046. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.46216/1.27089. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.46028/1.18390. Took 0.47 sec\n",
      "Epoch 68, Loss(train/val) 0.45403/1.32221. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45302/1.35655. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.43514/1.41904. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.43576/1.36092. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.43422/1.40754. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.42011/1.47857. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.40889/1.44806. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.43922/1.56123. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.42326/1.48081. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.40450/1.49292. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.39548/1.51492. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.39196/1.50501. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.39781/1.50703. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.40515/1.50628. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.39054/1.54012. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.38017/1.50734. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.37487/1.54514. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.37871/1.49932. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.38224/1.53831. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.37964/1.57507. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.40046/1.66608. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.39687/1.43465. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.38051/1.53325. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.37558/1.50383. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.36754/1.62656. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.38544/1.68185. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.35171/1.74271. Took 0.48 sec\n",
      "Epoch 95, Loss(train/val) 0.36493/1.69465. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.34737/1.57816. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.37536/1.61452. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.33196/1.59381. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.33242/1.70821. Took 0.47 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.70370/0.75188. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69441/0.75415. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69051/0.75113. Took 0.49 sec\n",
      "Epoch 3, Loss(train/val) 0.68748/0.75154. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68761/0.76028. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69070/0.76421. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68346/0.76331. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68009/0.76313. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68124/0.77913. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67272/0.77647. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67828/0.78339. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67908/0.77213. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66993/0.79927. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66742/0.78677. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66446/0.81908. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66567/0.80525. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66235/0.81344. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66978/0.82404. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.66096/0.80122. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.66224/0.79659. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65935/0.78526. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64796/0.80615. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64870/0.79577. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64863/0.77885. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64649/0.80626. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63971/0.82433. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.63814/0.82506. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.64496/0.83835. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62487/0.83365. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.63142/0.85620. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63654/0.86291. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63119/0.87590. Took 0.47 sec\n",
      "Epoch 32, Loss(train/val) 0.62640/0.85441. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62468/0.85314. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.62182/0.90076. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62252/0.89178. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61650/0.90718. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.62185/0.91788. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61400/0.90471. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.60729/0.94021. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.61557/0.89093. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.61107/0.91233. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.59271/0.92767. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.60690/0.90777. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.59272/0.94716. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.59299/0.95436. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.59557/0.94165. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.58246/0.97795. Took 0.43 sec\n",
      "Epoch 48, Loss(train/val) 0.58399/0.90880. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.59340/0.93136. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.58015/0.93581. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.58043/0.93194. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.57216/0.96198. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.57158/0.97926. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.57797/0.90610. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.55856/1.00347. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56610/0.97340. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.56582/1.00811. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.54973/1.05570. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.55787/1.05296. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54139/0.99940. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.53718/1.11486. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.53049/1.10589. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.52773/1.13738. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.54274/1.12039. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.51490/1.19058. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.53895/1.14029. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.52274/1.17770. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.51791/1.17475. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.51684/1.14167. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49322/1.17629. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.50119/1.25230. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.49199/1.24154. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48709/1.20889. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.51305/1.22623. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48906/1.25447. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46965/1.28183. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.48841/1.21267. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.49416/1.30748. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.49851/1.14694. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.46101/1.24948. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.47028/1.26546. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.44890/1.23733. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.45067/1.26918. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.44505/1.25321. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.43531/1.30092. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.44182/1.27797. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.43023/1.46982. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.43045/1.37805. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.42707/1.33551. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.43672/1.30842. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.41834/1.36761. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.41955/1.37527. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.39549/1.43533. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.40792/1.44558. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.40677/1.44447. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.40051/1.41453. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.37664/1.41678. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.38409/1.49594. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.39262/1.49180. Took 0.44 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.71092/0.71199. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70938/0.70136. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.70153/0.69191. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69684/0.69446. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.69136/0.69875. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.69236/0.71289. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69459/0.70416. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69071/0.71003. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69450/0.70782. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68489/0.71098. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.69201/0.70760. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.68254/0.71368. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.68300/0.71029. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.67927/0.71101. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.68067/0.73319. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67842/0.73697. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.68030/0.74177. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.67169/0.74523. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.66820/0.74714. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66104/0.73219. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65087/0.74095. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65584/0.75528. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65449/0.77852. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.65058/0.76487. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64415/0.78256. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63928/0.79366. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.63212/0.79608. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.62669/0.80279. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61684/0.81204. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62851/0.78484. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63097/0.78470. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.61702/0.78993. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.60367/0.79995. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60410/0.82410. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.60272/0.84202. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60773/0.82605. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.59134/0.81454. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.59136/0.83790. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.59251/0.83973. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.58351/0.84466. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58769/0.82580. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.57713/0.83916. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56009/0.87796. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.56821/0.88363. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.57266/0.88859. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.56837/0.90550. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.55478/0.91023. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55291/0.91741. Took 0.47 sec\n",
      "Epoch 48, Loss(train/val) 0.57265/0.93088. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.54436/0.95063. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54074/0.94159. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.54622/0.93453. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.53149/0.96560. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.53708/0.97169. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52779/1.01301. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.53002/1.01057. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.53066/1.01549. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51366/1.02899. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50458/1.08748. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.51287/1.08096. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49967/1.14135. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.48937/1.16251. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51023/1.14529. Took 0.47 sec\n",
      "Epoch 63, Loss(train/val) 0.48084/1.19923. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49707/1.11599. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.48726/1.23727. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.49031/1.23317. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.49028/1.21208. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.46806/1.23326. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46617/1.26721. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.46309/1.28540. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.47010/1.28387. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.45512/1.30768. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.44821/1.32380. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.47598/1.33834. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.46435/1.32358. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46539/1.27474. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.45010/1.31494. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43167/1.33312. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.43281/1.40843. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.41872/1.38920. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.42482/1.39865. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.43944/1.43418. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.44071/1.39887. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.41236/1.45696. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.42816/1.38495. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.42254/1.45306. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.39407/1.44910. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.40531/1.47768. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.39671/1.47231. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.39367/1.49017. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.40560/1.49392. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.39326/1.43813. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.40025/1.46165. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.40147/1.51750. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.40714/1.42809. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.41885/1.42455. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.38250/1.43758. Took 0.47 sec\n",
      "Epoch 98, Loss(train/val) 0.37422/1.48911. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.35854/1.54649. Took 0.45 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.70875/0.70693. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69549/0.71092. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69713/0.70403. Took 0.49 sec\n",
      "Epoch 3, Loss(train/val) 0.69701/0.69507. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.69424/0.69504. Took 0.49 sec\n",
      "Epoch 5, Loss(train/val) 0.69402/0.70229. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.69138/0.70466. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68953/0.70718. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68981/0.69453. Took 0.49 sec\n",
      "Epoch 9, Loss(train/val) 0.68530/0.70193. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68517/0.70762. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68840/0.69860. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.68404/0.69152. Took 0.49 sec\n",
      "Epoch 13, Loss(train/val) 0.68717/0.68422. Took 0.48 sec\n",
      "Epoch 14, Loss(train/val) 0.67662/0.68398. Took 0.48 sec\n",
      "Epoch 15, Loss(train/val) 0.68268/0.70039. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.67594/0.70976. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.67644/0.70141. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.67077/0.71198. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.67047/0.72599. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.66313/0.73526. Took 0.47 sec\n",
      "Epoch 21, Loss(train/val) 0.66151/0.73354. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65959/0.71985. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.65624/0.73884. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65519/0.78517. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65354/0.73250. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.64530/0.74841. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.64614/0.81333. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63853/0.77690. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.63882/0.79822. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63365/0.80461. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63263/0.82255. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62635/0.89658. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62847/0.85125. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.61986/0.88015. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61845/0.88538. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60830/0.89612. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60598/0.89653. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.61021/0.94462. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.61541/0.89961. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.60192/0.89967. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59365/0.89498. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.58993/0.88454. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59301/0.90298. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.58844/0.87338. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.59232/0.98373. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.57742/0.94719. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.58171/0.88958. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58989/0.88587. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.56885/0.90051. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.57593/0.88112. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55918/0.93072. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.55137/0.93910. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.56077/0.96513. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.55020/0.93418. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55613/0.97623. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.54473/1.01082. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.55193/0.98998. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.54135/1.03271. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.51270/1.06387. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.53259/0.98892. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.51452/1.06399. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.51430/1.11295. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.52202/1.01696. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50301/1.02037. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.49275/1.10401. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.50657/1.07289. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.49383/1.03477. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.48194/1.15995. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.48584/1.14082. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.46889/1.07714. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.47309/1.11813. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46825/1.09114. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.46910/1.05087. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.46033/1.13729. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.44961/1.10548. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.44402/1.13128. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.44001/1.08468. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.42843/1.16254. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44385/1.12922. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.44660/1.10851. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.43783/1.14662. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.43907/1.14617. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40916/1.14430. Took 0.47 sec\n",
      "Epoch 84, Loss(train/val) 0.40692/1.19343. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.42012/1.17548. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.40330/1.18925. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.39946/1.22847. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.40183/1.25122. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.38923/1.23600. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.37434/1.21688. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.37073/1.24779. Took 0.47 sec\n",
      "Epoch 92, Loss(train/val) 0.35124/1.30551. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.36389/1.30540. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.34853/1.35456. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.33721/1.41094. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.34174/1.31060. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.33756/1.35553. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.33549/1.43330. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.31849/1.39988. Took 0.46 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.70033/0.69328. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69664/0.68895. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69276/0.68921. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68779/0.68674. Took 0.49 sec\n",
      "Epoch 4, Loss(train/val) 0.68458/0.68457. Took 0.48 sec\n",
      "Epoch 5, Loss(train/val) 0.68167/0.69112. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68091/0.68594. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67990/0.69741. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67478/0.70309. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67388/0.70312. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67194/0.71383. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.66910/0.71595. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.66706/0.72643. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66721/0.72410. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66474/0.72576. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.65816/0.73892. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65333/0.73437. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65393/0.75738. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.64976/0.77021. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65423/0.76740. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64518/0.76662. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.64414/0.76339. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63687/0.77326. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.63887/0.76125. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.62981/0.76399. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62853/0.75655. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.62665/0.75102. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.61437/0.76667. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.60869/0.77638. Took 0.47 sec\n",
      "Epoch 29, Loss(train/val) 0.60887/0.76993. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.59622/0.78922. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.59594/0.78904. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59425/0.79397. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.58672/0.79324. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.57221/0.79077. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.56668/0.82646. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.57675/0.82604. Took 0.47 sec\n",
      "Epoch 37, Loss(train/val) 0.55573/0.83120. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.56443/0.84130. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.55836/0.84860. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.55392/0.85638. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.54487/0.84498. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.55451/0.84551. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.53028/0.86159. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.53062/0.85895. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.52240/0.84036. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.52246/0.84942. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.51590/0.87252. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.52561/0.82275. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.51091/0.86464. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.49673/0.87635. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.49291/0.89835. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.49143/0.86111. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.48849/0.88827. Took 0.43 sec\n",
      "Epoch 54, Loss(train/val) 0.48309/0.87841. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.46219/0.92848. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.47630/0.99269. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.47314/0.95899. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.44987/0.95623. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.44979/0.97004. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.44852/0.99660. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.44444/0.95771. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.44306/0.98872. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.42203/0.98061. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.43758/0.98178. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.42163/0.99820. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.39445/1.04345. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.38850/1.05251. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.41334/1.08326. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.38465/1.10852. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.38866/1.09338. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.40228/1.08509. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.38798/1.10561. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.37026/1.14225. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.36961/1.20850. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.35613/1.18811. Took 0.47 sec\n",
      "Epoch 76, Loss(train/val) 0.34061/1.16798. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.35907/1.18238. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.35179/1.23746. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.34248/1.20439. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.35109/1.29597. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.34566/1.31259. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.32641/1.21515. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.32632/1.32517. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.31064/1.27700. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.31467/1.36547. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.31530/1.23718. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.31566/1.31627. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.28795/1.34654. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.28434/1.31956. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.29010/1.32734. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.27556/1.37961. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.27447/1.41122. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.25478/1.48490. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.25423/1.43417. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.26155/1.48396. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.27297/1.44781. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.26276/1.50898. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.25423/1.50417. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.25895/1.51704. Took 0.44 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.71234/0.67795. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.71866/0.69190. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.71028/0.69551. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70839/0.69120. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.70780/0.69811. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.70882/0.69014. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69618/0.70011. Took 0.47 sec\n",
      "Epoch 7, Loss(train/val) 0.69509/0.69994. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.69217/0.68255. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68369/0.68433. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68441/0.68709. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.68566/0.68656. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68217/0.68368. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67894/0.69941. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.68698/0.70943. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67806/0.69413. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67716/0.71634. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.67357/0.71706. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.68123/0.73108. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67709/0.70564. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.67448/0.72754. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.67481/0.70723. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.67719/0.70665. Took 0.47 sec\n",
      "Epoch 23, Loss(train/val) 0.66606/0.71448. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.67006/0.72296. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66921/0.73270. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.65901/0.72328. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65513/0.73048. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.66285/0.73755. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.65785/0.74145. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.64961/0.73943. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64689/0.73599. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.64665/0.74856. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.64552/0.75132. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.64577/0.74541. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.64131/0.75569. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.64005/0.75517. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.64597/0.76078. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.64085/0.76262. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.63319/0.74217. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.62957/0.74727. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.63089/0.79025. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.62245/0.77189. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.63038/0.75807. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.62555/0.76357. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.62829/0.76839. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.61414/0.78364. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.61633/0.79457. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.62267/0.79092. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.60943/0.74886. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.60965/0.77122. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.60674/0.74582. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.62233/0.80279. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.60200/0.75766. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.59910/0.77818. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.59147/0.78639. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.60260/0.78013. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.60155/0.78291. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.58965/0.77312. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.58581/0.81041. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.58367/0.78244. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.57150/0.85547. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.58351/0.79805. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.55440/0.80796. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.55910/0.84086. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.57917/0.84751. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.56134/0.83760. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.55310/0.82423. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.55600/0.83947. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.53048/0.90826. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.54081/0.91102. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.53637/0.87423. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.53113/0.91873. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.53672/0.89785. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.52375/0.90418. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.52998/0.92467. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.53159/0.93542. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.51011/0.97876. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.50215/0.98465. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.51610/1.01812. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.50861/1.00392. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.49917/1.03303. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.48390/1.05971. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.49324/1.06274. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.48232/1.10161. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.49202/1.08472. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.46351/1.11482. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.45872/1.10331. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.46080/1.10088. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.46380/1.10668. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.47075/1.11502. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.48696/1.03709. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.46989/1.02030. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.43864/1.11060. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.45783/1.08149. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.43229/1.07383. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.43916/1.13422. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.49246/0.98390. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.44476/1.09853. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.41693/1.10792. Took 0.46 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69544/0.69929. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69368/0.69673. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69029/0.69872. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69019/0.69868. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68894/0.69643. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.68658/0.69451. Took 0.49 sec\n",
      "Epoch 6, Loss(train/val) 0.68633/0.69185. Took 0.49 sec\n",
      "Epoch 7, Loss(train/val) 0.68350/0.69259. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68238/0.69469. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.68105/0.70207. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67747/0.69751. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.67424/0.69773. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67484/0.69957. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66897/0.70750. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66692/0.71504. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.66366/0.72072. Took 0.43 sec\n",
      "Epoch 16, Loss(train/val) 0.65973/0.72558. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65766/0.72813. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65508/0.74756. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65015/0.75113. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65246/0.74186. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.64071/0.77793. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63325/0.80863. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.62737/0.85271. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.62063/0.85574. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.61459/0.84637. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.60586/0.88772. Took 0.47 sec\n",
      "Epoch 27, Loss(train/val) 0.60071/0.89649. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.59513/0.91280. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.58415/0.92882. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.58683/0.92516. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.58212/0.95044. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.56816/0.90975. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.56849/0.93267. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.54824/0.99441. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.55690/1.02079. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.53573/1.04234. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.54200/1.00010. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.53109/0.99792. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.54209/1.00895. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.53179/0.96863. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.53040/1.01316. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.51910/1.00263. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.50765/1.09568. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.49833/1.10099. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.51126/1.11310. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.51135/1.01611. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.50514/1.06796. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.48719/1.09720. Took 0.43 sec\n",
      "Epoch 49, Loss(train/val) 0.48920/1.13177. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.47536/1.12633. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.47245/1.10005. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.46120/1.13555. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.48138/1.18532. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.45010/1.20758. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.46239/1.20344. Took 0.43 sec\n",
      "Epoch 56, Loss(train/val) 0.46525/1.20318. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.44952/1.15433. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.43426/1.17096. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.44897/1.09495. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.47081/1.16432. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.44588/1.20893. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.42948/1.18305. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.44460/1.17837. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.44393/1.12208. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.41308/1.16492. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.39809/1.25088. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.39461/1.28575. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.38138/1.24778. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.40528/1.27661. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.39607/1.30378. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.39801/1.27492. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.39484/1.27747. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.36414/1.25882. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.38835/1.25025. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.37941/1.34699. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.38578/1.31800. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.37063/1.31212. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.33355/1.35873. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.36871/1.43151. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.35903/1.34575. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.32982/1.36942. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.33634/1.45541. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.32956/1.37555. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.38151/1.31232. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.33534/1.40656. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.31800/1.28595. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.29986/1.38659. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.31335/1.42564. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.30319/1.41717. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.30187/1.47045. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.28752/1.40806. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.28234/1.53226. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.27686/1.50379. Took 0.47 sec\n",
      "Epoch 94, Loss(train/val) 0.26508/1.68137. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.28365/1.79583. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.37797/1.58865. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.38063/1.57060. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.30310/1.60340. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.26928/1.68913. Took 0.45 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.70191/0.68810. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.68877/0.69751. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69426/0.68957. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69070/0.69693. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68562/0.69372. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.68358/0.69541. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67790/0.72388. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67215/0.73381. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67238/0.72543. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67359/0.71715. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67365/0.72243. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67090/0.75594. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67068/0.75872. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.66382/0.77803. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66434/0.77153. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66734/0.78188. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65677/0.77645. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66151/0.80275. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65953/0.80670. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65731/0.79690. Took 0.47 sec\n",
      "Epoch 20, Loss(train/val) 0.65586/0.78955. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64878/0.77446. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65021/0.77567. Took 0.47 sec\n",
      "Epoch 23, Loss(train/val) 0.64690/0.77578. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64169/0.78437. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64315/0.77598. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.63907/0.78997. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63247/0.74698. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63833/0.72739. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.61460/0.79017. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.62501/0.77481. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62053/0.80647. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61309/0.82306. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.61703/0.80748. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61095/0.77116. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61189/0.78745. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.59577/0.77024. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.60949/0.75915. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60020/0.77591. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59886/0.79739. Took 0.47 sec\n",
      "Epoch 40, Loss(train/val) 0.58934/0.77861. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59798/0.81678. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57891/0.81039. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.57077/0.81053. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57003/0.80988. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.57748/0.84666. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.56364/0.83856. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56572/0.85797. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56645/0.83637. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.56325/0.86385. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.55799/0.89673. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54498/0.86645. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.54828/0.93064. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.53650/0.88807. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54258/0.92618. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.54874/0.89995. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.52871/0.93369. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52681/0.97900. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.53237/0.97295. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.52611/0.92091. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.52795/0.96330. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.51968/1.03016. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.51216/1.04015. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.51547/1.01460. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50277/1.08883. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.50707/1.01470. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.50542/1.15006. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.50111/1.03861. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49918/1.12592. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.49125/1.09322. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48419/1.11044. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.47645/1.18717. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.45888/1.17093. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.45353/1.27027. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.45984/1.21208. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.46044/1.22120. Took 0.43 sec\n",
      "Epoch 76, Loss(train/val) 0.44987/1.26671. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.43348/1.22809. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44370/1.25457. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.43646/1.29879. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.43029/1.29495. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.44199/1.32619. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.43669/1.33127. Took 0.47 sec\n",
      "Epoch 83, Loss(train/val) 0.41119/1.36572. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.39524/1.31728. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.44879/1.36106. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.42767/1.42082. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.40695/1.44958. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.38617/1.49419. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.39036/1.44677. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.38139/1.44721. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.35819/1.49805. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.40234/1.48379. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.37058/1.47489. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.36623/1.60366. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.34487/1.56052. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.37746/1.51121. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.34777/1.67971. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.33861/1.64872. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.42450/1.48356. Took 0.45 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.71605/0.71433. Took 0.63 sec\n",
      "Epoch 1, Loss(train/val) 0.71122/0.69744. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.70159/0.70440. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69100/0.71118. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68907/0.69973. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68132/0.72205. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68281/0.72759. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67834/0.72609. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67317/0.74008. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.66868/0.72798. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66593/0.72838. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.66902/0.74372. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66435/0.73893. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66142/0.72548. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66119/0.72630. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.64883/0.73314. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.64778/0.75154. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.64420/0.75937. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.64011/0.76234. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64145/0.73366. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64935/0.72012. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63462/0.72942. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64428/0.73284. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63893/0.75716. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.63723/0.73828. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.63859/0.72549. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62136/0.75053. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62901/0.75954. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61490/0.77876. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61287/0.78648. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.60630/0.78118. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61467/0.77524. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.60107/0.78199. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.60646/0.83666. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59809/0.83453. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.60946/0.81202. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58951/0.82249. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.58477/0.85344. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.59275/0.86642. Took 0.43 sec\n",
      "Epoch 39, Loss(train/val) 0.58045/0.90021. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.57422/0.85438. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.57042/0.88716. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56465/0.85389. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.55679/0.89341. Took 0.47 sec\n",
      "Epoch 44, Loss(train/val) 0.55977/0.86999. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.55552/0.84767. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.55187/0.88521. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.54612/0.91027. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.53508/0.90460. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.53255/0.98117. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.53492/0.95786. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.53100/1.02369. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.50307/1.02080. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.51445/1.07226. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.51212/1.07587. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.50132/1.07735. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.50072/1.07934. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.49685/1.14084. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.49248/1.15029. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.47205/1.24187. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49195/1.27574. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.47135/1.21391. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.47574/1.22550. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.45773/1.27274. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.48967/1.11778. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.47538/1.17114. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.46579/1.21860. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.45806/1.31620. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.44807/1.33656. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.44080/1.34327. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.45197/1.45112. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48427/1.28226. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.43946/1.27360. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.43226/1.41850. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.43964/1.44219. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.43564/1.37180. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.41421/1.45877. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.40770/1.56418. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.41445/1.48583. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.41903/1.49108. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.40417/1.49388. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.40286/1.59751. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.38880/1.55154. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.39780/1.44819. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.38889/1.59375. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.37196/1.60557. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.42023/1.48848. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.40409/1.33751. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.39726/1.48593. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.39035/1.42965. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.37825/1.48286. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.35551/1.52895. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.36462/1.46750. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.38221/1.48512. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.34600/1.71413. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.32963/1.58350. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.35888/1.71542. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.34608/1.71983. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.33699/1.68285. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.35363/1.62366. Took 0.44 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.70998/0.71740. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69514/0.69710. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69128/0.71670. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68673/0.70835. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68885/0.72774. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68268/0.72479. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.68322/0.71631. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67153/0.70455. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.66587/0.68494. Took 0.49 sec\n",
      "Epoch 9, Loss(train/val) 0.67037/0.70828. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.66711/0.72138. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66648/0.72180. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66760/0.73773. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.65775/0.74397. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.65632/0.72469. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.65656/0.71915. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65662/0.74239. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65311/0.73397. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64858/0.75350. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.64108/0.73782. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.63872/0.76161. Took 0.47 sec\n",
      "Epoch 21, Loss(train/val) 0.64654/0.75850. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.63674/0.75393. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.65860/0.77919. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64640/0.77874. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63469/0.77983. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64546/0.80497. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62725/0.78540. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61568/0.79372. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.62111/0.78383. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63280/0.78143. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62895/0.78093. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.60877/0.79586. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60932/0.78530. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.61105/0.79564. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.60616/0.80122. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59977/0.81282. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60515/0.78053. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.60676/0.78117. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.60187/0.78130. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59969/0.81223. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58018/0.80765. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.58509/0.83947. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58298/0.84472. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.57311/0.85184. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.56480/0.87931. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55806/0.88195. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56800/0.88555. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.56592/0.87163. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57051/0.90149. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.54389/0.90918. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.55624/0.86472. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.53878/0.87766. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.54651/0.91177. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53945/0.90991. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.52069/0.95022. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.51859/0.96559. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52285/0.94194. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.50351/0.95538. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.50580/0.96258. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.50793/0.98826. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.50528/0.95252. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.47737/1.01866. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.49120/1.01104. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.47983/0.97939. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.46444/1.02148. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.46047/1.01977. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.46467/1.04595. Took 0.47 sec\n",
      "Epoch 68, Loss(train/val) 0.44099/1.06831. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45540/1.03351. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.44295/1.06001. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.46646/1.11998. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.43942/1.12786. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.44033/1.16149. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.41746/1.14520. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.43200/1.07794. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43060/1.15330. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.41490/1.13775. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.41886/1.12376. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.40067/1.23809. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.39215/1.28972. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.37204/1.23271. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.41016/1.17444. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.38732/1.15826. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.37420/1.25639. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.37190/1.25197. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.36824/1.24275. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.36619/1.27304. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.35642/1.25803. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.37215/1.20351. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.38208/1.30506. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.38785/1.21344. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.35268/1.23158. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.33823/1.23128. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.34299/1.32420. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.34320/1.26528. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.35603/1.36040. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.34805/1.27666. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.35600/1.44257. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.34596/1.37221. Took 0.45 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.71249/0.67158. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70366/0.67717. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69875/0.67508. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69375/0.69370. Took 0.43 sec\n",
      "Epoch 4, Loss(train/val) 0.69004/0.70241. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68179/0.70115. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.69196/0.69094. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68433/0.69097. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.68291/0.71206. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67510/0.71926. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67427/0.74675. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67547/0.75843. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67123/0.76161. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.67045/0.78385. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65748/0.78690. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65583/0.81864. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65899/0.80400. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64538/0.84982. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64900/0.83573. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.64186/0.84930. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63781/0.87896. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63770/0.81870. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.63005/0.82583. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.63436/0.82573. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.61888/0.82663. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.63084/0.82761. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.62450/0.84484. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61915/0.77129. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61011/0.82477. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.60207/0.82970. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60086/0.81515. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.59373/0.83886. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.59005/0.81914. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.59308/0.84580. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.58802/0.86809. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58257/0.80913. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.57561/0.85387. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.57516/0.86561. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58245/0.85985. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.57005/0.87888. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.55609/0.91320. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.57450/0.90114. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.55500/0.93452. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.54617/0.94570. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56417/0.91038. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.53886/0.93229. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.52288/1.03459. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.51731/1.02774. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.51129/0.98448. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.50416/1.00854. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.52731/1.06447. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.52361/1.00671. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.50920/0.97968. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.50467/0.96341. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.48553/1.02882. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.50230/1.00294. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.49266/0.94340. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.48386/1.03133. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.48602/1.01473. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.47140/1.03622. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.47471/0.98494. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.47113/0.96373. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.45077/0.97952. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.48243/1.03775. Took 0.43 sec\n",
      "Epoch 64, Loss(train/val) 0.45732/1.00542. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.45000/1.04600. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.42300/1.08918. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.42294/1.09783. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.41151/1.08141. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.42463/1.07263. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.45491/1.04909. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.42895/1.02420. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.41008/1.00523. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.40905/1.06500. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.42239/1.08370. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.41027/1.06895. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.40861/1.10579. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.40268/1.08251. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.39367/1.14640. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.37749/1.10674. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.37132/1.13988. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.39944/1.01634. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.39797/1.07578. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.38486/1.07568. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.37978/1.05116. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.35432/1.05066. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.34933/1.02223. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.36294/1.01557. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.34403/1.11943. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.35600/1.13995. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36820/1.09121. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.38450/1.14418. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.36028/1.16473. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.33881/1.15960. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.35870/1.20542. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.32257/1.20087. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.33575/1.20806. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.34692/1.14688. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.32373/1.23232. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.32687/1.25529. Took 0.46 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69422/0.69839. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.68511/0.69841. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68229/0.70534. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.67746/0.69310. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.67628/0.68622. Took 0.50 sec\n",
      "Epoch 5, Loss(train/val) 0.67564/0.70334. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.66969/0.70273. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67072/0.71166. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.66721/0.71355. Took 0.43 sec\n",
      "Epoch 9, Loss(train/val) 0.66424/0.71863. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.65975/0.73111. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.65610/0.74247. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65449/0.74035. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.65773/0.74972. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65653/0.73109. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.64752/0.74572. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.64411/0.74479. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64059/0.76567. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.63684/0.75567. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.63227/0.80507. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.62964/0.82150. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.62699/0.81809. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.61914/0.84638. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.60731/0.84977. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.62259/0.81704. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.61132/0.83582. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.60464/0.83764. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.59579/0.86897. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.58427/0.86101. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60054/0.85681. Took 0.47 sec\n",
      "Epoch 30, Loss(train/val) 0.58275/0.88035. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.58226/0.88258. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.58483/0.91055. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.56391/0.87841. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.57822/0.91676. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.55988/0.90906. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.57264/0.87887. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.54467/0.89998. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.53789/0.98203. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.53841/0.99730. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.54697/0.94450. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.52060/1.04996. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.52436/1.05319. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.52228/1.04792. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.50076/1.09181. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.51840/1.03912. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.50208/1.05331. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.48810/1.17510. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.49330/1.07035. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.49394/1.09220. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.47469/1.12491. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.47017/1.14066. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.46170/1.15971. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.50199/1.07353. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.45422/1.15416. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.43845/1.18310. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.43760/1.16870. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.44417/1.19477. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.43698/1.24581. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.44154/1.21124. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.43337/1.24125. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.43848/1.23210. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.41400/1.27285. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.41154/1.29173. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.39872/1.27150. Took 0.47 sec\n",
      "Epoch 65, Loss(train/val) 0.41240/1.23783. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.38706/1.31396. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.40223/1.29016. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.37620/1.31881. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.38182/1.33869. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.38324/1.29879. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.37432/1.36463. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.37835/1.24218. Took 0.43 sec\n",
      "Epoch 73, Loss(train/val) 0.37242/1.35161. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.34123/1.35959. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.34024/1.41948. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.33903/1.39096. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.35473/1.47547. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.34343/1.44315. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.33123/1.42947. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.31644/1.47326. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.35261/1.50472. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.32924/1.44658. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.32212/1.51421. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.31417/1.49784. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.30356/1.54194. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.30312/1.57899. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.31071/1.50091. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.31447/1.51261. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.29791/1.49919. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.28639/1.53917. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.28081/1.57566. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.27842/1.56254. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.26672/1.55937. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.25922/1.57960. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.27266/1.62859. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.27398/1.67798. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.27519/1.63797. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.27619/1.68826. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.26235/1.63449. Took 0.46 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.68970/0.70076. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.68712/0.71335. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.68260/0.73787. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68312/0.74501. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.67505/0.75588. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68092/0.74209. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67344/0.75223. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67399/0.75761. Took 0.43 sec\n",
      "Epoch 8, Loss(train/val) 0.66779/0.76614. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66197/0.75595. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.66742/0.75844. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.66387/0.76717. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66460/0.75351. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65621/0.77210. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.65445/0.79058. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65135/0.77604. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65403/0.76353. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.64658/0.77860. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64255/0.77824. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.63714/0.77499. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63804/0.78002. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63213/0.79703. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.62705/0.80516. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63718/0.80968. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.61883/0.82308. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.62660/0.82666. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.62033/0.80914. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61752/0.80049. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.61077/0.81851. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.60124/0.83415. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61371/0.82762. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.59636/0.85009. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59669/0.86487. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.59453/0.88346. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.60123/0.87224. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.59233/0.84751. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.56671/0.89420. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.57097/0.88935. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57446/0.89249. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57424/0.90913. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.57458/0.91514. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.56816/0.88839. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.55866/0.87537. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.55251/0.88961. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.55542/0.88213. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.54150/0.86978. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.55210/0.89864. Took 0.43 sec\n",
      "Epoch 47, Loss(train/val) 0.53819/0.90135. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.52969/0.93576. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.52689/0.89563. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.51990/0.91215. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.52682/0.93067. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52335/0.91035. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.49885/0.92432. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.53969/0.90948. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.51332/0.90276. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.50966/0.89224. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.50302/0.93343. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.48023/1.01417. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.49378/1.00230. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.47068/1.07870. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.49531/1.04762. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48546/1.01513. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.49931/0.95855. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.48449/1.01433. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.47623/1.04426. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.46680/1.00527. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.46357/0.99232. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.46171/1.04357. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46275/1.04377. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.45124/1.04829. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.46397/1.05044. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46917/1.08373. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.44973/1.12091. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.46901/1.10268. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.45170/1.11520. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.43660/1.11146. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.45852/1.04989. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.43740/1.14590. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.43064/1.12629. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.41177/1.18487. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.44106/1.16862. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.44202/1.14775. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.43603/1.14342. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.41453/1.13265. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.40945/1.15522. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43390/1.15757. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.42708/1.13822. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.40443/1.17193. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.41224/1.17637. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.41436/1.23491. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.41804/1.18803. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.43793/1.15402. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.41516/1.19307. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.44620/1.14489. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.39660/1.19974. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.40273/1.18747. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.40305/1.22573. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.39134/1.20912. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.39656/1.27638. Took 0.44 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69812/0.70419. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.68488/0.70817. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68244/0.70592. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.67835/0.70225. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.67263/0.70431. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67108/0.70561. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.66692/0.72017. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.66091/0.72124. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.66264/0.73888. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.65933/0.72970. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.65411/0.72423. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.65673/0.72296. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65672/0.73206. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.64526/0.74609. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.64644/0.73663. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.64462/0.73957. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.63771/0.75725. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.63484/0.77788. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.62720/0.79482. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.63079/0.78720. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.62690/0.79812. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.62312/0.80903. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.63333/0.75974. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62208/0.75860. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.61073/0.77274. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.60645/0.76829. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.60474/0.78746. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.60652/0.77618. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60184/0.77008. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.59816/0.79058. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.59516/0.77533. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.58107/0.81010. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.57965/0.80754. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.57043/0.81777. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.58568/0.78971. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.56731/0.82091. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.57671/0.82332. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.56593/0.86287. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.55549/0.87683. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.55492/0.87028. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.54601/0.88507. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.54643/0.88409. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.53989/0.88626. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.55380/0.91376. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.53951/0.91078. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.52354/0.91137. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.51881/0.91923. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.50858/0.93007. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.52577/0.94732. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.51409/0.95413. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.51084/0.93834. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.49621/0.96602. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.49912/0.96809. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.48823/0.95846. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.49708/0.91679. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.48584/0.94850. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.49855/1.00014. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.47072/1.00283. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.49566/0.95855. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.46497/1.01241. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.46021/1.00809. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.45137/1.05511. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.46353/1.00984. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.43737/1.02267. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.44152/1.09120. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.42715/1.10795. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.43860/1.05783. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.42006/1.07831. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.42900/1.10747. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.43336/1.08947. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.41790/1.13058. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.42006/1.16646. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.41819/1.15127. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.41356/1.19671. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.39805/1.19255. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.40136/1.21586. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.40430/1.23892. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.36390/1.23806. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.36876/1.19033. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.35967/1.19315. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.36304/1.24185. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.36717/1.24928. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.38397/1.19010. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.35064/1.23325. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.34157/1.23006. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.33589/1.31973. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.35113/1.31065. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.40640/1.17606. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.37158/1.22826. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.34083/1.34752. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.32324/1.39271. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.32623/1.34536. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.30643/1.43289. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.31393/1.52637. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.30925/1.47021. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.29311/1.47792. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.28228/1.50288. Took 0.47 sec\n",
      "Epoch 97, Loss(train/val) 0.28769/1.51179. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.28586/1.44645. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.27912/1.51833. Took 0.45 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69252/0.70651. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68638/0.71184. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.68481/0.71109. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68186/0.71799. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.67754/0.71885. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.67555/0.73161. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.67006/0.73722. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.66972/0.76074. Took 0.47 sec\n",
      "Epoch 8, Loss(train/val) 0.66641/0.75692. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.66815/0.76082. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.66399/0.77845. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66224/0.78476. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66369/0.79455. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.65581/0.80516. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65799/0.81731. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.65050/0.82854. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.64965/0.85442. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64630/0.86747. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.64514/0.87696. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64378/0.89899. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.63109/0.92245. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63521/0.93324. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.62781/0.94336. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.62562/0.96456. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62183/0.98006. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.61810/0.99884. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.60960/1.01241. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.60527/1.00118. Took 0.47 sec\n",
      "Epoch 28, Loss(train/val) 0.60385/1.01636. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.59310/1.02041. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.58566/1.04574. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.59881/1.02819. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.57748/1.06918. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.57301/1.07111. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.57877/1.05009. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.57994/1.05763. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.57000/1.07504. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57845/1.03997. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.55758/1.09267. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.55060/1.10301. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.55171/1.12575. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.53424/1.14432. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.54868/1.16373. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.52468/1.16331. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.53610/1.16098. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.51541/1.15001. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.50666/1.19928. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.49387/1.21211. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.51008/1.19206. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.50138/1.21317. Took 0.47 sec\n",
      "Epoch 50, Loss(train/val) 0.49225/1.21160. Took 0.43 sec\n",
      "Epoch 51, Loss(train/val) 0.49518/1.22071. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.48364/1.24694. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.47310/1.24695. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.47590/1.25569. Took 0.47 sec\n",
      "Epoch 55, Loss(train/val) 0.46622/1.24791. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.47234/1.24379. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.45576/1.24986. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.44789/1.29878. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.43899/1.29399. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.43173/1.27502. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.44899/1.30573. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.42459/1.31667. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.41008/1.30939. Took 0.48 sec\n",
      "Epoch 64, Loss(train/val) 0.43092/1.34862. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.41674/1.33666. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.40295/1.36444. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.39753/1.36189. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.39878/1.36940. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.38494/1.36449. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.38913/1.46180. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.39952/1.39642. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.38511/1.45687. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.37740/1.45844. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.36691/1.43736. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.39584/1.46551. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.36346/1.43407. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.36154/1.37616. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.37998/1.47180. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.35994/1.42537. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.34488/1.47069. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.32914/1.51618. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.31376/1.54713. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.32740/1.51981. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.30210/1.60175. Took 0.47 sec\n",
      "Epoch 85, Loss(train/val) 0.31565/1.62385. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.33078/1.55814. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.33143/1.59099. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.31008/1.68175. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.30567/1.66224. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.29785/1.69132. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.31335/1.60742. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.28912/1.75156. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.29296/1.74625. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.30222/1.71306. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.26754/1.70380. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.24970/1.79366. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.29318/1.77489. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.33364/1.76941. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.28480/1.70012. Took 0.46 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.72489/0.71258. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.71005/0.71177. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.70582/0.71207. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69667/0.70944. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.69165/0.71446. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68160/0.72491. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67593/0.73688. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.67393/0.74591. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67559/0.73634. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67344/0.74385. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.66714/0.76975. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67452/0.76777. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.66371/0.76924. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66580/0.78023. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66369/0.80513. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.65753/0.78539. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65933/0.77938. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65258/0.79455. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65640/0.81767. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64096/0.83522. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.63850/0.83411. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64243/0.85205. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.64055/0.84740. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64392/0.87164. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63787/0.87376. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64312/0.89155. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.63989/0.84835. Took 0.43 sec\n",
      "Epoch 27, Loss(train/val) 0.63546/0.82457. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62338/0.82400. Took 0.43 sec\n",
      "Epoch 29, Loss(train/val) 0.62158/0.84050. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.63444/0.81691. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62730/0.82388. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61272/0.86597. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.61097/0.90304. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60749/0.92830. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.61154/0.93982. Took 0.47 sec\n",
      "Epoch 36, Loss(train/val) 0.59466/0.93058. Took 0.43 sec\n",
      "Epoch 37, Loss(train/val) 0.60188/0.94279. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.59028/0.98985. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.59916/0.97823. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58916/0.95966. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58106/0.97354. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.58256/0.99352. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57777/0.99634. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57727/1.00644. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.57923/0.97670. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.57085/1.00002. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.56452/1.06326. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55771/1.10330. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.56152/1.14276. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.55811/1.09113. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.54677/1.10370. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.55650/1.17273. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55106/1.17959. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.54789/1.20749. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.53439/1.27723. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52523/1.25728. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.54565/1.24401. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.52955/1.27970. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53927/1.32547. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.52503/1.31788. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.51880/1.33844. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51044/1.32312. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.52140/1.31884. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.51432/1.25705. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.49348/1.35194. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51784/1.32568. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.49778/1.28690. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49768/1.34491. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.46967/1.39655. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.48521/1.39344. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48786/1.36744. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.46831/1.39489. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48256/1.44263. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.47561/1.40903. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48366/1.42633. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46335/1.42022. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.45088/1.50169. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.45273/1.48845. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.45530/1.50448. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.45161/1.38035. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.45156/1.53993. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.43756/1.52313. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.46803/1.40023. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.44268/1.47550. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.41748/1.57813. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.42018/1.59251. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43219/1.62613. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.40607/1.63084. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.40557/1.68586. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.39328/1.69145. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.39717/1.67113. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.41876/1.68109. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.39173/1.88437. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.39621/1.70650. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.38554/1.83117. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.39223/1.77597. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.37482/1.81258. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.38312/1.90710. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.37559/1.75271. Took 0.44 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.71721/0.73258. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.70222/0.70598. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.70327/0.70021. Took 0.50 sec\n",
      "Epoch 3, Loss(train/val) 0.69068/0.70492. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68871/0.70497. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68057/0.72495. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67485/0.74415. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68520/0.73966. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.67091/0.75859. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67622/0.76240. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67758/0.78712. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.67337/0.80408. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66457/0.83055. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66291/0.87874. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66301/0.88989. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66249/0.89715. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65283/0.90888. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65400/0.93405. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.64911/0.95346. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64452/0.96962. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.63833/0.98616. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64078/1.02958. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63973/0.97805. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63231/0.99866. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.62244/1.05309. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62586/1.03375. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.61990/1.09610. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.61696/1.04700. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61904/1.06030. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.60871/1.12255. Took 0.43 sec\n",
      "Epoch 30, Loss(train/val) 0.61032/1.08356. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.61600/1.05851. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.59783/1.06261. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59543/1.12092. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.58585/1.14377. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.58676/1.19096. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58300/1.13097. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.59884/1.10888. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.58563/1.11483. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.58479/1.13089. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59029/1.11739. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.58096/1.13841. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56780/1.19245. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.57749/1.12392. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.57050/1.17334. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57552/1.21838. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.55998/1.28479. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.54851/1.32231. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.54444/1.28089. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.54356/1.27659. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55642/1.24608. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.53938/1.31668. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.55174/1.22376. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.52875/1.23992. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.54423/1.17499. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.52489/1.21573. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.51266/1.28606. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.50977/1.26260. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.52081/1.32286. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53138/1.27081. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.50658/1.31116. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.51348/1.27834. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48357/1.33689. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.49575/1.34177. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50686/1.30287. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.47141/1.37054. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.48963/1.40488. Took 0.43 sec\n",
      "Epoch 67, Loss(train/val) 0.50833/1.34825. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.47707/1.39102. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.48039/1.25177. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.47136/1.36624. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45122/1.44812. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.45636/1.37012. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.45844/1.48908. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.44647/1.46233. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.44911/1.39992. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43976/1.48691. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.42545/1.48241. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44487/1.46268. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.43954/1.44849. Took 0.47 sec\n",
      "Epoch 80, Loss(train/val) 0.44111/1.45181. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.40026/1.50992. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.40912/1.47984. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.43268/1.53846. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.41018/1.50122. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.43508/1.44378. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.40505/1.45766. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.38666/1.57314. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.38765/1.45188. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.37741/1.54230. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.39812/1.47772. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.37922/1.54728. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.41316/1.44399. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.40066/1.52344. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.38904/1.57470. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.37457/1.62570. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.35948/1.57717. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.38525/1.53281. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.35950/1.60290. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34916/1.58842. Took 0.47 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69899/0.69687. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69133/0.69775. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.69231/0.71227. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68654/0.70045. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68440/0.70637. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68492/0.71037. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.67904/0.70794. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67441/0.70979. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67248/0.71197. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.67176/0.72553. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67551/0.74385. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66563/0.75299. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66326/0.75636. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67006/0.76121. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66578/0.78149. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.66023/0.78226. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66215/0.78124. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65884/0.80965. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65653/0.79755. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65501/0.81861. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65709/0.82804. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65177/0.83315. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64997/0.83787. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64692/0.81882. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64207/0.84360. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64918/0.82463. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64245/0.82225. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63490/0.82676. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62854/0.83373. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.62699/0.86168. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62957/0.86831. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.64275/0.83098. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.62575/0.88952. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.62076/0.86052. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.61263/0.90394. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.61234/0.86704. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61428/0.87605. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.60408/0.92698. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.60096/0.86215. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60489/0.92248. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60500/0.91368. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.57875/0.89924. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58751/0.91746. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.58977/0.93587. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.58632/0.92040. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.58234/0.95305. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.58258/0.92631. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57350/0.93786. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.56721/0.94511. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.56156/0.97580. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55714/0.90664. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55091/0.93461. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.55620/1.05480. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55325/0.94256. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52701/0.94019. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.53558/0.95466. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53623/0.94813. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.54554/0.95649. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.54194/0.94863. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.52879/0.91895. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52963/0.95449. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.51552/0.95235. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.50394/0.89695. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.51528/0.94457. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.51480/0.88636. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.52525/0.90129. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.53105/0.94099. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.49221/0.92162. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.52019/0.95690. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.50350/0.95361. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49295/0.90297. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.48208/0.93777. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.47957/0.94204. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.46257/1.00793. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.46676/1.04551. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.46230/1.02909. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46060/1.02351. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.46005/1.03831. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.45769/1.03860. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.45431/0.97225. Took 0.43 sec\n",
      "Epoch 80, Loss(train/val) 0.46474/1.05613. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.45097/1.02037. Took 0.43 sec\n",
      "Epoch 82, Loss(train/val) 0.45804/1.02551. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.46806/1.05243. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.44645/1.04627. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.44504/1.05059. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.44406/1.08840. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.43751/1.07453. Took 0.43 sec\n",
      "Epoch 88, Loss(train/val) 0.44528/1.07791. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.41964/1.08643. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.43868/1.09029. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.41527/1.09791. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.42491/1.09319. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.42709/1.09350. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.42238/1.10244. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.42197/1.07490. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.40885/1.12543. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.39529/1.08375. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.41595/1.13454. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.38310/1.15322. Took 0.45 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70386/0.70179. Took 0.63 sec\n",
      "Epoch 1, Loss(train/val) 0.70073/0.72081. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69399/0.72017. Took 0.43 sec\n",
      "Epoch 3, Loss(train/val) 0.68760/0.72423. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68481/0.73876. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68020/0.72692. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.67454/0.73149. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67409/0.72817. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.66657/0.73316. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.66481/0.74273. Took 0.43 sec\n",
      "Epoch 10, Loss(train/val) 0.67084/0.75179. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66507/0.76551. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66054/0.76906. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66335/0.76670. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65674/0.76257. Took 0.47 sec\n",
      "Epoch 15, Loss(train/val) 0.65778/0.78921. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65968/0.75401. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65890/0.76416. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65069/0.78591. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65258/0.81382. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64105/0.81923. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64376/0.81401. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63762/0.81874. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63797/0.82236. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63035/0.84789. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.63765/0.83260. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62573/0.84226. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64126/0.82749. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61838/0.83953. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.62490/0.84204. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62088/0.83019. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.61757/0.85246. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61156/0.85825. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.61194/0.85912. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.60741/0.88145. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.60812/0.87561. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61249/0.89790. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.59865/0.89561. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60734/0.91139. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.59250/0.91240. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.58992/0.92853. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59120/0.91891. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58579/0.92175. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.58071/0.93111. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57512/0.93774. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.57933/0.95058. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.57064/0.98378. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.56960/0.97329. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55794/0.98759. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.55920/0.97129. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.56261/1.01617. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.56334/0.97961. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.54203/0.94475. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.53654/0.96898. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.54271/1.03066. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.54230/1.01056. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.53289/1.01666. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52845/1.01015. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.50944/1.00710. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.51891/1.04928. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.50371/1.02477. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.50055/1.06840. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49251/0.99505. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48703/1.02121. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.49907/1.03673. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.47814/1.03541. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.47273/1.00958. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.46962/1.08739. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.48702/1.03368. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.46212/1.03667. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.47658/1.08127. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.46686/1.03129. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.44973/1.10025. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.45604/1.04930. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.44824/1.06430. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.45689/1.02413. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43160/1.08315. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.44151/1.04105. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.43351/1.12562. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.41947/1.18115. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.44687/1.18380. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.42713/1.11319. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.41457/1.15037. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.38952/1.21817. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.40192/1.22889. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.39834/1.24077. Took 0.46 sec\n",
      "Epoch 86, Loss(train/val) 0.38747/1.27146. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.37991/1.19126. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.37697/1.30583. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.39231/1.32542. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.36806/1.34933. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.36422/1.28047. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.34992/1.30350. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.36258/1.42979. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.34549/1.32852. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.33794/1.33312. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.32778/1.39461. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.34787/1.46809. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.30972/1.49412. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.31173/1.47139. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.70563/0.70254. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69990/0.68843. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.69092/0.68154. Took 0.49 sec\n",
      "Epoch 3, Loss(train/val) 0.68718/0.69114. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68484/0.68699. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.67792/0.68062. Took 0.48 sec\n",
      "Epoch 6, Loss(train/val) 0.67472/0.67463. Took 0.48 sec\n",
      "Epoch 7, Loss(train/val) 0.67394/0.66947. Took 0.48 sec\n",
      "Epoch 8, Loss(train/val) 0.67021/0.66765. Took 0.49 sec\n",
      "Epoch 9, Loss(train/val) 0.66835/0.67302. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.65952/0.70080. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.65952/0.68132. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66436/0.72528. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65102/0.69860. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.65758/0.70576. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.64633/0.70097. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65008/0.72386. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.64315/0.71703. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.63455/0.71643. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.63397/0.71804. Took 0.47 sec\n",
      "Epoch 20, Loss(train/val) 0.63532/0.70097. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.62823/0.70781. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.62398/0.70980. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.62514/0.72893. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.61371/0.71793. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62146/0.72705. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.60841/0.72556. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.60695/0.73711. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.60811/0.75251. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.60468/0.76120. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.59214/0.75990. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.59671/0.78549. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.59984/0.76385. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.58335/0.78389. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.58883/0.75829. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.57953/0.77203. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58151/0.79537. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.57656/0.81497. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.57305/0.83871. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57023/0.81959. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.55987/0.84488. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.55113/0.83293. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.54316/0.88920. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.52983/0.86369. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.56183/0.84433. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.54507/0.85989. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55458/0.89456. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.53789/0.89812. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.53726/0.88711. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.53512/0.94636. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.52258/0.98168. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.50814/0.99479. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.50520/0.98630. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.50609/1.01549. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.49247/1.04127. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.50902/1.08192. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.49399/1.08324. Took 0.47 sec\n",
      "Epoch 57, Loss(train/val) 0.49540/1.10365. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.47808/1.13732. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.49302/1.13187. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.47860/1.13215. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.45460/1.16631. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48740/1.15977. Took 0.47 sec\n",
      "Epoch 63, Loss(train/val) 0.47214/1.18354. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.46523/1.13427. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.44261/1.19072. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.43517/1.24461. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.44943/1.21586. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.41652/1.27918. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45550/1.26458. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.41779/1.36782. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.40661/1.34997. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.43291/1.35734. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.41356/1.38767. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.41549/1.31525. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.41221/1.32171. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.39454/1.38286. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.41810/1.43091. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.39159/1.44175. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.40233/1.39210. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.41863/1.45105. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.39718/1.43583. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.38237/1.47539. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.37564/1.46220. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.37336/1.41649. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.37275/1.49720. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.33669/1.60071. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.34443/1.52382. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.35452/1.59018. Took 0.47 sec\n",
      "Epoch 89, Loss(train/val) 0.35291/1.57737. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.38267/1.62394. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.34143/1.63118. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.33159/1.61063. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.32180/1.67069. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.33829/1.60590. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.32774/1.63856. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.31180/1.73132. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.31405/1.71731. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.29188/1.76943. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.31907/1.77178. Took 0.45 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.72562/0.72194. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70594/0.72148. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.70255/0.69661. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.69087/0.70708. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.68454/0.71167. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68788/0.72331. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68483/0.70246. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68193/0.69257. Took 0.48 sec\n",
      "Epoch 8, Loss(train/val) 0.68380/0.69616. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67900/0.70182. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67733/0.70045. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.67917/0.70008. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67340/0.69605. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67863/0.72117. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67657/0.72033. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66838/0.71732. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66359/0.72451. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65975/0.71764. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.65604/0.72281. Took 0.43 sec\n",
      "Epoch 19, Loss(train/val) 0.65327/0.70064. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64734/0.73518. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.64957/0.74785. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64020/0.75258. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.63145/0.75799. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.63402/0.77328. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.63191/0.77297. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62908/0.79019. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63415/0.78407. Took 0.46 sec\n",
      "Epoch 28, Loss(train/val) 0.62641/0.79809. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61821/0.77469. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62582/0.78753. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.61690/0.77677. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60809/0.80507. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.60600/0.83397. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.60716/0.83751. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.59773/0.84308. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.60176/0.81735. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.59255/0.82027. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.58684/0.82770. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.59832/0.85386. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.58168/0.88032. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.59407/0.86293. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.57907/0.86848. Took 0.43 sec\n",
      "Epoch 43, Loss(train/val) 0.58296/0.91420. Took 0.47 sec\n",
      "Epoch 44, Loss(train/val) 0.56478/0.89968. Took 0.43 sec\n",
      "Epoch 45, Loss(train/val) 0.58706/0.89931. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.56623/0.85028. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.56243/0.85773. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57547/0.87602. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.56634/0.86899. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.55707/0.86573. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56408/0.88437. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.56046/0.86427. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.55686/0.85215. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.55088/0.89646. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.55652/0.89042. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53648/0.89678. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.54606/0.87772. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.55549/0.87218. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53511/0.88111. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.52602/0.87601. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.53784/0.92287. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.53802/0.91120. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.51680/0.90241. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.52130/0.93044. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.51741/0.92965. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.50636/0.91247. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.52489/0.89511. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50222/0.92280. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.51413/0.91287. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.48964/0.95607. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.52737/0.94342. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.49934/0.93819. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.48881/0.98037. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.48234/0.97582. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49384/0.97128. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.46800/0.97363. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.48526/0.98562. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.47589/0.97084. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.46309/0.97438. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.49303/1.01487. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.48234/0.99339. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.45012/1.03022. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.46699/1.00166. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.45703/1.01128. Took 0.43 sec\n",
      "Epoch 85, Loss(train/val) 0.44823/1.01932. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.46373/1.05258. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43943/1.03810. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.45160/1.04871. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.43028/1.11424. Took 0.47 sec\n",
      "Epoch 90, Loss(train/val) 0.44448/1.10818. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.43707/1.09894. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.43800/1.16039. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.41027/1.17131. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.44043/1.10037. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.42235/1.12086. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.40826/1.14295. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.46272/1.12634. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.40490/1.15999. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.41721/1.15055. Took 0.44 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.71164/0.71923. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.70061/0.70393. Took 0.49 sec\n",
      "Epoch 2, Loss(train/val) 0.69508/0.71227. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69292/0.72456. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69414/0.73364. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68983/0.73900. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68488/0.74400. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68644/0.73887. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68210/0.73052. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68246/0.72661. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.67369/0.73373. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67750/0.72645. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67001/0.73268. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.67659/0.73045. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67207/0.73086. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66305/0.74991. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66097/0.74737. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66209/0.75527. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.65401/0.77236. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.65052/0.78290. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65352/0.78617. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.64244/0.78719. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.64180/0.80187. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.65241/0.79557. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64369/0.81007. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.62690/0.83190. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.61922/0.86113. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.61617/0.87880. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61556/0.89457. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.60291/0.90332. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.59322/0.94542. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60549/0.92434. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59051/0.93132. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.59181/0.94695. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.58793/0.93277. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.57786/0.95364. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.57841/0.94968. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57287/0.94116. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.56689/0.94452. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.54320/0.98731. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.55174/0.94984. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.53461/0.95152. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.54149/0.93015. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.53864/0.95018. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.54783/0.91921. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.52455/0.91000. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.50752/0.92484. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.51777/0.93766. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.50787/0.94940. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.51889/0.91999. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.48978/0.97988. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.49831/1.02911. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.48717/1.02307. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.47784/1.05828. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.46746/1.09770. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.46712/1.05167. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.45066/1.11447. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.47305/1.10380. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.46962/1.08834. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.44523/1.14579. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.43133/1.12786. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.44784/1.17805. Took 0.47 sec\n",
      "Epoch 62, Loss(train/val) 0.42605/1.14239. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.43129/1.18556. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.41596/1.12477. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.43170/1.15991. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.40201/1.19113. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.39880/1.24129. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.40550/1.18975. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.38489/1.26800. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.39753/1.25246. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.37646/1.24513. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.40405/1.29652. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.39657/1.28432. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.38930/1.24098. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.40022/1.21079. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.37725/1.20970. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.35910/1.28747. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.34987/1.30694. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.34991/1.32043. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.37146/1.22336. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.35260/1.30557. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.35779/1.28273. Took 0.47 sec\n",
      "Epoch 83, Loss(train/val) 0.34137/1.35292. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.35897/1.26824. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.32931/1.31936. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.33106/1.39243. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.33678/1.33650. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.31404/1.29038. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.34350/1.36171. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.30927/1.43584. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.32007/1.42636. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.32250/1.33242. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.28987/1.36006. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.29177/1.45609. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.30063/1.35265. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.30458/1.41189. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.29643/1.44603. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.28481/1.52340. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.28296/1.48792. Took 0.46 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70041/0.69972. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.68934/0.70578. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.68600/0.70613. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68073/0.70460. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.67508/0.71371. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.67158/0.71670. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.66481/0.71944. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.66799/0.72040. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.66510/0.71842. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.65886/0.71901. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.65691/0.71359. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.65723/0.71038. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65803/0.71351. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.65485/0.70353. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.64827/0.71504. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.64739/0.72233. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.64260/0.71776. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.63901/0.71906. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.64144/0.71301. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.63321/0.73163. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.62528/0.73577. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.62393/0.74542. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.61591/0.77015. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.60232/0.80467. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.59411/0.81709. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.60541/0.83975. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.59280/0.87168. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.59099/0.87320. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.58807/0.90225. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.58265/0.91092. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.57673/0.94795. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.56674/0.98048. Took 0.47 sec\n",
      "Epoch 32, Loss(train/val) 0.56282/1.01163. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.54874/1.01424. Took 0.47 sec\n",
      "Epoch 34, Loss(train/val) 0.54645/0.98836. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.54289/1.02250. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.53411/1.05760. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.53372/1.05577. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.53835/1.02179. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.52774/1.03957. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.52091/1.06648. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.51763/1.07981. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.51108/1.06263. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.50283/1.10447. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.50013/1.09772. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.49381/1.10606. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.50744/1.10963. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.49374/1.12339. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.49407/1.15028. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.47599/1.17919. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.47300/1.20097. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.46457/1.22906. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.46451/1.25847. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.46735/1.23769. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.45855/1.27735. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.46002/1.20170. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.46288/1.22791. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.45281/1.24367. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.44763/1.33483. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.44840/1.28820. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.42968/1.31225. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.41637/1.35872. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.41978/1.38673. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.42193/1.39258. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.44053/1.36138. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.40180/1.37395. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.39741/1.43950. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.40293/1.37967. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.38680/1.43177. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.36573/1.42896. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.37843/1.42890. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.36576/1.42600. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.36362/1.51215. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.41227/1.44478. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.37547/1.44094. Took 0.43 sec\n",
      "Epoch 75, Loss(train/val) 0.34894/1.49246. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.33893/1.50237. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.31851/1.65561. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.31659/1.59220. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.31589/1.68261. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.28916/1.66109. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.30845/1.65643. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.30259/1.63718. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.31247/1.55992. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.31755/1.65849. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.29719/1.69012. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.29920/1.70986. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.28599/1.75081. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.24932/1.81862. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.25499/1.91002. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.25795/1.84668. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.23684/1.87747. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.26678/1.93999. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.30142/1.70176. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.26932/1.72683. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.23170/1.87459. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.23962/1.87566. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.22169/1.92917. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.24171/1.96864. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.27617/1.88690. Took 0.44 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.70027/0.70017. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70047/0.69682. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69454/0.70220. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69021/0.70285. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69182/0.72102. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.69549/0.71421. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69097/0.72902. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.69374/0.71244. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68630/0.72918. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68166/0.74297. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.67713/0.74556. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67610/0.74607. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.67628/0.76588. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.67224/0.76221. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.67417/0.76495. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67590/0.77400. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.66836/0.76737. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66991/0.77394. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66559/0.78652. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.66417/0.77921. Took 0.47 sec\n",
      "Epoch 20, Loss(train/val) 0.66142/0.79873. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65387/0.80420. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65286/0.82066. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.65874/0.82038. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64468/0.83328. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.65629/0.81239. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.64742/0.82242. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64487/0.82059. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.64654/0.81600. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.63765/0.84948. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63922/0.82236. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.62638/0.83385. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63707/0.83388. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.62469/0.84181. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.62945/0.82730. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.62225/0.82573. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62379/0.81844. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61270/0.83961. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.61225/0.79791. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.59918/0.79717. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.60193/0.79680. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.59318/0.81614. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.58980/0.85453. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.59413/0.85741. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59865/0.86725. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.58762/0.85101. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57502/0.84336. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.57796/0.88620. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.57036/0.93566. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57378/0.94207. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.56381/0.94816. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.55720/0.95143. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.53914/0.95215. Took 0.43 sec\n",
      "Epoch 53, Loss(train/val) 0.53745/1.01956. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.53673/1.02027. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.52702/1.00486. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.52344/1.04255. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51203/1.08202. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.51568/1.04643. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.49402/1.06720. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.49564/1.09112. Took 0.47 sec\n",
      "Epoch 61, Loss(train/val) 0.48891/1.10605. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.48681/1.07595. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.50559/1.03135. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.51240/1.07127. Took 0.43 sec\n",
      "Epoch 65, Loss(train/val) 0.48136/1.08536. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.46459/1.13608. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.48112/1.09933. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.46324/1.08749. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.45457/1.11755. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.45799/1.12814. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.45709/1.09622. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.47885/1.08904. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.44833/1.08078. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.45125/1.07866. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.44099/1.06638. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.43104/1.09006. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.40802/1.17022. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.44480/1.15551. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.42560/1.15278. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.42547/1.17220. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.39648/1.20571. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.40112/1.12173. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.38490/1.23220. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.40963/1.19123. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.39678/1.23578. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.37377/1.15937. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.37436/1.22972. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.36310/1.16793. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.36052/1.31507. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.37027/1.26932. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.34335/1.26634. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.34993/1.27378. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.33061/1.30172. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.32575/1.34966. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.33689/1.36121. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.32234/1.36305. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.33198/1.38457. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.33274/1.38408. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.33297/1.20892. Took 0.44 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.70689/0.71660. Took 0.51 sec\n",
      "Epoch 1, Loss(train/val) 0.69838/0.72014. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69528/0.72222. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69184/0.71603. Took 0.49 sec\n",
      "Epoch 4, Loss(train/val) 0.68673/0.71628. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.67885/0.72046. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67939/0.72890. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67702/0.74186. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.67069/0.74596. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67102/0.75157. Took 0.46 sec\n",
      "Epoch 10, Loss(train/val) 0.66338/0.75753. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.65814/0.77383. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65165/0.78655. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.64701/0.79634. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.64407/0.79239. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.64158/0.81252. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.63332/0.80648. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.62686/0.85151. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.62822/0.85479. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.62957/0.84831. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.62098/0.87991. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.62564/0.83435. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.60672/0.89389. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.61329/0.88529. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.59716/0.91941. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.59670/0.90347. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.59727/0.94684. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.59074/0.94416. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.58902/0.94048. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.58302/0.94053. Took 0.46 sec\n",
      "Epoch 30, Loss(train/val) 0.58601/0.94872. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.57924/0.96268. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.57326/0.94720. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.56636/0.97391. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.56012/0.98404. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.56050/0.96959. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.54199/1.01408. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.54064/1.00342. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.53775/1.05890. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.53608/1.02435. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.55029/1.00231. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.53408/0.97361. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.54225/1.01728. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.51980/1.01589. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.51762/1.03525. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.51837/1.06535. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.49855/1.11109. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.50803/1.05488. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.50609/1.06307. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.49331/1.06999. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.50856/1.05694. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.48899/1.07928. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.48078/1.10032. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.48652/1.04453. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.47727/1.06942. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.45423/1.12681. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.45784/1.16208. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.47376/1.08952. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.43952/1.15023. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.45528/1.08557. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.43675/1.15348. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.42894/1.14905. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.42124/1.17034. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.40398/1.22468. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.43112/1.20012. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.41171/1.20593. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.40003/1.21127. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.40098/1.20779. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.37764/1.21167. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.39077/1.21945. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.37160/1.26305. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.37380/1.27368. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.37969/1.23457. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.37169/1.24531. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.34898/1.25080. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.35242/1.28694. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.36537/1.32253. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.34920/1.26433. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.33040/1.24829. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.34246/1.29266. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.32561/1.35193. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.31711/1.36572. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.33734/1.38923. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.32436/1.39501. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.30036/1.38093. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.30758/1.40317. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.31711/1.38912. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.28722/1.38125. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.28946/1.43362. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.29161/1.48046. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.28555/1.41391. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.28880/1.42162. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.29747/1.50404. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.29386/1.42759. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.25937/1.42321. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.27314/1.39817. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.25756/1.40900. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.24099/1.44400. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.25293/1.58394. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.27359/1.66923. Took 0.46 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69202/0.71627. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69473/0.72598. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.68594/0.72508. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.67917/0.72753. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68642/0.72930. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68227/0.73595. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.67233/0.74555. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67150/0.74041. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.67640/0.74661. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67072/0.74894. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.66559/0.76057. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66636/0.76640. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66033/0.77193. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.64970/0.78207. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.64779/0.78968. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.65275/0.78700. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.64950/0.79782. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.64774/0.79299. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.63682/0.81517. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.64218/0.81263. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.63430/0.81076. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.62976/0.82111. Took 0.46 sec\n",
      "Epoch 22, Loss(train/val) 0.63066/0.80858. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.61734/0.81518. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.61741/0.82163. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.60984/0.82473. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.60784/0.82332. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.60186/0.81334. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.59756/0.80996. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.59467/0.81660. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.59453/0.82278. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.59591/0.84821. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.59884/0.81610. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.58603/0.82143. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.58622/0.83633. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.57046/0.85258. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.57078/0.85605. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.56304/0.85955. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.55110/0.88776. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.54681/0.88399. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.55479/0.89366. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.53959/0.89716. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.53664/0.87852. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.54578/0.88183. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.52524/0.89868. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.52076/0.87429. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.52872/0.86632. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.51779/0.86078. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.52531/0.87347. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.50741/0.94220. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.49418/0.89421. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.49616/0.90580. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.49583/0.89209. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.48683/0.94612. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.49858/0.96555. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.47459/0.85843. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.47818/0.91900. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.46222/0.93336. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.47024/0.95393. Took 0.43 sec\n",
      "Epoch 59, Loss(train/val) 0.46097/0.95605. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.45344/0.99188. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.44786/0.97707. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.45070/0.97865. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.45187/0.98857. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.45019/0.99397. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.43659/0.99916. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.41970/1.14455. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.42927/1.03205. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.42509/1.05068. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.41717/1.11812. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.39844/1.17027. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.39562/1.15637. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.37823/1.13576. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.39099/1.16483. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.38841/1.17719. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.37669/1.23654. Took 0.47 sec\n",
      "Epoch 76, Loss(train/val) 0.35823/1.20064. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.35937/1.27036. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.36695/1.23080. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.35873/1.13969. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.35691/1.25375. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.33783/1.29909. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.36218/1.22499. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.39554/1.24419. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.33306/1.31101. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.33254/1.33434. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.35505/1.35606. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.32870/1.25999. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.33225/1.33300. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.33131/1.38420. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.30992/1.30934. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.28996/1.34475. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.30733/1.39725. Took 0.43 sec\n",
      "Epoch 93, Loss(train/val) 0.29929/1.44571. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.27874/1.38766. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.27675/1.49215. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.26668/1.48545. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.29482/1.43387. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.33074/1.45860. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.29112/1.37113. Took 0.44 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.70391/0.69605. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69365/0.69154. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.68228/0.70417. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.68352/0.70298. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.67845/0.70362. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.67600/0.70595. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.67836/0.71034. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67091/0.71862. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.67261/0.71801. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.66808/0.71442. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66486/0.73693. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.66348/0.73242. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.65921/0.73129. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.65584/0.75193. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65309/0.77567. Took 0.47 sec\n",
      "Epoch 15, Loss(train/val) 0.64554/0.79434. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.64389/0.81492. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.64456/0.82390. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.63683/0.82276. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.62865/0.84373. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.63517/0.84921. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.62322/0.87622. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.61672/0.88123. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.61824/0.92014. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.61473/0.92546. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.61585/0.90142. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.60925/0.90927. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.59859/0.95405. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60338/0.93673. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.59219/0.94572. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.58110/0.96452. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.57761/0.93638. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.57414/0.94008. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.57131/0.90031. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.57480/0.95529. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.56143/0.92930. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.56199/0.94997. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.55313/0.97291. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.55250/0.96701. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.54364/0.99134. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.53823/1.00967. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.54471/1.02597. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.53014/1.05213. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.52131/1.01330. Took 0.47 sec\n",
      "Epoch 44, Loss(train/val) 0.52010/1.05504. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.52476/1.04186. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.51844/1.07827. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.51318/1.05237. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.50161/1.08858. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.50571/1.12136. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.49721/1.13040. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.49150/1.09629. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.50242/1.11119. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.49029/1.07110. Took 0.46 sec\n",
      "Epoch 54, Loss(train/val) 0.47835/1.12845. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.48165/1.12049. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.45963/1.17567. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.47664/1.10154. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.45910/1.12022. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.46257/1.14110. Took 0.46 sec\n",
      "Epoch 60, Loss(train/val) 0.45331/1.13200. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.43667/1.14997. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.44823/1.19473. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.43073/1.24502. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.41763/1.25928. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.41368/1.32138. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.45925/1.21129. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.41341/1.24573. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.40809/1.24429. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.39568/1.21777. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.39493/1.24544. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.40301/1.22323. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.42131/1.25374. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.39086/1.29418. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.36865/1.33651. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.37801/1.33417. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.35898/1.31029. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.36820/1.39341. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.37963/1.32168. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.41035/1.42447. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.36497/1.38472. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.35752/1.40874. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.37059/1.38197. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.35452/1.44518. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.33534/1.43429. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.32270/1.51497. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.33647/1.45506. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.30975/1.53818. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.32051/1.49729. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.32223/1.56334. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.28499/1.62285. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.31331/1.52240. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.30845/1.62050. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.30654/1.61437. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.31429/1.51031. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.29341/1.54466. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.30290/1.50544. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.28016/1.64913. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.27396/1.68001. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.26466/1.72190. Took 0.44 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69504/0.73375. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.68758/0.72511. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.68619/0.73741. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68300/0.73604. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68852/0.74214. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.67842/0.74071. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68200/0.75571. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67717/0.76539. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67826/0.77832. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67485/0.78186. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66823/0.79900. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66955/0.81478. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.66860/0.79375. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.66862/0.79696. Took 0.47 sec\n",
      "Epoch 14, Loss(train/val) 0.66336/0.79331. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.65450/0.80962. Took 0.47 sec\n",
      "Epoch 16, Loss(train/val) 0.65328/0.81287. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65586/0.78682. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65170/0.79012. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64129/0.79243. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64323/0.78562. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63123/0.81920. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.62981/0.82127. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62695/0.82939. Took 0.47 sec\n",
      "Epoch 24, Loss(train/val) 0.62351/0.83444. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.62375/0.83811. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.61535/0.85786. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.60829/0.85987. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60082/0.89904. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.59573/0.89390. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.57994/0.91882. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.59032/0.92313. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.58136/0.92484. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.57114/0.92510. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.56734/0.94838. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.56564/0.95843. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.55756/0.94221. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.54298/0.96038. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.54308/0.99564. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.54449/1.02161. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.54134/1.03681. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.53253/1.01902. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.51255/1.04828. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.51286/1.04738. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.51136/1.06965. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.51157/1.06750. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.50039/1.06128. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.47549/1.06945. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.49600/1.07024. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.47912/1.05665. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.47307/1.09294. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.47384/1.10750. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.45579/1.10372. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.44935/1.10408. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.44250/1.19174. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.42738/1.11454. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.43052/1.18859. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.42903/1.18055. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.42875/1.16805. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.40295/1.24023. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.40532/1.25284. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.39890/1.21853. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.38103/1.28590. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.38993/1.22210. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.38010/1.27242. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.37188/1.21045. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.36636/1.34525. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.36423/1.39284. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.37884/1.39938. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.35980/1.40511. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.33487/1.49807. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.33348/1.41154. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.34957/1.41026. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.33718/1.30721. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.32813/1.35771. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.33603/1.35108. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.31943/1.45174. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.31945/1.41861. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.30479/1.40451. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.34987/1.39927. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.33001/1.38556. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.30524/1.36668. Took 0.46 sec\n",
      "Epoch 82, Loss(train/val) 0.28684/1.46124. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.28575/1.44072. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.28529/1.51889. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.30547/1.35945. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.30261/1.37250. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.28192/1.41016. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.25321/1.48130. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.26652/1.58232. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.27600/1.61075. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.26204/1.49409. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.26217/1.56408. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.23761/1.55318. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.22438/1.71119. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.22893/1.63418. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.21788/1.71660. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.21969/1.76129. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.21923/1.82393. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.20084/1.78633. Took 0.45 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.71335/0.69802. Took 0.64 sec\n",
      "Epoch 1, Loss(train/val) 0.71758/0.69803. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.70962/0.68664. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.70093/0.68462. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.69713/0.68654. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.69380/0.68086. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.69256/0.68097. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68471/0.69316. Took 0.46 sec\n",
      "Epoch 8, Loss(train/val) 0.69355/0.69739. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.69093/0.69173. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68156/0.69642. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.68170/0.70029. Took 0.43 sec\n",
      "Epoch 12, Loss(train/val) 0.67312/0.70436. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67525/0.70289. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67983/0.69944. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66639/0.70289. Took 0.47 sec\n",
      "Epoch 16, Loss(train/val) 0.67633/0.69689. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66759/0.70277. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66915/0.69460. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.66071/0.70562. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.66228/0.70466. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.67483/0.68439. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.66136/0.68330. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66089/0.68403. Took 0.46 sec\n",
      "Epoch 24, Loss(train/val) 0.65288/0.69442. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66041/0.69891. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.65866/0.70894. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65104/0.69804. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64867/0.70096. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.64350/0.71055. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63219/0.71999. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63905/0.71901. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.63182/0.70692. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.63351/0.72241. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.62321/0.71243. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.62655/0.72867. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.62069/0.72465. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61160/0.73144. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.61326/0.74306. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60716/0.74405. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59681/0.73144. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.59830/0.72361. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59572/0.74825. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61328/0.73060. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.59007/0.75270. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59034/0.73998. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58839/0.74369. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.57757/0.75090. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57096/0.75799. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.58696/0.75978. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.58505/0.77244. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56370/0.78178. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.57236/0.77387. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.55498/0.76431. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.55979/0.76124. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.56540/0.81416. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.54939/0.76635. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.55644/0.79318. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.55201/0.79471. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.54185/0.80333. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.53673/0.81973. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.54715/0.83490. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.52577/0.80930. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.52084/0.81360. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.53311/0.80786. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.50934/0.84316. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.52696/0.81356. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.51145/0.82863. Took 0.47 sec\n",
      "Epoch 68, Loss(train/val) 0.49538/0.87911. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.49504/0.88360. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48918/0.89317. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.49729/0.92504. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.50454/0.90317. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48951/0.95250. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.47652/0.95880. Took 0.47 sec\n",
      "Epoch 75, Loss(train/val) 0.50338/0.93499. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.48916/0.93695. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.48825/0.97891. Took 0.47 sec\n",
      "Epoch 78, Loss(train/val) 0.50473/0.88182. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.47910/0.96966. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.47352/0.96091. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.47123/0.98944. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.46634/1.03152. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.44767/1.05589. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.43543/1.05109. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.45453/1.08719. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43089/1.08475. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.44271/1.13983. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.47017/1.02166. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.45576/1.03542. Took 0.46 sec\n",
      "Epoch 90, Loss(train/val) 0.43166/1.02713. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.42557/1.02845. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.41476/1.14559. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.40219/1.14354. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.41945/1.23348. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.41756/1.22348. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.41361/1.17726. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.39941/1.18191. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.38978/1.25859. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.42248/1.08194. Took 0.45 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.71206/0.68303. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.70588/0.69823. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.68522/0.69294. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69450/0.69935. Took 0.46 sec\n",
      "Epoch 4, Loss(train/val) 0.69600/0.71089. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68490/0.69933. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68872/0.71192. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67695/0.71340. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68173/0.70280. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67631/0.71773. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67728/0.71205. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66968/0.71549. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67022/0.70799. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66265/0.71500. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66966/0.72128. Took 0.43 sec\n",
      "Epoch 15, Loss(train/val) 0.66456/0.73783. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.66317/0.74359. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.65490/0.75021. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64758/0.76712. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.63854/0.78198. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64413/0.78907. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63731/0.80286. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.64046/0.81243. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.62746/0.82541. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62455/0.81504. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.62144/0.82211. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61669/0.81852. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.60899/0.83326. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.61302/0.82225. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.60497/0.81925. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61322/0.83352. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.58943/0.84477. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.60698/0.82902. Took 0.43 sec\n",
      "Epoch 33, Loss(train/val) 0.58281/0.84001. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.58263/0.84186. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58122/0.91369. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.58741/0.92939. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57806/0.95690. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.56671/0.96300. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.57036/0.99953. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.57081/1.00076. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.55921/1.03495. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56961/1.01231. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.56113/1.03805. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55694/1.02422. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55550/1.04154. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.53531/1.07185. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.53720/1.08685. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.54024/1.09867. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.52372/1.16387. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.53195/1.17175. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.53561/1.11769. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.51746/1.16819. Took 0.47 sec\n",
      "Epoch 53, Loss(train/val) 0.50408/1.19707. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.50644/1.17943. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.50530/1.22775. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.51072/1.17854. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.50351/1.21581. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.50332/1.18880. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.49170/1.26516. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.48073/1.20442. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.48895/1.23423. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.45512/1.27115. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.48286/1.27521. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.45840/1.24806. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.46630/1.31880. Took 0.43 sec\n",
      "Epoch 66, Loss(train/val) 0.46490/1.29820. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.46555/1.23831. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.46045/1.25802. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.46402/1.30856. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.44052/1.28070. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.43005/1.33321. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.45444/1.36043. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.44614/1.29687. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.41514/1.39453. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.42959/1.38337. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.41669/1.34798. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.40162/1.35754. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.40180/1.39032. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.43520/1.27026. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.42735/1.38494. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.41982/1.27167. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.41528/1.31622. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.40651/1.32076. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.39034/1.35396. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.39299/1.43795. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.37906/1.40111. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.37428/1.47404. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.37285/1.44929. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.34598/1.53539. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.37984/1.51729. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.37032/1.37535. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.38713/1.45969. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.36875/1.42503. Took 0.46 sec\n",
      "Epoch 94, Loss(train/val) 0.36926/1.53196. Took 0.43 sec\n",
      "Epoch 95, Loss(train/val) 0.35922/1.57234. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.36404/1.49311. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.34516/1.52455. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.32236/1.58277. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.34388/1.51377. Took 0.44 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.72449/0.73875. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.70984/0.74281. Took 0.46 sec\n",
      "Epoch 2, Loss(train/val) 0.70434/0.74868. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70446/0.73651. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.70380/0.73216. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.69151/0.71922. Took 0.48 sec\n",
      "Epoch 6, Loss(train/val) 0.68654/0.72120. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68297/0.71925. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68417/0.71058. Took 0.49 sec\n",
      "Epoch 9, Loss(train/val) 0.68417/0.71787. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68072/0.72258. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.67978/0.71165. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68511/0.72395. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67771/0.73565. Took 0.47 sec\n",
      "Epoch 14, Loss(train/val) 0.68743/0.72830. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67599/0.72909. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67265/0.73134. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.67263/0.73344. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66883/0.75197. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66200/0.76828. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65544/0.78755. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.65871/0.81210. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.66922/0.81020. Took 0.47 sec\n",
      "Epoch 23, Loss(train/val) 0.66273/0.78595. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65182/0.79451. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.64869/0.81964. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.64711/0.82159. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.64538/0.81549. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64393/0.81515. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.63420/0.82538. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.62977/0.84600. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.63864/0.84877. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.63587/0.85391. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.63106/0.86077. Took 0.47 sec\n",
      "Epoch 34, Loss(train/val) 0.62526/0.87428. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.62917/0.89699. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.61713/0.90998. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.61632/0.90711. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.61878/0.89912. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60354/0.93950. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60900/0.95143. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.59687/0.93363. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.59911/0.92531. Took 0.47 sec\n",
      "Epoch 43, Loss(train/val) 0.59271/0.94067. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59975/0.96985. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.58942/0.98096. Took 0.43 sec\n",
      "Epoch 46, Loss(train/val) 0.59161/0.99285. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58078/1.03839. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.55961/1.02248. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.57604/1.07115. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.57391/1.00856. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.55279/1.08863. Took 0.47 sec\n",
      "Epoch 52, Loss(train/val) 0.56575/1.06016. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.56352/1.10352. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54780/1.10433. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.55864/1.14166. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.54568/1.12977. Took 0.43 sec\n",
      "Epoch 57, Loss(train/val) 0.55395/1.10136. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.54861/1.16741. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53991/1.13432. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.54409/1.16627. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.52451/1.18948. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.53311/1.21767. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.52813/1.20624. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.52423/1.20581. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.52376/1.21171. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.50841/1.25119. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.49693/1.28401. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50314/1.24939. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.49469/1.26643. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49377/1.26897. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.49047/1.30002. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.50107/1.25084. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48896/1.33750. Took 0.43 sec\n",
      "Epoch 74, Loss(train/val) 0.46773/1.39452. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.46201/1.38566. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.45407/1.35550. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.46329/1.37112. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.44209/1.35444. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.46761/1.35839. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.44256/1.41179. Took 0.43 sec\n",
      "Epoch 81, Loss(train/val) 0.45105/1.38311. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.42453/1.41939. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.43821/1.50807. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.44421/1.42748. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.42191/1.54811. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.42374/1.58628. Took 0.43 sec\n",
      "Epoch 87, Loss(train/val) 0.40113/1.51797. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.41831/1.60524. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.39582/1.70482. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.43477/1.50602. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.41549/1.57069. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.40287/1.64236. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.37430/1.67495. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.36481/1.61955. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.37631/1.68503. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.36323/1.57209. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.38806/1.66671. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.35399/1.65172. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.36045/1.74957. Took 0.45 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70302/0.69539. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69466/0.70199. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69534/0.70195. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69363/0.70892. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69103/0.71605. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68781/0.72350. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68475/0.73166. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.68194/0.73568. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68169/0.75622. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68122/0.74261. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67473/0.74337. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67504/0.75523. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66931/0.75918. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66802/0.76942. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.66029/0.77924. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.66085/0.79046. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65182/0.79111. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.65452/0.80641. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65298/0.81667. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64678/0.82463. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64498/0.83891. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63646/0.83556. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64308/0.82549. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.63896/0.82765. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.63216/0.82346. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.62763/0.83557. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.62410/0.84443. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62358/0.84542. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.62500/0.85451. Took 0.47 sec\n",
      "Epoch 29, Loss(train/val) 0.62238/0.85531. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61613/0.86194. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.60549/0.87419. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61072/0.87406. Took 0.44 sec\n",
      "Epoch 33, Loss(train/val) 0.59617/0.90651. Took 0.46 sec\n",
      "Epoch 34, Loss(train/val) 0.58715/0.90718. Took 0.43 sec\n",
      "Epoch 35, Loss(train/val) 0.58701/0.92590. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.59051/0.92041. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.58387/0.95532. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.56778/0.98982. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.57698/1.00251. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.56470/0.96714. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.58424/0.97657. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56888/1.00037. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.55202/1.03832. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55872/0.99868. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.55246/1.01275. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.54522/1.03742. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.54893/1.10753. Took 0.46 sec\n",
      "Epoch 48, Loss(train/val) 0.54166/1.05023. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.54613/1.03609. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.53730/1.08905. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.52801/1.09913. Took 0.46 sec\n",
      "Epoch 52, Loss(train/val) 0.53037/1.10009. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.51236/1.12739. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52020/1.12099. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.52421/1.09921. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.49760/1.12517. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.51995/1.07601. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.52400/1.09327. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.51144/1.11047. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.50112/1.13760. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.49223/1.12498. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50764/1.08982. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.49532/1.13363. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.48452/1.10640. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.48686/1.13074. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.47948/1.15178. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.48028/1.16124. Took 0.47 sec\n",
      "Epoch 68, Loss(train/val) 0.48018/1.18353. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.47067/1.14852. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.46517/1.17197. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.44687/1.24133. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.44378/1.21762. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.44679/1.17450. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.44060/1.24432. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.42981/1.22036. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46079/1.17128. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.43432/1.25157. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.41918/1.25993. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.43364/1.20068. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.41118/1.33703. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.42675/1.34053. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.42703/1.24854. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.43078/1.18489. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.42010/1.24525. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.41941/1.23847. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.39265/1.27392. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.38686/1.27722. Took 0.47 sec\n",
      "Epoch 88, Loss(train/val) 0.41572/1.24623. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.40557/1.25352. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.44547/1.20488. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.39705/1.20249. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.38897/1.29981. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.38735/1.28004. Took 0.43 sec\n",
      "Epoch 94, Loss(train/val) 0.38263/1.33875. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.36827/1.32263. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.36408/1.38874. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.34084/1.35350. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.39057/1.36006. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.34968/1.43818. Took 0.45 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.71526/0.70872. Took 0.52 sec\n",
      "Epoch 1, Loss(train/val) 0.70409/0.72003. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.70731/0.71104. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.70508/0.70417. Took 0.49 sec\n",
      "Epoch 4, Loss(train/val) 0.69590/0.71651. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.69075/0.71783. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.69883/0.72337. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.69457/0.71114. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68604/0.71711. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.68486/0.71469. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67897/0.71427. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.67528/0.72391. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67821/0.74776. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.67832/0.72558. Took 0.46 sec\n",
      "Epoch 14, Loss(train/val) 0.68072/0.73293. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.68441/0.72884. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.67732/0.72687. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66984/0.74905. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.67644/0.72351. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.67275/0.73906. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66827/0.75277. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.67215/0.73475. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66290/0.74698. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.66802/0.73527. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.65968/0.73945. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.66513/0.74857. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.65724/0.74889. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.65749/0.75573. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.65120/0.76822. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.65096/0.76491. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63152/0.80635. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.63809/0.82208. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64150/0.84934. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.63134/0.85057. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62932/0.85334. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.62592/0.84869. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.61719/0.86921. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.61429/0.87889. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.61652/0.88894. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.61786/0.88499. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.60258/0.91558. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.60579/0.88300. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60635/0.90892. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.59846/0.90925. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.58769/0.90512. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.59805/0.92237. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.59081/0.90612. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.58728/0.89085. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56473/0.94681. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.58270/0.91841. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.57092/0.94086. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.56211/0.91030. Took 0.47 sec\n",
      "Epoch 52, Loss(train/val) 0.55337/0.94186. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.57599/0.91757. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.55293/0.92410. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.54104/0.98925. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.53864/0.99319. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.52373/1.01783. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.54494/1.01326. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.54328/0.99383. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.53295/1.01845. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.52542/1.06079. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.51136/1.04432. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.52411/1.05801. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.52308/1.06097. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.51988/1.09498. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.51238/1.08098. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.49735/1.14052. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.48774/1.17632. Took 0.47 sec\n",
      "Epoch 69, Loss(train/val) 0.49797/1.14870. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.47764/1.15636. Took 0.44 sec\n",
      "Epoch 71, Loss(train/val) 0.47990/1.12486. Took 0.46 sec\n",
      "Epoch 72, Loss(train/val) 0.49227/1.12622. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.48011/1.15024. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.46988/1.19697. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.47716/1.16858. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46812/1.19326. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.47705/1.18092. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.46137/1.17741. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.46184/1.21481. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.47722/1.19237. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.45508/1.14945. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.43938/1.20756. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.43519/1.21904. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.43467/1.21845. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.44426/1.16435. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.41744/1.28069. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43031/1.15519. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.44642/1.15564. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.42581/1.20671. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.41586/1.21245. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.40697/1.28373. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.41561/1.26685. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.41439/1.18422. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.41210/1.16118. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.39053/1.27117. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.38611/1.28104. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.38103/1.22609. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.37368/1.31037. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.38794/1.26476. Took 0.45 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69385/0.69022. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.68478/0.68564. Took 0.49 sec\n",
      "Epoch 2, Loss(train/val) 0.68684/0.68251. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.68166/0.68032. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.68044/0.67808. Took 0.48 sec\n",
      "Epoch 5, Loss(train/val) 0.67848/0.67379. Took 0.47 sec\n",
      "Epoch 6, Loss(train/val) 0.68040/0.66762. Took 0.47 sec\n",
      "Epoch 7, Loss(train/val) 0.67609/0.66376. Took 0.52 sec\n",
      "Epoch 8, Loss(train/val) 0.67364/0.65762. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.67004/0.66370. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.66656/0.66784. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66735/0.67233. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66886/0.67473. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66285/0.67198. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66252/0.67030. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65828/0.67113. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65086/0.67364. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65470/0.67651. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64782/0.66670. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.64510/0.67078. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.64209/0.66790. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.63213/0.67098. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63144/0.68337. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.63048/0.68255. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62558/0.67426. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.62485/0.71029. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62333/0.70774. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.61072/0.71244. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61712/0.70368. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.61143/0.70566. Took 0.47 sec\n",
      "Epoch 30, Loss(train/val) 0.60570/0.68939. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.60859/0.71130. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59618/0.71751. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.59541/0.71062. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.59516/0.71988. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.58116/0.76111. Took 0.46 sec\n",
      "Epoch 36, Loss(train/val) 0.57919/0.79350. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.57936/0.79538. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57703/0.80333. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.56931/0.78165. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56630/0.79465. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.56622/0.79486. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55019/0.80331. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.55213/0.80164. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.53549/0.83468. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.53498/0.82049. Took 0.46 sec\n",
      "Epoch 46, Loss(train/val) 0.53168/0.81284. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.52576/0.85592. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.51992/0.86664. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.52179/0.88657. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.51940/0.85503. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.51238/0.85348. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.51193/0.89405. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.49500/0.92245. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.49974/0.89456. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.48601/0.88210. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.47733/0.92053. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.47497/0.93719. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.47852/0.91772. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.47384/0.92497. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.46953/0.90533. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.45952/0.98974. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.47943/0.94889. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.46509/1.02452. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.45379/1.00912. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.45695/1.03359. Took 0.46 sec\n",
      "Epoch 66, Loss(train/val) 0.45807/0.97124. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.43426/0.99700. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.44241/1.02462. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.43927/1.11363. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.43562/1.09843. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.45099/1.02067. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.41490/1.05338. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.41802/1.02774. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.41154/1.07240. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.40481/1.04504. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.38484/1.07121. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.38582/1.11433. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.40283/1.10813. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.38504/1.04834. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.36941/1.04973. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.37097/1.03783. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.37705/1.10374. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.36719/1.05465. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.38508/1.09682. Took 0.44 sec\n",
      "Epoch 85, Loss(train/val) 0.36759/1.08628. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.35480/1.07833. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.35980/1.07763. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.34488/1.16561. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.36525/1.03724. Took 0.47 sec\n",
      "Epoch 90, Loss(train/val) 0.35420/1.10946. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.36439/1.19037. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.35241/1.14981. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.32822/1.15544. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.33148/1.17741. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.32774/1.15913. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.30123/1.14596. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.32597/1.17272. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.31645/1.23054. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.29514/1.24370. Took 0.45 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69667/0.73299. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69514/0.73951. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.69159/0.73968. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69300/0.74223. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68398/0.73295. Took 0.48 sec\n",
      "Epoch 5, Loss(train/val) 0.68751/0.74451. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68490/0.75421. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.67911/0.76738. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67840/0.75638. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.66732/0.75740. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67374/0.75265. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66484/0.74740. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.66917/0.75630. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66953/0.74537. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65964/0.74892. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.65929/0.76515. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.66225/0.77286. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65150/0.77408. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.64134/0.79698. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65488/0.76815. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64519/0.80184. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.64238/0.78947. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64038/0.80762. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.64094/0.81869. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.63738/0.81605. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.63221/0.82846. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63302/0.82699. Took 0.44 sec\n",
      "Epoch 27, Loss(train/val) 0.63003/0.84151. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.63172/0.85462. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.63393/0.85657. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.63751/0.86228. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.62969/0.86925. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.63040/0.86580. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.62519/0.88455. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.62179/0.87861. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.62302/0.86822. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.60952/0.87350. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.61102/0.88101. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.61108/0.89504. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.60035/0.88966. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.60083/0.87614. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.58339/0.89366. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58897/0.91309. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.59422/0.92489. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.58382/0.92719. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.58294/0.94915. Took 0.45 sec\n",
      "Epoch 46, Loss(train/val) 0.58267/0.90278. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.56675/0.93777. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57672/0.95029. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.56077/0.95774. Took 0.46 sec\n",
      "Epoch 50, Loss(train/val) 0.55925/0.94471. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.56348/0.94671. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.56038/0.94301. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.56013/0.91495. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.54393/0.91969. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.54264/0.94187. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.54312/0.93912. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.54207/0.93143. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.53534/0.96464. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.53834/0.89529. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52850/0.93723. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.51879/0.97304. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.52819/0.90704. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.51511/0.91087. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.51093/0.89441. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.50682/0.95211. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.49612/0.97936. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.49166/0.96924. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49502/0.98192. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.50111/0.96039. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49517/0.97884. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.49679/0.96903. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.48003/0.97937. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.48341/1.00047. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.49373/0.95644. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.48009/0.94016. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46258/0.96446. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.47027/0.98776. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.46561/0.99796. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.47250/0.95450. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.45100/0.98770. Took 0.44 sec\n",
      "Epoch 81, Loss(train/val) 0.46422/0.98170. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.46880/1.01985. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.46681/0.94338. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.44259/1.02096. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.45802/1.03162. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.44909/1.02391. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.43438/1.09907. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.42664/1.06415. Took 0.43 sec\n",
      "Epoch 89, Loss(train/val) 0.42511/1.03795. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.43572/1.09689. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.41884/1.07673. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.41856/1.07984. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.40374/1.12668. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.39585/1.12092. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.41787/1.13051. Took 0.46 sec\n",
      "Epoch 96, Loss(train/val) 0.41132/1.22781. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.41615/1.13038. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.42534/1.19840. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.39992/1.20959. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.70037/0.73946. Took 0.47 sec\n",
      "Epoch 1, Loss(train/val) 0.69577/0.72171. Took 0.49 sec\n",
      "Epoch 2, Loss(train/val) 0.68675/0.72713. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.68416/0.72398. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68304/0.72718. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68163/0.71348. Took 0.50 sec\n",
      "Epoch 6, Loss(train/val) 0.68241/0.70939. Took 0.48 sec\n",
      "Epoch 7, Loss(train/val) 0.68101/0.72710. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67701/0.71777. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.68038/0.71844. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67294/0.72321. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.66953/0.73924. Took 0.46 sec\n",
      "Epoch 12, Loss(train/val) 0.66656/0.74446. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66589/0.75491. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66730/0.75683. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.65409/0.75858. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65124/0.76592. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.65034/0.78175. Took 0.46 sec\n",
      "Epoch 18, Loss(train/val) 0.64130/0.79949. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.62988/0.82770. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.63257/0.81934. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.62967/0.81617. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.62599/0.83635. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.61285/0.84168. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.60892/0.84154. Took 0.44 sec\n",
      "Epoch 25, Loss(train/val) 0.60392/0.90137. Took 0.46 sec\n",
      "Epoch 26, Loss(train/val) 0.60181/0.87800. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.59181/0.87897. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.58729/0.91703. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.58614/0.94168. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.59269/0.90826. Took 0.44 sec\n",
      "Epoch 31, Loss(train/val) 0.57976/0.89861. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.56815/0.90626. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.58456/0.90668. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.56523/0.85834. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.56546/0.88339. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.56389/0.86958. Took 0.44 sec\n",
      "Epoch 37, Loss(train/val) 0.54315/0.91145. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.53663/0.95758. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.52194/0.97398. Took 0.46 sec\n",
      "Epoch 40, Loss(train/val) 0.52618/0.95677. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.51000/0.96014. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.52468/0.92693. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.51140/0.97671. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.51095/0.95727. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.51304/0.93053. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.50560/0.91400. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.49318/0.89111. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.48881/0.90280. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.48614/0.92185. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.48679/1.00354. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.47992/0.88534. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.47208/0.99177. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.47716/1.01524. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.47564/1.00529. Took 0.44 sec\n",
      "Epoch 55, Loss(train/val) 0.46908/0.95733. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.45387/0.92941. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.43521/1.06476. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.44495/0.96136. Took 0.47 sec\n",
      "Epoch 59, Loss(train/val) 0.43831/1.03124. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.44323/1.00137. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.43059/0.97530. Took 0.45 sec\n",
      "Epoch 62, Loss(train/val) 0.41980/1.05857. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.40183/1.04145. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.38794/1.05504. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.41250/1.12296. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.38493/1.11443. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.40112/1.13347. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.39617/1.06705. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.39972/1.13738. Took 0.46 sec\n",
      "Epoch 70, Loss(train/val) 0.38775/1.09106. Took 0.43 sec\n",
      "Epoch 71, Loss(train/val) 0.36334/1.10707. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.36050/1.14553. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.36583/1.15685. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.35038/1.20530. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.35056/1.19730. Took 0.46 sec\n",
      "Epoch 76, Loss(train/val) 0.35445/1.19430. Took 0.44 sec\n",
      "Epoch 77, Loss(train/val) 0.34434/1.23625. Took 0.46 sec\n",
      "Epoch 78, Loss(train/val) 0.31663/1.31869. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.34223/1.22413. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.32252/1.20892. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.31844/1.25244. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.33878/1.17829. Took 0.44 sec\n",
      "Epoch 83, Loss(train/val) 0.32236/1.23873. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.30159/1.21198. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.31519/1.19947. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.30999/1.19541. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.30338/1.25081. Took 0.46 sec\n",
      "Epoch 88, Loss(train/val) 0.27186/1.29860. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.28027/1.33122. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.28511/1.42725. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.28070/1.34428. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.30505/1.43526. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.30974/1.26595. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.28876/1.47645. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.25996/1.41892. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.26340/1.40459. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.23637/1.41600. Took 0.46 sec\n",
      "Epoch 98, Loss(train/val) 0.25376/1.39662. Took 0.44 sec\n",
      "Epoch 99, Loss(train/val) 0.25113/1.55308. Took 0.44 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.70582/0.72301. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69287/0.72908. Took 0.43 sec\n",
      "Epoch 2, Loss(train/val) 0.69065/0.71620. Took 0.47 sec\n",
      "Epoch 3, Loss(train/val) 0.69289/0.69950. Took 0.49 sec\n",
      "Epoch 4, Loss(train/val) 0.69393/0.69097. Took 0.49 sec\n",
      "Epoch 5, Loss(train/val) 0.68761/0.69718. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68798/0.71940. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.67884/0.72929. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.68100/0.74395. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.67147/0.75613. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67038/0.76720. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66572/0.78592. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.65931/0.79830. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.65972/0.81459. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65385/0.82363. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.65020/0.82618. Took 0.46 sec\n",
      "Epoch 16, Loss(train/val) 0.64201/0.82436. Took 0.43 sec\n",
      "Epoch 17, Loss(train/val) 0.64353/0.81991. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64491/0.82484. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.63989/0.84121. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.62891/0.81634. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.62672/0.81292. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.61945/0.81647. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.61989/0.80836. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.61406/0.80573. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.61702/0.80755. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61127/0.80928. Took 0.47 sec\n",
      "Epoch 27, Loss(train/val) 0.60653/0.82060. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60367/0.81919. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.60140/0.80062. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.59327/0.84353. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.58882/0.83364. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.58951/0.84423. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.56992/0.86927. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.57229/0.90145. Took 0.44 sec\n",
      "Epoch 35, Loss(train/val) 0.57490/0.87377. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.57972/0.87719. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.56708/0.92951. Took 0.46 sec\n",
      "Epoch 38, Loss(train/val) 0.55677/0.90379. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.55737/0.93645. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.55519/0.95082. Took 0.44 sec\n",
      "Epoch 41, Loss(train/val) 0.53703/0.98927. Took 0.46 sec\n",
      "Epoch 42, Loss(train/val) 0.54696/0.98027. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.55028/0.96486. Took 0.43 sec\n",
      "Epoch 44, Loss(train/val) 0.53546/0.98517. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.52899/0.98441. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.53372/1.01844. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.52092/1.06193. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.51371/1.04725. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.51929/1.06152. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.51033/1.03515. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.49363/1.13769. Took 0.43 sec\n",
      "Epoch 52, Loss(train/val) 0.51167/1.16485. Took 0.46 sec\n",
      "Epoch 53, Loss(train/val) 0.49117/1.13373. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.48680/1.22188. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.49203/1.19444. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.48871/1.14555. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.47401/1.18816. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.47379/1.15772. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.47071/1.12975. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.47313/1.17300. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.45511/1.19675. Took 0.46 sec\n",
      "Epoch 62, Loss(train/val) 0.44765/1.29324. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.45619/1.23138. Took 0.46 sec\n",
      "Epoch 64, Loss(train/val) 0.43937/1.34986. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.43545/1.22698. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.42840/1.21386. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.43178/1.27386. Took 0.46 sec\n",
      "Epoch 68, Loss(train/val) 0.42539/1.36682. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.42025/1.20507. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.42252/1.25002. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.42826/1.30652. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.40594/1.38938. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.41160/1.34624. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.38487/1.37539. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.40546/1.37381. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.39746/1.37141. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.38725/1.49677. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.39222/1.28866. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.39332/1.38021. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.39777/1.37918. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.37604/1.32657. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.36636/1.34796. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.37269/1.30723. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.38404/1.35526. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.35963/1.49170. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.37721/1.37900. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.36290/1.55587. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.33190/1.55268. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.33950/1.56340. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.33854/1.51232. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.35561/1.60294. Took 0.46 sec\n",
      "Epoch 92, Loss(train/val) 0.34509/1.55449. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.33686/1.51622. Took 0.45 sec\n",
      "Epoch 94, Loss(train/val) 0.34384/1.49697. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.33463/1.48872. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.33954/1.40034. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.31469/1.40271. Took 0.43 sec\n",
      "Epoch 98, Loss(train/val) 0.30831/1.60730. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.30929/1.47707. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69692/0.70798. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69462/0.70657. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69047/0.70801. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.68746/0.70647. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.68585/0.71099. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.68517/0.70774. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68047/0.70520. Took 0.50 sec\n",
      "Epoch 7, Loss(train/val) 0.67979/0.71319. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67658/0.72540. Took 0.44 sec\n",
      "Epoch 9, Loss(train/val) 0.67223/0.72229. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.66841/0.74202. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66633/0.74915. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66368/0.75032. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.65830/0.76629. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.65308/0.78022. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.64870/0.78652. Took 0.47 sec\n",
      "Epoch 16, Loss(train/val) 0.63884/0.81971. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.63341/0.82916. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.62943/0.85739. Took 0.44 sec\n",
      "Epoch 19, Loss(train/val) 0.62869/0.83467. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.62144/0.83742. Took 0.44 sec\n",
      "Epoch 21, Loss(train/val) 0.61808/0.84826. Took 0.45 sec\n",
      "Epoch 22, Loss(train/val) 0.62128/0.86206. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.61355/0.85945. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.60783/0.88973. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.59884/0.86751. Took 0.47 sec\n",
      "Epoch 26, Loss(train/val) 0.59007/0.89078. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.58407/0.88745. Took 0.45 sec\n",
      "Epoch 28, Loss(train/val) 0.58875/0.87191. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.56955/0.88670. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.57628/0.90038. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.56183/0.90057. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.57341/0.88229. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.56313/0.88429. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.54644/0.87908. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.54037/0.88658. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.53319/0.91327. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.52880/0.91460. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.52410/0.90126. Took 0.44 sec\n",
      "Epoch 39, Loss(train/val) 0.50946/0.91975. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.50898/0.98343. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.50587/0.91434. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.49336/0.95268. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.48095/1.00022. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.48680/0.98924. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.46746/1.00116. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.47358/1.04801. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.45638/1.02468. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.45023/1.01984. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.43584/1.06654. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.42797/1.09449. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.42245/1.09933. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.41561/1.06043. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.41256/1.05896. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.38688/1.07909. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.39387/1.12465. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.39432/1.13737. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.37594/1.11889. Took 0.46 sec\n",
      "Epoch 58, Loss(train/val) 0.38768/1.08316. Took 0.44 sec\n",
      "Epoch 59, Loss(train/val) 0.38266/1.14728. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.36482/1.15789. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.37171/1.16205. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.34812/1.14901. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.34400/1.11236. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.33522/1.17358. Took 0.47 sec\n",
      "Epoch 65, Loss(train/val) 0.37320/1.14215. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.33853/1.15636. Took 0.47 sec\n",
      "Epoch 67, Loss(train/val) 0.33302/1.13479. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.32149/1.21307. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.30732/1.20990. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.31923/1.22363. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.33526/1.19438. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.31429/1.19655. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.29119/1.19875. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.28104/1.26756. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.29019/1.34662. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.29441/1.32702. Took 0.47 sec\n",
      "Epoch 77, Loss(train/val) 0.28594/1.28202. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.27551/1.30822. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.24790/1.31674. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.25673/1.30409. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.25849/1.39349. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.23315/1.47467. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.26054/1.40483. Took 0.45 sec\n",
      "Epoch 84, Loss(train/val) 0.24261/1.41378. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.27489/1.45344. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.25821/1.45969. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.23755/1.39398. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.23256/1.42561. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.22379/1.54505. Took 0.45 sec\n",
      "Epoch 90, Loss(train/val) 0.21716/1.45282. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.23344/1.59979. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.20096/1.54999. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.18878/1.59387. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.19266/1.67431. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.19166/1.61492. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.17197/1.59068. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.19545/1.68694. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.20224/1.62722. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.18358/1.52931. Took 0.45 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.70977/0.68907. Took 0.60 sec\n",
      "Epoch 1, Loss(train/val) 0.70141/0.69607. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69466/0.70187. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.69075/0.70999. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68697/0.70967. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68996/0.70189. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68428/0.72617. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.68389/0.72957. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68134/0.74008. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.67805/0.73953. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67403/0.75279. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.67024/0.75193. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66930/0.76707. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.66398/0.76744. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66427/0.77499. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.66013/0.77900. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65929/0.79650. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65616/0.80520. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64869/0.80061. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.64123/0.81665. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.64940/0.81830. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.63922/0.81929. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64224/0.81415. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.63964/0.83829. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62767/0.85296. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.61957/0.86316. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.63133/0.84798. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.62162/0.86150. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.62694/0.85880. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.61203/0.85193. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.61279/0.85583. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.59604/0.87125. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.59974/0.86329. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.60071/0.85295. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.58877/0.88113. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.58064/0.87793. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58264/0.91068. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.56838/0.94954. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.56020/0.93257. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.56745/0.86926. Took 0.43 sec\n",
      "Epoch 40, Loss(train/val) 0.55381/0.91746. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.54998/0.90771. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.55396/0.89461. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.54234/0.87715. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.53695/0.94727. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.54669/0.96244. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.53725/0.90200. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.54890/0.93105. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.52237/0.98495. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.51985/0.93798. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.50724/0.97750. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.51384/1.03457. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.49785/1.06862. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.51679/0.94177. Took 0.45 sec\n",
      "Epoch 54, Loss(train/val) 0.48480/1.02025. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.47918/0.96743. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.47094/1.07396. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.46084/1.05040. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.45808/1.08150. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.45987/1.16579. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.46762/1.16704. Took 0.47 sec\n",
      "Epoch 61, Loss(train/val) 0.45311/1.11348. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.44364/1.17369. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.43214/1.27023. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.45044/1.26486. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.41018/1.26899. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.40815/1.35396. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.42741/1.37392. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.42789/1.29006. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.41451/1.27712. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.40343/1.32820. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.39878/1.38871. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.42189/1.39589. Took 0.44 sec\n",
      "Epoch 73, Loss(train/val) 0.41045/1.20008. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.45448/1.24711. Took 0.47 sec\n",
      "Epoch 75, Loss(train/val) 0.39180/1.38442. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.38242/1.42158. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.39362/1.33339. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.36376/1.46853. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.38808/1.42732. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.39927/1.52312. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.40199/1.48637. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.35428/1.42774. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.35432/1.47021. Took 0.43 sec\n",
      "Epoch 84, Loss(train/val) 0.38451/1.41517. Took 0.47 sec\n",
      "Epoch 85, Loss(train/val) 0.38398/1.44198. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.33817/1.42073. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.33115/1.52003. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.30996/1.58524. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.32849/1.60798. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.38027/1.57686. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.32876/1.55375. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.32308/1.49579. Took 0.47 sec\n",
      "Epoch 93, Loss(train/val) 0.31072/1.56444. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.31418/1.60307. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.29446/1.74692. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.28564/1.67629. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.27462/1.69110. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.29035/1.68737. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.32929/1.71768. Took 0.44 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.71180/0.70394. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.70641/0.70656. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69407/0.71177. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.68842/0.73474. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69108/0.71645. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68496/0.70668. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.67797/0.70628. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68752/0.72421. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68178/0.73678. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.68010/0.73315. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67896/0.76285. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67330/0.75695. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67141/0.77387. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.66740/0.77460. Took 0.45 sec\n",
      "Epoch 14, Loss(train/val) 0.66917/0.76615. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67635/0.76289. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67287/0.76480. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.66631/0.77642. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66631/0.78564. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.66274/0.81130. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65450/0.80697. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66747/0.80756. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65512/0.82185. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.65357/0.83418. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65331/0.84419. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.66566/0.83962. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65173/0.85113. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.63843/0.87392. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63567/0.91445. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.64095/0.90592. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63919/0.90928. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.63323/0.89712. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.66183/0.85579. Took 0.47 sec\n",
      "Epoch 33, Loss(train/val) 0.64192/0.85363. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.63157/0.90347. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.62906/0.89694. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62222/0.92691. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.62974/0.87851. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.63166/0.89733. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.61758/0.91209. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.62336/0.91408. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.61581/0.90894. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.60525/0.93610. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.60520/0.95517. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.60749/0.95945. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.59908/0.95928. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59310/0.96635. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.59103/0.99975. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.60053/0.99280. Took 0.47 sec\n",
      "Epoch 49, Loss(train/val) 0.60540/0.99206. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.58007/1.04171. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.58038/1.04668. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.58717/1.07201. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.57192/1.10221. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.58040/1.08076. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.56657/1.07635. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.57229/1.08916. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.55532/1.11309. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.55601/1.11404. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.55360/1.13510. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.54586/1.09334. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.53638/1.12000. Took 0.43 sec\n",
      "Epoch 62, Loss(train/val) 0.55157/1.15178. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.53979/1.11766. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.52231/1.15252. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.51365/1.19153. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.53810/1.11418. Took 0.47 sec\n",
      "Epoch 67, Loss(train/val) 0.51539/1.15577. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.51040/1.20551. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.49802/1.21033. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.51213/1.21460. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.48410/1.29954. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.49483/1.30618. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.49100/1.24791. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.51200/1.16402. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.49050/1.22720. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.49961/1.17678. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.47477/1.19875. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.47279/1.15819. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.46527/1.23326. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.47165/1.25360. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.46733/1.22646. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.45288/1.23950. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.46908/1.24622. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.45467/1.22586. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.45759/1.28126. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.43966/1.18126. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.46920/1.26592. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.44346/1.19915. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.43802/1.17119. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.45105/1.17712. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.43604/1.12827. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.42322/1.16657. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.42404/1.25965. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.41980/1.22889. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.41114/1.21860. Took 0.43 sec\n",
      "Epoch 96, Loss(train/val) 0.40270/1.24056. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.39025/1.19821. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.40783/1.29555. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.42182/1.21238. Took 0.44 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.70589/0.70265. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69319/0.70539. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69593/0.70856. Took 0.44 sec\n",
      "Epoch 3, Loss(train/val) 0.69365/0.71405. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68818/0.72767. Took 0.44 sec\n",
      "Epoch 5, Loss(train/val) 0.68897/0.73234. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68547/0.72829. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68833/0.72899. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68331/0.72014. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.67921/0.72151. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67591/0.73394. Took 0.44 sec\n",
      "Epoch 11, Loss(train/val) 0.67816/0.72813. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67753/0.73904. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67439/0.72607. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66999/0.73820. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.67784/0.74309. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67080/0.74431. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.66765/0.77084. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66556/0.76939. Took 0.47 sec\n",
      "Epoch 19, Loss(train/val) 0.65568/0.80896. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66836/0.80125. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.66239/0.81547. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65953/0.81834. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.65772/0.82046. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.65833/0.82601. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.65784/0.82943. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.65406/0.81830. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63935/0.82812. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.64359/0.84725. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.64044/0.86203. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.63578/0.87628. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.63514/0.87109. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.62935/0.86172. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.62479/0.88002. Took 0.48 sec\n",
      "Epoch 34, Loss(train/val) 0.61432/0.88555. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.62052/0.90121. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61355/0.86545. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60349/0.90772. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60211/0.96153. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.60241/0.88703. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.60300/0.91431. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.58983/0.90976. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58624/0.95236. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.59507/0.92337. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.58511/0.94200. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.59934/0.83857. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.58002/0.85950. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.57159/0.88473. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.57881/0.91486. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.56729/0.90051. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.55821/0.93824. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.56608/0.89407. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.55519/0.92571. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.54336/0.93974. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53788/0.98826. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.54003/0.96955. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.55009/0.91505. Took 0.47 sec\n",
      "Epoch 57, Loss(train/val) 0.55213/0.89238. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54079/0.87843. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.52307/1.00007. Took 0.43 sec\n",
      "Epoch 60, Loss(train/val) 0.52282/0.94484. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.50663/1.03657. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.50389/0.93623. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.51593/0.95625. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50532/1.03786. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.52955/0.96222. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.52371/0.91845. Took 0.44 sec\n",
      "Epoch 67, Loss(train/val) 0.51316/0.96949. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.49983/1.03676. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.48990/1.01163. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.48350/1.00669. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.46732/1.08114. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.45911/1.10676. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.47090/1.10230. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.46509/0.98990. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.47935/1.12155. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.44965/1.07133. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.45775/1.12676. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.47319/1.03832. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.46339/1.09692. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.44519/1.12504. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.42906/1.12735. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.45776/1.00450. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.43351/1.11643. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.41888/1.17516. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.42663/1.25465. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.42935/1.09519. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.45245/1.15797. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.43129/1.16774. Took 0.47 sec\n",
      "Epoch 89, Loss(train/val) 0.41058/1.21184. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.39282/1.17263. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.38871/1.19691. Took 0.45 sec\n",
      "Epoch 92, Loss(train/val) 0.36532/1.17648. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.38628/1.28233. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.38786/1.23817. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.38267/1.25381. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.38783/1.28643. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.41148/1.33247. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.40223/1.23435. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.38343/1.24051. Took 0.44 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69825/0.68709. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69467/0.69366. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.69314/0.69583. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.69031/0.69995. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68804/0.70304. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.68347/0.70633. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68129/0.72662. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68061/0.73174. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.67642/0.74845. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.67377/0.75870. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67159/0.75975. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.66753/0.77483. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67442/0.78276. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.66247/0.78616. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66355/0.80491. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66003/0.83410. Took 0.45 sec\n",
      "Epoch 16, Loss(train/val) 0.65627/0.81841. Took 0.46 sec\n",
      "Epoch 17, Loss(train/val) 0.65241/0.82596. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.65119/0.81849. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.64936/0.83194. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65037/0.81767. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.63835/0.85657. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.63862/0.83884. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.63361/0.86012. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62252/0.83615. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.61303/0.86092. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.60921/0.87043. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.60416/0.88394. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.60955/0.85859. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.59996/0.85209. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.58338/0.84369. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.58193/0.87515. Took 0.45 sec\n",
      "Epoch 32, Loss(train/val) 0.57590/0.82866. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.57517/0.81412. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.56772/0.86495. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.56267/0.86530. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.56985/0.85428. Took 0.47 sec\n",
      "Epoch 37, Loss(train/val) 0.55138/0.86326. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.54472/0.87194. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.54394/0.87304. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.54314/0.88583. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.51812/0.86901. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.52131/0.89069. Took 0.46 sec\n",
      "Epoch 43, Loss(train/val) 0.51667/0.93753. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.49786/0.90434. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.51853/0.90929. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.49648/0.91792. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.50014/0.95874. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.49318/0.95941. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.48332/1.05906. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.47528/0.94816. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.47055/0.97862. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.45921/1.02212. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.44357/1.04935. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.46603/0.98616. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.46434/0.99390. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.42204/1.00458. Took 0.46 sec\n",
      "Epoch 57, Loss(train/val) 0.45307/1.10934. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.44262/1.02663. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.43593/0.98215. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.41179/0.99191. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.42254/1.08721. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.41753/1.06684. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.40318/1.06821. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.39876/1.02642. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.39616/1.08005. Took 0.45 sec\n",
      "Epoch 66, Loss(train/val) 0.40090/1.01139. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.43511/1.03657. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.39113/1.05047. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.36403/1.02200. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.35718/1.16742. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.38397/1.06217. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.37909/1.07469. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.37228/1.14415. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.36520/1.14111. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.35077/1.18609. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.33932/1.17392. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.35741/1.18467. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.36671/1.12383. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.36449/1.21433. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.34112/1.31474. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.33457/1.20483. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.32037/1.12823. Took 0.47 sec\n",
      "Epoch 83, Loss(train/val) 0.31388/1.18421. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.32129/1.24532. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.32090/1.22572. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.32679/1.35221. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.30795/1.23249. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.28180/1.34771. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.30397/1.37384. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.32572/1.37916. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.30794/1.35970. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.29639/1.32384. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.33040/1.49788. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.27989/1.43511. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.25455/1.32821. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.25648/1.35704. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.24333/1.44445. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.27697/1.46238. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.25938/1.41483. Took 0.44 sec\n",
      "ACC: 0.3958333333333333\n",
      "Epoch 0, Loss(train/val) 0.70126/0.71109. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.69201/0.72027. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.68533/0.73110. Took 0.45 sec\n",
      "Epoch 3, Loss(train/val) 0.68288/0.73069. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.68602/0.74390. Took 0.47 sec\n",
      "Epoch 5, Loss(train/val) 0.68860/0.74827. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68099/0.77159. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.67891/0.78306. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68250/0.75114. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.68283/0.73659. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.68136/0.73286. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 0.68151/0.74975. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68287/0.74797. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.67348/0.75322. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66892/0.76569. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.67109/0.77474. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66900/0.78407. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66574/0.79184. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.66343/0.80306. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.65766/0.80436. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.65806/0.81821. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.65376/0.82227. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65518/0.84302. Took 0.46 sec\n",
      "Epoch 23, Loss(train/val) 0.65506/0.83498. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64383/0.84633. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.64621/0.84004. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64529/0.85143. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.63868/0.86338. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.63458/0.88161. Took 0.47 sec\n",
      "Epoch 29, Loss(train/val) 0.63430/0.89652. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.62579/0.86673. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.62629/0.87337. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61905/0.88089. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.62157/0.86607. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.61318/0.84800. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.60668/0.83808. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.61041/0.86191. Took 0.47 sec\n",
      "Epoch 37, Loss(train/val) 0.60835/0.82795. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60409/0.85003. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.58856/0.88089. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59151/0.87589. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.58477/0.89559. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58478/0.89205. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.57081/0.95251. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.57395/0.91487. Took 0.47 sec\n",
      "Epoch 45, Loss(train/val) 0.57619/0.90698. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55028/0.95557. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.55116/0.94910. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.56163/0.96749. Took 0.47 sec\n",
      "Epoch 49, Loss(train/val) 0.54838/0.94674. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54261/0.96732. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.52934/0.99361. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52385/1.00022. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.52533/0.97306. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.51664/1.02593. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.50526/1.08789. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.50744/1.02962. Took 0.47 sec\n",
      "Epoch 57, Loss(train/val) 0.49075/1.04087. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.51006/0.99406. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.48752/1.04679. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.49416/1.07133. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.50956/1.02199. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.49437/0.99314. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.48484/1.04738. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.46081/1.03483. Took 0.47 sec\n",
      "Epoch 65, Loss(train/val) 0.46748/1.12115. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.45385/1.13497. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.44033/1.05697. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.45150/1.05241. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.42961/1.06093. Took 0.43 sec\n",
      "Epoch 70, Loss(train/val) 0.43928/1.11863. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.43255/1.06463. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.43033/1.06745. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.42599/1.11511. Took 0.45 sec\n",
      "Epoch 74, Loss(train/val) 0.44028/1.15490. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.41892/1.13019. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.42949/1.08771. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.41002/1.14766. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.40501/1.17102. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.39340/1.15944. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.42210/1.21973. Took 0.48 sec\n",
      "Epoch 81, Loss(train/val) 0.40175/1.17570. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.40238/1.19280. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.39963/1.10033. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.40004/1.21605. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.38779/1.21758. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.39854/1.16770. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.37239/1.23543. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.35676/1.29200. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.39217/1.19398. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.38495/1.34944. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.39047/1.25529. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.36375/1.20828. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.34681/1.30722. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.39188/1.25847. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.36464/1.30170. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.35255/1.32719. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.35189/1.26989. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.34081/1.27639. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.39597/1.37397. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.70940/0.70093. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.70599/0.71405. Took 0.45 sec\n",
      "Epoch 2, Loss(train/val) 0.70234/0.69105. Took 0.50 sec\n",
      "Epoch 3, Loss(train/val) 0.69517/0.68916. Took 0.48 sec\n",
      "Epoch 4, Loss(train/val) 0.69309/0.69231. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.69404/0.69307. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68962/0.69067. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68712/0.69250. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68389/0.69596. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.68270/0.70966. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.68201/0.73502. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.67967/0.73586. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67228/0.75471. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67320/0.76997. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.66491/0.76190. Took 0.45 sec\n",
      "Epoch 15, Loss(train/val) 0.66331/0.77964. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.67255/0.77012. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.66093/0.78032. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65946/0.80647. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.66615/0.80069. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.65688/0.83511. Took 0.47 sec\n",
      "Epoch 21, Loss(train/val) 0.65805/0.84474. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.65660/0.84179. Took 0.44 sec\n",
      "Epoch 23, Loss(train/val) 0.64713/0.80155. Took 0.45 sec\n",
      "Epoch 24, Loss(train/val) 0.64550/0.83743. Took 0.47 sec\n",
      "Epoch 25, Loss(train/val) 0.63288/0.86590. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.64145/0.86801. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.62744/0.83523. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.62963/0.84234. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.63029/0.83563. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.62023/0.87002. Took 0.46 sec\n",
      "Epoch 31, Loss(train/val) 0.62233/0.88563. Took 0.43 sec\n",
      "Epoch 32, Loss(train/val) 0.61433/0.87728. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.60235/0.89961. Took 0.43 sec\n",
      "Epoch 34, Loss(train/val) 0.59873/0.88943. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.59349/0.94394. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.58609/0.93071. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.58351/0.93889. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.57795/0.95839. Took 0.46 sec\n",
      "Epoch 39, Loss(train/val) 0.59068/0.94660. Took 0.45 sec\n",
      "Epoch 40, Loss(train/val) 0.57733/0.96278. Took 0.46 sec\n",
      "Epoch 41, Loss(train/val) 0.57992/0.96710. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.56581/1.01383. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.56378/0.98331. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.55788/1.03047. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.56146/1.00812. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.55089/0.98444. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55170/1.02381. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.54074/1.04872. Took 0.45 sec\n",
      "Epoch 49, Loss(train/val) 0.54446/1.08022. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.54685/1.04306. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.52487/1.13697. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.51720/1.13687. Took 0.47 sec\n",
      "Epoch 53, Loss(train/val) 0.52540/1.12184. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.52016/1.15844. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.52223/1.19541. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.50411/1.18637. Took 0.47 sec\n",
      "Epoch 57, Loss(train/val) 0.50194/1.16290. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.49545/1.23394. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.49371/1.22786. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.48456/1.22341. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.48117/1.25906. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48036/1.27898. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.46047/1.23743. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.48182/1.27403. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.45536/1.27992. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.46101/1.28170. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.45574/1.32038. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.43576/1.34783. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.44940/1.27635. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.44224/1.30836. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.42953/1.37293. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.41004/1.39440. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.42303/1.39609. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.43382/1.44328. Took 0.44 sec\n",
      "Epoch 75, Loss(train/val) 0.40934/1.39039. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.41740/1.39737. Took 0.46 sec\n",
      "Epoch 77, Loss(train/val) 0.42061/1.59033. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.40091/1.47932. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.43824/1.50741. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.37188/1.47599. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.39759/1.57162. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.39406/1.47126. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.37492/1.51451. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.38727/1.51367. Took 0.47 sec\n",
      "Epoch 85, Loss(train/val) 0.36693/1.60079. Took 0.43 sec\n",
      "Epoch 86, Loss(train/val) 0.37876/1.52483. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.34494/1.72898. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.36862/1.66880. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.35103/1.70040. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.38660/1.69007. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.41628/1.56776. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.38842/1.56654. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.36133/1.64064. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.37111/1.55160. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.34982/1.64020. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.33683/1.73318. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.34502/1.66827. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.33203/1.75536. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.33783/1.76205. Took 0.44 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.69883/0.70332. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.69906/0.69630. Took 0.48 sec\n",
      "Epoch 2, Loss(train/val) 0.69414/0.69501. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.68962/0.69962. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.69040/0.70476. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68376/0.70125. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.68598/0.70019. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68845/0.70640. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68230/0.70471. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.68199/0.70498. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.67689/0.70679. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.67839/0.70029. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.67382/0.71121. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67505/0.71869. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66823/0.73283. Took 0.47 sec\n",
      "Epoch 15, Loss(train/val) 0.66678/0.73456. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.66722/0.75311. Took 0.47 sec\n",
      "Epoch 17, Loss(train/val) 0.65762/0.77482. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.65644/0.78172. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.65475/0.80180. Took 0.44 sec\n",
      "Epoch 20, Loss(train/val) 0.66052/0.78194. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.65351/0.78322. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.64661/0.79090. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.64402/0.80571. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.64617/0.78786. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.64170/0.81554. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.62751/0.82259. Took 0.47 sec\n",
      "Epoch 27, Loss(train/val) 0.63147/0.79337. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61708/0.80228. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.62596/0.81632. Took 0.45 sec\n",
      "Epoch 30, Loss(train/val) 0.60701/0.84765. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.61407/0.87316. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.61322/0.85552. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.61374/0.88063. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.60777/0.89349. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.59497/0.91370. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.59691/0.94068. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.60030/1.01161. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.61871/0.90815. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.61414/0.91465. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.59680/0.91817. Took 0.47 sec\n",
      "Epoch 41, Loss(train/val) 0.59565/0.93471. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.58958/0.93570. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.59738/0.97340. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.57273/1.00280. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.58694/0.99497. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.57602/1.01377. Took 0.44 sec\n",
      "Epoch 47, Loss(train/val) 0.55831/1.08320. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58532/0.99716. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.57791/0.99518. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.57039/0.95746. Took 0.47 sec\n",
      "Epoch 51, Loss(train/val) 0.56999/0.97764. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.57019/0.97590. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.56248/0.98478. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.55196/0.98299. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.53571/0.99940. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.53208/1.02237. Took 0.44 sec\n",
      "Epoch 57, Loss(train/val) 0.53712/1.03497. Took 0.43 sec\n",
      "Epoch 58, Loss(train/val) 0.52893/1.01748. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.52707/0.94485. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.52763/0.97288. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.51405/1.04024. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.51235/1.06324. Took 0.45 sec\n",
      "Epoch 63, Loss(train/val) 0.51725/1.00279. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.49378/1.05696. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.49667/1.04828. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.49655/0.99211. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.51067/1.04476. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.50346/1.06627. Took 0.44 sec\n",
      "Epoch 69, Loss(train/val) 0.48572/1.10576. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.46987/1.11605. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.48267/1.06534. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.47832/1.05348. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.46660/1.02064. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.46284/1.14209. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.46012/1.09637. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.44849/1.15867. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.44349/1.11365. Took 0.45 sec\n",
      "Epoch 78, Loss(train/val) 0.47108/1.11372. Took 0.45 sec\n",
      "Epoch 79, Loss(train/val) 0.45718/1.11804. Took 0.45 sec\n",
      "Epoch 80, Loss(train/val) 0.42770/1.26441. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.44288/1.22316. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.44635/1.23820. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.47052/1.15562. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.46189/1.15083. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.42803/1.11504. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.42340/1.14522. Took 0.45 sec\n",
      "Epoch 87, Loss(train/val) 0.42157/1.13807. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.40666/1.16720. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.42115/1.29939. Took 0.43 sec\n",
      "Epoch 90, Loss(train/val) 0.43403/1.29708. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.42004/1.27673. Took 0.43 sec\n",
      "Epoch 92, Loss(train/val) 0.42195/1.22580. Took 0.47 sec\n",
      "Epoch 93, Loss(train/val) 0.40178/1.24212. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.41443/1.26563. Took 0.46 sec\n",
      "Epoch 95, Loss(train/val) 0.38914/1.39371. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.38622/1.36537. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.36673/1.41630. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.40173/1.30005. Took 0.47 sec\n",
      "Epoch 99, Loss(train/val) 0.39514/1.25881. Took 0.44 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69873/0.69500. Took 0.49 sec\n",
      "Epoch 1, Loss(train/val) 0.68902/0.69299. Took 0.47 sec\n",
      "Epoch 2, Loss(train/val) 0.68589/0.70295. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.67963/0.70786. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.68095/0.71237. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.68123/0.71003. Took 0.45 sec\n",
      "Epoch 6, Loss(train/val) 0.68034/0.70948. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.67258/0.71088. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.67272/0.71188. Took 0.47 sec\n",
      "Epoch 9, Loss(train/val) 0.67504/0.71535. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.66880/0.69494. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.66193/0.70125. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66342/0.69742. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.65687/0.69665. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.65335/0.68924. Took 0.49 sec\n",
      "Epoch 15, Loss(train/val) 0.65490/0.70359. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.64569/0.69243. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.64509/0.69130. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64668/0.70670. Took 0.47 sec\n",
      "Epoch 19, Loss(train/val) 0.63399/0.70653. Took 0.45 sec\n",
      "Epoch 20, Loss(train/val) 0.62843/0.71473. Took 0.46 sec\n",
      "Epoch 21, Loss(train/val) 0.63244/0.71569. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.62567/0.74932. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.61842/0.77053. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62483/0.77486. Took 0.46 sec\n",
      "Epoch 25, Loss(train/val) 0.61498/0.76923. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.60783/0.80099. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.60032/0.82051. Took 0.43 sec\n",
      "Epoch 28, Loss(train/val) 0.60000/0.77652. Took 0.46 sec\n",
      "Epoch 29, Loss(train/val) 0.58434/0.80827. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.60030/0.78622. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.58374/0.78683. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.57683/0.78936. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.57522/0.82913. Took 0.45 sec\n",
      "Epoch 34, Loss(train/val) 0.57388/0.85372. Took 0.47 sec\n",
      "Epoch 35, Loss(train/val) 0.56301/0.85491. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.56196/0.85030. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.57422/0.84903. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.54988/0.89817. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.53720/0.84353. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.54908/0.86678. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.53269/0.87321. Took 0.43 sec\n",
      "Epoch 42, Loss(train/val) 0.53017/0.87349. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.52277/0.84341. Took 0.45 sec\n",
      "Epoch 44, Loss(train/val) 0.51106/0.90288. Took 0.47 sec\n",
      "Epoch 45, Loss(train/val) 0.51471/0.94018. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.49385/0.94059. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.50927/0.93212. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.50022/0.91289. Took 0.44 sec\n",
      "Epoch 49, Loss(train/val) 0.50471/0.95486. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.48448/0.97563. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.46675/1.00795. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.48574/0.97042. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.46096/1.02906. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.45082/1.00713. Took 0.46 sec\n",
      "Epoch 55, Loss(train/val) 0.44728/0.99150. Took 0.45 sec\n",
      "Epoch 56, Loss(train/val) 0.43201/1.04287. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.44269/1.03671. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.43060/1.14802. Took 0.46 sec\n",
      "Epoch 59, Loss(train/val) 0.42419/1.07793. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.43729/1.03830. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.41540/1.14514. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.40285/1.08586. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.42046/1.12387. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.39874/1.13111. Took 0.45 sec\n",
      "Epoch 65, Loss(train/val) 0.38842/1.12888. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.39626/1.19383. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.40875/1.17685. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.40496/1.10785. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.39428/1.19904. Took 0.45 sec\n",
      "Epoch 70, Loss(train/val) 0.37409/1.16600. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.36571/1.16735. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.36073/1.17068. Took 0.47 sec\n",
      "Epoch 73, Loss(train/val) 0.36900/1.13905. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.35415/1.20685. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.36354/1.23350. Took 0.45 sec\n",
      "Epoch 76, Loss(train/val) 0.37655/1.21739. Took 0.47 sec\n",
      "Epoch 77, Loss(train/val) 0.34696/1.20824. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.32335/1.32787. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.34525/1.27535. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.33429/1.33984. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.30595/1.36060. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.33023/1.31527. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.31337/1.31773. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.31012/1.41150. Took 0.46 sec\n",
      "Epoch 85, Loss(train/val) 0.30563/1.49478. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.29687/1.43522. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.28365/1.39185. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.28382/1.58988. Took 0.46 sec\n",
      "Epoch 89, Loss(train/val) 0.28393/1.52703. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.28151/1.50301. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.27781/1.53141. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.24773/1.48104. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.23742/1.52563. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.24598/1.55131. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.24665/1.63007. Took 0.45 sec\n",
      "Epoch 96, Loss(train/val) 0.24416/1.65389. Took 0.46 sec\n",
      "Epoch 97, Loss(train/val) 0.26219/1.64991. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.24939/1.67118. Took 0.48 sec\n",
      "Epoch 99, Loss(train/val) 0.23787/1.67831. Took 0.45 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.74048/0.74801. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 0.73529/0.78245. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.71558/0.73673. Took 0.48 sec\n",
      "Epoch 3, Loss(train/val) 0.71111/0.76941. Took 0.44 sec\n",
      "Epoch 4, Loss(train/val) 0.70919/0.77704. Took 0.45 sec\n",
      "Epoch 5, Loss(train/val) 0.70285/0.75352. Took 0.44 sec\n",
      "Epoch 6, Loss(train/val) 0.69948/0.76950. Took 0.47 sec\n",
      "Epoch 7, Loss(train/val) 0.69902/0.75402. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.68937/0.80309. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.69725/0.81307. Took 0.44 sec\n",
      "Epoch 10, Loss(train/val) 0.69542/0.82153. Took 0.45 sec\n",
      "Epoch 11, Loss(train/val) 0.68735/0.78452. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.68582/0.79943. Took 0.46 sec\n",
      "Epoch 13, Loss(train/val) 0.68857/0.76164. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.67053/0.75962. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.69109/0.72743. Took 0.48 sec\n",
      "Epoch 16, Loss(train/val) 0.67845/0.74513. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.68030/0.72342. Took 0.48 sec\n",
      "Epoch 18, Loss(train/val) 0.67001/0.74083. Took 0.45 sec\n",
      "Epoch 19, Loss(train/val) 0.67231/0.74625. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.67572/0.75619. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.66425/0.76435. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.66862/0.74567. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.67144/0.74884. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.66298/0.74637. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.66201/0.74239. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.66521/0.75444. Took 0.47 sec\n",
      "Epoch 27, Loss(train/val) 0.66626/0.76223. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.65243/0.75257. Took 0.45 sec\n",
      "Epoch 29, Loss(train/val) 0.65632/0.79334. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.65097/0.77570. Took 0.47 sec\n",
      "Epoch 31, Loss(train/val) 0.65172/0.79583. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.64073/0.78499. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.64065/0.80623. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.63680/0.82942. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.63472/0.82467. Took 0.44 sec\n",
      "Epoch 36, Loss(train/val) 0.62961/0.84542. Took 0.46 sec\n",
      "Epoch 37, Loss(train/val) 0.63572/0.88313. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.60995/0.83383. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.62356/0.86124. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.61983/0.84254. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.60635/0.89953. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.60468/0.88763. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.61210/0.90961. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.59256/0.88498. Took 0.45 sec\n",
      "Epoch 45, Loss(train/val) 0.58774/0.93026. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.59332/0.94325. Took 0.46 sec\n",
      "Epoch 47, Loss(train/val) 0.59604/0.90838. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.58587/0.99393. Took 0.47 sec\n",
      "Epoch 49, Loss(train/val) 0.58012/0.90755. Took 0.44 sec\n",
      "Epoch 50, Loss(train/val) 0.57184/0.94983. Took 0.45 sec\n",
      "Epoch 51, Loss(train/val) 0.57870/1.02724. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.58611/0.91357. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.56189/0.96225. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.56956/1.09183. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.56291/0.96864. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.56402/1.05634. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.54649/0.96700. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.54932/1.05690. Took 0.47 sec\n",
      "Epoch 59, Loss(train/val) 0.53795/0.99544. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.53913/1.11630. Took 0.46 sec\n",
      "Epoch 61, Loss(train/val) 0.55679/0.94993. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.53797/1.01765. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.53844/0.97199. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.51036/1.05330. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.52426/1.04294. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.52918/1.16892. Took 0.46 sec\n",
      "Epoch 67, Loss(train/val) 0.55904/0.94579. Took 0.44 sec\n",
      "Epoch 68, Loss(train/val) 0.54058/0.98401. Took 0.46 sec\n",
      "Epoch 69, Loss(train/val) 0.53664/0.96581. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.51687/1.02256. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.49964/1.08156. Took 0.45 sec\n",
      "Epoch 72, Loss(train/val) 0.50673/1.03856. Took 0.46 sec\n",
      "Epoch 73, Loss(train/val) 0.50185/1.14919. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.50205/1.07282. Took 0.46 sec\n",
      "Epoch 75, Loss(train/val) 0.47808/1.10194. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.48991/1.16680. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.50198/1.13992. Took 0.44 sec\n",
      "Epoch 78, Loss(train/val) 0.47737/1.19682. Took 0.46 sec\n",
      "Epoch 79, Loss(train/val) 0.50548/1.06642. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.52958/1.10764. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.50292/1.04513. Took 0.44 sec\n",
      "Epoch 82, Loss(train/val) 0.47751/1.11708. Took 0.46 sec\n",
      "Epoch 83, Loss(train/val) 0.47152/1.17361. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.46914/1.17763. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.44611/1.28684. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.45703/1.18895. Took 0.46 sec\n",
      "Epoch 87, Loss(train/val) 0.46471/1.25477. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.45377/1.27192. Took 0.47 sec\n",
      "Epoch 89, Loss(train/val) 0.48179/1.23677. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.43680/1.28842. Took 0.46 sec\n",
      "Epoch 91, Loss(train/val) 0.44477/1.31626. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.44045/1.25466. Took 0.45 sec\n",
      "Epoch 93, Loss(train/val) 0.44162/1.25553. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.43053/1.36804. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.43470/1.25191. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.47537/1.34341. Took 0.44 sec\n",
      "Epoch 97, Loss(train/val) 0.45122/1.26573. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.44190/1.24296. Took 0.48 sec\n",
      "Epoch 99, Loss(train/val) 0.46146/1.28030. Took 0.44 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.73096/0.71412. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 0.70630/0.71996. Took 0.44 sec\n",
      "Epoch 2, Loss(train/val) 0.70336/0.75101. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.68644/0.75797. Took 0.45 sec\n",
      "Epoch 4, Loss(train/val) 0.69100/0.74472. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.68778/0.76329. Took 0.43 sec\n",
      "Epoch 6, Loss(train/val) 0.68691/0.76477. Took 0.46 sec\n",
      "Epoch 7, Loss(train/val) 0.67985/0.75346. Took 0.44 sec\n",
      "Epoch 8, Loss(train/val) 0.66657/0.75142. Took 0.46 sec\n",
      "Epoch 9, Loss(train/val) 0.68004/0.74130. Took 0.45 sec\n",
      "Epoch 10, Loss(train/val) 0.67804/0.76097. Took 0.46 sec\n",
      "Epoch 11, Loss(train/val) 0.67285/0.79041. Took 0.44 sec\n",
      "Epoch 12, Loss(train/val) 0.66773/0.77060. Took 0.45 sec\n",
      "Epoch 13, Loss(train/val) 0.67212/0.78203. Took 0.44 sec\n",
      "Epoch 14, Loss(train/val) 0.66616/0.79819. Took 0.46 sec\n",
      "Epoch 15, Loss(train/val) 0.66191/0.81746. Took 0.44 sec\n",
      "Epoch 16, Loss(train/val) 0.65226/0.82546. Took 0.45 sec\n",
      "Epoch 17, Loss(train/val) 0.65157/0.82684. Took 0.44 sec\n",
      "Epoch 18, Loss(train/val) 0.64964/0.84633. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.63678/0.85559. Took 0.43 sec\n",
      "Epoch 20, Loss(train/val) 0.63286/0.85934. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.63298/0.87974. Took 0.43 sec\n",
      "Epoch 22, Loss(train/val) 0.62798/0.89007. Took 0.47 sec\n",
      "Epoch 23, Loss(train/val) 0.61806/0.91128. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.62162/0.93356. Took 0.45 sec\n",
      "Epoch 25, Loss(train/val) 0.61458/0.93916. Took 0.44 sec\n",
      "Epoch 26, Loss(train/val) 0.61335/0.97175. Took 0.46 sec\n",
      "Epoch 27, Loss(train/val) 0.59609/0.99806. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.61336/1.00459. Took 0.47 sec\n",
      "Epoch 29, Loss(train/val) 0.59961/1.01221. Took 0.44 sec\n",
      "Epoch 30, Loss(train/val) 0.59702/1.01602. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.58887/1.00306. Took 0.44 sec\n",
      "Epoch 32, Loss(train/val) 0.60162/1.01820. Took 0.46 sec\n",
      "Epoch 33, Loss(train/val) 0.59965/1.02531. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.57368/1.03354. Took 0.46 sec\n",
      "Epoch 35, Loss(train/val) 0.57901/1.03723. Took 0.43 sec\n",
      "Epoch 36, Loss(train/val) 0.57779/1.05693. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.57778/1.04908. Took 0.45 sec\n",
      "Epoch 38, Loss(train/val) 0.57606/1.11598. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.56232/1.16372. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.56330/1.13130. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.56889/1.10036. Took 0.45 sec\n",
      "Epoch 42, Loss(train/val) 0.56198/1.17193. Took 0.45 sec\n",
      "Epoch 43, Loss(train/val) 0.54628/1.23648. Took 0.44 sec\n",
      "Epoch 44, Loss(train/val) 0.56371/1.21607. Took 0.46 sec\n",
      "Epoch 45, Loss(train/val) 0.54532/1.20528. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.54655/1.23228. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.54259/1.22253. Took 0.44 sec\n",
      "Epoch 48, Loss(train/val) 0.55808/1.17060. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.53792/1.25195. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.54329/1.22404. Took 0.46 sec\n",
      "Epoch 51, Loss(train/val) 0.54145/1.22841. Took 0.44 sec\n",
      "Epoch 52, Loss(train/val) 0.52711/1.20519. Took 0.45 sec\n",
      "Epoch 53, Loss(train/val) 0.51905/1.26937. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.53254/1.29635. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.53838/1.22504. Took 0.44 sec\n",
      "Epoch 56, Loss(train/val) 0.52247/1.24452. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.52454/1.20888. Took 0.44 sec\n",
      "Epoch 58, Loss(train/val) 0.53004/1.18917. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.50753/1.21676. Took 0.45 sec\n",
      "Epoch 60, Loss(train/val) 0.50794/1.21541. Took 0.45 sec\n",
      "Epoch 61, Loss(train/val) 0.49840/1.28516. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.48933/1.26585. Took 0.46 sec\n",
      "Epoch 63, Loss(train/val) 0.50248/1.27593. Took 0.44 sec\n",
      "Epoch 64, Loss(train/val) 0.50594/1.22619. Took 0.46 sec\n",
      "Epoch 65, Loss(train/val) 0.49690/1.22060. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.49653/1.22503. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.48308/1.25594. Took 0.43 sec\n",
      "Epoch 68, Loss(train/val) 0.51914/1.34328. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.48380/1.28780. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.49365/1.25075. Took 0.45 sec\n",
      "Epoch 71, Loss(train/val) 0.47282/1.34133. Took 0.44 sec\n",
      "Epoch 72, Loss(train/val) 0.46985/1.35807. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.46594/1.32923. Took 0.44 sec\n",
      "Epoch 74, Loss(train/val) 0.45886/1.42873. Took 0.45 sec\n",
      "Epoch 75, Loss(train/val) 0.45363/1.41618. Took 0.44 sec\n",
      "Epoch 76, Loss(train/val) 0.46291/1.40987. Took 0.45 sec\n",
      "Epoch 77, Loss(train/val) 0.43061/1.40340. Took 0.43 sec\n",
      "Epoch 78, Loss(train/val) 0.45739/1.43925. Took 0.44 sec\n",
      "Epoch 79, Loss(train/val) 0.44968/1.39522. Took 0.44 sec\n",
      "Epoch 80, Loss(train/val) 0.46228/1.42531. Took 0.46 sec\n",
      "Epoch 81, Loss(train/val) 0.44849/1.40898. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.42628/1.54615. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.42999/1.51844. Took 0.44 sec\n",
      "Epoch 84, Loss(train/val) 0.42651/1.52686. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.44203/1.46771. Took 0.44 sec\n",
      "Epoch 86, Loss(train/val) 0.44958/1.43506. Took 0.47 sec\n",
      "Epoch 87, Loss(train/val) 0.43938/1.50414. Took 0.44 sec\n",
      "Epoch 88, Loss(train/val) 0.41207/1.47709. Took 0.45 sec\n",
      "Epoch 89, Loss(train/val) 0.43104/1.58259. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.42006/1.60663. Took 0.45 sec\n",
      "Epoch 91, Loss(train/val) 0.40834/1.61267. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.40505/1.53609. Took 0.46 sec\n",
      "Epoch 93, Loss(train/val) 0.40025/1.55904. Took 0.44 sec\n",
      "Epoch 94, Loss(train/val) 0.41835/1.50426. Took 0.44 sec\n",
      "Epoch 95, Loss(train/val) 0.39136/1.56125. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.39340/1.71226. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.38629/1.71602. Took 0.44 sec\n",
      "Epoch 98, Loss(train/val) 0.40359/1.62256. Took 0.45 sec\n",
      "Epoch 99, Loss(train/val) 0.37700/1.63158. Took 0.44 sec\n",
      "ACC: 0.5833333333333334\n"
     ]
    }
   ],
   "source": [
    "## 실행파일\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from Stock_dataloader_csv_ti import stock_csv_read\n",
    "from Stock_Dataset import StockDataset\n",
    "\n",
    "\n",
    "args.data_list = os.listdir(r\"C:\\Users\\lab\\Desktop\\Autoformer\\data\\kdd17\\price_long_50\")\n",
    "\n",
    "with open(args.save_file_path + '\\\\' + 'Autoformer_result_t.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"model\", \"stock\", \"entire_exp_time\",  \"avg_test_ACC\", \"avg_test_ACC_std\"])\n",
    "\n",
    "    for data in args.data_list:\n",
    "        est = time.time()\n",
    "\n",
    "        stock = data.split('.')[0]\n",
    "\n",
    "        setattr(args, 'symbol', stock)\n",
    "        args.new_file_path = args.save_file_path + '\\\\' + \"Autoformer_\" + args.symbol\n",
    "        os.makedirs(args.new_file_path)\n",
    "        \n",
    "        csv_read = stock_csv_read(data, args.ts_len,args.target_len)\n",
    "        split_data_list = csv_read.cv_split()\n",
    "\n",
    "        with open(args.new_file_path + '\\\\'+ str(args.symbol)+'test_acc_list' +'.csv', 'w',newline='') as alist:\n",
    "            www = csv.writer(alist)\n",
    "            www.writerow([\"acc_list\"])\n",
    "\n",
    "            ACC_cv = []\n",
    "            for i, data in enumerate(split_data_list):\n",
    "                \n",
    "                args.sp_ith = i\n",
    "\n",
    "                args.split_file_path = args.new_file_path + \"\\\\\" + str(i) +\"th_iter\"\n",
    "                os.makedirs(args.split_file_path)\n",
    "                ## Model                 \n",
    "                Autoformer = args.Autoformer()\n",
    "                \n",
    "                Autoformer.to(args.device)\n",
    "\n",
    "                ##Optimizer\n",
    "                Autoformer_optimizer = optim.Adam(Autoformer.parameters(), lr=args.lr, weight_decay=args.L2)\n",
    "\n",
    "                ## training\n",
    "                Train_losses = []\n",
    "                Validation_losses = []\n",
    "                for epoch in range(args.epoch):\n",
    "                    ts = time.time()\n",
    "\n",
    "                    trainset = StockDataset(data[0])\n",
    "                    valset = StockDataset(data[1])\n",
    "                    testset = StockDataset(data[2])\n",
    "\n",
    "\n",
    "                    partition = {'train': trainset, 'val': valset, 'test': testset}     \n",
    "\n",
    "                    Autoformer, train_loss = train(Autoformer,Autoformer_optimizer, args, partition)\n",
    "                    Autoformer, validation_loss = validation(Autoformer, args, partition)\n",
    "\n",
    "\n",
    "                    ## .state_dict() : model의 parameter(W)만을 저장하는것임 => 다시 불러올 때 모델의 파라미터를 알고있어야함\n",
    "                    if len(Validation_losses) == 0:\n",
    "                        torch.save(Autoformer.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_Autoformer' +'.pt')\n",
    "                    elif min(Validation_losses) > validation_loss:\n",
    "                        torch.save(Autoformer.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_Autoformer' +'.pt')\n",
    "                    \n",
    "                    Train_losses.append(train_loss)\n",
    "                    Validation_losses.append(validation_loss)\n",
    "                    \n",
    "                    te = time.time()\n",
    "\n",
    "                    print('Epoch {}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "                    .format(epoch, train_loss, validation_loss, te - ts))\n",
    "\n",
    "                ## Test\n",
    "                # state_dict로 저장했기 때문에 model의 hyperparameter를 불러와야함\n",
    "                Autoformer = args.Autoformer()\n",
    "                \n",
    "                Autoformer.to(args.device)\n",
    "\n",
    "                # Model_selection\n",
    "                min_val_losses = Validation_losses.index(min(Validation_losses)) ## 10 epoch일 경우 0번째~9번째 까지로 나옴\n",
    "\n",
    "                Autoformer.load_state_dict(torch.load(args.split_file_path + '\\\\' + str(min_val_losses) +'_Autoformer' + '.pt'))\n",
    "\n",
    "                ACC = test(Autoformer, args, partition)\n",
    "                www.writerow([ACC])\n",
    "                print('ACC: {}'.format(ACC))\n",
    "\n",
    "                with open(args.split_file_path + '\\\\'+ str(min_val_losses)+'Epoch_test_metric' +'.csv', 'w') as fd:\n",
    "                    print('ACC: {}'.format(ACC), file=fd)\n",
    "\n",
    "                result = {}\n",
    "\n",
    "                result['train_losses'] = Train_losses\n",
    "                result['val_losses'] = Validation_losses\n",
    "                result['ACC'] = ACC\n",
    "\n",
    "                eet = time.time()\n",
    "                entire_exp_time = eet - est\n",
    "                \n",
    "\n",
    "                ## draw loss curve\n",
    "                fig = plt.figure()\n",
    "                plt.plot(result['train_losses'])\n",
    "                plt.plot(result['val_losses'])\n",
    "                plt.legend(['train_losses', 'val_losses'], fontsize=15)\n",
    "                plt.xlabel('epoch', fontsize=15)\n",
    "                plt.ylabel('loss', fontsize=15)\n",
    "                plt.grid()\n",
    "                plt.savefig(args.split_file_path + '\\\\' + 'fig' + '.png')\n",
    "                plt.close(fig)\n",
    "                ACC_cv.append(result['ACC'])\n",
    "\n",
    "        ACC_cv_ar = np.array(ACC_cv)\n",
    "        acc_avg = np.mean(ACC_cv_ar)\n",
    "        acc_std = np.std(ACC_cv_ar)\n",
    "\n",
    "        wr.writerow([\"Autoformer\", args.symbol, entire_exp_time, acc_avg, acc_std])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('taewon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "013403e7ebf8f35ee0411721c7e4b108aa3c3f8cb903b89610d110413a68ec3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
