{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Stock_Dataset.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\USER\\\\JupyterProjects\\\\conv_biLSTM_attention')\n",
    "from Stock_Dataset import StockDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from CONV_Att_BILSTM.ipynb\n",
      "importing Jupyter notebook from attention.ipynb\n",
      "importing Jupyter notebook from metric.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "from CONV_Att_BILSTM import Conv_attLSTM\n",
    "import numpy as np\n",
    "import time\n",
    "from metric import metric_acc as ACC\n",
    "from metric import metric_mcc as MCC\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "from Stock_dataloader_csv import stock_csv_read\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "\n",
    "\n",
    "def train(conv_attLSTM, lstm_optimizer,Partition, args): ## Data, loss function, argument\n",
    "    trainloader = DataLoader(Partition['train'],\n",
    "                             batch_size = args.batch_size,\n",
    "                             shuffle=False, drop_last=True)\n",
    "    conv_attLSTM.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for i, (x,y) in enumerate(trainloader):\n",
    "        \n",
    "        lstm_optimizer.zero_grad()\n",
    "        true_y = y.squeeze().float().to(args.device)\n",
    "        x = x.to(args.device)\n",
    "        \n",
    "        conv_attLSTM.hidden = [hidden.to(args.device) for hidden in conv_attLSTM.init_hidden()]\n",
    "        \n",
    "        yhat, attention_weight, attn_applied = conv_attLSTM(x)\n",
    "        # print(es.size()) [128, 20]\n",
    "        \n",
    "\n",
    "        loss = args.loss_fn(yhat, true_y)\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        lstm_optimizer.step()## parameter 갱신\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    return conv_attLSTM, train_loss\n",
    "\n",
    "\n",
    "def validation(conv_attLSTM, partition, args):\n",
    "    valloader = DataLoader(partition['val'], \n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    conv_attLSTM.eval()\n",
    "    \n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(valloader):\n",
    "\n",
    "            true_y = y.squeeze().float().to(args.device)\n",
    "            x = x.to(args.device)\n",
    "\n",
    "            conv_attLSTM.hidden = [conv_attLSTM.to(args.device) for hidden in conv_attLSTM.init_hidden()]\n",
    "\n",
    "            yhat, attention_weight, attn_applied = conv_attLSTM(x)\n",
    "\n",
    "\n",
    "            # output_ = torch.where(output1 >= 0.5, 1.0, 0.0)\n",
    "            # output_.requires_grad=True\n",
    "\n",
    "            loss = args.loss_fn(yhat, true_y)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss = val_loss / len(valloader)\n",
    "        return conv_attLSTM, val_loss\n",
    "\n",
    "\n",
    "def test(conv_attLSTM,partition, args):\n",
    "    testloader = DataLoader(partition['test'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    conv_attLSTM.eval()\n",
    "\n",
    "    ACC_metric = 0.0\n",
    "    MCC_metric = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(testloader):\n",
    "\n",
    "            # feature transform\n",
    "            true_y = y.squeeze().float().to(args.device)\n",
    "            x = x.to(args.device)\n",
    "\n",
    "            conv_attLSTM.hidden = [hidden.to(args.device) for hidden in conv_attLSTM.init_hidden()]\n",
    "\n",
    "            yhat, attention_weight, attn_applied = conv_attLSTM(x)\n",
    "\n",
    "            output_ = torch.where(yhat >= 0.5, 1.0, 0.0)\n",
    "\n",
    "            output_.requires_grad = True\n",
    "\n",
    "            perc_y_pred = output_.cpu().detach().numpy()\n",
    "            perc_y_true = true_y.cpu().detach().numpy()\n",
    "\n",
    "            acc = accuracy_score(perc_y_true, perc_y_pred)\n",
    "            mcc = matthews_corrcoef(perc_y_true, perc_y_pred)\n",
    "\n",
    "            ACC_metric += acc\n",
    "            MCC_metric += mcc\n",
    "\n",
    "        ACC_metric = ACC_metric / len(testloader)\n",
    "        MCC_metric = MCC_metric / len(testloader)\n",
    "\n",
    "        return ACC_metric, MCC_metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# ========= experiment setting ========== #\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args.save_file_path = r\"D:\\conv_bilstm_results\"\n",
    "\n",
    "# ====== hyperparameter ======= #\n",
    "args.batch_size = 64\n",
    "\n",
    "args.dropout = 0.2\n",
    "args.use_bn = True\n",
    "args.loss_fn = nn.L1Loss()  ## loss function for classification : cross entropy\n",
    "args.optim = 'Adam'\n",
    "args.lr = 0.0005\n",
    "args.l2 = 0.00001 #?\n",
    "args.epoch = 100\n",
    "# ============= model ================== #\n",
    "args.Conv_attLSTM = Conv_attLSTM\n",
    "\n",
    "# ====== att_lstm hyperparameter ======= #\n",
    "args.x_frames = 10\n",
    "args.y_frames = 1\n",
    "\n",
    "args.input_dim = 64\n",
    "args.hid_dim = 64\n",
    "args.output_dim = 1\n",
    "\n",
    "args.attention_head = 1\n",
    "args.attn_size = 10\n",
    "args.num_layers = 1\n",
    "args.attLSTM_x_frames = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss(train/val) 0.50062/0.49768. Took 1.24 sec\n",
      "Epoch 1, Loss(train/val) 0.49911/0.49486. Took 0.06 sec\n",
      "Epoch 2, Loss(train/val) 0.49681/0.48942. Took 0.07 sec\n",
      "Epoch 3, Loss(train/val) 0.49398/0.48101. Took 0.06 sec\n",
      "Epoch 4, Loss(train/val) 0.48972/0.47244. Took 0.05 sec\n",
      "Epoch 5, Loss(train/val) 0.48475/0.46525. Took 0.05 sec\n",
      "Epoch 6, Loss(train/val) 0.47447/0.45977. Took 0.06 sec\n",
      "Epoch 7, Loss(train/val) 0.46061/0.45686. Took 0.06 sec\n",
      "Epoch 8, Loss(train/val) 0.45111/0.45538. Took 0.06 sec\n",
      "Epoch 9, Loss(train/val) 0.46156/0.45173. Took 0.05 sec\n",
      "Epoch 10, Loss(train/val) 0.45237/0.45402. Took 0.05 sec\n",
      "Epoch 11, Loss(train/val) 0.44313/0.45399. Took 0.05 sec\n",
      "Epoch 12, Loss(train/val) 0.44117/0.45431. Took 0.06 sec\n",
      "Epoch 13, Loss(train/val) 0.44329/0.45416. Took 0.06 sec\n",
      "Epoch 14, Loss(train/val) 0.43770/0.45624. Took 0.06 sec\n",
      "Epoch 15, Loss(train/val) 0.43038/0.45707. Took 0.06 sec\n",
      "Epoch 16, Loss(train/val) 0.42149/0.45656. Took 0.05 sec\n",
      "Epoch 17, Loss(train/val) 0.41013/0.45564. Took 0.06 sec\n",
      "Epoch 18, Loss(train/val) 0.41581/0.46179. Took 0.05 sec\n",
      "Epoch 19, Loss(train/val) 0.40468/0.47298. Took 0.05 sec\n",
      "Epoch 20, Loss(train/val) 0.39674/0.48401. Took 0.06 sec\n",
      "Epoch 21, Loss(train/val) 0.39655/0.47904. Took 0.06 sec\n",
      "Epoch 22, Loss(train/val) 0.40001/0.47188. Took 0.06 sec\n",
      "Epoch 23, Loss(train/val) 0.39043/0.48696. Took 0.06 sec\n",
      "Epoch 24, Loss(train/val) 0.38387/0.47187. Took 0.06 sec\n",
      "Epoch 25, Loss(train/val) 0.38775/0.47716. Took 0.06 sec\n",
      "Epoch 26, Loss(train/val) 0.38443/0.48824. Took 0.05 sec\n",
      "Epoch 27, Loss(train/val) 0.38054/0.49516. Took 0.06 sec\n",
      "Epoch 28, Loss(train/val) 0.36645/0.47732. Took 0.06 sec\n",
      "Epoch 29, Loss(train/val) 0.37201/0.49841. Took 0.05 sec\n",
      "Epoch 30, Loss(train/val) 0.37129/0.47792. Took 0.06 sec\n",
      "Epoch 31, Loss(train/val) 0.37059/0.47408. Took 0.06 sec\n",
      "Epoch 32, Loss(train/val) 0.36553/0.49495. Took 0.05 sec\n",
      "Epoch 33, Loss(train/val) 0.36839/0.48491. Took 0.06 sec\n",
      "Epoch 34, Loss(train/val) 0.35686/0.50404. Took 0.05 sec\n",
      "Epoch 35, Loss(train/val) 0.35753/0.50045. Took 0.06 sec\n",
      "Epoch 36, Loss(train/val) 0.35159/0.49975. Took 0.06 sec\n",
      "Epoch 37, Loss(train/val) 0.35031/0.50455. Took 0.06 sec\n",
      "Epoch 38, Loss(train/val) 0.35023/0.50603. Took 0.06 sec\n",
      "Epoch 39, Loss(train/val) 0.35117/0.51306. Took 0.06 sec\n",
      "Epoch 40, Loss(train/val) 0.35265/0.51389. Took 0.06 sec\n",
      "Epoch 41, Loss(train/val) 0.34598/0.51214. Took 0.06 sec\n",
      "Epoch 42, Loss(train/val) 0.34080/0.52079. Took 0.05 sec\n",
      "Epoch 43, Loss(train/val) 0.33550/0.52231. Took 0.06 sec\n",
      "Epoch 44, Loss(train/val) 0.33310/0.51803. Took 0.06 sec\n",
      "Epoch 45, Loss(train/val) 0.32831/0.47271. Took 0.05 sec\n",
      "Epoch 46, Loss(train/val) 0.32696/0.50882. Took 0.06 sec\n",
      "Epoch 47, Loss(train/val) 0.32933/0.47042. Took 0.05 sec\n",
      "Epoch 48, Loss(train/val) 0.32639/0.47150. Took 0.06 sec\n",
      "Epoch 49, Loss(train/val) 0.31310/0.49922. Took 0.05 sec\n",
      "Epoch 50, Loss(train/val) 0.31653/0.47859. Took 0.05 sec\n",
      "Epoch 51, Loss(train/val) 0.31087/0.47385. Took 0.06 sec\n",
      "Epoch 52, Loss(train/val) 0.31012/0.48688. Took 0.06 sec\n",
      "Epoch 53, Loss(train/val) 0.30332/0.47954. Took 0.06 sec\n",
      "Epoch 54, Loss(train/val) 0.30445/0.48351. Took 0.07 sec\n",
      "Epoch 55, Loss(train/val) 0.29486/0.47891. Took 0.06 sec\n"
     ]
    }
   ],
   "source": [
    "## 실행 파일\n",
    "args.data_list = os.listdir(\"./data/kdd17/ourpped\")\n",
    "\n",
    "\n",
    "with open(args.save_file_path + '\\\\' + 'CONV_ATTBILSTM_result_t.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"model\", \"stock\", \"entire_exp_time\",  \"avg_test_ACC\", \"avg_test_MCC\"])\n",
    "\n",
    "    for data in args.data_list:\n",
    "        \n",
    "        stock = data.split('.')[0]\n",
    "\n",
    "        est = time.time()\n",
    "        setattr(args, 'symbol', stock)\n",
    "        args.new_file_path = args.save_file_path + '\\\\' + \"CONV_ATTBILSTM_\" + args.symbol\n",
    "        os.makedirs(args.new_file_path)\n",
    "        \n",
    "        \n",
    "        csv_read = stock_csv_read(data,args.x_frames,args.y_frames)\n",
    "        split_data_list = csv_read.cv_split()\n",
    "        \n",
    "        with open(args.new_file_path + '/'+ str(args.symbol)+'test_acc_list' +'.csv', 'w',newline='') as alist:\n",
    "            www = csv.writer(alist)\n",
    "            www.writerow([\"acc_list\"])\n",
    "\n",
    "            ACC_cv = []\n",
    "            for i, data in enumerate(split_data_list):\n",
    "                args.split_file_path = args.new_file_path + \"\\\\\" + str(i) +\"th_iter\"\n",
    "                os.makedirs(args.split_file_path)\n",
    "\n",
    "                # 0번째에 index 1번째에 stock 1개가 input으로 들어감\n",
    "                trainset = StockDataset(data[0])\n",
    "                valset = StockDataset(data[1])\n",
    "                testset = StockDataset(data[2])\n",
    "            \n",
    "\n",
    "                partition = {'train': trainset, 'val': valset, 'test': testset}\n",
    "\n",
    "\n",
    "                conv_attLSTM = args.Conv_attLSTM(args.input_dim, args.hid_dim, args.output_dim, args.num_layers, args.batch_size,\n",
    "                                            args.dropout, args.use_bn, args.attention_head, args.attn_size,activation=\"ReLU\")\n",
    "\n",
    "                conv_attLSTM.to(args.device)\n",
    "\n",
    "\n",
    "                if args.optim == 'SGD':\n",
    "                    lstm_optimizer = optim.SGD(conv_attLSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "                elif args.optim == 'RMSprop':\n",
    "                    lstm_optimizer = optim.RMSprop(conv_attLSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "                elif args.optim == 'Adam':\n",
    "                    lstm_optimizer = optim.Adam(conv_attLSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "                else:\n",
    "                    raise ValueError('In-valid optimizer choice')\n",
    "\n",
    "                # ===== List for epoch-wise data ====== #\n",
    "                train_losses = []\n",
    "                val_losses = []\n",
    "                # ===================================== #\n",
    "                for epoch in range(args.epoch):\n",
    "                    ts = time.time()\n",
    "                    conv_attLSTM, train_loss = train(conv_attLSTM, lstm_optimizer, partition, args)\n",
    "\n",
    "                    conv_attLSTM, val_loss = validation(conv_attLSTM, partition, args)\n",
    "\n",
    "                    te = time.time()\n",
    "\n",
    "                    ## 각 에폭마다 모델을 저장하기 위한 코드\n",
    "                    if len(val_losses) == 0:\n",
    "                        torch.save(conv_attLSTM.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'conv_attLSTM' +'.pt')\n",
    "                    elif min(val_losses) > val_loss:\n",
    "                        torch.save(conv_attLSTM.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'conv_attLSTM' +'.pt')\n",
    "\n",
    "                    train_losses.append(train_loss)\n",
    "                    val_losses.append(val_loss)\n",
    "\n",
    "                    print('Epoch {}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "                        .format(epoch, train_loss, val_loss, te - ts))\n",
    "\n",
    "                ## val_losses에서 가장 값이 최소인 위치를 저장함\n",
    "                site_val_losses = val_losses.index(min(val_losses)) ## 10 epoch일 경우 0번째~9번째 까지로 나옴\n",
    "                conv_attLSTM = args.Conv_attLSTM(args.input_dim, args.hid_dim, args.output_dim, args.num_layers, args.batch_size,\n",
    "                                                args.dropout, args.use_bn, args.attention_head, args.attn_size, activation=\"ReLU\")\n",
    "\n",
    "                conv_attLSTM.to(args.device)\n",
    "\n",
    "\n",
    "                conv_attLSTM.load_state_dict(torch.load(args.split_file_path + '\\\\' + str(site_val_losses) +'conv_attLSTM'+ '.pt'))\n",
    "\n",
    "                ACC, MCC = test(conv_attLSTM, partition, args)\n",
    "                print('ACC: {}, MCC: {}'.format(ACC, MCC))\n",
    "                www.writerow([ACC])\n",
    "                with open(args.split_file_path + '\\\\'+ str(site_val_losses)+'Epoch_test_metric' +'.csv', 'w') as fd:\n",
    "                    print('ACC: {}, MCC: {}'.format(ACC, MCC), file=fd)\n",
    "\n",
    "                result = {}\n",
    "\n",
    "                result['train_losses'] = train_losses\n",
    "                result['val_losses'] = val_losses\n",
    "                result['ACC'] = ACC\n",
    "                result['MCC'] = MCC\n",
    "                eet = time.time()\n",
    "                entire_exp_time = eet - est\n",
    "\n",
    "                fig = plt.figure()\n",
    "                plt.plot(result['train_losses'])\n",
    "                plt.plot(result['val_losses'])\n",
    "                plt.legend(['train_losses', 'val_losses'], fontsize=15)\n",
    "                plt.xlabel('epoch', fontsize=15)\n",
    "                plt.ylabel('loss', fontsize=15)\n",
    "                plt.grid()\n",
    "                plt.savefig(args.split_file_path + '\\\\' + str(args.symbol) + '_fig' + '.png')\n",
    "                plt.close(fig)\n",
    "                ACC_cv.append(result['ACC'])\n",
    "        ACC_cv_ar = np.array(ACC_cv)\n",
    "        acc_avg = np.mean(ACC_cv_ar)\n",
    "        acc_std = np.std(ACC_cv_ar)\n",
    "\n",
    "        wr.writerow([\"CONV_BILSTM_ATTENTION\", args.symbol, entire_exp_time, acc_avg, acc_std, result['MCC']])\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.9.12 ('taewon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "013403e7ebf8f35ee0411721c7e4b108aa3c3f8cb903b89610d110413a68ec3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
